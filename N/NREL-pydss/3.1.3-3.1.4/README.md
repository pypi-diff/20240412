# Comparing `tmp/nrel_pydss-3.1.3.tar.gz` & `tmp/nrel_pydss-3.1.4.tar.gz`

## Comparing `nrel_pydss-3.1.3.tar` & `nrel_pydss-3.1.4.tar`

### file list

```diff
@@ -1,165 +1,165 @@
--rw-r--r--   0        0        0    21669 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/ResultData.py
--rw-r--r--   0        0        0     1134 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/SolveMode.py
--rw-r--r--   0        0        0      102 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/__init__.py
--rw-r--r--   0        0        0     8485 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/common.py
--rw-r--r--   0        0        0     4542 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/config_data.py
--rw-r--r--   0        0        0    12846 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/controllers.py
--rw-r--r--   0        0        0     9325 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/dataset_buffer.py
--rw-r--r--   0        0        0     2466 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/dssBus.py
--rw-r--r--   0        0        0      950 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/dssCircuit.py
--rw-r--r--   0        0        0     6947 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/dssElement.py
--rw-r--r--   0        0        0      480 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/dssElementFactory.py
--rw-r--r--   0        0        0    25419 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/dssInstance.py
--rw-r--r--   0        0        0     4268 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/dssObjectBase.py
--rw-r--r--   0        0        0     2463 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/dssTransformer.py
--rw-r--r--   0        0        0     4808 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/element_fields.py
--rw-r--r--   0        0        0     1563 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/element_options.py
--rw-r--r--   0        0        0      939 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/exceptions.py
--rw-r--r--   0        0        0    16911 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/export_list_reader.py
--rw-r--r--   0        0        0     5284 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/get_snapshot_timepoints.py
--rw-r--r--   0        0        0    16499 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/helics_interface.py
--rw-r--r--   0        0        0    45756 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/metrics.py
--rw-r--r--   0        0        0     3598 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/naerm.py
--rw-r--r--   0        0        0    21013 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/node_voltage_metrics.py
--rw-r--r--   0        0        0     3861 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyContrReader.py
--rw-r--r--   0        0        0     2535 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyDSS.py
--rw-r--r--   0        0        0     4382 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyResults.py
--rw-r--r--   0        0        0    11487 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pydss_fs_interface.py
--rw-r--r--   0        0        0    29286 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pydss_project.py
--rw-r--r--   0        0        0    39419 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pydss_results.py
--rw-r--r--   0        0        0     9683 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/registry.py
--rw-r--r--   0        0        0    40114 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/simulation_input_models.py
--rw-r--r--   0        0        0     8787 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/storage_filters.py
--rw-r--r--   0        0        0    16130 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/thermal_metrics.py
--rw-r--r--   0        0        0     4744 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/unitDefinations.py
--rw-r--r--   0        0        0    20134 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/value_storage.py
--rw-r--r--   0        0        0     2767 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/Extensions/MonteCarlo.py
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/Extensions/__init__.py
--rw-r--r--   0        0        0     1093 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/ProfileManager/ProfileInterface.py
--rw-r--r--   0        0        0       51 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/ProfileManager/__init__.py
--rw-r--r--   0        0        0     1729 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/ProfileManager/base_definitions.py
--rw-r--r--   0        0        0      423 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/ProfileManager/common.py
--rw-r--r--   0        0        0     6735 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/ProfileManager/hooks/MongoDB.py
--rw-r--r--   0        0        0       51 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/ProfileManager/hooks/__init__.py
--rw-r--r--   0        0        0     7811 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/ProfileManager/hooks/h5.py
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/__init__.py
--rw-r--r--   0        0        0     2040 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/endpoints.yaml
--rw-r--r--   0        0        0     2705 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/logging.yaml
--rw-r--r--   0        0        0     2992 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/server.py
--rw-r--r--   0        0        0    32649 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/schema/PyDSS.v1.json
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/schema/__init__.py
--rwxr-xr-x   0        0        0      124 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/src/RunServer.bat
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/src/__init__.py
--rw-r--r--   0        0        0       37 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/src/config.yaml
--rw-r--r--   0        0        0      540 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/src/app/DataWriter.py
--rw-r--r--   0        0        0     2800 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/src/app/HDF5.py
--rw-r--r--   0        0        0     4943 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/src/app/JSON_writer.py
--rw-r--r--   0        0        0     1542 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/src/app/Tester.py
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/src/app/__init__.py
--rw-r--r--   0        0        0     5017 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/src/app/pydss.py
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/src/web/__init__.py
--rw-r--r--   0        0        0      268 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/src/web/create_schema.py
--rw-r--r--   0        0        0    23008 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/src/web/handler.py
--rw-r--r--   0        0        0     4685 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/api/src/web/parser.py
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/apps/__init__.py
--rw-r--r--   0        0        0    16715 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/apps/data_viewer.py
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/cli/__init__.py
--rw-r--r--   0        0        0      616 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/cli/add_post_process.py
--rw-r--r--   0        0        0     2689 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/cli/add_scenario.py
--rw-r--r--   0        0        0     1668 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/cli/controllers.py
--rw-r--r--   0        0        0     1419 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/cli/convert.py
--rw-r--r--   0        0        0     3067 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/cli/create_project.py
--rw-r--r--   0        0        0     1381 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/cli/edit_scenario.py
--rw-r--r--   0        0        0     1472 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/cli/export.py
--rw-r--r--   0        0        0     3267 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/cli/extract.py
--rw-r--r--   0        0        0     1294 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/cli/pydss.py
--rw-r--r--   0        0        0     4484 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/cli/reports.py
--rw-r--r--   0        0        0     3691 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/cli/run.py
--rw-r--r--   0        0        0      536 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/cli/run_server.py
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/__init__.py
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/plots.toml
--rw-r--r--   0        0        0      550 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/ExportLists/ExportMode-byClass.toml
--rw-r--r--   0        0        0      128 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/ExportLists/ExportMode-byElement.toml
--rw-r--r--   0        0        0      716 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/ExportLists/Exports.toml
--rw-r--r--   0        0        0      489 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/ExportLists/Subscriptions.toml
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/ExportLists/__init__.py
--rw-r--r--   0        0        0      373 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/Monte_Carlo/MonteCarloSettings.toml
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/Monte_Carlo/__init__.py
--rw-r--r--   0        0        0      144 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/FaultController.toml
--rw-r--r--   0        0        0      260 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/GenController.toml
--rw-r--r--   0        0        0      328 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/MotorStall.toml
--rw-r--r--   0        0        0      142 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/MotorStallSimple.toml
--rw-r--r--   0        0        0     1240 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/PvController.toml
--rw-r--r--   0        0        0     1006 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/PvDynamic.toml
--rw-r--r--   0        0        0     1005 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/PvFrequencyRideThru.toml
--rw-r--r--   0        0        0     2533 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/PvVoltageRideThru.toml
--rw-r--r--   0        0        0      152 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/SocketController.toml
--rw-r--r--   0        0        0      593 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/StorageController.toml
--rw-r--r--   0        0        0       81 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/ThermostaticLoad.toml
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/__init__.py
--rw-r--r--   0        0        0       44 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/xfmrController.toml
--rw-r--r--   0        0        0      278 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyPlotList/FrequencySweep.toml
--rw-r--r--   0        0        0      140 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyPlotList/Histogram.toml
--rw-r--r--   0        0        0      289 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyPlotList/NetworkGraph.toml
--rw-r--r--   0        0        0      404 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyPlotList/Table.toml
--rw-r--r--   0        0        0      404 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyPlotList/ThreeDim.toml
--rw-r--r--   0        0        0      211 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyPlotList/TimeSeries.toml
--rw-r--r--   0        0        0      404 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyPlotList/Topology.toml
--rw-r--r--   0        0        0      404 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyPlotList/VoltageDistance.toml
--rw-r--r--   0        0        0      423 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyPlotList/XY.toml
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/defaults/pyPlotList/__init__.py
--rw-r--r--   0        0        0     2631 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/modes/Dynamic.py
--rw-r--r--   0        0        0     2633 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/modes/QSTS.py
--rw-r--r--   0        0        0      937 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/modes/Snapshot.py
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/modes/__init__.py
--rw-r--r--   0        0        0     2893 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/modes/solver_base.py
--rw-r--r--   0        0        0       46 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/__init__.py
--rw-r--r--   0        0        0     6939 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/enumerations.py
--rw-r--r--   0        0        0    16684 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/models.py
--rw-r--r--   0        0        0     1414 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/pyController.py
--rw-r--r--   0        0        0      498 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/pyControllerAbstract.py
--rw-r--r--   0        0        0     3323 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/FaultController.py
--rw-r--r--   0        0        0     5588 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/GenController.py
--rw-r--r--   0        0        0     6498 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/MotorStall.py
--rw-r--r--   0        0        0     9103 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/MotorStallBackup.py
--rw-r--r--   0        0        0     3595 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/MotorStallSimple.py
--rw-r--r--   0        0        0    13026 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/PvController.py
--rw-r--r--   0        0        0    13853 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/PvDynamic.py
--rw-r--r--   0        0        0    21560 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/PvFrequencyRideThru.py
--rw-r--r--   0        0        0    20790 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/PvVoltageRideThru.py
--rw-r--r--   0        0        0     3598 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/SocketController.py
--rw-r--r--   0        0        0    23446 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/StorageController.py
--rw-r--r--   0        0        0     2215 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/ThermostaticLoad.py
--rw-r--r--   0        0        0        2 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/__init__.py
--rw-r--r--   0        0        0     2825 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/xfmrController.py
--rw-r--r--   0        0        0     3397 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/Settings/PvControllers.toml
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/Settings/__init__.py
--rw-r--r--   0        0        0     2533 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyControllers/Settings/PvControllers.toml
--rw-r--r--   0        0        0       47 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyPostprocessor/__init__.py
--rw-r--r--   0        0        0     1164 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyPostprocessor/pyPostprocess.py
--rw-r--r--   0        0        0     3199 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyPostprocessor/pyPostprocessAbstract.py
--rw-r--r--   0        0        0    19588 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyPostprocessor/PostprocessScripts/DERMSOptimizer.py
--rw-r--r--   0        0        0    24064 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyPostprocessor/PostprocessScripts/EdLiFoControl.py
--rw-r--r--   0        0        0       49 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyPostprocessor/PostprocessScripts/__init__.py
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyPostprocessor/PostprocessScripts/DERMSOptimizer_helper_modules/__init__.py
--rw-r--r--   0        0        0    20556 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/pyPostprocessor/PostprocessScripts/DERMSOptimizer_helper_modules/opt_funcs.py
--rw-r--r--   0        0        0      182 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/reports/__init__.py
--rw-r--r--   0        0        0     1656 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/reports/capacitor_change_count.py
--rw-r--r--   0        0        0     7152 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/reports/feeder_losses.py
--rw-r--r--   0        0        0    15011 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/reports/pv_reports.py
--rw-r--r--   0        0        0     1628 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/reports/reg_control_tap_number_change_count.py
--rw-r--r--   0        0        0     9396 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/reports/reports.py
--rw-r--r--   0        0        0     7624 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/reports/thermal_metrics.py
--rw-r--r--   0        0        0    11243 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/reports/voltage_metrics.py
--rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/utils/__init__.py
--rw-r--r--   0        0        0     4759 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/utils/dataframe_utils.py
--rw-r--r--   0        0        0     4767 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/utils/dss_utils.py
--rw-r--r--   0        0        0      834 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/utils/pydss_utils.py
--rw-r--r--   0        0        0     5204 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/utils/simulation_utils.py
--rw-r--r--   0        0        0     4953 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/utils/timing_utils.py
--rw-r--r--   0        0        0     4847 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/src/pydss/utils/utils.py
--rw-r--r--   0        0        0     2723 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/.gitignore
--rw-r--r--   0        0        0     2900 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/LICENSE
--rw-r--r--   0        0        0      257 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/README.md
--rw-r--r--   0        0        0     1486 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/pyproject.toml
--rw-r--r--   0        0        0     1762 2020-02-02 00:00:00.000000 nrel_pydss-3.1.3/PKG-INFO
+-rw-r--r--   0        0        0    21149 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/ResultData.py
+-rw-r--r--   0        0        0     1109 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/SolveMode.py
+-rw-r--r--   0        0        0       98 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/__init__.py
+-rw-r--r--   0        0        0     8218 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/common.py
+-rw-r--r--   0        0        0     4377 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/config_data.py
+-rw-r--r--   0        0        0    12484 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/controllers.py
+-rw-r--r--   0        0        0     9045 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/dataset_buffer.py
+-rw-r--r--   0        0        0     2377 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/dssBus.py
+-rw-r--r--   0        0        0      916 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/dssCircuit.py
+-rw-r--r--   0        0        0     6742 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/dssElement.py
+-rw-r--r--   0        0        0      466 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/dssElementFactory.py
+-rw-r--r--   0        0        0    24890 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/dssInstance.py
+-rw-r--r--   0        0        0     4141 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/dssObjectBase.py
+-rw-r--r--   0        0        0     2381 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/dssTransformer.py
+-rw-r--r--   0        0        0     4649 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/element_fields.py
+-rw-r--r--   0        0        0     1518 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/element_options.py
+-rw-r--r--   0        0        0      906 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/exceptions.py
+-rw-r--r--   0        0        0    16434 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/export_list_reader.py
+-rw-r--r--   0        0        0     5189 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/get_snapshot_timepoints.py
+-rw-r--r--   0        0        0    16131 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/helics_interface.py
+-rw-r--r--   0        0        0    44535 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/metrics.py
+-rw-r--r--   0        0        0     3469 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/naerm.py
+-rw-r--r--   0        0        0    20437 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/node_voltage_metrics.py
+-rw-r--r--   0        0        0     3767 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyContrReader.py
+-rw-r--r--   0        0        0     2469 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyDSS.py
+-rw-r--r--   0        0        0     4297 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyResults.py
+-rw-r--r--   0        0        0    11068 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pydss_fs_interface.py
+-rw-r--r--   0        0        0    28397 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pydss_project.py
+-rw-r--r--   0        0        0    38303 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pydss_results.py
+-rw-r--r--   0        0        0     9438 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/registry.py
+-rw-r--r--   0        0        0    38847 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/simulation_input_models.py
+-rw-r--r--   0        0        0     8528 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/storage_filters.py
+-rw-r--r--   0        0        0    15736 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/thermal_metrics.py
+-rw-r--r--   0        0        0     4619 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/unitDefinations.py
+-rw-r--r--   0        0        0    19454 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/value_storage.py
+-rw-r--r--   0        0        0     2708 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/Extensions/MonteCarlo.py
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/Extensions/__init__.py
+-rw-r--r--   0        0        0     1067 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/ProfileManager/ProfileInterface.py
+-rw-r--r--   0        0        0       50 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/ProfileManager/__init__.py
+-rw-r--r--   0        0        0     1669 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/ProfileManager/base_definitions.py
+-rw-r--r--   0        0        0      403 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/ProfileManager/common.py
+-rw-r--r--   0        0        0     6590 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/ProfileManager/hooks/MongoDB.py
+-rw-r--r--   0        0        0       50 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/ProfileManager/hooks/__init__.py
+-rw-r--r--   0        0        0     7626 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/ProfileManager/hooks/h5.py
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/__init__.py
+-rw-r--r--   0        0        0     1951 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/endpoints.yaml
+-rw-r--r--   0        0        0     2640 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/logging.yaml
+-rw-r--r--   0        0        0     2911 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/server.py
+-rw-r--r--   0        0        0    31947 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/schema/PyDSS.v1.json
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/schema/__init__.py
+-rw-r--r--   0        0        0      121 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/src/RunServer.bat
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/src/__init__.py
+-rw-r--r--   0        0        0       35 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/src/config.yaml
+-rw-r--r--   0        0        0      525 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/src/app/DataWriter.py
+-rw-r--r--   0        0        0     2727 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/src/app/HDF5.py
+-rw-r--r--   0        0        0     4817 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/src/app/JSON_writer.py
+-rw-r--r--   0        0        0     1496 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/src/app/Tester.py
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/src/app/__init__.py
+-rw-r--r--   0        0        0     4885 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/src/app/pydss.py
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/src/web/__init__.py
+-rw-r--r--   0        0        0      258 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/src/web/create_schema.py
+-rw-r--r--   0        0        0    22395 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/src/web/handler.py
+-rw-r--r--   0        0        0     4575 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/api/src/web/parser.py
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/apps/__init__.py
+-rw-r--r--   0        0        0    16311 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/apps/data_viewer.py
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/cli/__init__.py
+-rw-r--r--   0        0        0      596 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/cli/add_post_process.py
+-rw-r--r--   0        0        0     2631 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/cli/add_scenario.py
+-rw-r--r--   0        0        0     1604 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/cli/controllers.py
+-rw-r--r--   0        0        0     1364 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/cli/convert.py
+-rw-r--r--   0        0        0     2957 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/cli/create_project.py
+-rw-r--r--   0        0        0     1326 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/cli/edit_scenario.py
+-rw-r--r--   0        0        0     1407 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/cli/export.py
+-rw-r--r--   0        0        0     3148 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/cli/extract.py
+-rw-r--r--   0        0        0     1251 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/cli/pydss.py
+-rw-r--r--   0        0        0     4355 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/cli/reports.py
+-rw-r--r--   0        0        0     3575 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/cli/run.py
+-rw-r--r--   0        0        0      513 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/cli/run_server.py
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/__init__.py
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/plots.toml
+-rw-r--r--   0        0        0      523 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/ExportLists/ExportMode-byClass.toml
+-rw-r--r--   0        0        0      121 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/ExportLists/ExportMode-byElement.toml
+-rw-r--r--   0        0        0      674 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/ExportLists/Exports.toml
+-rw-r--r--   0        0        0      463 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/ExportLists/Subscriptions.toml
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/ExportLists/__init__.py
+-rw-r--r--   0        0        0      352 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/Monte_Carlo/MonteCarloSettings.toml
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/Monte_Carlo/__init__.py
+-rw-r--r--   0        0        0      139 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/FaultController.toml
+-rw-r--r--   0        0        0      245 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/GenController.toml
+-rw-r--r--   0        0        0      304 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/MotorStall.toml
+-rw-r--r--   0        0        0      135 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/MotorStallSimple.toml
+-rw-r--r--   0        0        0     1161 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/PvController.toml
+-rw-r--r--   0        0        0      960 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/PvDynamic.toml
+-rw-r--r--   0        0        0      959 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/PvFrequencyRideThru.toml
+-rw-r--r--   0        0        0     2425 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/PvVoltageRideThru.toml
+-rw-r--r--   0        0        0      144 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/SocketController.toml
+-rw-r--r--   0        0        0      558 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/StorageController.toml
+-rw-r--r--   0        0        0       74 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/ThermostaticLoad.toml
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/__init__.py
+-rw-r--r--   0        0        0       43 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/xfmrController.toml
+-rw-r--r--   0        0        0      263 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyPlotList/FrequencySweep.toml
+-rw-r--r--   0        0        0      132 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyPlotList/Histogram.toml
+-rw-r--r--   0        0        0      278 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyPlotList/NetworkGraph.toml
+-rw-r--r--   0        0        0      379 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyPlotList/Table.toml
+-rw-r--r--   0        0        0      379 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyPlotList/ThreeDim.toml
+-rw-r--r--   0        0        0      200 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyPlotList/TimeSeries.toml
+-rw-r--r--   0        0        0      379 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyPlotList/Topology.toml
+-rw-r--r--   0        0        0      379 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyPlotList/VoltageDistance.toml
+-rw-r--r--   0        0        0      406 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyPlotList/XY.toml
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/defaults/pyPlotList/__init__.py
+-rw-r--r--   0        0        0     2560 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/modes/Dynamic.py
+-rw-r--r--   0        0        0     2569 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/modes/QSTS.py
+-rw-r--r--   0        0        0      906 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/modes/Snapshot.py
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/modes/__init__.py
+-rw-r--r--   0        0        0     2794 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/modes/solver_base.py
+-rw-r--r--   0        0        0       45 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/__init__.py
+-rw-r--r--   0        0        0     6715 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/enumerations.py
+-rw-r--r--   0        0        0    16226 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/models.py
+-rw-r--r--   0        0        0     1379 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/pyController.py
+-rw-r--r--   0        0        0      475 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/pyControllerAbstract.py
+-rw-r--r--   0        0        0     3255 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/FaultController.py
+-rw-r--r--   0        0        0     5440 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/GenController.py
+-rw-r--r--   0        0        0     6364 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/MotorStall.py
+-rw-r--r--   0        0        0     8907 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/MotorStallBackup.py
+-rw-r--r--   0        0        0     3515 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/MotorStallSimple.py
+-rw-r--r--   0        0        0    12715 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/PvController.py
+-rw-r--r--   0        0        0    13552 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/PvDynamic.py
+-rw-r--r--   0        0        0    21098 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/PvFrequencyRideThru.py
+-rw-r--r--   0        0        0    20345 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/PvVoltageRideThru.py
+-rw-r--r--   0        0        0     3521 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/SocketController.py
+-rw-r--r--   0        0        0    22893 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/StorageController.py
+-rw-r--r--   0        0        0     2149 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/ThermostaticLoad.py
+-rw-r--r--   0        0        0        1 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/__init__.py
+-rw-r--r--   0        0        0     2762 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/xfmrController.py
+-rw-r--r--   0        0        0     3234 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/Settings/PvControllers.toml
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/Settings/__init__.py
+-rw-r--r--   0        0        0     2425 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyControllers/Settings/PvControllers.toml
+-rw-r--r--   0        0        0       47 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyPostprocessor/__init__.py
+-rw-r--r--   0        0        0     1129 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyPostprocessor/pyPostprocess.py
+-rw-r--r--   0        0        0     3121 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyPostprocessor/pyPostprocessAbstract.py
+-rw-r--r--   0        0        0    19150 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyPostprocessor/PostprocessScripts/DERMSOptimizer.py
+-rw-r--r--   0        0        0    23379 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyPostprocessor/PostprocessScripts/EdLiFoControl.py
+-rw-r--r--   0        0        0       48 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyPostprocessor/PostprocessScripts/__init__.py
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyPostprocessor/PostprocessScripts/DERMSOptimizer_helper_modules/__init__.py
+-rw-r--r--   0        0        0    20117 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/pyPostprocessor/PostprocessScripts/DERMSOptimizer_helper_modules/opt_funcs.py
+-rw-r--r--   0        0        0      175 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/reports/__init__.py
+-rw-r--r--   0        0        0     1605 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/reports/capacitor_change_count.py
+-rw-r--r--   0        0        0     6953 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/reports/feeder_losses.py
+-rw-r--r--   0        0        0    14649 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/reports/pv_reports.py
+-rw-r--r--   0        0        0     1579 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/reports/reg_control_tap_number_change_count.py
+-rw-r--r--   0        0        0     9129 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/reports/reports.py
+-rw-r--r--   0        0        0     7439 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/reports/thermal_metrics.py
+-rw-r--r--   0        0        0    10965 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/reports/voltage_metrics.py
+-rw-r--r--   0        0        0        0 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/utils/__init__.py
+-rw-r--r--   0        0        0     4606 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/utils/dataframe_utils.py
+-rw-r--r--   0        0        0     4579 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/utils/dss_utils.py
+-rw-r--r--   0        0        0      810 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/utils/pydss_utils.py
+-rw-r--r--   0        0        0     5009 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/utils/simulation_utils.py
+-rw-r--r--   0        0        0     4760 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/utils/timing_utils.py
+-rw-r--r--   0        0        0     4639 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/src/pydss/utils/utils.py
+-rw-r--r--   0        0        0     2545 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/.gitignore
+-rw-r--r--   0        0        0     2877 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/LICENSE
+-rw-r--r--   0        0        0      251 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/README.md
+-rw-r--r--   0        0        0     1414 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/pyproject.toml
+-rw-r--r--   0        0        0     1762 2020-02-02 00:00:00.000000 nrel_pydss-3.1.4/PKG-INFO
```

### Comparing `nrel_pydss-3.1.3/src/pydss/ResultData.py` & `nrel_pydss-3.1.4/src/pydss/ResultData.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,520 +1,520 @@
-
-
-from datetime import datetime
-import pathlib
-import os
-
-import opendssdirect as dss
-from loguru import logger
-import pandas as pd
-
-from pydss.unitDefinations import unit_info
-from pydss.common import (
-    PV_LOAD_SHAPE_FILENAME,
-    PV_PROFILES_FILENAME,
-    NODE_NAMES_BY_TYPE_FILENAME,
-    DatasetPropertyType,
-)
-from pydss.dataset_buffer import DatasetBuffer
-from pydss.utils.dss_utils import get_node_names_by_type
-from pydss.exceptions import InvalidConfiguration, InvalidParameter
-from pydss.export_list_reader import ExportListReader, StoreValuesType
-from pydss.reports.reports import Reports, ReportGranularity
-from pydss.simulation_input_models import SimulationSettingsModel
-from pydss.utils.dataframe_utils import write_dataframe
-from pydss.utils.utils import dump_data
-from pydss.utils.simulation_utils import (
-    create_datetime_index_from_settings,
-    create_loadshape_pmult_dataframe_for_simulation,
-)
-from pydss.utils.timing_utils import Timer, timer_stats_collector, track_timing
-from pydss.value_storage import ValueContainer, ValueByNumber
-from pydss.metrics import (
-    OpenDssPropertyMetric,
-    SummedElementsByGroupOpenDssPropertyMetric,
-    SummedElementsOpenDssPropertyMetric,
-)
-
-class ResultData:
-    """Exports data to files."""
-
-    METADATA_FILENAME = "metadata.json"
-    INDICES_BASENAME = "indices"
-
-    def __init__(self, settings: SimulationSettingsModel, system_paths, dss_objects,
-                 dss_objects_by_class, dss_buses, dss_solver, dss_command,
-                 dss_instance):
-        self._dss_solver = dss_solver
-        self._results = {}
-        self._buses = dss_buses
-        self._objects_by_element = dss_objects
-        self._objects_by_class = dss_objects_by_class
-        self.system_paths = system_paths
-        self._element_metrics = {}  # (elem_class, prop_name) to OpenDssPropertyMetric
-        self._summed_element_metrics = {}
-        self._settings = settings
-        self._cur_step = 0
-        self._current_results = {}
-
-        self._dss_command = dss_command
-        self._start_day = dss_solver.StartDay
-        self._end_day = dss_solver.EndDay
-        self._time_dataset = None
-        self._frequency_dataset = None
-        self._mode_dataset = None
-        self._simulation_mode = []
-        self._hdf_store = None
-        self._scenario = settings.project.active_scenario
-        self._base_scenario = settings.project.active_scenario
-        self._export_format = settings.exports.export_format
-        self._export_compression = settings.exports.export_compression
-        self._max_chunk_bytes = settings.exports.hdf_max_chunk_bytes
-        self._export_dir = os.path.join(
-            self.system_paths["Export"],
-            settings.project.active_scenario,
-        )
-        # Use / because this is used in HDFStore
-        self._export_relative_dir = "Exports/" + settings.project.active_scenario
-        self._store_frequency = False
-        self._store_mode = False
-        if settings.frequency.enable_frequency_sweep:
-            self._store_frequency = True
-            self._store_mode = True
-
-        pathlib.Path(self._export_dir).mkdir(parents=True, exist_ok=True)
-
-        export_list_filename = os.path.join(
-            system_paths["ExportLists"],
-            "Exports.toml",
-        )
-        if not os.path.exists(export_list_filename):
-            export_list_filename = os.path.join(
-                system_paths["ExportLists"],
-                "ExportMode-byClass.toml",
-            )
-        self._export_list = ExportListReader(export_list_filename)
-        Reports.append_required_exports(self._export_list, settings)
-        dump_data(
-            self._export_list.serialize(),
-            os.path.join(self._export_dir, "ExportsActual.toml"),
-        )
-        self._circuit_metrics = {}
-        self._create_exports()
-
-    def _create_exports(self):
-        for elem_class in self._export_list.list_element_classes():
-            if elem_class in ("Buses", "Nodes"):
-                objs = self._buses
-            elif elem_class in self._objects_by_class:
-                objs = self._objects_by_class[elem_class]
-            elif elem_class == "FeederHead":
-                objs = self._objects_by_class["Circuits"]
-            elif elem_class != "CktElement":  # TODO
-                continue
-            for prop in self._export_list.iter_export_properties(elem_class=elem_class):
-                if prop.opendss_classes:
-                    dss_objs = []
-                    for cls in prop.opendss_classes:
-                        if cls not in self._objects_by_class:
-                            logger.warning("Export class=%s is not present in the circuit", cls)
-                            continue
-
-                        for obj in self._objects_by_class[cls].values():
-                            if obj.Enabled and prop.should_store_name(obj.FullName):
-                                dss_objs.append(obj)
-                else:
-                    dss_objs = [x for x in objs.values() if x.Enabled and prop.should_store_name(x.FullName)]
-                if prop.custom_metric is None:
-                    self._add_opendss_metric(prop, dss_objs)
-                else:
-                    self._add_custom_metric(prop, dss_objs)
-
-    def _add_opendss_metric(self, prop, dss_objs):
-        obj = dss_objs[0]
-        if not obj.IsValidAttribute(prop.name):
-            raise InvalidParameter(f"{obj.FullName} / {prop.name} cannot be exported")
-        key = (prop.elem_class, prop.name)
-        if prop.sum_elements:
-            metric = self._summed_element_metrics.get(key)
-            if metric is None:
-                if prop.sum_groups:
-                    cls = SummedElementsByGroupOpenDssPropertyMetric
-                else:
-                    cls = SummedElementsOpenDssPropertyMetric
-                metric = cls(prop, dss_objs, self._settings)
-                self._summed_element_metrics[key] = metric
-            else:
-                metric.add_dss_obj(obj)
-        else:
-            metric = self._element_metrics.get(key)
-            if metric is None:
-                metric = OpenDssPropertyMetric(prop, dss_objs, self._settings)
-                self._element_metrics[key] = metric
-            else:
-                metric.add_property(prop)
-
-    def _add_custom_metric(self, prop, dss_objs):
-        cls = prop.custom_metric
-        if cls.is_circuit_wide():
-            metric = self._circuit_metrics.get(cls)
-            if metric is None:
-                metric = cls(prop, dss_objs, self._settings)
-                self._circuit_metrics[cls] = metric
-            else:
-                metric.add_property(prop)
-        else:
-            key = (prop.elem_class, prop.name)
-            metric = self._element_metrics.get(key)
-            if metric is None:
-                metric = cls(prop, dss_objs, self._settings)
-                self._element_metrics[key] = metric
-            else:
-                metric.add_property(prop)
-
-    def InitializeDataStore(self, hdf_store, num_steps, MC_scenario_number=None):
-        if MC_scenario_number is not None:
-            self._scenario = self._base_scenario + f"_MC{MC_scenario_number}"
-        self._hdf_store = hdf_store
-        self._time_dataset = DatasetBuffer(
-            hdf_store=hdf_store,
-            path=f"Exports/{self._scenario}/Timestamp",
-            max_size=num_steps,
-            dtype=float,
-            columns=("Timestamp",),
-            max_chunk_bytes=self._max_chunk_bytes
-        )
-        self._frequency_dataset = DatasetBuffer(
-            hdf_store=hdf_store,
-            path=f"Exports/{self._scenario}/Frequency",
-            max_size=num_steps,
-            dtype=float,
-            columns=("Frequency",),
-            max_chunk_bytes=self._max_chunk_bytes
-        )
-        self._mode_dataset = DatasetBuffer(
-            hdf_store=hdf_store,
-            path=f"Exports/{self._scenario}/Mode",
-            max_size=num_steps,
-            dtype="S10",
-            columns=("Mode",),
-            max_chunk_bytes=self._max_chunk_bytes
-        )
-        self._cur_step = 0
-
-        base_path = "Exports/" + self._scenario
-        for metric in self._iter_metrics():
-            metric.initialize_data_store(hdf_store, base_path, num_steps)
-
-    def _iter_metrics(self):
-        for metric in self._element_metrics.values():
-            yield metric
-        for metric in self._summed_element_metrics.values():
-            yield metric
-        for metric in self._circuit_metrics.values():
-            yield metric
-
-    @property
-    def CurrentResults(self):
-        return self._current_results
-
-    @track_timing(timer_stats_collector)
-    def UpdateResults(self, store_nan=False):
-        self._current_results.clear()
-
-        # Get the number of seconds since the Epoch without any timezone conversions.
-        timestamp = (self._dss_solver.GetDateTime() - datetime(1970, 1, 1, 0, 0)).total_seconds() #  datetime.utcfromtimestamp(0) is now depriciated
-        self._time_dataset.write_value([timestamp])
-        self._frequency_dataset.write_value([self._dss_solver.getFrequency()])
-        self._mode_dataset.write_value([self._dss_solver.getMode()])
-
-        for metric in self._iter_metrics():
-            with Timer(timer_stats_collector, metric.label()):
-                data = metric.append_values(self._cur_step, store_nan=store_nan)
-
-            if isinstance(data, dict):
-                # TODO: reconsider
-                # Something is only returned for OpenDSS properties
-                self._current_results.update(data)
-
-        self._cur_step += 1
-        return self._current_results
-
-    def ExportResults(self):
-        metadata = {
-            "event_log": None,
-            "element_info_files": [],
-        }
-
-        if self._settings.exports.export_event_log:
-            self._export_event_log(metadata)
-        if self._settings.exports.export_elements:
-            self._export_elements(metadata, set(self._settings.exports.export_element_types))
-            self._export_feeder_head_info(metadata)
-        if self._settings.exports.export_pv_profiles:
-            self._export_pv_profiles()
-        if self._settings.exports.export_node_names_by_type:
-            self._export_node_names_by_type()
-
-        filename = os.path.join(self._export_dir, self.METADATA_FILENAME)
-        dump_data(metadata, filename, indent=4)
-        logger.info(f"Exported metadata to {filename}", )
-        self._hdf_store = None
-
-    def Close(self):
-        for dataset in (self._time_dataset, self._frequency_dataset, self._mode_dataset):
-            dataset.flush_data()
-        for metric in self._iter_metrics():
-            metric.close()
-
-    def _export_event_log(self, metadata):
-        event_log = "event_log.csv"
-        file_path = os.path.join(self._export_dir, event_log)
-        if os.path.exists(file_path):
-            os.remove(file_path)
-
-        orig = os.getcwd()
-        os.chdir(self._export_dir)
-        try:
-            cmd = "Export EventLog {}".format(event_log)
-            out = self._dss_command(cmd)
-            if out != event_log:
-                raise Exception(f"Failed to export EventLog:  {out}")
-            logger.info(f"Exported OpenDSS event log to {out}", )
-            metadata["event_log"] = self._export_relative_dir + f"/{event_log}"
-        finally:
-            os.chdir(orig)
-
-    def _export_dataframe(self, df, basename):
-        filename = basename + "." + self._export_format
-        write_dataframe(df, filename, compress=self._export_compression)
-        logger.info("Exported %s", filename)
-
-    def _find_feeder_head_line(self):
-        feeder_head_line = None
-        flag = dss.Topology.First()
-        while flag > 0:
-            if 'line' in dss.Topology.BranchName().lower():
-                feeder_head_line = dss.Topology.BranchName()
-                break
-            else:
-                flag = dss.Topology.Next()
-
-        return feeder_head_line
-
-    def _get_feeder_head_loading(self):
-        head_line = self._find_feeder_head_line()
-        if head_line is not None:
-            flag = dss.Circuit.SetActiveElement(head_line)
-
-            if flag>0:
-                n_phases = dss.CktElement.NumPhases()
-                max_amp = dss.CktElement.NormalAmps()
-                Currents = dss.CktElement.CurrentsMagAng()[:2*n_phases]
-                Current_magnitude = Currents[::2]
-
-                max_flow = max(max(Current_magnitude), 1e-10)
-                loading = max_flow/max_amp
-
-                return loading
-            else:
-                return None
-        else:
-            return None
-
-    def _reverse_powerflow(self):
-        reverse_pf = dss.Circuit.TotalPower()[0] > 0 # total substation power is an injection(-) or a consumption(+)
-        return reverse_pf
-
-    def _export_feeder_head_info(self, metadata):
-        """
-        Gets feeder head information comprising:
-        1- The name of the feeder head line
-        2- The feeder head loading in per unit
-        3- The feeder head load in (kW, kVar). Negative in case of power injection
-        4- The reverse power flow flag. True if power is flowing back to the feeder head, False otherwise
-        """
-        if not "feeder_head_info_files" in metadata.keys():
-            metadata["feeder_head_info_files"] = []
-
-        total_power = dss.Circuit.TotalPower()
-        df_dict = {"FeederHeadLine": self._find_feeder_head_line(),
-                   "FeederHeadLoading": self._get_feeder_head_loading(),
-                   "FeederHeadLoadKW": total_power[0],
-                   "FeederHeadLoadKVar": total_power[1],
-                   "ReversePowerFlow": self._reverse_powerflow()
-                  }
-
-        filename = "FeederHeadInfo"
-        fname = filename + ".json"
-        relpath = os.path.join(self._export_relative_dir, fname)
-        filepath = os.path.join(self._export_dir, fname)
-
-        #write_dataframe(df, filepath)
-        dump_data(df_dict, filepath)
-        metadata["feeder_head_info_files"].append(relpath)
-        logger.info("Exported %s information to %s.", filename, filepath)
-
-    def _export_elements(self, metadata, element_types):
-        exports = [
-            # TODO: opendssdirect does not provide a function to export Bus information.
-            ("Capacitor", "CapacitorsInfo", dss.Capacitors.Count),
-            ("Fuse", "FusesInfo", dss.Fuses.Count),
-            ("Generator", "GeneratorsInfo", dss.Generators.Count),
-            ("Isource", "IsourceInfo", dss.Isource.Count),
-            ("Line", "LinesInfo", dss.Lines.Count),
-            ("Load", "LoadsInfo", dss.Loads.Count),
-            ("Monitor", "MonitorsInfo", dss.Monitors.Count),
-            ("PVSystem", "PVSystemsInfo", dss.PVsystems.Count),
-            ("Recloser", "ReclosersInfo", dss.Reclosers.Count),
-            ("RegControl", "RegControlsInfo", dss.RegControls.Count),
-            ("Relay", "RelaysInfo", dss.Relays.Count),
-            ("Sensor", "SensorsInfo", dss.Sensors.Count),
-            ("Transformer", "TransformersInfo", dss.Transformers.Count),
-            ("Vsource", "VsourcesInfo", dss.Vsources.Count),
-            ("XYCurve", "XYCurvesInfo", dss.XYCurves.Count),
-            # TODO This can be very large. Consider making it configurable.
-            #("LoadShape", "LoadShapeInfo", dss.LoadShape.Count),
-        ]
-        if element_types:
-            types = set()
-            for elem_type in element_types:
-                if elem_type.endswith("s"):
-                    # Maintain compatibility with old format used plural names.
-                    elem_type = elem_type[:-1]
-                types.add(elem_type)
-            exports = [x for x in exports if x[0] in types]
-
-        for class_name, filename, count_func in exports:
-            df = dss.utils.class_to_dataframe(class_name)
-            # Always record in CSV format for readability.
-            # There are also warning messages from PyTables because the
-            # data may contain strings.
-            fname = filename + ".csv"
-            relpath = os.path.join(self._export_relative_dir, fname)
-            filepath = os.path.join(self._export_dir, fname)
-            write_dataframe(df, filepath)
-            metadata["element_info_files"].append(relpath)
-            logger.info("Exported %s information to %s.", filename, filepath)
-
-        if not element_types or "Transformer" in element_types or "Transformers" in element_types:
-            self._export_transformers(metadata)
-
-    def _export_transformers(self, metadata):
-        df_dict = {"Transformer": [], "HighSideConnection": [], "NumPhases": []}
-
-        dss.Circuit.SetActiveClass("Transformer")
-        flag = dss.ActiveClass.First()
-        while flag > 0:
-            name = dss.CktElement.Name()
-            df_dict["Transformer"].append(name)
-            df_dict["HighSideConnection"].append(dss.Properties.Value("conns").split("[")[1].split(",")[0].strip(" ").lower())
-            df_dict["NumPhases"].append(dss.CktElement.NumPhases())
-            flag = dss.ActiveClass.Next()
-
-        df = pd.DataFrame.from_dict(df_dict)
-
-        relpath = os.path.join(self._export_relative_dir, "TransformersPhaseInfo.csv")
-        filepath = os.path.join(self._export_dir, "TransformersPhaseInfo.csv")
-        write_dataframe(df, filepath)
-        metadata["element_info_files"].append(relpath)
-        logger.info("Exported transformer phase information to %s", filepath)
-
-    def _export_pv_profiles(self):
-        granularity = self._settings.reports.granularity
-        pv_systems = self._objects_by_class.get("PVSystems")
-        if pv_systems is None:
-            logger.info("No PVSystems are present")
-            return
-
-        pv_infos = []
-        profiles = set()
-        for full_name, obj in pv_systems.items():
-            profile_name = obj.GetParameter("yearly").lower()
-            if profile_name != "":
-                profiles.add(profile_name)
-            pv_infos.append({
-                "irradiance": obj.GetParameter("irradiance"),
-                "name": full_name,
-                "pmpp": obj.GetParameter("pmpp"),
-                "load_shape_profile": profile_name,
-            })
-
-        pmult_sums = {}
-        if dss.LoadShape.First() == 0:
-            logger.warning("There are no load shapes.")
-            return
-
-        sim_resolution = self._settings.project.step_resolution_sec
-        per_time_point = (
-            ReportGranularity.PER_ELEMENT_PER_TIME_POINT,
-            ReportGranularity.ALL_ELEMENTS_PER_TIME_POINT,
-        )
-        load_shape_data = {}
-        while True:
-            name = dss.LoadShape.Name().lower()
-            if name in profiles:
-                sinterval = dss.LoadShape.SInterval()
-                assert sim_resolution >= sinterval, f"{sim_resolution} >= {sinterval}"
-                df = create_loadshape_pmult_dataframe_for_simulation(self._settings)
-                sum_values = df.iloc[:, 0].sum()
-                if granularity in per_time_point:
-                    load_shape_data[name] = df.iloc[:, 0].values
-                    pmult_sums[name] = sum_values
-                else:
-                    pmult_sums[name] = sum_values
-            if dss.LoadShape.Next() == 0:
-                break
-
-        if load_shape_data and granularity in per_time_point:
-            filename = os.path.join(self._export_dir, PV_LOAD_SHAPE_FILENAME)
-            index = create_datetime_index_from_settings(self._settings)
-            df = pd.DataFrame(load_shape_data, index=index)
-            write_dataframe(df, filename, compress=True)
-
-        for pv_info in pv_infos:
-            profile = pv_info["load_shape_profile"]
-            if profile == "":
-                pv_info["load_shape_pmult_sum"] = 0
-            else:
-                pv_info["load_shape_pmult_sum"] = pmult_sums[profile]
-
-        data = {"pv_systems": pv_infos}
-        filename = os.path.join(self._export_dir, PV_PROFILES_FILENAME)
-        dump_data(data, filename, indent=2)
-        logger.info("Exported PV profile information to %s", filename)
-
-    def _export_node_names_by_type(self):
-        data = get_node_names_by_type()
-        filename = os.path.join(self._export_dir, NODE_NAMES_BY_TYPE_FILENAME)
-        dump_data(data, filename, indent=2)
-        logger.info("Exported node names by type to %s", filename)
-
-    @staticmethod
-    def get_units(prop, index=None):
-        units = unit_info.get(prop)
-        if units is None:
-            raise InvalidParameter(f"no units are stored for {prop}")
-
-        if isinstance(units, dict):
-            if index is None:
-                raise InvalidParameter(f"index must be provided for {prop}")
-            if index == 0:
-                return units["E"]
-            if index == 1:
-                return units["O"]
-            raise InvalidParameter("index must be 0 or 1")
-
-        return units
-
-    def max_num_bytes(self):
-        """Return the maximum number of bytes the container could hold.
-
-        Returns
-        -------
-        int
-
-        """
-        total = 0
-        for metric in self._iter_metrics():
-            total += metric.max_num_bytes()
-        return total
+
+
+from datetime import datetime
+import pathlib
+import os
+
+import opendssdirect as dss
+from loguru import logger
+import pandas as pd
+
+from pydss.unitDefinations import unit_info
+from pydss.common import (
+    PV_LOAD_SHAPE_FILENAME,
+    PV_PROFILES_FILENAME,
+    NODE_NAMES_BY_TYPE_FILENAME,
+    DatasetPropertyType,
+)
+from pydss.dataset_buffer import DatasetBuffer
+from pydss.utils.dss_utils import get_node_names_by_type
+from pydss.exceptions import InvalidConfiguration, InvalidParameter
+from pydss.export_list_reader import ExportListReader, StoreValuesType
+from pydss.reports.reports import Reports, ReportGranularity
+from pydss.simulation_input_models import SimulationSettingsModel
+from pydss.utils.dataframe_utils import write_dataframe
+from pydss.utils.utils import dump_data
+from pydss.utils.simulation_utils import (
+    create_datetime_index_from_settings,
+    create_loadshape_pmult_dataframe_for_simulation,
+)
+from pydss.utils.timing_utils import Timer, timer_stats_collector, track_timing
+from pydss.value_storage import ValueContainer, ValueByNumber
+from pydss.metrics import (
+    OpenDssPropertyMetric,
+    SummedElementsByGroupOpenDssPropertyMetric,
+    SummedElementsOpenDssPropertyMetric,
+)
+
+class ResultData:
+    """Exports data to files."""
+
+    METADATA_FILENAME = "metadata.json"
+    INDICES_BASENAME = "indices"
+
+    def __init__(self, settings: SimulationSettingsModel, system_paths, dss_objects,
+                 dss_objects_by_class, dss_buses, dss_solver, dss_command,
+                 dss_instance):
+        self._dss_solver = dss_solver
+        self._results = {}
+        self._buses = dss_buses
+        self._objects_by_element = dss_objects
+        self._objects_by_class = dss_objects_by_class
+        self.system_paths = system_paths
+        self._element_metrics = {}  # (elem_class, prop_name) to OpenDssPropertyMetric
+        self._summed_element_metrics = {}
+        self._settings = settings
+        self._cur_step = 0
+        self._current_results = {}
+
+        self._dss_command = dss_command
+        self._start_day = dss_solver.StartDay
+        self._end_day = dss_solver.EndDay
+        self._time_dataset = None
+        self._frequency_dataset = None
+        self._mode_dataset = None
+        self._simulation_mode = []
+        self._hdf_store = None
+        self._scenario = settings.project.active_scenario
+        self._base_scenario = settings.project.active_scenario
+        self._export_format = settings.exports.export_format
+        self._export_compression = settings.exports.export_compression
+        self._max_chunk_bytes = settings.exports.hdf_max_chunk_bytes
+        self._export_dir = os.path.join(
+            self.system_paths["Export"],
+            settings.project.active_scenario,
+        )
+        # Use / because this is used in HDFStore
+        self._export_relative_dir = "Exports/" + settings.project.active_scenario
+        self._store_frequency = False
+        self._store_mode = False
+        if settings.frequency.enable_frequency_sweep:
+            self._store_frequency = True
+            self._store_mode = True
+
+        pathlib.Path(self._export_dir).mkdir(parents=True, exist_ok=True)
+
+        export_list_filename = os.path.join(
+            system_paths["ExportLists"],
+            "Exports.toml",
+        )
+        if not os.path.exists(export_list_filename):
+            export_list_filename = os.path.join(
+                system_paths["ExportLists"],
+                "ExportMode-byClass.toml",
+            )
+        self._export_list = ExportListReader(export_list_filename)
+        Reports.append_required_exports(self._export_list, settings)
+        dump_data(
+            self._export_list.serialize(),
+            os.path.join(self._export_dir, "ExportsActual.toml"),
+        )
+        self._circuit_metrics = {}
+        self._create_exports()
+
+    def _create_exports(self):
+        for elem_class in self._export_list.list_element_classes():
+            if elem_class in ("Buses", "Nodes"):
+                objs = self._buses
+            elif elem_class in self._objects_by_class:
+                objs = self._objects_by_class[elem_class]
+            elif elem_class == "FeederHead":
+                objs = self._objects_by_class["Circuits"]
+            elif elem_class != "CktElement":  # TODO
+                continue
+            for prop in self._export_list.iter_export_properties(elem_class=elem_class):
+                if prop.opendss_classes:
+                    dss_objs = []
+                    for cls in prop.opendss_classes:
+                        if cls not in self._objects_by_class:
+                            logger.warning("Export class=%s is not present in the circuit", cls)
+                            continue
+
+                        for obj in self._objects_by_class[cls].values():
+                            if obj.Enabled and prop.should_store_name(obj.FullName):
+                                dss_objs.append(obj)
+                else:
+                    dss_objs = [x for x in objs.values() if x.Enabled and prop.should_store_name(x.FullName)]
+                if prop.custom_metric is None:
+                    self._add_opendss_metric(prop, dss_objs)
+                else:
+                    self._add_custom_metric(prop, dss_objs)
+
+    def _add_opendss_metric(self, prop, dss_objs):
+        obj = dss_objs[0]
+        if not obj.IsValidAttribute(prop.name):
+            raise InvalidParameter(f"{obj.FullName} / {prop.name} cannot be exported")
+        key = (prop.elem_class, prop.name)
+        if prop.sum_elements:
+            metric = self._summed_element_metrics.get(key)
+            if metric is None:
+                if prop.sum_groups:
+                    cls = SummedElementsByGroupOpenDssPropertyMetric
+                else:
+                    cls = SummedElementsOpenDssPropertyMetric
+                metric = cls(prop, dss_objs, self._settings)
+                self._summed_element_metrics[key] = metric
+            else:
+                metric.add_dss_obj(obj)
+        else:
+            metric = self._element_metrics.get(key)
+            if metric is None:
+                metric = OpenDssPropertyMetric(prop, dss_objs, self._settings)
+                self._element_metrics[key] = metric
+            else:
+                metric.add_property(prop)
+
+    def _add_custom_metric(self, prop, dss_objs):
+        cls = prop.custom_metric
+        if cls.is_circuit_wide():
+            metric = self._circuit_metrics.get(cls)
+            if metric is None:
+                metric = cls(prop, dss_objs, self._settings)
+                self._circuit_metrics[cls] = metric
+            else:
+                metric.add_property(prop)
+        else:
+            key = (prop.elem_class, prop.name)
+            metric = self._element_metrics.get(key)
+            if metric is None:
+                metric = cls(prop, dss_objs, self._settings)
+                self._element_metrics[key] = metric
+            else:
+                metric.add_property(prop)
+
+    def InitializeDataStore(self, hdf_store, num_steps, MC_scenario_number=None):
+        if MC_scenario_number is not None:
+            self._scenario = self._base_scenario + f"_MC{MC_scenario_number}"
+        self._hdf_store = hdf_store
+        self._time_dataset = DatasetBuffer(
+            hdf_store=hdf_store,
+            path=f"Exports/{self._scenario}/Timestamp",
+            max_size=num_steps,
+            dtype=float,
+            columns=("Timestamp",),
+            max_chunk_bytes=self._max_chunk_bytes
+        )
+        self._frequency_dataset = DatasetBuffer(
+            hdf_store=hdf_store,
+            path=f"Exports/{self._scenario}/Frequency",
+            max_size=num_steps,
+            dtype=float,
+            columns=("Frequency",),
+            max_chunk_bytes=self._max_chunk_bytes
+        )
+        self._mode_dataset = DatasetBuffer(
+            hdf_store=hdf_store,
+            path=f"Exports/{self._scenario}/Mode",
+            max_size=num_steps,
+            dtype="S10",
+            columns=("Mode",),
+            max_chunk_bytes=self._max_chunk_bytes
+        )
+        self._cur_step = 0
+
+        base_path = "Exports/" + self._scenario
+        for metric in self._iter_metrics():
+            metric.initialize_data_store(hdf_store, base_path, num_steps)
+
+    def _iter_metrics(self):
+        for metric in self._element_metrics.values():
+            yield metric
+        for metric in self._summed_element_metrics.values():
+            yield metric
+        for metric in self._circuit_metrics.values():
+            yield metric
+
+    @property
+    def CurrentResults(self):
+        return self._current_results
+
+    @track_timing(timer_stats_collector)
+    def UpdateResults(self, store_nan=False):
+        self._current_results.clear()
+
+        # Get the number of seconds since the Epoch without any timezone conversions.
+        timestamp = (self._dss_solver.GetDateTime() - datetime(1970, 1, 1, 0, 0)).total_seconds() #  datetime.utcfromtimestamp(0) is now depriciated
+        self._time_dataset.write_value([timestamp])
+        self._frequency_dataset.write_value([self._dss_solver.getFrequency()])
+        self._mode_dataset.write_value([self._dss_solver.getMode()])
+
+        for metric in self._iter_metrics():
+            with Timer(timer_stats_collector, metric.label()):
+                data = metric.append_values(self._cur_step, store_nan=store_nan)
+
+            if isinstance(data, dict):
+                # TODO: reconsider
+                # Something is only returned for OpenDSS properties
+                self._current_results.update(data)
+
+        self._cur_step += 1
+        return self._current_results
+
+    def ExportResults(self):
+        metadata = {
+            "event_log": None,
+            "element_info_files": [],
+        }
+
+        if self._settings.exports.export_event_log:
+            self._export_event_log(metadata)
+        if self._settings.exports.export_elements:
+            self._export_elements(metadata, set(self._settings.exports.export_element_types))
+            self._export_feeder_head_info(metadata)
+        if self._settings.exports.export_pv_profiles:
+            self._export_pv_profiles()
+        if self._settings.exports.export_node_names_by_type:
+            self._export_node_names_by_type()
+
+        filename = os.path.join(self._export_dir, self.METADATA_FILENAME)
+        dump_data(metadata, filename, indent=4)
+        logger.info(f"Exported metadata to {filename}", )
+        self._hdf_store = None
+
+    def Close(self):
+        for dataset in (self._time_dataset, self._frequency_dataset, self._mode_dataset):
+            dataset.flush_data()
+        for metric in self._iter_metrics():
+            metric.close()
+
+    def _export_event_log(self, metadata):
+        event_log = "event_log.csv"
+        file_path = os.path.join(self._export_dir, event_log)
+        if os.path.exists(file_path):
+            os.remove(file_path)
+
+        orig = os.getcwd()
+        os.chdir(self._export_dir)
+        try:
+            cmd = "Export EventLog {}".format(event_log)
+            out = self._dss_command(cmd)
+            if out != event_log:
+                raise Exception(f"Failed to export EventLog:  {out}")
+            logger.info(f"Exported OpenDSS event log to {out}", )
+            metadata["event_log"] = self._export_relative_dir + f"/{event_log}"
+        finally:
+            os.chdir(orig)
+
+    def _export_dataframe(self, df, basename):
+        filename = basename + "." + self._export_format
+        write_dataframe(df, filename, compress=self._export_compression)
+        logger.info("Exported %s", filename)
+
+    def _find_feeder_head_line(self):
+        feeder_head_line = None
+        flag = dss.Topology.First()
+        while flag > 0:
+            if 'line' in dss.Topology.BranchName().lower():
+                feeder_head_line = dss.Topology.BranchName()
+                break
+            else:
+                flag = dss.Topology.Next()
+
+        return feeder_head_line
+
+    def _get_feeder_head_loading(self):
+        head_line = self._find_feeder_head_line()
+        if head_line is not None:
+            flag = dss.Circuit.SetActiveElement(head_line)
+
+            if flag>0:
+                n_phases = dss.CktElement.NumPhases()
+                max_amp = dss.CktElement.NormalAmps()
+                Currents = dss.CktElement.CurrentsMagAng()[:2*n_phases]
+                Current_magnitude = Currents[::2]
+
+                max_flow = max(max(Current_magnitude), 1e-10)
+                loading = max_flow/max_amp
+
+                return loading
+            else:
+                return None
+        else:
+            return None
+
+    def _reverse_powerflow(self):
+        reverse_pf = dss.Circuit.TotalPower()[0] > 0 # total substation power is an injection(-) or a consumption(+)
+        return reverse_pf
+
+    def _export_feeder_head_info(self, metadata):
+        """
+        Gets feeder head information comprising:
+        1- The name of the feeder head line
+        2- The feeder head loading in per unit
+        3- The feeder head load in (kW, kVar). Negative in case of power injection
+        4- The reverse power flow flag. True if power is flowing back to the feeder head, False otherwise
+        """
+        if not "feeder_head_info_files" in metadata.keys():
+            metadata["feeder_head_info_files"] = []
+
+        total_power = dss.Circuit.TotalPower()
+        df_dict = {"FeederHeadLine": self._find_feeder_head_line(),
+                   "FeederHeadLoading": self._get_feeder_head_loading(),
+                   "FeederHeadLoadKW": total_power[0],
+                   "FeederHeadLoadKVar": total_power[1],
+                   "ReversePowerFlow": self._reverse_powerflow()
+                  }
+
+        filename = "FeederHeadInfo"
+        fname = filename + ".json"
+        relpath = os.path.join(self._export_relative_dir, fname)
+        filepath = os.path.join(self._export_dir, fname)
+
+        #write_dataframe(df, filepath)
+        dump_data(df_dict, filepath)
+        metadata["feeder_head_info_files"].append(relpath)
+        logger.info("Exported %s information to %s.", filename, filepath)
+
+    def _export_elements(self, metadata, element_types):
+        exports = [
+            # TODO: opendssdirect does not provide a function to export Bus information.
+            ("Capacitor", "CapacitorsInfo", dss.Capacitors.Count),
+            ("Fuse", "FusesInfo", dss.Fuses.Count),
+            ("Generator", "GeneratorsInfo", dss.Generators.Count),
+            ("Isource", "IsourceInfo", dss.Isource.Count),
+            ("Line", "LinesInfo", dss.Lines.Count),
+            ("Load", "LoadsInfo", dss.Loads.Count),
+            ("Monitor", "MonitorsInfo", dss.Monitors.Count),
+            ("PVSystem", "PVSystemsInfo", dss.PVsystems.Count),
+            ("Recloser", "ReclosersInfo", dss.Reclosers.Count),
+            ("RegControl", "RegControlsInfo", dss.RegControls.Count),
+            ("Relay", "RelaysInfo", dss.Relays.Count),
+            ("Sensor", "SensorsInfo", dss.Sensors.Count),
+            ("Transformer", "TransformersInfo", dss.Transformers.Count),
+            ("Vsource", "VsourcesInfo", dss.Vsources.Count),
+            ("XYCurve", "XYCurvesInfo", dss.XYCurves.Count),
+            # TODO This can be very large. Consider making it configurable.
+            #("LoadShape", "LoadShapeInfo", dss.LoadShape.Count),
+        ]
+        if element_types:
+            types = set()
+            for elem_type in element_types:
+                if elem_type.endswith("s"):
+                    # Maintain compatibility with old format used plural names.
+                    elem_type = elem_type[:-1]
+                types.add(elem_type)
+            exports = [x for x in exports if x[0] in types]
+
+        for class_name, filename, count_func in exports:
+            df = dss.utils.class_to_dataframe(class_name)
+            # Always record in CSV format for readability.
+            # There are also warning messages from PyTables because the
+            # data may contain strings.
+            fname = filename + ".csv"
+            relpath = os.path.join(self._export_relative_dir, fname)
+            filepath = os.path.join(self._export_dir, fname)
+            write_dataframe(df, filepath)
+            metadata["element_info_files"].append(relpath)
+            logger.info("Exported %s information to %s.", filename, filepath)
+
+        if not element_types or "Transformer" in element_types or "Transformers" in element_types:
+            self._export_transformers(metadata)
+
+    def _export_transformers(self, metadata):
+        df_dict = {"Transformer": [], "HighSideConnection": [], "NumPhases": []}
+
+        dss.Circuit.SetActiveClass("Transformer")
+        flag = dss.ActiveClass.First()
+        while flag > 0:
+            name = dss.CktElement.Name()
+            df_dict["Transformer"].append(name)
+            df_dict["HighSideConnection"].append(dss.Properties.Value("conns").split("[")[1].split(",")[0].strip(" ").lower())
+            df_dict["NumPhases"].append(dss.CktElement.NumPhases())
+            flag = dss.ActiveClass.Next()
+
+        df = pd.DataFrame.from_dict(df_dict)
+
+        relpath = os.path.join(self._export_relative_dir, "TransformersPhaseInfo.csv")
+        filepath = os.path.join(self._export_dir, "TransformersPhaseInfo.csv")
+        write_dataframe(df, filepath)
+        metadata["element_info_files"].append(relpath)
+        logger.info("Exported transformer phase information to %s", filepath)
+
+    def _export_pv_profiles(self):
+        granularity = self._settings.reports.granularity
+        pv_systems = self._objects_by_class.get("PVSystems")
+        if pv_systems is None:
+            logger.info("No PVSystems are present")
+            return
+
+        pv_infos = []
+        profiles = set()
+        for full_name, obj in pv_systems.items():
+            profile_name = obj.GetParameter("yearly").lower()
+            if profile_name != "":
+                profiles.add(profile_name)
+            pv_infos.append({
+                "irradiance": obj.GetParameter("irradiance"),
+                "name": full_name,
+                "pmpp": obj.GetParameter("pmpp"),
+                "load_shape_profile": profile_name,
+            })
+
+        pmult_sums = {}
+        if dss.LoadShape.First() == 0:
+            logger.warning("There are no load shapes.")
+            return
+
+        sim_resolution = self._settings.project.step_resolution_sec
+        per_time_point = (
+            ReportGranularity.PER_ELEMENT_PER_TIME_POINT,
+            ReportGranularity.ALL_ELEMENTS_PER_TIME_POINT,
+        )
+        load_shape_data = {}
+        while True:
+            name = dss.LoadShape.Name().lower()
+            if name in profiles:
+                sinterval = dss.LoadShape.SInterval()
+                assert sim_resolution >= sinterval, f"{sim_resolution} >= {sinterval}"
+                df = create_loadshape_pmult_dataframe_for_simulation(self._settings)
+                sum_values = df.iloc[:, 0].sum()
+                if granularity in per_time_point:
+                    load_shape_data[name] = df.iloc[:, 0].values
+                    pmult_sums[name] = sum_values
+                else:
+                    pmult_sums[name] = sum_values
+            if dss.LoadShape.Next() == 0:
+                break
+
+        if load_shape_data and granularity in per_time_point:
+            filename = os.path.join(self._export_dir, PV_LOAD_SHAPE_FILENAME)
+            index = create_datetime_index_from_settings(self._settings)
+            df = pd.DataFrame(load_shape_data, index=index)
+            write_dataframe(df, filename, compress=True)
+
+        for pv_info in pv_infos:
+            profile = pv_info["load_shape_profile"]
+            if profile == "":
+                pv_info["load_shape_pmult_sum"] = 0
+            else:
+                pv_info["load_shape_pmult_sum"] = pmult_sums[profile]
+
+        data = {"pv_systems": pv_infos}
+        filename = os.path.join(self._export_dir, PV_PROFILES_FILENAME)
+        dump_data(data, filename, indent=2)
+        logger.info("Exported PV profile information to %s", filename)
+
+    def _export_node_names_by_type(self):
+        data = get_node_names_by_type()
+        filename = os.path.join(self._export_dir, NODE_NAMES_BY_TYPE_FILENAME)
+        dump_data(data, filename, indent=2)
+        logger.info("Exported node names by type to %s", filename)
+
+    @staticmethod
+    def get_units(prop, index=None):
+        units = unit_info.get(prop)
+        if units is None:
+            raise InvalidParameter(f"no units are stored for {prop}")
+
+        if isinstance(units, dict):
+            if index is None:
+                raise InvalidParameter(f"index must be provided for {prop}")
+            if index == 0:
+                return units["E"]
+            if index == 1:
+                return units["O"]
+            raise InvalidParameter("index must be 0 or 1")
+
+        return units
+
+    def max_num_bytes(self):
+        """Return the maximum number of bytes the container could hold.
+
+        Returns
+        -------
+        int
+
+        """
+        total = 0
+        for metric in self._iter_metrics():
+            total += metric.max_num_bytes()
+        return total
```

### Comparing `nrel_pydss-3.1.3/src/pydss/SolveMode.py` & `nrel_pydss-3.1.4/src/pydss/SolveMode.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,25 @@
-from loguru import logger
-import opendssdirect as dss
-
-from pydss.common import SimulationType
-from pydss.exceptions import InvalidParameter
-from pydss.modes.Dynamic import Dynamic
-from pydss.modes.Snapshot import Snapshot
-from pydss.modes.QSTS import QSTS
-from pydss.simulation_input_models import ProjectModel, SimulationSettingsModel
-
-
-def GetSolver(settings: SimulationSettingsModel, dssInstance):
-    logger.info('Setting solver to %s mode.', settings.project.simulation_type.value)
-    return get_solver_from_simulation_type(settings.project)
-
-
-def get_solver_from_simulation_type(settings: ProjectModel):
-    """Return a solver from the simulation type."""
-    if settings.simulation_type == SimulationType.SNAPSHOT:
-        return Snapshot(dssInstance=dss, settings=settings)
-    elif settings.simulation_type == SimulationType.QSTS:
-        return QSTS(dssInstance=dss, settings=settings)
-    elif settings.simulation_type == SimulationType.DYNAMIC:
-        return Dynamic(dssInstance=dss, settings=settings)
-    raise InvalidParameter(f"{settings.simulation_type} does not have a supported solver")
+from loguru import logger
+import opendssdirect as dss
+
+from pydss.common import SimulationType
+from pydss.exceptions import InvalidParameter
+from pydss.modes.Dynamic import Dynamic
+from pydss.modes.Snapshot import Snapshot
+from pydss.modes.QSTS import QSTS
+from pydss.simulation_input_models import ProjectModel, SimulationSettingsModel
+
+
+def GetSolver(settings: SimulationSettingsModel, dssInstance):
+    logger.info('Setting solver to %s mode.', settings.project.simulation_type.value)
+    return get_solver_from_simulation_type(settings.project)
+
+
+def get_solver_from_simulation_type(settings: ProjectModel):
+    """Return a solver from the simulation type."""
+    if settings.simulation_type == SimulationType.SNAPSHOT:
+        return Snapshot(dssInstance=dss, settings=settings)
+    elif settings.simulation_type == SimulationType.QSTS:
+        return QSTS(dssInstance=dss, settings=settings)
+    elif settings.simulation_type == SimulationType.DYNAMIC:
+        return Dynamic(dssInstance=dss, settings=settings)
+    raise InvalidParameter(f"{settings.simulation_type} does not have a supported solver")
```

### Comparing `nrel_pydss-3.1.3/src/pydss/common.py` & `nrel_pydss-3.1.4/src/pydss/common.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,267 +1,267 @@
-
-import enum
-import os
-from collections import namedtuple
-
-import pydss
-from pydss.utils.utils import load_data
-
-DATE_FORMAT = '%Y-%m-%d %H:%M:%S.%f' # '%Y-%m-%d %H:%M:%S.%f', "%m/%d/%Y %H:%M:%S"
-TIME_FORMAT = '%H:%M:%S'
-
-PLOTS_FILENAME = "plots.toml"
-SIMULATION_SETTINGS_FILENAME = "simulation.toml"
-RUN_SIMULATION_FILENAME = "simulation-run.toml"
-MONTE_CARLO_SETTINGS_FILENAME = "MonteCarloSettings.toml"
-OPENDSS_MASTER_FILENAME = "Master.dss"
-SUBSCRIPTIONS_FILENAME = "Subscriptions.toml"
-PROJECT_TAR = "project.tar"
-PROJECT_ZIP = "project.zip"
-PROFILE_MAPPING = "mapping.toml"
-PROFILE_SRC_H5 = "profiles.h5"
-PV_LOAD_SHAPE_FILENAME = "pv_load_shape_data.h5"
-PV_PROFILES_FILENAME = "pv_profiles.json"
-NODE_NAMES_BY_TYPE_FILENAME = "node_names_by_type.json"
-INTEGER_NAN = -9999
-
-class ControllerType(enum.Enum):
-    FAULT_CONTROLLER = "FaultController"
-    GENERATOR_CONTROLLER = "GenController"
-    MOTOR_STALL = "MotorStall"
-    MOTOR_STALL_SIMPLE = "MotorStallSimple"
-    PV_CONTROLLER = "PvController"
-    PV_DYNAMIC = "PvDynamic"
-    PV_FREQUENCY_RIDETHROUGH = "PvFrequencyRideThru"
-    PV_VOLTAGE_RIDETHROUGH = "PvVoltageRideThru"
-    SOCKET_CONTROLLER = "SocketController"
-    STORAGE_CONTROLLER = "StorageController"
-    THERMOSTATIC_LOAD_CONTROLLER = "ThermostaticLoad"
-    XMFR_CONTROLLER = "xmfrController"
-
-CONTROLLER_TYPES = tuple(x.value for x in ControllerType)
-CONFIG_EXT = ".toml"
-
-class ExportMode(enum.Enum):
-    BY_CLASS = "ExportMode-byClass"
-    BY_ELEMENT = "ExportMode-byElement"
-    SUBSCRIPTIONS = 'Subscriptions'
-    EXPORTS = "Exports"
-
-def filename_from_enum(obj):
-    return obj.value + CONFIG_EXT
-
-FAULT_CONTROLLER_FILENAME = filename_from_enum(ControllerType.FAULT_CONTROLLER)
-GENERATOR_CONTROLLER_FILENAME = filename_from_enum(ControllerType.GENERATOR_CONTROLLER)
-MOTOR_STALL_FILENAME = filename_from_enum(ControllerType.MOTOR_STALL)
-MOTOR_STALL_SIMPLE_FILENAME = filename_from_enum(ControllerType.MOTOR_STALL_SIMPLE)
-PV_CONTROLLER_FILENAME = filename_from_enum(ControllerType.PV_CONTROLLER)
-PV_DYNAMIC_FILENAME = filename_from_enum(ControllerType.PV_DYNAMIC)
-PV_FREQUENCY_RIDETHROUGH_FILENAME = filename_from_enum(ControllerType.PV_FREQUENCY_RIDETHROUGH)
-PV_VOLTAGE_RIDETHROUGH_FILENAME = filename_from_enum(ControllerType.PV_VOLTAGE_RIDETHROUGH)
-SOCKET_CONTROLLER_FILENAME = filename_from_enum(ControllerType.SOCKET_CONTROLLER)
-STORAGE_CONTROLLER_FILENAME = filename_from_enum(ControllerType.STORAGE_CONTROLLER)
-THERMOSTATIC_LOAD_CONTROLLER_FILENAME = filename_from_enum(ControllerType.THERMOSTATIC_LOAD_CONTROLLER)
-XMFR_CONTROLLER_FILENAME = filename_from_enum(ControllerType.XMFR_CONTROLLER)
-
-EXPORT_BY_CLASS_FILENAME = filename_from_enum(ExportMode.BY_CLASS)
-EXPORT_BY_ELEMENT_FILENAME = filename_from_enum(ExportMode.BY_ELEMENT)
-EXPORTS_FILENAME = filename_from_enum(ExportMode.EXPORTS)
-
-DEFAULT_FAULT_CONTROLLER_CONFIG_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "pyControllerList",
-    FAULT_CONTROLLER_FILENAME,
-)
-DEFAULT_GENERATOR_CONTROLLER_CONFIG_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "pyControllerList",
-    GENERATOR_CONTROLLER_FILENAME,
-)
-DEFAULT_MOTOR_STALL_CONTROLLER_CONFIG_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "pyControllerList",
-    MOTOR_STALL_FILENAME,
-)
-DEFAULT_PV_CONTROLLER_CONFIG_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "pyControllerList",
-    PV_CONTROLLER_FILENAME,
-)
-DEFAULT_PV_DYNAMIC_CONTROLLER_CONFIG_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "pyControllerList",
-    PV_DYNAMIC_FILENAME,
-)
-DEFAULT_PV_FREQUENCY_RIDETHROUGH_CONTROLLER_CONFIG_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "pyControllerList",
-    PV_FREQUENCY_RIDETHROUGH_FILENAME,
-)
-DEFAULT_PV_VOLTAGE_RIDETHROUGH_CONTROLLER_CONFIG_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "pyControllerList",
-    PV_VOLTAGE_RIDETHROUGH_FILENAME,
-)
-DEFAULT_SOCKET_CONTROLLER_CONFIG_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "pyControllerList",
-    SOCKET_CONTROLLER_FILENAME,
-)
-DEFAULT_STORAGE_CONTROLLER_CONFIG_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "pyControllerList",
-    STORAGE_CONTROLLER_FILENAME,
-)
-DEFAULT_THERMOSTATIC_LOAD_CONTROLLER_CONFIG_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "pyControllerList",
-    THERMOSTATIC_LOAD_CONTROLLER_FILENAME,
-)
-DEFAULT_XMFR_CONTROLLER_CONFIG_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "pyControllerList",
-    XMFR_CONTROLLER_FILENAME,
-)
-
-DEFAULT_SUBSCRIPTIONS_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "ExportLists",
-    SUBSCRIPTIONS_FILENAME,
-)
-DEFAULT_SIMULATION_SETTINGS_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    SIMULATION_SETTINGS_FILENAME,
-)
-
-DEFAULT_PLOT_SETTINGS_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    PLOTS_FILENAME
-)
-DEFAULT_EXPORT_BY_CLASS_SETTINGS_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "ExportLists",
-    EXPORT_BY_CLASS_FILENAME,
-)
-DEFAULT_EXPORT_BY_ELEMENT_SETTINGS_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "ExportLists",
-    EXPORTS_FILENAME,
-)
-DEFAULT_EXPORTS_SETTINGS_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "ExportLists",
-    EXPORT_BY_ELEMENT_FILENAME,
-)
-DEFAULT_MONTE_CARLO_SETTINGS_FILE = os.path.join(
-    os.path.dirname(getattr(pydss, "__path__")[0]),
-    "pydss",
-    "defaults",
-    "Monte_Carlo",
-    MONTE_CARLO_SETTINGS_FILENAME,
-)
-class ControlMode(enum.Enum):
-    """Supported control modes"""
-    STATIC = "Static"
-    TIME = "Time"
-class DataConversion(enum.Enum):
-    NONE = "none"
-    ABS = "abs"
-    ABS_SUM = "abs_sum"
-    SUM = "sum"
-    SUM_REAL = "sum_real"
-    SUM_ABS_REAL = "sum_abs_real"
-class DatasetPropertyType(enum.Enum):
-    PER_TIME_POINT = "per_time_point"  # data is stored at every time point
-    FILTERED = "filtered"  # data is stored after being filtered
-    METADATA = "metadata"  # metadata for another dataset
-    TIME_STEP = "time_step"  # data are time indices, tied to FILTERED
-    VALUE = "value"  # Only a single value is written for each element
-class FileFormat(enum.Enum):
-    """Supported file formats"""
-    CSV = "csv"
-    HDF5 = "h5"
-
-
-class LimitsFilter(enum.Enum):
-    INSIDE = "inside"
-    OUTSIDE = "outside"
-
-
-class LoggingLevel(enum.Enum):
-    """Supported logging levels"""
-    DEBUG = "debug"
-    INFO = "info"
-    WARNING = "warning"
-    ERROR = "error"
-
-
-class ReportGranularity(enum.Enum):
-    """Specifies the granularity on which data is collected."""
-    PER_ELEMENT_PER_TIME_POINT = "per_element_per_time_point"
-    PER_ELEMENT_TOTAL = "per_element_total"
-    ALL_ELEMENTS_PER_TIME_POINT = "all_elements_per_time_point"
-    ALL_ELEMENTS_TOTAL = "all_elements_total"
-
-
-class SimulationType(enum.Enum):
-    """Supported simulation types"""
-    DYNAMIC = "dynamic"
-    QSTS = "qsts"
-    SNAPSHOT = "snapshot"
-
-
-class SnapshotTimePointSelectionMode(enum.Enum):
-    """Defines methods by which snapshot time points can be calculated."""
-
-    MAX_PV_LOAD_RATIO = "max_pv_load_ratio"
-    MAX_LOAD = "max_load"
-    DAYTIME_MIN_LOAD = "daytime_min_load"
-    MAX_PV_MINUS_LOAD = "pv_minus_load"
-    NONE = "none"
-
-
-class StoreValuesType(enum.Enum):
-    ALL = "all"
-    CHANGE_COUNT = "change_count"
-    MAX = "max"
-    MIN = "min"
-    MOVING_AVERAGE = "moving_average"
-    MOVING_AVERAGE_MAX = "moving_average_max"
-    SUM = "sum"
-
-
-MinMax = namedtuple("MinMax", "min, max")
+
+import enum
+import os
+from collections import namedtuple
+
+import pydss
+from pydss.utils.utils import load_data
+
+DATE_FORMAT = '%Y-%m-%d %H:%M:%S.%f' # '%Y-%m-%d %H:%M:%S.%f', "%m/%d/%Y %H:%M:%S"
+TIME_FORMAT = '%H:%M:%S'
+
+PLOTS_FILENAME = "plots.toml"
+SIMULATION_SETTINGS_FILENAME = "simulation.toml"
+RUN_SIMULATION_FILENAME = "simulation-run.toml"
+MONTE_CARLO_SETTINGS_FILENAME = "MonteCarloSettings.toml"
+OPENDSS_MASTER_FILENAME = "Master.dss"
+SUBSCRIPTIONS_FILENAME = "Subscriptions.toml"
+PROJECT_TAR = "project.tar"
+PROJECT_ZIP = "project.zip"
+PROFILE_MAPPING = "mapping.toml"
+PROFILE_SRC_H5 = "profiles.h5"
+PV_LOAD_SHAPE_FILENAME = "pv_load_shape_data.h5"
+PV_PROFILES_FILENAME = "pv_profiles.json"
+NODE_NAMES_BY_TYPE_FILENAME = "node_names_by_type.json"
+INTEGER_NAN = -9999
+
+class ControllerType(enum.Enum):
+    FAULT_CONTROLLER = "FaultController"
+    GENERATOR_CONTROLLER = "GenController"
+    MOTOR_STALL = "MotorStall"
+    MOTOR_STALL_SIMPLE = "MotorStallSimple"
+    PV_CONTROLLER = "PvController"
+    PV_DYNAMIC = "PvDynamic"
+    PV_FREQUENCY_RIDETHROUGH = "PvFrequencyRideThru"
+    PV_VOLTAGE_RIDETHROUGH = "PvVoltageRideThru"
+    SOCKET_CONTROLLER = "SocketController"
+    STORAGE_CONTROLLER = "StorageController"
+    THERMOSTATIC_LOAD_CONTROLLER = "ThermostaticLoad"
+    XMFR_CONTROLLER = "xmfrController"
+
+CONTROLLER_TYPES = tuple(x.value for x in ControllerType)
+CONFIG_EXT = ".toml"
+
+class ExportMode(enum.Enum):
+    BY_CLASS = "ExportMode-byClass"
+    BY_ELEMENT = "ExportMode-byElement"
+    SUBSCRIPTIONS = 'Subscriptions'
+    EXPORTS = "Exports"
+
+def filename_from_enum(obj):
+    return obj.value + CONFIG_EXT
+
+FAULT_CONTROLLER_FILENAME = filename_from_enum(ControllerType.FAULT_CONTROLLER)
+GENERATOR_CONTROLLER_FILENAME = filename_from_enum(ControllerType.GENERATOR_CONTROLLER)
+MOTOR_STALL_FILENAME = filename_from_enum(ControllerType.MOTOR_STALL)
+MOTOR_STALL_SIMPLE_FILENAME = filename_from_enum(ControllerType.MOTOR_STALL_SIMPLE)
+PV_CONTROLLER_FILENAME = filename_from_enum(ControllerType.PV_CONTROLLER)
+PV_DYNAMIC_FILENAME = filename_from_enum(ControllerType.PV_DYNAMIC)
+PV_FREQUENCY_RIDETHROUGH_FILENAME = filename_from_enum(ControllerType.PV_FREQUENCY_RIDETHROUGH)
+PV_VOLTAGE_RIDETHROUGH_FILENAME = filename_from_enum(ControllerType.PV_VOLTAGE_RIDETHROUGH)
+SOCKET_CONTROLLER_FILENAME = filename_from_enum(ControllerType.SOCKET_CONTROLLER)
+STORAGE_CONTROLLER_FILENAME = filename_from_enum(ControllerType.STORAGE_CONTROLLER)
+THERMOSTATIC_LOAD_CONTROLLER_FILENAME = filename_from_enum(ControllerType.THERMOSTATIC_LOAD_CONTROLLER)
+XMFR_CONTROLLER_FILENAME = filename_from_enum(ControllerType.XMFR_CONTROLLER)
+
+EXPORT_BY_CLASS_FILENAME = filename_from_enum(ExportMode.BY_CLASS)
+EXPORT_BY_ELEMENT_FILENAME = filename_from_enum(ExportMode.BY_ELEMENT)
+EXPORTS_FILENAME = filename_from_enum(ExportMode.EXPORTS)
+
+DEFAULT_FAULT_CONTROLLER_CONFIG_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "pyControllerList",
+    FAULT_CONTROLLER_FILENAME,
+)
+DEFAULT_GENERATOR_CONTROLLER_CONFIG_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "pyControllerList",
+    GENERATOR_CONTROLLER_FILENAME,
+)
+DEFAULT_MOTOR_STALL_CONTROLLER_CONFIG_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "pyControllerList",
+    MOTOR_STALL_FILENAME,
+)
+DEFAULT_PV_CONTROLLER_CONFIG_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "pyControllerList",
+    PV_CONTROLLER_FILENAME,
+)
+DEFAULT_PV_DYNAMIC_CONTROLLER_CONFIG_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "pyControllerList",
+    PV_DYNAMIC_FILENAME,
+)
+DEFAULT_PV_FREQUENCY_RIDETHROUGH_CONTROLLER_CONFIG_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "pyControllerList",
+    PV_FREQUENCY_RIDETHROUGH_FILENAME,
+)
+DEFAULT_PV_VOLTAGE_RIDETHROUGH_CONTROLLER_CONFIG_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "pyControllerList",
+    PV_VOLTAGE_RIDETHROUGH_FILENAME,
+)
+DEFAULT_SOCKET_CONTROLLER_CONFIG_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "pyControllerList",
+    SOCKET_CONTROLLER_FILENAME,
+)
+DEFAULT_STORAGE_CONTROLLER_CONFIG_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "pyControllerList",
+    STORAGE_CONTROLLER_FILENAME,
+)
+DEFAULT_THERMOSTATIC_LOAD_CONTROLLER_CONFIG_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "pyControllerList",
+    THERMOSTATIC_LOAD_CONTROLLER_FILENAME,
+)
+DEFAULT_XMFR_CONTROLLER_CONFIG_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "pyControllerList",
+    XMFR_CONTROLLER_FILENAME,
+)
+
+DEFAULT_SUBSCRIPTIONS_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "ExportLists",
+    SUBSCRIPTIONS_FILENAME,
+)
+DEFAULT_SIMULATION_SETTINGS_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    SIMULATION_SETTINGS_FILENAME,
+)
+
+DEFAULT_PLOT_SETTINGS_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    PLOTS_FILENAME
+)
+DEFAULT_EXPORT_BY_CLASS_SETTINGS_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "ExportLists",
+    EXPORT_BY_CLASS_FILENAME,
+)
+DEFAULT_EXPORT_BY_ELEMENT_SETTINGS_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "ExportLists",
+    EXPORTS_FILENAME,
+)
+DEFAULT_EXPORTS_SETTINGS_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "ExportLists",
+    EXPORT_BY_ELEMENT_FILENAME,
+)
+DEFAULT_MONTE_CARLO_SETTINGS_FILE = os.path.join(
+    os.path.dirname(getattr(pydss, "__path__")[0]),
+    "pydss",
+    "defaults",
+    "Monte_Carlo",
+    MONTE_CARLO_SETTINGS_FILENAME,
+)
+class ControlMode(enum.Enum):
+    """Supported control modes"""
+    STATIC = "Static"
+    TIME = "Time"
+class DataConversion(enum.Enum):
+    NONE = "none"
+    ABS = "abs"
+    ABS_SUM = "abs_sum"
+    SUM = "sum"
+    SUM_REAL = "sum_real"
+    SUM_ABS_REAL = "sum_abs_real"
+class DatasetPropertyType(enum.Enum):
+    PER_TIME_POINT = "per_time_point"  # data is stored at every time point
+    FILTERED = "filtered"  # data is stored after being filtered
+    METADATA = "metadata"  # metadata for another dataset
+    TIME_STEP = "time_step"  # data are time indices, tied to FILTERED
+    VALUE = "value"  # Only a single value is written for each element
+class FileFormat(enum.Enum):
+    """Supported file formats"""
+    CSV = "csv"
+    HDF5 = "h5"
+
+
+class LimitsFilter(enum.Enum):
+    INSIDE = "inside"
+    OUTSIDE = "outside"
+
+
+class LoggingLevel(enum.Enum):
+    """Supported logging levels"""
+    DEBUG = "debug"
+    INFO = "info"
+    WARNING = "warning"
+    ERROR = "error"
+
+
+class ReportGranularity(enum.Enum):
+    """Specifies the granularity on which data is collected."""
+    PER_ELEMENT_PER_TIME_POINT = "per_element_per_time_point"
+    PER_ELEMENT_TOTAL = "per_element_total"
+    ALL_ELEMENTS_PER_TIME_POINT = "all_elements_per_time_point"
+    ALL_ELEMENTS_TOTAL = "all_elements_total"
+
+
+class SimulationType(enum.Enum):
+    """Supported simulation types"""
+    DYNAMIC = "dynamic"
+    QSTS = "qsts"
+    SNAPSHOT = "snapshot"
+
+
+class SnapshotTimePointSelectionMode(enum.Enum):
+    """Defines methods by which snapshot time points can be calculated."""
+
+    MAX_PV_LOAD_RATIO = "max_pv_load_ratio"
+    MAX_LOAD = "max_load"
+    DAYTIME_MIN_LOAD = "daytime_min_load"
+    MAX_PV_MINUS_LOAD = "pv_minus_load"
+    NONE = "none"
+
+
+class StoreValuesType(enum.Enum):
+    ALL = "all"
+    CHANGE_COUNT = "change_count"
+    MAX = "max"
+    MIN = "min"
+    MOVING_AVERAGE = "moving_average"
+    MOVING_AVERAGE_MAX = "moving_average_max"
+    SUM = "sum"
+
+
+MinMax = namedtuple("MinMax", "min, max")
```

### Comparing `nrel_pydss-3.1.3/src/pydss/dataset_buffer.py` & `nrel_pydss-3.1.4/src/pydss/dataset_buffer.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,280 +1,280 @@
-"""Contains DatasetBuffer"""
-
-from loguru import logger
-import pandas as pd
-import numpy as np
-
-from pydss.exceptions import InvalidConfiguration
-from pydss.utils.utils import make_timestamps
-from pydss.common import DatasetPropertyType
-
-
-
-KiB = 1024
-MiB = KiB * KiB
-GiB = MiB * MiB
-
-# The optimal number of chunks to store in memory will vary widely.
-# The h5py docs recommend keeping chunk byte sizes between 10 KiB - 1 MiB.
-# It needs to be larger than the biggest possible row and also cover enough
-# columns to compress duplicate values. Since we might store thousands of
-# elements in one dataset, make it the max by default.
-# Note that the downside to making this larger is that any read causes the
-# entire chunk to be read.
-DEFAULT_MAX_CHUNK_BYTES = 1 * MiB
-
-class DatasetBuffer:
-    """Provides a write buffer to an HDF dataset to increase performance.
-    Users must call flush_data before the object goes out of scope to ensure
-    that all data is flushed.
-
-    """
-    # TODO add support for context manager, though pydss wouldn't be able to
-    # take advantage in its current implementation.
-
-    def __init__(
-            self, hdf_store, path, max_size, dtype, columns, scaleoffset=None,
-            max_chunk_bytes=None, attributes=None, names=None,
-            column_ranges_per_name=None, data=None
-        ):
-        if max_chunk_bytes is None:
-            max_chunk_bytes = DEFAULT_MAX_CHUNK_BYTES
-        self._buf_index = 0
-        self._hdf_store = hdf_store
-        self._max_size = max_size
-        num_columns = len(columns)
-        if data is None:
-            self.chunk_count = self.compute_chunk_count(
-                num_columns,
-                max_size,
-                dtype,
-                max_chunk_bytes,
-            )
-            shape = (self._max_size, num_columns)
-            chunks = (self.chunk_count, num_columns)
-        else:
-            self.chunk_count = None
-            shape = None
-            chunks = None
-
-        dim = len(shape)
-        print(path)
-        self._dataset = self._hdf_store.create_dataset(
-            name=path,
-            shape=shape,
-            data=data,
-            chunks=chunks,
-            dtype=dtype,
-            compression="gzip",
-            compression_opts=4,
-            shuffle=True,
-            maxshape=[None for _ in range(dim)],
-            # Does not preserve NaN, so don't use it.
-            #scaleoffset=scaleoffset,
-        )
-
-        # Columns, names, and column_ranges_per_name can't be stored as
-        # attributes because they can exceed the size limit. Store as datasets
-        # instead.
-        column_dataset_path = path + "Columns"
-        column_dataset = self._hdf_store.create_dataset(
-            name=column_dataset_path,
-            data=np.array(columns, dtype="S"),
-            maxshape=(None, ),
-        )
-        column_dataset.attrs["type"] = DatasetPropertyType.METADATA.value
-        self._dataset.attrs["column_dataset_path"] = column_dataset_path
-
-        if names is not None:
-            name_dataset_path = path + "Names"
-            name_dataset = self._hdf_store.create_dataset(
-                name = name_dataset_path,
-                data = np.array(names, dtype="S"),
-                maxshape=(None, ),
-            )
-            name_dataset.attrs["type"] = DatasetPropertyType.METADATA.value
-            self._dataset.attrs["name_dataset_path"] = name_dataset_path
-
-        if column_ranges_per_name is not None:
-            column_ranges_dataset_path = path + "ColumnRanges"
-            data = np.array(column_ranges_per_name)
-            dim = len(data.shape)
-            column_ranges_dataset = self._hdf_store.create_dataset(
-                name=column_ranges_dataset_path,
-                data=data,
-                maxshape=[None for _ in range(dim)],
-            )
-            column_ranges_dataset.attrs["type"] = DatasetPropertyType.METADATA.value
-            self._dataset.attrs["column_ranges_dataset_path"] = column_ranges_dataset_path
-
-        self._dataset.attrs["length"] = 0
-        self._dataset_index = 0
-        self._buf = np.empty(chunks, dtype=dtype)
-
-        if attributes is not None:
-            for attr, val in attributes.items():
-                self._dataset.attrs[attr] = val
-
-        logger.debug("Created DatasetBuffer path=%s shape=%s chunks=%s",
-                     path, shape, chunks)
-
-    def __del__(self):
-        assert self._buf_index == 0, \
-            f"DatasetBuffer destructed with data in memory: {self._dataset.name}"
-
-    def flush_data(self):
-        """Flush the data in the temporary buffer to storage."""
-        length = self._buf_index
-        if length == 0:
-            return
-
-        new_index = self._dataset_index + length
-        
-        if new_index > self._dataset.shape[0]:
-            new_dimensions = (new_index, self._dataset.shape[1])
-            self._dataset.resize(new_dimensions)
-            logger.warning(f"result index {new_index} exceed dataset dimension {self._dataset.shape[0]} for dataset {self._dataset.name}. Resizig dataset to {new_dimensions}")
-  
-        self._dataset[self._dataset_index:new_index] = self._buf[0:length]
-        
-        self._buf_index = 0
-        self._dataset_index = new_index
-        self._dataset.attrs["length"] = new_index
-        self._dataset.flush()
-
-    def max_num_bytes(self):
-        """Return the maximum number of bytes the container could hold.
-
-        Returns
-        -------
-        int
-
-        """
-        size_row = self._buf.size * self._buf.itemsize / len(self._buf)
-        return size_row * self._max_size
-
-    def write_value(self, value):
-        """Write the value to the internal buffer, flushing when full."""
-
-        self._buf[self._buf_index] = value
-        self._buf_index += 1
-        if self._buf_index == self.chunk_count:
-            self.flush_data()
-
-    def write_data(self, values):
-        """Write the data to the dataset."""
-        new_index = self._dataset_index + len(values)
-        self._dataset[self._dataset_index:new_index] = values
-        self._dataset_index = new_index
-        self._dataset.attrs["length"] = new_index
-
-    @staticmethod
-    def compute_chunk_count(
-            num_columns,
-            max_size,
-            dtype,
-            max_chunk_bytes=DEFAULT_MAX_CHUNK_BYTES
-        ):
-        assert max_size > 0, f"max_size={max_size}"
-        tmp = np.empty((1, num_columns), dtype=dtype)
-        size_row = tmp.size * tmp.itemsize
-        chunk_count = min(int(max_chunk_bytes / size_row), max_size)
-        if chunk_count == 0:
-            raise InvalidConfiguration(
-                f"HDF Max Chunk Bytes is smaller than the size of a row. Please increase it. " \
-                f"max_chunk_bytes={max_chunk_bytes} num_columns={num_columns} " \
-                f"size_row={size_row}"
-            )
-
-        return chunk_count
-
-    @staticmethod
-    def get_column_ranges(dataset):
-        """Return the column ranges per name for the dataset.
-
-        Parameters
-        ----------
-        dataset : h5py.Dataset
-
-        Returns
-        -------
-        list
-
-        """
-        column_ranges_dataset = dataset.file[dataset.attrs["column_ranges_dataset_path"]]
-        return column_ranges_dataset[:]
-
-    @staticmethod
-    def get_columns(dataset):
-        """Return the columns for the dataset.
-
-        Parameters
-        ----------
-        dataset : h5py.Dataset
-
-        Returns
-        -------
-        list
-
-        """
-        col_dataset = dataset.file[dataset.attrs["column_dataset_path"]]
-        return [x.decode("utf8") for x in col_dataset[:]]
-
-    @staticmethod
-    def get_names(dataset):
-        """Return the names for the dataset.
-
-        Parameters
-        ----------
-        dataset : h5py.Dataset
-
-        Returns
-        -------
-        list
-
-        """
-        name_dataset = dataset.file[dataset.attrs["name_dataset_path"]]
-        return [x.decode("utf8") for x in name_dataset[:]]
-
-    @staticmethod
-    def to_dataframe(dataset, column_range=None):
-        """Create a pandas DataFrame from a dataset created with this class.
-
-        Parameters
-        ----------
-        dataset : h5py.Dataset
-        column_range : None | list
-            first element is column start, second element is length
-
-        Returns
-        -------
-        pd.DataFrame
-
-        """
-        length = dataset.attrs["length"]
-        columns = DatasetBuffer.get_columns(dataset)
-        if column_range is None:
-            return pd.DataFrame(dataset[:length], columns=columns)
-
-        start = column_range[0]
-        end = start + column_range[1]
-        return pd.DataFrame(
-            dataset[:length, start:end],
-            columns=columns[start:end],
-        )
-
-    @staticmethod
-    def to_datetime(dataset):
-        """Create a pandas DatetimeIndex from a dataset.
-
-        Parameters
-        ----------
-        dataset : h5py.Dataset
-
-        Returns
-        -------
-        pd.DatetimeIndex
-
-        """
-        length = dataset.attrs["length"]
-        return make_timestamps(dataset[:length])
+"""Contains DatasetBuffer"""
+
+from loguru import logger
+import pandas as pd
+import numpy as np
+
+from pydss.exceptions import InvalidConfiguration
+from pydss.utils.utils import make_timestamps
+from pydss.common import DatasetPropertyType
+
+
+
+KiB = 1024
+MiB = KiB * KiB
+GiB = MiB * MiB
+
+# The optimal number of chunks to store in memory will vary widely.
+# The h5py docs recommend keeping chunk byte sizes between 10 KiB - 1 MiB.
+# It needs to be larger than the biggest possible row and also cover enough
+# columns to compress duplicate values. Since we might store thousands of
+# elements in one dataset, make it the max by default.
+# Note that the downside to making this larger is that any read causes the
+# entire chunk to be read.
+DEFAULT_MAX_CHUNK_BYTES = 1 * MiB
+
+class DatasetBuffer:
+    """Provides a write buffer to an HDF dataset to increase performance.
+    Users must call flush_data before the object goes out of scope to ensure
+    that all data is flushed.
+
+    """
+    # TODO add support for context manager, though pydss wouldn't be able to
+    # take advantage in its current implementation.
+
+    def __init__(
+            self, hdf_store, path, max_size, dtype, columns, scaleoffset=None,
+            max_chunk_bytes=None, attributes=None, names=None,
+            column_ranges_per_name=None, data=None
+        ):
+        if max_chunk_bytes is None:
+            max_chunk_bytes = DEFAULT_MAX_CHUNK_BYTES
+        self._buf_index = 0
+        self._hdf_store = hdf_store
+        self._max_size = max_size
+        num_columns = len(columns)
+        if data is None:
+            self.chunk_count = self.compute_chunk_count(
+                num_columns,
+                max_size,
+                dtype,
+                max_chunk_bytes,
+            )
+            shape = (self._max_size, num_columns)
+            chunks = (self.chunk_count, num_columns)
+        else:
+            self.chunk_count = None
+            shape = None
+            chunks = None
+
+        dim = len(shape)
+        print(path)
+        self._dataset = self._hdf_store.create_dataset(
+            name=path,
+            shape=shape,
+            data=data,
+            chunks=chunks,
+            dtype=dtype,
+            compression="gzip",
+            compression_opts=4,
+            shuffle=True,
+            maxshape=[None for _ in range(dim)],
+            # Does not preserve NaN, so don't use it.
+            #scaleoffset=scaleoffset,
+        )
+
+        # Columns, names, and column_ranges_per_name can't be stored as
+        # attributes because they can exceed the size limit. Store as datasets
+        # instead.
+        column_dataset_path = path + "Columns"
+        column_dataset = self._hdf_store.create_dataset(
+            name=column_dataset_path,
+            data=np.array(columns, dtype="S"),
+            maxshape=(None, ),
+        )
+        column_dataset.attrs["type"] = DatasetPropertyType.METADATA.value
+        self._dataset.attrs["column_dataset_path"] = column_dataset_path
+
+        if names is not None:
+            name_dataset_path = path + "Names"
+            name_dataset = self._hdf_store.create_dataset(
+                name = name_dataset_path,
+                data = np.array(names, dtype="S"),
+                maxshape=(None, ),
+            )
+            name_dataset.attrs["type"] = DatasetPropertyType.METADATA.value
+            self._dataset.attrs["name_dataset_path"] = name_dataset_path
+
+        if column_ranges_per_name is not None:
+            column_ranges_dataset_path = path + "ColumnRanges"
+            data = np.array(column_ranges_per_name)
+            dim = len(data.shape)
+            column_ranges_dataset = self._hdf_store.create_dataset(
+                name=column_ranges_dataset_path,
+                data=data,
+                maxshape=[None for _ in range(dim)],
+            )
+            column_ranges_dataset.attrs["type"] = DatasetPropertyType.METADATA.value
+            self._dataset.attrs["column_ranges_dataset_path"] = column_ranges_dataset_path
+
+        self._dataset.attrs["length"] = 0
+        self._dataset_index = 0
+        self._buf = np.empty(chunks, dtype=dtype)
+
+        if attributes is not None:
+            for attr, val in attributes.items():
+                self._dataset.attrs[attr] = val
+
+        logger.debug("Created DatasetBuffer path=%s shape=%s chunks=%s",
+                     path, shape, chunks)
+
+    def __del__(self):
+        assert self._buf_index == 0, \
+            f"DatasetBuffer destructed with data in memory: {self._dataset.name}"
+
+    def flush_data(self):
+        """Flush the data in the temporary buffer to storage."""
+        length = self._buf_index
+        if length == 0:
+            return
+
+        new_index = self._dataset_index + length
+        
+        if new_index > self._dataset.shape[0]:
+            new_dimensions = (new_index, self._dataset.shape[1])
+            self._dataset.resize(new_dimensions)
+            logger.warning(f"result index {new_index} exceed dataset dimension {self._dataset.shape[0]} for dataset {self._dataset.name}. Resizig dataset to {new_dimensions}")
+  
+        self._dataset[self._dataset_index:new_index] = self._buf[0:length]
+        
+        self._buf_index = 0
+        self._dataset_index = new_index
+        self._dataset.attrs["length"] = new_index
+        self._dataset.flush()
+
+    def max_num_bytes(self):
+        """Return the maximum number of bytes the container could hold.
+
+        Returns
+        -------
+        int
+
+        """
+        size_row = self._buf.size * self._buf.itemsize / len(self._buf)
+        return size_row * self._max_size
+
+    def write_value(self, value):
+        """Write the value to the internal buffer, flushing when full."""
+
+        self._buf[self._buf_index] = value
+        self._buf_index += 1
+        if self._buf_index == self.chunk_count:
+            self.flush_data()
+
+    def write_data(self, values):
+        """Write the data to the dataset."""
+        new_index = self._dataset_index + len(values)
+        self._dataset[self._dataset_index:new_index] = values
+        self._dataset_index = new_index
+        self._dataset.attrs["length"] = new_index
+
+    @staticmethod
+    def compute_chunk_count(
+            num_columns,
+            max_size,
+            dtype,
+            max_chunk_bytes=DEFAULT_MAX_CHUNK_BYTES
+        ):
+        assert max_size > 0, f"max_size={max_size}"
+        tmp = np.empty((1, num_columns), dtype=dtype)
+        size_row = tmp.size * tmp.itemsize
+        chunk_count = min(int(max_chunk_bytes / size_row), max_size)
+        if chunk_count == 0:
+            raise InvalidConfiguration(
+                f"HDF Max Chunk Bytes is smaller than the size of a row. Please increase it. " \
+                f"max_chunk_bytes={max_chunk_bytes} num_columns={num_columns} " \
+                f"size_row={size_row}"
+            )
+
+        return chunk_count
+
+    @staticmethod
+    def get_column_ranges(dataset):
+        """Return the column ranges per name for the dataset.
+
+        Parameters
+        ----------
+        dataset : h5py.Dataset
+
+        Returns
+        -------
+        list
+
+        """
+        column_ranges_dataset = dataset.file[dataset.attrs["column_ranges_dataset_path"]]
+        return column_ranges_dataset[:]
+
+    @staticmethod
+    def get_columns(dataset):
+        """Return the columns for the dataset.
+
+        Parameters
+        ----------
+        dataset : h5py.Dataset
+
+        Returns
+        -------
+        list
+
+        """
+        col_dataset = dataset.file[dataset.attrs["column_dataset_path"]]
+        return [x.decode("utf8") for x in col_dataset[:]]
+
+    @staticmethod
+    def get_names(dataset):
+        """Return the names for the dataset.
+
+        Parameters
+        ----------
+        dataset : h5py.Dataset
+
+        Returns
+        -------
+        list
+
+        """
+        name_dataset = dataset.file[dataset.attrs["name_dataset_path"]]
+        return [x.decode("utf8") for x in name_dataset[:]]
+
+    @staticmethod
+    def to_dataframe(dataset, column_range=None):
+        """Create a pandas DataFrame from a dataset created with this class.
+
+        Parameters
+        ----------
+        dataset : h5py.Dataset
+        column_range : None | list
+            first element is column start, second element is length
+
+        Returns
+        -------
+        pd.DataFrame
+
+        """
+        length = dataset.attrs["length"]
+        columns = DatasetBuffer.get_columns(dataset)
+        if column_range is None:
+            return pd.DataFrame(dataset[:length], columns=columns)
+
+        start = column_range[0]
+        end = start + column_range[1]
+        return pd.DataFrame(
+            dataset[:length, start:end],
+            columns=columns[start:end],
+        )
+
+    @staticmethod
+    def to_datetime(dataset):
+        """Create a pandas DatetimeIndex from a dataset.
+
+        Parameters
+        ----------
+        dataset : h5py.Dataset
+
+        Returns
+        -------
+        pd.DatetimeIndex
+
+        """
+        length = dataset.attrs["length"]
+        return make_timestamps(dataset[:length])
```

### Comparing `nrel_pydss-3.1.3/src/pydss/dssInstance.py` & `nrel_pydss-3.1.4/src/pydss/dssInstance.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,529 +1,529 @@
-from pydss.common import SimulationType
-from pydss.simulation_input_models import SimulationSettingsModel
-from pydss.pyContrReader import read_controller_settings_from_registry
-from pydss.dssElementFactory import create_dss_element
-from pydss.utils.utils import make_human_readable_size
-from pydss.pyContrReader import pyContrReader as pcr
-from pydss.exceptions import (
-    InvalidConfiguration, PyDssConvergenceError, PyDssConvergenceErrorCountExceeded,
-    PyDssConvergenceMaxError, OpenDssModelError, OpenDssConvergenceErrorCountExceeded
-)
-from pydss.ProfileManager import ProfileInterface
-from pydss.pyPostprocessor import pyPostprocess
-import pydss.pyControllers as pyControllers
-from pydss import helics_interface as HI
-from pydss.ResultData import ResultData
-from pydss.dssCircuit import dssCircuit
-from pydss.common import SnapshotTimePointSelectionMode, DATE_FORMAT
-from pydss.dssBus import dssBus
-from pydss import SolveMode
-from pydss.simulation_input_models import SimulationSettingsModel
-from pydss.utils.simulation_utils import SimulationFilteredTimeRange
-from pydss.utils.timing_utils import Timer, timer_stats_collector, track_timing
-from pydss.get_snapshot_timepoints import get_snapshot_timepoint
-
-import opendssdirect as dss
-import numpy as np
-from loguru import logger
-import json
-import time
-import os
-from collections import defaultdict
-from pathlib import Path
-
-from opendssdirect.utils import run_command
-
-CONTROLLER_PRIORITIES = 3
-
-class OpenDSS:
-    def __init__(self, settings: SimulationSettingsModel):
-        self._dssInstance = dss
-        self._TempResultList = []
-        self._dssBuses = {}
-        self._dssObjects = {}
-        self._dssObjectsByClass = {}
-        self._DelFlag = 0
-        self._settings = settings
-        self._convergenceErrors = 0
-        self._convergenceErrorsOpenDSS = 0
-        self._maxConvergenceErrorCount = None
-        self._maxConvergenceError = 0.0
-        self._controller_iteration_counts = {}
-        self._simulation_range = SimulationFilteredTimeRange.from_settings(settings)
-
-        root_path = settings.project.project_path
-        active_project_path = root_path / settings.project.active_project
-        import_path = active_project_path / 'Scenarios'
-        active_scenario_path = import_path / settings.project.active_scenario
-        self._ActiveProject = settings.project.active_project
-
-        self._dssPath = {
-            'root': root_path,
-            'Import': import_path,
-            'ExportLists': active_scenario_path / 'ExportLists',
-            'pyControllers': active_scenario_path / 'pyControllerList',
-            'Export': active_project_path /  'Exports',
-            'Log': active_project_path / 'Logs',
-            'dssFiles': active_project_path / 'DSSfiles',
-            'dssFilePath': active_project_path / 'DSSfiles' / settings.project.dss_file,
-        }
-
-        if settings.project.dss_file_absolute_path:
-            self._dssPath['dssFilePath'] = Path(settings.project.dss_file)
-
-        if not self._dssPath['dssFilePath'].exists():
-            raise InvalidConfiguration(f"DSS file {self._dssPath['dssFilePath']} does not exist")
-
-        logger.info('An instance of OpenDSS version ' + self._dssInstance.__version__ + ' has been created.')
-
-        for key, path in self._dssPath.items():
-            if path.name == "pyControllerList" and not path.exists():
-                # This will happen if a zipped project with no controllers is unzipped and then run.
-                path.mkdir()
-            else:
-                assert path.exists(), '{} path: {} does not exist!'.format(key, path)
-
-        self._compile_model()
-
-        logger.info('OpenDSS fundamental frequency set to :  ' + str(settings.frequency.fundamental_frequency) + ' Hz')
-
-        #run_command('Set %SeriesRL={}'.format(settings.frequency.percentage_load_in_series))
-        if settings.frequency.neglect_shunt_admittance:
-            run_command('Set NeglectLoadY=Yes')
-
-        active_scenario = self._GetActiveScenario()
-        if active_scenario.snapshot_time_point_selection_config.mode != SnapshotTimePointSelectionMode.NONE:
-            self._SetSnapshotTimePoint(active_scenario)
-
-        self._dssCircuit = self._dssInstance.Circuit
-        self._dssElement = self._dssInstance.Element
-        self._dssBus = self._dssInstance.Bus
-        self._dssClass = self._dssInstance.ActiveClass
-        self._dssCommand = run_command
-        self._dssSolution = self._dssInstance.Solution
-        self._dssSolver = SolveMode.GetSolver(settings=settings, dssInstance=self._dssInstance)
-        self._dssBuses = self.CreateBusObjects()
-        self._dssObjects, self._dssObjectsByClass = self.CreateDssObjects(self._dssBuses)
-        self._dssSolver.reSolve()
-
-        if settings.profiles.use_profile_manager:
-            #TODO: disable internal profiles
-            logger.info('Disabling internal yearly and duty-cycle profiles.')
-            for m in ["Loads", "PVSystem", "Generator", "Storage"]:
-                run_command(f'BatchEdit {m}..* yearly=NONE duty=None')
-            profileSettings = self._settings.profiles.settings
-            profileSettings["objects"] = self._dssObjects
-            self.profileStore = ProfileInterface.Create(
-                self._dssInstance, self._dssSolver, self._settings, logger, **profileSettings
-            )
-
-        self.ResultContainer = ResultData(settings, self._dssPath,  self._dssObjects, self._dssObjectsByClass,
-                                          self._dssBuses, self._dssSolver, self._dssCommand, self._dssInstance)
-
-        if settings.project.use_controller_registry:
-            ControllerList = read_controller_settings_from_registry(self._dssPath['pyControllers'])
-        else:
-            pyCtrlReader = pcr(self._dssPath['pyControllers'])
-            ControllerList = pyCtrlReader.pyControllers
-
-        if ControllerList is not None:
-            self._CreateControllers(ControllerList)
-
-        self._increment_flag = True
-        if settings.helics.co_simulation_mode:
-            self._heilcs_interface = HI.helics_interface(self._dssSolver, self._dssObjects, self._dssObjectsByClass, settings,
-                                           self._dssPath)
-        logger.info("Simulation initialization complete")
-        return
-
-    @track_timing(timer_stats_collector)
-    def _compile_model(self):
-        self._dssInstance.Basic.ClearAll()
-        self._dssInstance.utils.run_command('Log=NO')
-        run_command('Clear')
-        logger.info('Loading OpenDSS model')
-        reply = ""
-        try:
-            orig_dir = os.getcwd()
-            reply = run_command('compile ' + str(self._dssPath['dssFilePath']))
-        finally:
-            os.chdir(orig_dir)
-
-        logger.info('OpenDSS:  ' + reply)
-        if reply != "":
-            raise OpenDssModelError(f"Error compiling OpenDSS model: {reply}")
-
-    def _CreateControllers(self, ControllerDict):
-        self._pyControls = {}
-        self._pyControls_types = {}
-        for ControllerType, ElementsDict in ControllerDict.items():
-            for ElmName, SettingsDict in ElementsDict.items():
-                Controller = pyControllers.pyController.Create(ElmName, ControllerType, SettingsDict, self._dssObjects,
-                                                  self._dssInstance, self._dssSolver)
-                if Controller != -1:
-                    controller_name = 'Controller.' + ElmName
-                    self._pyControls[controller_name] = Controller
-                    class_name, element_name = Controller.ControlledElement().split(".")
-                    if controller_name not in self._pyControls_types:
-                        self._pyControls_types[controller_name] = class_name
-                    logger.info('Created pyController -> Controller.' + ElmName)
-        return
-
-    def _update_controllers(self, Priority, Time, Iteration, UpdateResults):
-        errors = []
-        maxError = 0
-        _pyControls_types = set(self._pyControls_types.values())
-
-        for class_name in _pyControls_types:
-            self._dssInstance.Basic.SetActiveClass(class_name)
-            elm = self._dssInstance.ActiveClass.First()
-            while elm:
-                element_name = self._dssInstance.CktElement.Name()
-                controller_name = 'Controller.' + element_name
-                if controller_name in self._pyControls:
-                    controller = self._pyControls[controller_name]
-                    error = controller.Update(Priority, Time, UpdateResults)
-                    maxError = error if error > maxError else maxError
-                elm = self._dssInstance.ActiveClass.Next()
-        return maxError < self._settings.project.error_tolerance, maxError
-
-    @staticmethod
-    def CreateBusObjects():
-        dssBuses = {}
-        BusNames = dss.Circuit.AllBusNames()
-        dss.run_command('New  Fault.DEFAULT Bus1={} enabled=no r=0.01'.format(BusNames[0]))
-        for BusName in BusNames:
-            dss.Circuit.SetActiveBus(BusName)
-            dssBuses[BusName] = dssBus()
-        return dssBuses
-
-    @staticmethod
-    def CreateDssObjects(dssBuses):
-        dssObjects = {}
-        dssObjectsByClass = defaultdict(dict)
-
-        InvalidSelection = ['Settings', 'ActiveClass', 'dss', 'utils', 'PDElements', 'XYCurves', 'Bus', 'Properties']
-        # TODO: this causes a segmentation fault. Aadil says it may not be needed.
-        #self._dssObjectsByClass={'LoadShape': self._get_relavent_object_dict('LoadShape')}
-
-        for ElmName in dss.Circuit.AllElementNames():
-            Class, Name =  ElmName.split('.', 1)
-            ClassName = Class + 's'
-            dss.Circuit.SetActiveElement(ElmName)
-            dssObjectsByClass[ClassName][ElmName] = create_dss_element(Class, Name)
-            dssObjects[ElmName] = dssObjectsByClass[ClassName][ElmName]
-
-        for ObjName in dssObjects.keys():
-            ClassName = ObjName.split('.')[0] + 's'
-            if ObjName not in dssObjectsByClass[Class]:
-                dssObjectsByClass[Class][ObjName] = dssObjects[ObjName]
-
-        dssObjects['Circuit.' + dss.Circuit.Name()] = dssCircuit()
-        dssObjectsByClass['Circuits'] = {
-            'Circuit.' + dss.Circuit.Name(): dssObjects['Circuit.' + dss.Circuit.Name()]
-        }
-        dssObjectsByClass['Buses'] = dssBuses
-
-        return dssObjects, dssObjectsByClass
-
-    def _get_relavent_object_dict(self, key):
-        object_list = {}
-        element_collection = getattr(self._dssInstance, key)
-        element = element_collection.First()
-        while element:
-            fullName = self._dssInstance.Element.Name()
-            object, name =  fullName.split('.', 1)
-            object_list[fullName] = create_dss_element(object, name, self._dssInstance)
-            element = element_collection.Next()
-        return object_list
-
-    @track_timing(timer_stats_collector)
-    def RunStep(self, step, updateObjects=None):
-        # updating parameters before simulation run
-        if self._settings.logging.log_time_step_updates:
-            logger.info(f'Pydss datetime - {self._dssSolver.GetDateTime()}')
-            logger.info(f'OpenDSS time [h] - {self._dssSolver.GetOpenDSSTime()}')
-        if self._settings.profiles.use_profile_manager:
-            self.profileStore.update()
-
-        if self._settings.helics.co_simulation_mode:
-            self._heilcs_interface.updateHelicsSubscriptions()
-        else:
-            if updateObjects:
-                for object, params in updateObjects.items():
-                    cl, name = object.split('.')
-                    self._Modifier.Edit_Element(cl, name, params)
-
-        # run simulation time step and get results
-        time_step_has_converged = True
-        if not self._settings.project.disable_pydss_controllers:
-            with Timer(timer_stats_collector, "UpdateControllers"):
-                for priority in range(CONTROLLER_PRIORITIES):
-                    priority_has_converged = False
-                    for i in range(self._settings.project.max_control_iterations):
-                        has_converged, error = self._update_controllers(priority, step, i, UpdateResults=False)
-                        logger.debug('Control Loop {} convergence error: {}'.format(priority, error))
-                        if has_converged:
-                            priority_has_converged = True
-                            break
-                        self._dssSolver.reSolve()
-                    if i == 0:
-                        # Don't track 0.
-                        pass
-                    elif i not in self._controller_iteration_counts:
-                        self._controller_iteration_counts[i] = 1
-                    else:
-                        self._controller_iteration_counts[i] += 1
-                    if not priority_has_converged:
-                        time_step_has_converged = False
-                        logger.warning('Control Loop {} no convergence @ {} '.format(priority, step))
-                        self._HandleConvergenceErrorChecks(step, error)
-
-
-        if self._settings.frequency.enable_frequency_sweep and \
-                self._settings.project.simulation_type != SimulationType.DYNAMIC:
-            self._dssSolver.setMode('Harmonic')
-            for frequency in np.arange(self._settings.frequency.start_frequency,
-                                      self._settings.frequency.end_frequency + 1,
-                                      self._settings.frequency.frequency_increment):
-                self._dssSolver.setFrequency(frequency * self._settings.frequency.fundamental_frequency)
-                self._dssSolver.reSolve()
-                if self._settings.exports.export_results:
-                    self.ResultContainer.UpdateResults()
-            if self._settings.project.simulation_type != SimulationType.SNAPSHOT:
-                self._dssSolver.setMode('Snapshot')
-            else:
-                self._dssSolver.setMode('Yearly')
-
-        if self._settings.helics.co_simulation_mode:
-            self._heilcs_interface.updateHelicsPublications()
-            self._increment_flag, helics_time = self._heilcs_interface.request_time_increment()
-
-        return time_step_has_converged
-
-    def _HandleConvergenceErrorChecks(self, step, error):
-        self._convergenceErrors += 1
-
-        if self._maxConvergenceError != 0.0 and error > self._maxConvergenceError:
-            logger.error("Convergence error %s exceeded max value %s at step %s", error, self._maxConvergenceError, step)
-            raise PyDssConvergenceMaxError(f"Exceeded max convergence error {error}")
-
-        if self._maxConvergenceErrorCount is not None and self._convergenceErrors > self._maxConvergenceErrorCount:
-            logger.error("Exceeded convergence error count threshold at step %s", step)
-            raise PyDssConvergenceErrorCountExceeded(f"{self._convergenceErrors} errors occurred")
-
-    def _HandleOpenDSSConvergenceErrorChecks(self, step):
-        self._convergenceErrorsOpenDSS += 1
-
-        if self._maxConvergenceErrorCount is not None and self._convergenceErrorsOpenDSS > self._maxConvergenceErrorCount:
-            logger.error("Exceeded OpenDSS convergence error count threshold at step %s", step)
-            raise OpenDssConvergenceErrorCountExceeded(f"{self._convergenceErrorsOpenDSS} errors occurred")
-
-    def DryRunSimulation(self, project, scenario):
-        """Run one time point for getting estimated space."""
-        if not self._settings.exports.export_results:
-            raise InvalidConfiguration("Log Reults must set to be True.")
-
-        Steps, _, _ = self._dssSolver.SimulationSteps()
-        logger.info('Dry run simulation...')
-        self.ResultContainer.InitializeDataStore(project.hdf_store, Steps)
-
-        try:
-            self.RunStep(0)
-            self.ResultContainer.UpdateResults()
-        finally:
-            self.ResultContainer.Close()
-
-        return self.ResultContainer.max_num_bytes()
-
-    def initStore(self, hdf_store, Steps, MC_scenario_number=None):
-        self.ResultContainer.InitializeDataStore(hdf_store, Steps, MC_scenario_number)
-
-    def RunSimulation(self, project, scenario, MC_scenario_number=None):
-        """Yields a tuple of the results of each step.
-
-        Yields
-        ------
-        tuple
-            is_complete, step, has_converged, results
-
-        """
-        startTime = time.time()
-        Steps, sTime, eTime = self._dssSolver.SimulationSteps()
-        threshold = self._settings.project.convergence_error_percent_threshold
-        if threshold > 0:
-            self._maxConvergenceErrorCount = round(threshold * .01 * Steps)
-        self._maxConvergenceError = self._settings.project.max_error_tolerance
-        dss.Solution.Convergence(self._settings.project.error_tolerance)
-        logger.info('Running simulation from {} till {}.'.format(sTime, eTime))
-        logger.info('Simulation time step {}.'.format(Steps))
-        logger.info("Set OpenDSS convergence to %s", dss.Solution.Convergence())
-        logger.info('Max convergence error count {}.'.format(self._maxConvergenceErrorCount))
-        logger.info("initializing store")
-        self.ResultContainer.InitializeDataStore(project.hdf_store, Steps, MC_scenario_number)
-        postprocessors = [
-            pyPostprocess.Create(
-                project,
-                scenario,
-                ppInfo,
-                self._dssInstance,
-                self._dssSolver,
-                self._dssObjects,
-                self._dssObjectsByClass,
-                self._settings,
-                logger,
-            ) for ppInfo in scenario.post_process_infos
-        ]
-        if not postprocessors:
-            logger.info('No post processing script selected')
-
-        is_complete = False
-        step = 0
-        has_converged = False
-        current_results = {}
-        try:
-            while step < Steps:
-                pydss_has_converged = True
-                opendss_has_converged = True
-                within_range = self._simulation_range.is_within_range(self._dssSolver.GetDateTime())
-                if within_range:
-                    pydss_has_converged = self.RunStep(step)
-                    opendss_has_converged = dss.Solution.Converged()
-                    if not opendss_has_converged:
-                        logger.error("OpenDSS did not converge at step=%s pydss_converged=%s",
-                                            step, pydss_has_converged)
-                        self._HandleOpenDSSConvergenceErrorChecks(step)
-                has_converged = pydss_has_converged and opendss_has_converged
-                if step == 0 and self.ResultContainer is not None:
-                    size = make_human_readable_size(self.ResultContainer.max_num_bytes())
-                    logger.info('Storage requirement estimation: %s, estimated based on first time step run.', size)
-                if postprocessors and within_range:
-                    step, has_converged = self._RunPostProcessors(step, Steps, postprocessors)
-                if self._increment_flag:
-                    step += 1
-
-                # In the case of a frequency sweep, the code updates results at each frequency.
-                # Doing so again would cause a duplicate result.
-                if (
-                    self._settings.exports.export_results and not
-                    (self._settings.frequency.enable_frequency_sweep and \
-                     self._settings.project.simulation_type != SimulationType.DYNAMIC)
-                ):
-                    store_nan = (
-                        not within_range or
-                        (not has_converged and
-                         self._settings.project.skip_export_on_convergence_error)
-                    )
-                    self.ResultContainer.UpdateResults(store_nan=store_nan)
-
-                if self._settings.helics.co_simulation_mode:
-                    if self._increment_flag:
-                        self._dssSolver.IncStep()
-                    else:
-                        self._dssSolver.reSolve()
-                else:
-                    self._dssSolver.IncStep()
-
-                if self._settings.exports.export_results:
-                    current_results = self.ResultContainer.CurrentResults
-                yield False, step, has_converged, current_results
-
-        finally:
-            if self._settings and self._settings.exports.export_results:
-                # This is here to guarantee that DatasetBuffers aren't left
-                # with any data in memory.
-                self.ResultContainer.Close()
-
-            for postprocessor in postprocessors:
-                postprocessor.finalize()
-
-        if self._settings and self._settings.exports.export_results:
-            self.ResultContainer.ExportResults()
-
-        timer_stats_collector.log_stats(clear=True)
-        if self._controller_iteration_counts:
-            data = {
-                "Report": "ControllerIterationCounts",
-                "Scenario": self._settings.project.active_scenario,
-                "Counts": self._controller_iteration_counts,
-            }
-        
-        logger.info(f'Simulation completed in { time.time() - startTime} seconds',)
-        logger.info('End of simulation')
-        yield True, step, has_converged, current_results
-
-    def _RunPostProcessors(self, step, Steps, postprocessors):
-        for postprocessor in postprocessors:
-            orig_step = step
-            step, has_converged, error = postprocessor.run(step, Steps, simulation=self)
-            assert step <= orig_step, "step cannot increment in postprocessor"
-            if not has_converged:
-                name = postprocessor.__class__.__name__
-                logger.warn("postprocessor %s reported a convergence error at step %s", name, step)
-                self._HandleConvergenceErrorChecks(step, error)
-
-        return step, has_converged
-
-    def RunMCsimulation(self, project, scenario, samples):
-        from pydss.Extensions.MonteCarlo import MonteCarloSim
-        MC = MonteCarloSim(self._settings, self._dssPath, self._dssObjects, self._dssObjectsByClass)
-        for i in range(samples):
-            MC.Create_Scenario()
-            for is_complete, _, _, _ in self.RunSimulation(project, scenario, i):
-                if is_complete:
-                    break
-        return
-
-    def _GetActiveScenario(self):
-        active_scenario = self._settings.project.active_scenario
-        for scenario in self._settings.project.scenarios:
-            if scenario.name == active_scenario:
-                return scenario
-        raise InvalidConfiguration(f"Active Scenario {active_scenario} is not present")
-
-    @track_timing(timer_stats_collector)
-    def _SetSnapshotTimePoint(self, scenario):
-        """Adjusts the time parameters based on the mode."""
-        p_settings = self._settings.project
-        config = scenario.snapshot_time_point_selection_config
-        mode = config.mode
-        assert mode != SnapshotTimePointSelectionMode.NONE, mode
-
-        if mode != SnapshotTimePointSelectionMode.NONE:
-            if p_settings.simulation_type != SimulationType.QSTS:
-                raise InvalidConfiguration(f"{mode} is only supported with QSTS simulations")
-
-            # These settings have to be temporarily overridden because of the underlying
-            # implementation to create a load shape dataframes..
-            orig_start = p_settings.start_time
-            orig_duration = p_settings.simulation_duration_min
-            if orig_duration != p_settings.step_resolution_sec / 60:
-                raise InvalidConfiguration("Simulation duration must be the same as resolution")
-            try:
-                p_settings.start_time = config.start_time
-                p_settings.simulation_duration_min = config.search_duration_min
-                new_start = get_snapshot_timepoint(self._settings, mode).strftime(DATE_FORMAT)
-                p_settings.start_time = new_start
-                logger.info("Changed simulation start time from %s to %s",
-                    orig_start,
-                    new_start,
-                )
-            except Exception:
-                p_settings.start_time = orig_start
-                raise
-            finally:
-                p_settings.simulation_duration_min = orig_duration
-        else:
-            assert False, f"unsupported mode {mode}"
-
-    # def __del__(self):
-    #     logger.info('An instance of OpenDSS (' + str(self) + ') has been deleted.')
-    #     loggers = [logger, self._reportsLogger]
-    #     if self._settings["Logging"]["Log to external file"]:
-    #         for L in loggers:
-    #             handlers = list(L.handlers)
-    #             for filehandler in handlers:
-    #                 filehandler.flush()
-    #                 filehandler.close()
-    #                 L.removeHandler(filehandler)
-    #     return
+from pydss.common import SimulationType
+from pydss.simulation_input_models import SimulationSettingsModel
+from pydss.pyContrReader import read_controller_settings_from_registry
+from pydss.dssElementFactory import create_dss_element
+from pydss.utils.utils import make_human_readable_size
+from pydss.pyContrReader import pyContrReader as pcr
+from pydss.exceptions import (
+    InvalidConfiguration, PyDssConvergenceError, PyDssConvergenceErrorCountExceeded,
+    PyDssConvergenceMaxError, OpenDssModelError, OpenDssConvergenceErrorCountExceeded
+)
+from pydss.ProfileManager import ProfileInterface
+from pydss.pyPostprocessor import pyPostprocess
+import pydss.pyControllers as pyControllers
+from pydss import helics_interface as HI
+from pydss.ResultData import ResultData
+from pydss.dssCircuit import dssCircuit
+from pydss.common import SnapshotTimePointSelectionMode, DATE_FORMAT
+from pydss.dssBus import dssBus
+from pydss import SolveMode
+from pydss.simulation_input_models import SimulationSettingsModel
+from pydss.utils.simulation_utils import SimulationFilteredTimeRange
+from pydss.utils.timing_utils import Timer, timer_stats_collector, track_timing
+from pydss.get_snapshot_timepoints import get_snapshot_timepoint
+
+import opendssdirect as dss
+import numpy as np
+from loguru import logger
+import json
+import time
+import os
+from collections import defaultdict
+from pathlib import Path
+
+from opendssdirect.utils import run_command
+
+CONTROLLER_PRIORITIES = 3
+
+class OpenDSS:
+    def __init__(self, settings: SimulationSettingsModel):
+        self._dssInstance = dss
+        self._TempResultList = []
+        self._dssBuses = {}
+        self._dssObjects = {}
+        self._dssObjectsByClass = {}
+        self._DelFlag = 0
+        self._settings = settings
+        self._convergenceErrors = 0
+        self._convergenceErrorsOpenDSS = 0
+        self._maxConvergenceErrorCount = None
+        self._maxConvergenceError = 0.0
+        self._controller_iteration_counts = {}
+        self._simulation_range = SimulationFilteredTimeRange.from_settings(settings)
+
+        root_path = settings.project.project_path
+        active_project_path = root_path / settings.project.active_project
+        import_path = active_project_path / 'Scenarios'
+        active_scenario_path = import_path / settings.project.active_scenario
+        self._ActiveProject = settings.project.active_project
+
+        self._dssPath = {
+            'root': root_path,
+            'Import': import_path,
+            'ExportLists': active_scenario_path / 'ExportLists',
+            'pyControllers': active_scenario_path / 'pyControllerList',
+            'Export': active_project_path /  'Exports',
+            'Log': active_project_path / 'Logs',
+            'dssFiles': active_project_path / 'DSSfiles',
+            'dssFilePath': active_project_path / 'DSSfiles' / settings.project.dss_file,
+        }
+
+        if settings.project.dss_file_absolute_path:
+            self._dssPath['dssFilePath'] = Path(settings.project.dss_file)
+
+        if not self._dssPath['dssFilePath'].exists():
+            raise InvalidConfiguration(f"DSS file {self._dssPath['dssFilePath']} does not exist")
+
+        logger.info('An instance of OpenDSS version ' + self._dssInstance.__version__ + ' has been created.')
+
+        for key, path in self._dssPath.items():
+            if path.name == "pyControllerList" and not path.exists():
+                # This will happen if a zipped project with no controllers is unzipped and then run.
+                path.mkdir()
+            else:
+                assert path.exists(), '{} path: {} does not exist!'.format(key, path)
+
+        self._compile_model()
+
+        logger.info('OpenDSS fundamental frequency set to :  ' + str(settings.frequency.fundamental_frequency) + ' Hz')
+
+        #run_command('Set %SeriesRL={}'.format(settings.frequency.percentage_load_in_series))
+        if settings.frequency.neglect_shunt_admittance:
+            run_command('Set NeglectLoadY=Yes')
+
+        active_scenario = self._GetActiveScenario()
+        if active_scenario.snapshot_time_point_selection_config.mode != SnapshotTimePointSelectionMode.NONE:
+            self._SetSnapshotTimePoint(active_scenario)
+
+        self._dssCircuit = self._dssInstance.Circuit
+        self._dssElement = self._dssInstance.Element
+        self._dssBus = self._dssInstance.Bus
+        self._dssClass = self._dssInstance.ActiveClass
+        self._dssCommand = run_command
+        self._dssSolution = self._dssInstance.Solution
+        self._dssSolver = SolveMode.GetSolver(settings=settings, dssInstance=self._dssInstance)
+        self._dssBuses = self.CreateBusObjects()
+        self._dssObjects, self._dssObjectsByClass = self.CreateDssObjects(self._dssBuses)
+        self._dssSolver.reSolve()
+
+        if settings.profiles.use_profile_manager:
+            #TODO: disable internal profiles
+            logger.info('Disabling internal yearly and duty-cycle profiles.')
+            for m in ["Loads", "PVSystem", "Generator", "Storage"]:
+                run_command(f'BatchEdit {m}..* yearly=NONE duty=None')
+            profileSettings = self._settings.profiles.settings
+            profileSettings["objects"] = self._dssObjects
+            self.profileStore = ProfileInterface.Create(
+                self._dssInstance, self._dssSolver, self._settings, logger, **profileSettings
+            )
+
+        self.ResultContainer = ResultData(settings, self._dssPath,  self._dssObjects, self._dssObjectsByClass,
+                                          self._dssBuses, self._dssSolver, self._dssCommand, self._dssInstance)
+
+        if settings.project.use_controller_registry:
+            ControllerList = read_controller_settings_from_registry(self._dssPath['pyControllers'])
+        else:
+            pyCtrlReader = pcr(self._dssPath['pyControllers'])
+            ControllerList = pyCtrlReader.pyControllers
+
+        if ControllerList is not None:
+            self._CreateControllers(ControllerList)
+
+        self._increment_flag = True
+        if settings.helics.co_simulation_mode:
+            self._heilcs_interface = HI.helics_interface(self._dssSolver, self._dssObjects, self._dssObjectsByClass, settings,
+                                           self._dssPath)
+        logger.info("Simulation initialization complete")
+        return
+
+    @track_timing(timer_stats_collector)
+    def _compile_model(self):
+        self._dssInstance.Basic.ClearAll()
+        self._dssInstance.utils.run_command('Log=NO')
+        run_command('Clear')
+        logger.info('Loading OpenDSS model')
+        reply = ""
+        try:
+            orig_dir = os.getcwd()
+            reply = run_command('compile ' + str(self._dssPath['dssFilePath']))
+        finally:
+            os.chdir(orig_dir)
+
+        logger.info('OpenDSS:  ' + reply)
+        if reply != "":
+            raise OpenDssModelError(f"Error compiling OpenDSS model: {reply}")
+
+    def _CreateControllers(self, ControllerDict):
+        self._pyControls = {}
+        self._pyControls_types = {}
+        for ControllerType, ElementsDict in ControllerDict.items():
+            for ElmName, SettingsDict in ElementsDict.items():
+                Controller = pyControllers.pyController.Create(ElmName, ControllerType, SettingsDict, self._dssObjects,
+                                                  self._dssInstance, self._dssSolver)
+                if Controller != -1:
+                    controller_name = 'Controller.' + ElmName
+                    self._pyControls[controller_name] = Controller
+                    class_name, element_name = Controller.ControlledElement().split(".")
+                    if controller_name not in self._pyControls_types:
+                        self._pyControls_types[controller_name] = class_name
+                    logger.info('Created pyController -> Controller.' + ElmName)
+        return
+
+    def _update_controllers(self, Priority, Time, Iteration, UpdateResults):
+        errors = []
+        maxError = 0
+        _pyControls_types = set(self._pyControls_types.values())
+
+        for class_name in _pyControls_types:
+            self._dssInstance.Basic.SetActiveClass(class_name)
+            elm = self._dssInstance.ActiveClass.First()
+            while elm:
+                element_name = self._dssInstance.CktElement.Name()
+                controller_name = 'Controller.' + element_name
+                if controller_name in self._pyControls:
+                    controller = self._pyControls[controller_name]
+                    error = controller.Update(Priority, Time, UpdateResults)
+                    maxError = error if error > maxError else maxError
+                elm = self._dssInstance.ActiveClass.Next()
+        return maxError < self._settings.project.error_tolerance, maxError
+
+    @staticmethod
+    def CreateBusObjects():
+        dssBuses = {}
+        BusNames = dss.Circuit.AllBusNames()
+        dss.run_command('New  Fault.DEFAULT Bus1={} enabled=no r=0.01'.format(BusNames[0]))
+        for BusName in BusNames:
+            dss.Circuit.SetActiveBus(BusName)
+            dssBuses[BusName] = dssBus()
+        return dssBuses
+
+    @staticmethod
+    def CreateDssObjects(dssBuses):
+        dssObjects = {}
+        dssObjectsByClass = defaultdict(dict)
+
+        InvalidSelection = ['Settings', 'ActiveClass', 'dss', 'utils', 'PDElements', 'XYCurves', 'Bus', 'Properties']
+        # TODO: this causes a segmentation fault. Aadil says it may not be needed.
+        #self._dssObjectsByClass={'LoadShape': self._get_relavent_object_dict('LoadShape')}
+
+        for ElmName in dss.Circuit.AllElementNames():
+            Class, Name =  ElmName.split('.', 1)
+            ClassName = Class + 's'
+            dss.Circuit.SetActiveElement(ElmName)
+            dssObjectsByClass[ClassName][ElmName] = create_dss_element(Class, Name)
+            dssObjects[ElmName] = dssObjectsByClass[ClassName][ElmName]
+
+        for ObjName in dssObjects.keys():
+            ClassName = ObjName.split('.')[0] + 's'
+            if ObjName not in dssObjectsByClass[Class]:
+                dssObjectsByClass[Class][ObjName] = dssObjects[ObjName]
+
+        dssObjects['Circuit.' + dss.Circuit.Name()] = dssCircuit()
+        dssObjectsByClass['Circuits'] = {
+            'Circuit.' + dss.Circuit.Name(): dssObjects['Circuit.' + dss.Circuit.Name()]
+        }
+        dssObjectsByClass['Buses'] = dssBuses
+
+        return dssObjects, dssObjectsByClass
+
+    def _get_relavent_object_dict(self, key):
+        object_list = {}
+        element_collection = getattr(self._dssInstance, key)
+        element = element_collection.First()
+        while element:
+            fullName = self._dssInstance.Element.Name()
+            object, name =  fullName.split('.', 1)
+            object_list[fullName] = create_dss_element(object, name, self._dssInstance)
+            element = element_collection.Next()
+        return object_list
+
+    @track_timing(timer_stats_collector)
+    def RunStep(self, step, updateObjects=None):
+        # updating parameters before simulation run
+        if self._settings.logging.log_time_step_updates:
+            logger.info(f'Pydss datetime - {self._dssSolver.GetDateTime()}')
+            logger.info(f'OpenDSS time [h] - {self._dssSolver.GetOpenDSSTime()}')
+        if self._settings.profiles.use_profile_manager:
+            self.profileStore.update()
+
+        if self._settings.helics.co_simulation_mode:
+            self._heilcs_interface.updateHelicsSubscriptions()
+        else:
+            if updateObjects:
+                for object, params in updateObjects.items():
+                    cl, name = object.split('.')
+                    self._Modifier.Edit_Element(cl, name, params)
+
+        # run simulation time step and get results
+        time_step_has_converged = True
+        if not self._settings.project.disable_pydss_controllers:
+            with Timer(timer_stats_collector, "UpdateControllers"):
+                for priority in range(CONTROLLER_PRIORITIES):
+                    priority_has_converged = False
+                    for i in range(self._settings.project.max_control_iterations):
+                        has_converged, error = self._update_controllers(priority, step, i, UpdateResults=False)
+                        logger.debug('Control Loop {} convergence error: {}'.format(priority, error))
+                        if has_converged:
+                            priority_has_converged = True
+                            break
+                        self._dssSolver.reSolve()
+                    if i == 0:
+                        # Don't track 0.
+                        pass
+                    elif i not in self._controller_iteration_counts:
+                        self._controller_iteration_counts[i] = 1
+                    else:
+                        self._controller_iteration_counts[i] += 1
+                    if not priority_has_converged:
+                        time_step_has_converged = False
+                        logger.warning('Control Loop {} no convergence @ {} '.format(priority, step))
+                        self._HandleConvergenceErrorChecks(step, error)
+
+
+        if self._settings.frequency.enable_frequency_sweep and \
+                self._settings.project.simulation_type != SimulationType.DYNAMIC:
+            self._dssSolver.setMode('Harmonic')
+            for frequency in np.arange(self._settings.frequency.start_frequency,
+                                      self._settings.frequency.end_frequency + 1,
+                                      self._settings.frequency.frequency_increment):
+                self._dssSolver.setFrequency(frequency * self._settings.frequency.fundamental_frequency)
+                self._dssSolver.reSolve()
+                if self._settings.exports.export_results:
+                    self.ResultContainer.UpdateResults()
+            if self._settings.project.simulation_type != SimulationType.SNAPSHOT:
+                self._dssSolver.setMode('Snapshot')
+            else:
+                self._dssSolver.setMode('Yearly')
+
+        if self._settings.helics.co_simulation_mode:
+            self._heilcs_interface.updateHelicsPublications()
+            self._increment_flag, helics_time = self._heilcs_interface.request_time_increment()
+
+        return time_step_has_converged
+
+    def _HandleConvergenceErrorChecks(self, step, error):
+        self._convergenceErrors += 1
+
+        if self._maxConvergenceError != 0.0 and error > self._maxConvergenceError:
+            logger.error("Convergence error %s exceeded max value %s at step %s", error, self._maxConvergenceError, step)
+            raise PyDssConvergenceMaxError(f"Exceeded max convergence error {error}")
+
+        if self._maxConvergenceErrorCount is not None and self._convergenceErrors > self._maxConvergenceErrorCount:
+            logger.error("Exceeded convergence error count threshold at step %s", step)
+            raise PyDssConvergenceErrorCountExceeded(f"{self._convergenceErrors} errors occurred")
+
+    def _HandleOpenDSSConvergenceErrorChecks(self, step):
+        self._convergenceErrorsOpenDSS += 1
+
+        if self._maxConvergenceErrorCount is not None and self._convergenceErrorsOpenDSS > self._maxConvergenceErrorCount:
+            logger.error("Exceeded OpenDSS convergence error count threshold at step %s", step)
+            raise OpenDssConvergenceErrorCountExceeded(f"{self._convergenceErrorsOpenDSS} errors occurred")
+
+    def DryRunSimulation(self, project, scenario):
+        """Run one time point for getting estimated space."""
+        if not self._settings.exports.export_results:
+            raise InvalidConfiguration("Log Reults must set to be True.")
+
+        Steps, _, _ = self._dssSolver.SimulationSteps()
+        logger.info('Dry run simulation...')
+        self.ResultContainer.InitializeDataStore(project.hdf_store, Steps)
+
+        try:
+            self.RunStep(0)
+            self.ResultContainer.UpdateResults()
+        finally:
+            self.ResultContainer.Close()
+
+        return self.ResultContainer.max_num_bytes()
+
+    def initStore(self, hdf_store, Steps, MC_scenario_number=None):
+        self.ResultContainer.InitializeDataStore(hdf_store, Steps, MC_scenario_number)
+
+    def RunSimulation(self, project, scenario, MC_scenario_number=None):
+        """Yields a tuple of the results of each step.
+
+        Yields
+        ------
+        tuple
+            is_complete, step, has_converged, results
+
+        """
+        startTime = time.time()
+        Steps, sTime, eTime = self._dssSolver.SimulationSteps()
+        threshold = self._settings.project.convergence_error_percent_threshold
+        if threshold > 0:
+            self._maxConvergenceErrorCount = round(threshold * .01 * Steps)
+        self._maxConvergenceError = self._settings.project.max_error_tolerance
+        dss.Solution.Convergence(self._settings.project.error_tolerance)
+        logger.info('Running simulation from {} till {}.'.format(sTime, eTime))
+        logger.info('Simulation time step {}.'.format(Steps))
+        logger.info("Set OpenDSS convergence to %s", dss.Solution.Convergence())
+        logger.info('Max convergence error count {}.'.format(self._maxConvergenceErrorCount))
+        logger.info("initializing store")
+        self.ResultContainer.InitializeDataStore(project.hdf_store, Steps, MC_scenario_number)
+        postprocessors = [
+            pyPostprocess.Create(
+                project,
+                scenario,
+                ppInfo,
+                self._dssInstance,
+                self._dssSolver,
+                self._dssObjects,
+                self._dssObjectsByClass,
+                self._settings,
+                logger,
+            ) for ppInfo in scenario.post_process_infos
+        ]
+        if not postprocessors:
+            logger.info('No post processing script selected')
+
+        is_complete = False
+        step = 0
+        has_converged = False
+        current_results = {}
+        try:
+            while step < Steps:
+                pydss_has_converged = True
+                opendss_has_converged = True
+                within_range = self._simulation_range.is_within_range(self._dssSolver.GetDateTime())
+                if within_range:
+                    pydss_has_converged = self.RunStep(step)
+                    opendss_has_converged = dss.Solution.Converged()
+                    if not opendss_has_converged:
+                        logger.error("OpenDSS did not converge at step=%s pydss_converged=%s",
+                                            step, pydss_has_converged)
+                        self._HandleOpenDSSConvergenceErrorChecks(step)
+                has_converged = pydss_has_converged and opendss_has_converged
+                if step == 0 and self.ResultContainer is not None:
+                    size = make_human_readable_size(self.ResultContainer.max_num_bytes())
+                    logger.info('Storage requirement estimation: %s, estimated based on first time step run.', size)
+                if postprocessors and within_range:
+                    step, has_converged = self._RunPostProcessors(step, Steps, postprocessors)
+                if self._increment_flag:
+                    step += 1
+
+                # In the case of a frequency sweep, the code updates results at each frequency.
+                # Doing so again would cause a duplicate result.
+                if (
+                    self._settings.exports.export_results and not
+                    (self._settings.frequency.enable_frequency_sweep and \
+                     self._settings.project.simulation_type != SimulationType.DYNAMIC)
+                ):
+                    store_nan = (
+                        not within_range or
+                        (not has_converged and
+                         self._settings.project.skip_export_on_convergence_error)
+                    )
+                    self.ResultContainer.UpdateResults(store_nan=store_nan)
+
+                if self._settings.helics.co_simulation_mode:
+                    if self._increment_flag:
+                        self._dssSolver.IncStep()
+                    else:
+                        self._dssSolver.reSolve()
+                else:
+                    self._dssSolver.IncStep()
+
+                if self._settings.exports.export_results:
+                    current_results = self.ResultContainer.CurrentResults
+                yield False, step, has_converged, current_results
+
+        finally:
+            if self._settings and self._settings.exports.export_results:
+                # This is here to guarantee that DatasetBuffers aren't left
+                # with any data in memory.
+                self.ResultContainer.Close()
+
+            for postprocessor in postprocessors:
+                postprocessor.finalize()
+
+        if self._settings and self._settings.exports.export_results:
+            self.ResultContainer.ExportResults()
+
+        timer_stats_collector.log_stats(clear=True)
+        if self._controller_iteration_counts:
+            data = {
+                "Report": "ControllerIterationCounts",
+                "Scenario": self._settings.project.active_scenario,
+                "Counts": self._controller_iteration_counts,
+            }
+        
+        logger.info(f'Simulation completed in { time.time() - startTime} seconds',)
+        logger.info('End of simulation')
+        yield True, step, has_converged, current_results
+
+    def _RunPostProcessors(self, step, Steps, postprocessors):
+        for postprocessor in postprocessors:
+            orig_step = step
+            step, has_converged, error = postprocessor.run(step, Steps, simulation=self)
+            assert step <= orig_step, "step cannot increment in postprocessor"
+            if not has_converged:
+                name = postprocessor.__class__.__name__
+                logger.warn("postprocessor %s reported a convergence error at step %s", name, step)
+                self._HandleConvergenceErrorChecks(step, error)
+
+        return step, has_converged
+
+    def RunMCsimulation(self, project, scenario, samples):
+        from pydss.Extensions.MonteCarlo import MonteCarloSim
+        MC = MonteCarloSim(self._settings, self._dssPath, self._dssObjects, self._dssObjectsByClass)
+        for i in range(samples):
+            MC.Create_Scenario()
+            for is_complete, _, _, _ in self.RunSimulation(project, scenario, i):
+                if is_complete:
+                    break
+        return
+
+    def _GetActiveScenario(self):
+        active_scenario = self._settings.project.active_scenario
+        for scenario in self._settings.project.scenarios:
+            if scenario.name == active_scenario:
+                return scenario
+        raise InvalidConfiguration(f"Active Scenario {active_scenario} is not present")
+
+    @track_timing(timer_stats_collector)
+    def _SetSnapshotTimePoint(self, scenario):
+        """Adjusts the time parameters based on the mode."""
+        p_settings = self._settings.project
+        config = scenario.snapshot_time_point_selection_config
+        mode = config.mode
+        assert mode != SnapshotTimePointSelectionMode.NONE, mode
+
+        if mode != SnapshotTimePointSelectionMode.NONE:
+            if p_settings.simulation_type != SimulationType.QSTS:
+                raise InvalidConfiguration(f"{mode} is only supported with QSTS simulations")
+
+            # These settings have to be temporarily overridden because of the underlying
+            # implementation to create a load shape dataframes..
+            orig_start = p_settings.start_time
+            orig_duration = p_settings.simulation_duration_min
+            if orig_duration != p_settings.step_resolution_sec / 60:
+                raise InvalidConfiguration("Simulation duration must be the same as resolution")
+            try:
+                p_settings.start_time = config.start_time
+                p_settings.simulation_duration_min = config.search_duration_min
+                new_start = get_snapshot_timepoint(self._settings, mode).strftime(DATE_FORMAT)
+                p_settings.start_time = new_start
+                logger.info("Changed simulation start time from %s to %s",
+                    orig_start,
+                    new_start,
+                )
+            except Exception:
+                p_settings.start_time = orig_start
+                raise
+            finally:
+                p_settings.simulation_duration_min = orig_duration
+        else:
+            assert False, f"unsupported mode {mode}"
+
+    # def __del__(self):
+    #     logger.info('An instance of OpenDSS (' + str(self) + ') has been deleted.')
+    #     loggers = [logger, self._reportsLogger]
+    #     if self._settings["Logging"]["Log to external file"]:
+    #         for L in loggers:
+    #             handlers = list(L.handlers)
+    #             for filehandler in handlers:
+    #                 filehandler.flush()
+    #                 filehandler.close()
+    #                 L.removeHandler(filehandler)
+    #     return
```

### Comparing `nrel_pydss-3.1.3/src/pydss/dssTransformer.py` & `nrel_pydss-3.1.4/src/pydss/dssTransformer.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,154 +1,149 @@
-00000000: 0d0a 6672 6f6d 2070 7964 7373 2e64 7373  ..from pydss.dss
-00000010: 456c 656d 656e 7420 696d 706f 7274 2064  Element import d
-00000020: 7373 456c 656d 656e 740d 0a66 726f 6d20  ssElement..from 
-00000030: 7079 6473 732e 7661 6c75 655f 7374 6f72  pydss.value_stor
-00000040: 6167 6520 696d 706f 7274 2056 616c 7565  age import Value
-00000050: 4279 4e75 6d62 6572 0d0a 6672 6f6d 2070  ByNumber..from p
-00000060: 7964 7373 2e76 616c 7565 5f73 746f 7261  ydss.value_stora
-00000070: 6765 2069 6d70 6f72 7420 5661 6c75 6542  ge import ValueB
-00000080: 794c 6973 740d 0a69 6d70 6f72 7420 6173  yList..import as
-00000090: 740d 0a66 726f 6d20 7079 6473 732e 6578  t..from pydss.ex
-000000a0: 6365 7074 696f 6e73 2069 6d70 6f72 7420  ceptions import 
-000000b0: 496e 7661 6c69 6450 6172 616d 6574 6572  InvalidParameter
-000000c0: 0d0a 0d0a 636c 6173 7320 6473 7354 7261  ....class dssTra
-000000d0: 6e73 666f 726d 6572 2864 7373 456c 656d  nsformer(dssElem
-000000e0: 656e 7429 3a0d 0a0d 0a20 2020 2056 4152  ent):....    VAR
-000000f0: 4941 424c 455f 4f55 5450 5554 535f 4259  IABLE_OUTPUTS_BY
-00000100: 5f4c 4142 454c 203d 207b 0d0a 2020 2020  _LABEL = {..    
-00000110: 2020 2020 2243 7572 7265 6e74 7322 3a20      "Currents": 
-00000120: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-00000130: 6973 5f63 6f6d 706c 6578 223a 2054 7275  is_complex": Tru
-00000140: 652c 0d0a 2020 2020 2020 2020 2020 2020  e,..            
-00000150: 2275 6e69 7473 223a 205b 275b 416d 7073  "units": ['[Amps
-00000160: 5d27 5d0d 0a20 2020 2020 2020 207d 2c0d  ]']..        },.
-00000170: 0a20 2020 2020 2020 2022 4375 7272 656e  .        "Curren
-00000180: 7473 4d61 6741 6e67 223a 207b 0d0a 2020  tsMagAng": {..  
-00000190: 2020 2020 2020 2020 2020 2269 735f 636f            "is_co
-000001a0: 6d70 6c65 7822 3a20 4661 6c73 652c 0d0a  mplex": False,..
-000001b0: 2020 2020 2020 2020 2020 2020 2275 6e69              "uni
-000001c0: 7473 223a 205b 275b 416d 7073 5d27 2c20  ts": ['[Amps]', 
-000001d0: 275b 4465 675d 275d 0d0a 2020 2020 2020  '[Deg]']..      
-000001e0: 2020 7d2c 0d0a 2020 2020 2020 2020 2250    },..        "P
-000001f0: 6f77 6572 7322 3a20 7b0d 0a20 2020 2020  owers": {..     
-00000200: 2020 2020 2020 2022 6973 5f63 6f6d 706c         "is_compl
-00000210: 6578 223a 2054 7275 652c 0d0a 2020 2020  ex": True,..    
-00000220: 2020 2020 2020 2020 2275 6e69 7473 223a          "units":
-00000230: 205b 275b 6b56 415d 275d 0d0a 2020 2020   ['[kVA]']..    
-00000240: 2020 2020 7d2c 0d0a 2020 2020 2020 2020      },..        
-00000250: 2256 6f6c 7461 6765 7322 3a20 7b0d 0a20  "Voltages": {.. 
-00000260: 2020 2020 2020 2020 2020 2022 6973 5f63             "is_c
-00000270: 6f6d 706c 6578 223a 2054 7275 652c 0d0a  omplex": True,..
-00000280: 2020 2020 2020 2020 2020 2020 2275 6e69              "uni
-00000290: 7473 223a 205b 275b 6b56 5d27 5d0d 0a20  ts": ['[kV]'].. 
-000002a0: 2020 2020 2020 207d 2c0d 0a20 2020 2020         },..     
-000002b0: 2020 2027 566f 6c74 6167 6573 4d61 6741     'VoltagesMagA
-000002c0: 6e67 273a 207b 0d0a 2020 2020 2020 2020  ng': {..        
-000002d0: 2020 2020 2269 735f 636f 6d70 6c65 7822      "is_complex"
-000002e0: 3a20 4661 6c73 652c 0d0a 2020 2020 2020  : False,..      
-000002f0: 2020 2020 2020 2275 6e69 7473 223a 205b        "units": [
-00000300: 275b 6b56 5d27 2c20 275b 4465 675d 275d  '[kV]', '[Deg]']
-00000310: 0d0a 2020 2020 2020 2020 7d2c 0d0a 2020  ..        },..  
-00000320: 2020 2020 2020 2743 706c 7853 6571 4375        'CplxSeqCu
-00000330: 7272 656e 7473 273a 207b 0d0a 2020 2020  rrents': {..    
-00000340: 2020 2020 2020 2020 2269 735f 636f 6d70          "is_comp
-00000350: 6c65 7822 3a20 5472 7565 2c0d 0a20 2020  lex": True,..   
-00000360: 2020 2020 2020 2020 2022 756e 6974 7322           "units"
-00000370: 3a20 5b27 5b41 6d70 735d 275d 0d0a 2020  : ['[Amps]']..  
-00000380: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-00000390: 2020 2753 6571 4375 7272 656e 7473 273a    'SeqCurrents':
-000003a0: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-000003b0: 2269 735f 636f 6d70 6c65 7822 3a20 4661  "is_complex": Fa
-000003c0: 6c73 652c 0d0a 2020 2020 2020 2020 2020  lse,..          
-000003d0: 2020 2275 6e69 7473 223a 205b 275b 416d    "units": ['[Am
-000003e0: 7073 5d27 2c20 275b 4465 675d 275d 0d0a  ps]', '[Deg]']..
-000003f0: 2020 2020 2020 2020 7d2c 0d0a 2020 2020          },..    
-00000400: 2020 2020 2753 6571 506f 7765 7273 273a      'SeqPowers':
-00000410: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-00000420: 2269 735f 636f 6d70 6c65 7822 3a20 4661  "is_complex": Fa
-00000430: 6c73 652c 0d0a 2020 2020 2020 2020 2020  lse,..          
-00000440: 2020 2275 6e69 7473 223a 205b 275b 6b56    "units": ['[kV
-00000450: 415d 272c 2027 5b44 6567 5d27 5d0d 0a20  A]', '[Deg]'].. 
-00000460: 2020 2020 2020 207d 0d0a 2020 2020 7d0d         }..    }.
-00000470: 0a0d 0a20 2020 2056 4152 4941 424c 455f  ...    VARIABLE_
-00000480: 4f55 5450 5554 535f 434f 4d50 4c45 5820  OUTPUTS_COMPLEX 
-00000490: 3d20 280d 0a20 2020 2020 2020 2027 4c6f  = (..        'Lo
-000004a0: 7373 6573 272c 0d0a 2020 2020 290d 0a0d  sses',..    )...
-000004b0: 0a20 2020 2056 4152 4941 424c 455f 4f55  .    VARIABLE_OU
-000004c0: 5450 5554 535f 4259 5f4c 4953 5420 3d20  TPUTS_BY_LIST = 
-000004d0: 5b0d 0a20 2020 2020 2020 2027 7461 7073  [..        'taps
-000004e0: 270d 0a20 2020 205d 0d0a 0d0a 2020 2020  '..    ]....    
-000004f0: 6465 6620 5f5f 696e 6974 5f5f 2873 656c  def __init__(sel
-00000500: 662c 2064 7373 496e 7374 616e 6365 293a  f, dssInstance):
-00000510: 0d0a 2020 2020 2020 2020 7375 7065 7228  ..        super(
-00000520: 6473 7354 7261 6e73 666f 726d 6572 2c20  dssTransformer, 
-00000530: 7365 6c66 292e 5f5f 696e 6974 5f5f 2864  self).__init__(d
-00000540: 7373 496e 7374 616e 6365 290d 0a20 2020  ssInstance)..   
-00000550: 2020 2020 2073 656c 662e 5f4e 756d 5769       self._NumWi
-00000560: 6e64 696e 6773 203d 2064 7373 496e 7374  ndings = dssInst
-00000570: 616e 6365 2e54 7261 6e73 666f 726d 6572  ance.Transformer
-00000580: 732e 4e75 6d57 696e 6469 6e67 7328 290d  s.NumWindings().
-00000590: 0a20 2020 2020 2020 2073 656c 662e 5f64  .        self._d
-000005a0: 7373 496e 7374 616e 6365 203d 2064 7373  ssInstance = dss
-000005b0: 496e 7374 616e 6365 0d0a 0d0a 2020 2020  Instance....    
-000005c0: 4070 726f 7065 7274 790d 0a20 2020 2064  @property..    d
-000005d0: 6566 204e 756d 5769 6e64 696e 6773 2873  ef NumWindings(s
-000005e0: 656c 6629 3a0d 0a20 2020 2020 2020 2072  elf):..        r
-000005f0: 6574 7572 6e20 7365 6c66 2e5f 4e75 6d57  eturn self._NumW
-00000600: 696e 6469 6e67 730d 0a0d 0a20 2020 2040  indings....    @
-00000610: 7374 6174 6963 6d65 7468 6f64 0d0a 2020  staticmethod..  
-00000620: 2020 6465 6620 6368 756e 6b5f 6c69 7374    def chunk_list
-00000630: 2876 616c 7565 732c 206e 4c69 7374 7329  (values, nLists)
-00000640: 3a0d 0a20 2020 2020 2020 2072 6574 7572  :..        retur
-00000650: 6e20 5b76 616c 7565 735b 6920 2a20 6e4c  n [values[i * nL
-00000660: 6973 7473 3a28 6920 2b20 3129 202a 206e  ists:(i + 1) * n
-00000670: 4c69 7374 735d 2066 6f72 2069 2069 6e20  Lists] for i in 
-00000680: 7261 6e67 6528 286c 656e 2876 616c 7565  range((len(value
-00000690: 7329 202b 206e 4c69 7374 7320 2d20 3129  s) + nLists - 1)
-000006a0: 202f 2f20 6e4c 6973 7473 295d 0d0a 0d0a   // nLists)]....
-000006b0: 2020 2020 6465 6620 4765 7456 616c 7565      def GetValue
-000006c0: 2873 656c 662c 2056 6172 4e61 6d65 2c20  (self, VarName, 
-000006d0: 636f 6e76 6572 743d 4661 6c73 6529 3a0d  convert=False):.
-000006e0: 0a20 2020 2020 2020 2069 6620 5661 724e  .        if VarN
-000006f0: 616d 6520 696e 2073 656c 662e 5f56 6172  ame in self._Var
-00000700: 6961 626c 6573 3a0d 0a20 2020 2020 2020  iables:..       
-00000710: 2020 2020 2056 6172 5661 6c75 6520 3d20       VarValue = 
-00000720: 7365 6c66 2e47 6574 5661 7269 6162 6c65  self.GetVariable
-00000730: 2856 6172 4e61 6d65 2c20 636f 6e76 6572  (VarName, conver
-00000740: 743d 636f 6e76 6572 7429 0d0a 2020 2020  t=convert)..    
-00000750: 2020 2020 656c 6966 2056 6172 4e61 6d65      elif VarName
-00000760: 2069 6e20 7365 6c66 2e5f 5061 7261 6d65   in self._Parame
-00000770: 7465 7273 3a0d 0a20 2020 2020 2020 2020  ters:..         
-00000780: 2020 2056 6172 5661 6c75 6520 3d20 7365     VarValue = se
-00000790: 6c66 2e47 6574 5061 7261 6d65 7465 7228  lf.GetParameter(
-000007a0: 5661 724e 616d 6529 0d0a 2020 2020 2020  VarName)..      
-000007b0: 2020 2020 2020 6966 2063 6f6e 7665 7274        if convert
-000007c0: 3a0d 0a20 2020 2020 2020 2020 2020 2020  :..             
-000007d0: 2020 2069 6620 5661 724e 616d 6520 696e     if VarName in
-000007e0: 2073 656c 662e 5641 5249 4142 4c45 5f4f   self.VARIABLE_O
-000007f0: 5554 5055 5453 5f42 595f 4c49 5354 3a0d  UTPUTS_BY_LIST:.
-00000800: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000810: 2020 2020 2056 6172 5661 6c75 6520 3d20       VarValue = 
-00000820: 5661 7256 616c 7565 5b3a 7365 6c66 2e4e  VarValue[:self.N
-00000830: 756d 5769 6e64 696e 6773 5d0d 0a20 2020  umWindings]..   
-00000840: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000850: 2056 6172 5661 6c75 6520 3d20 5661 6c75   VarValue = Valu
-00000860: 6542 794c 6973 7428 0d0a 2020 2020 2020  eByList(..      
-00000870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000880: 2020 7365 6c66 2e5f 4675 6c6c 4e61 6d65    self._FullName
-00000890: 2c20 5661 724e 616d 652c 2056 6172 5661  , VarName, VarVa
-000008a0: 6c75 652c 205b 2777 6467 7b7d 272e 666f  lue, ['wdg{}'.fo
-000008b0: 726d 6174 2869 2b31 2920 666f 7220 6920  rmat(i+1) for i 
-000008c0: 696e 2072 616e 6765 2873 656c 662e 4e75  in range(self.Nu
-000008d0: 6d57 696e 6469 6e67 7329 5d0d 0a20 2020  mWindings)]..   
-000008e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000008f0: 2029 0d0a 2020 2020 2020 2020 2020 2020   )..            
-00000900: 2020 2020 656c 7365 3a0d 0a20 2020 2020      else:..     
-00000910: 2020 2020 2020 2020 2020 2020 2020 2056                 V
-00000920: 6172 5661 6c75 6520 3d20 5661 6c75 6542  arValue = ValueB
-00000930: 794e 756d 6265 7228 7365 6c66 2e5f 4675  yNumber(self._Fu
-00000940: 6c6c 4e61 6d65 2c20 5661 724e 616d 652c  llName, VarName,
-00000950: 2056 6172 5661 6c75 6529 0d0a 0d0a 2020   VarValue)....  
-00000960: 2020 2020 2020 656c 7365 3a0d 0a20 2020        else:..   
-00000970: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
-00000980: 4e6f 6e65 0d0a 2020 2020 2020 2020 7265  None..        re
-00000990: 7475 726e 2056 6172 5661 6c75 650d 0a    turn VarValue..
+00000000: 0a66 726f 6d20 7079 6473 732e 6473 7345  .from pydss.dssE
+00000010: 6c65 6d65 6e74 2069 6d70 6f72 7420 6473  lement import ds
+00000020: 7345 6c65 6d65 6e74 0a66 726f 6d20 7079  sElement.from py
+00000030: 6473 732e 7661 6c75 655f 7374 6f72 6167  dss.value_storag
+00000040: 6520 696d 706f 7274 2056 616c 7565 4279  e import ValueBy
+00000050: 4e75 6d62 6572 0a66 726f 6d20 7079 6473  Number.from pyds
+00000060: 732e 7661 6c75 655f 7374 6f72 6167 6520  s.value_storage 
+00000070: 696d 706f 7274 2056 616c 7565 4279 4c69  import ValueByLi
+00000080: 7374 0a69 6d70 6f72 7420 6173 740a 6672  st.import ast.fr
+00000090: 6f6d 2070 7964 7373 2e65 7863 6570 7469  om pydss.excepti
+000000a0: 6f6e 7320 696d 706f 7274 2049 6e76 616c  ons import Inval
+000000b0: 6964 5061 7261 6d65 7465 720a 0a63 6c61  idParameter..cla
+000000c0: 7373 2064 7373 5472 616e 7366 6f72 6d65  ss dssTransforme
+000000d0: 7228 6473 7345 6c65 6d65 6e74 293a 0a0a  r(dssElement):..
+000000e0: 2020 2020 5641 5249 4142 4c45 5f4f 5554      VARIABLE_OUT
+000000f0: 5055 5453 5f42 595f 4c41 4245 4c20 3d20  PUTS_BY_LABEL = 
+00000100: 7b0a 2020 2020 2020 2020 2243 7572 7265  {.        "Curre
+00000110: 6e74 7322 3a20 7b0a 2020 2020 2020 2020  nts": {.        
+00000120: 2020 2020 2269 735f 636f 6d70 6c65 7822      "is_complex"
+00000130: 3a20 5472 7565 2c0a 2020 2020 2020 2020  : True,.        
+00000140: 2020 2020 2275 6e69 7473 223a 205b 275b      "units": ['[
+00000150: 416d 7073 5d27 5d0a 2020 2020 2020 2020  Amps]'].        
+00000160: 7d2c 0a20 2020 2020 2020 2022 4375 7272  },.        "Curr
+00000170: 656e 7473 4d61 6741 6e67 223a 207b 0a20  entsMagAng": {. 
+00000180: 2020 2020 2020 2020 2020 2022 6973 5f63             "is_c
+00000190: 6f6d 706c 6578 223a 2046 616c 7365 2c0a  omplex": False,.
+000001a0: 2020 2020 2020 2020 2020 2020 2275 6e69              "uni
+000001b0: 7473 223a 205b 275b 416d 7073 5d27 2c20  ts": ['[Amps]', 
+000001c0: 275b 4465 675d 275d 0a20 2020 2020 2020  '[Deg]'].       
+000001d0: 207d 2c0a 2020 2020 2020 2020 2250 6f77   },.        "Pow
+000001e0: 6572 7322 3a20 7b0a 2020 2020 2020 2020  ers": {.        
+000001f0: 2020 2020 2269 735f 636f 6d70 6c65 7822      "is_complex"
+00000200: 3a20 5472 7565 2c0a 2020 2020 2020 2020  : True,.        
+00000210: 2020 2020 2275 6e69 7473 223a 205b 275b      "units": ['[
+00000220: 6b56 415d 275d 0a20 2020 2020 2020 207d  kVA]'].        }
+00000230: 2c0a 2020 2020 2020 2020 2256 6f6c 7461  ,.        "Volta
+00000240: 6765 7322 3a20 7b0a 2020 2020 2020 2020  ges": {.        
+00000250: 2020 2020 2269 735f 636f 6d70 6c65 7822      "is_complex"
+00000260: 3a20 5472 7565 2c0a 2020 2020 2020 2020  : True,.        
+00000270: 2020 2020 2275 6e69 7473 223a 205b 275b      "units": ['[
+00000280: 6b56 5d27 5d0a 2020 2020 2020 2020 7d2c  kV]'].        },
+00000290: 0a20 2020 2020 2020 2027 566f 6c74 6167  .        'Voltag
+000002a0: 6573 4d61 6741 6e67 273a 207b 0a20 2020  esMagAng': {.   
+000002b0: 2020 2020 2020 2020 2022 6973 5f63 6f6d           "is_com
+000002c0: 706c 6578 223a 2046 616c 7365 2c0a 2020  plex": False,.  
+000002d0: 2020 2020 2020 2020 2020 2275 6e69 7473            "units
+000002e0: 223a 205b 275b 6b56 5d27 2c20 275b 4465  ": ['[kV]', '[De
+000002f0: 675d 275d 0a20 2020 2020 2020 207d 2c0a  g]'].        },.
+00000300: 2020 2020 2020 2020 2743 706c 7853 6571          'CplxSeq
+00000310: 4375 7272 656e 7473 273a 207b 0a20 2020  Currents': {.   
+00000320: 2020 2020 2020 2020 2022 6973 5f63 6f6d           "is_com
+00000330: 706c 6578 223a 2054 7275 652c 0a20 2020  plex": True,.   
+00000340: 2020 2020 2020 2020 2022 756e 6974 7322           "units"
+00000350: 3a20 5b27 5b41 6d70 735d 275d 0a20 2020  : ['[Amps]'].   
+00000360: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+00000370: 2753 6571 4375 7272 656e 7473 273a 207b  'SeqCurrents': {
+00000380: 0a20 2020 2020 2020 2020 2020 2022 6973  .            "is
+00000390: 5f63 6f6d 706c 6578 223a 2046 616c 7365  _complex": False
+000003a0: 2c0a 2020 2020 2020 2020 2020 2020 2275  ,.            "u
+000003b0: 6e69 7473 223a 205b 275b 416d 7073 5d27  nits": ['[Amps]'
+000003c0: 2c20 275b 4465 675d 275d 0a20 2020 2020  , '[Deg]'].     
+000003d0: 2020 207d 2c0a 2020 2020 2020 2020 2753     },.        'S
+000003e0: 6571 506f 7765 7273 273a 207b 0a20 2020  eqPowers': {.   
+000003f0: 2020 2020 2020 2020 2022 6973 5f63 6f6d           "is_com
+00000400: 706c 6578 223a 2046 616c 7365 2c0a 2020  plex": False,.  
+00000410: 2020 2020 2020 2020 2020 2275 6e69 7473            "units
+00000420: 223a 205b 275b 6b56 415d 272c 2027 5b44  ": ['[kVA]', '[D
+00000430: 6567 5d27 5d0a 2020 2020 2020 2020 7d0a  eg]'].        }.
+00000440: 2020 2020 7d0a 0a20 2020 2056 4152 4941      }..    VARIA
+00000450: 424c 455f 4f55 5450 5554 535f 434f 4d50  BLE_OUTPUTS_COMP
+00000460: 4c45 5820 3d20 280a 2020 2020 2020 2020  LEX = (.        
+00000470: 274c 6f73 7365 7327 2c0a 2020 2020 290a  'Losses',.    ).
+00000480: 0a20 2020 2056 4152 4941 424c 455f 4f55  .    VARIABLE_OU
+00000490: 5450 5554 535f 4259 5f4c 4953 5420 3d20  TPUTS_BY_LIST = 
+000004a0: 5b0a 2020 2020 2020 2020 2774 6170 7327  [.        'taps'
+000004b0: 0a20 2020 205d 0a0a 2020 2020 6465 6620  .    ]..    def 
+000004c0: 5f5f 696e 6974 5f5f 2873 656c 662c 2064  __init__(self, d
+000004d0: 7373 496e 7374 616e 6365 293a 0a20 2020  ssInstance):.   
+000004e0: 2020 2020 2073 7570 6572 2864 7373 5472       super(dssTr
+000004f0: 616e 7366 6f72 6d65 722c 2073 656c 6629  ansformer, self)
+00000500: 2e5f 5f69 6e69 745f 5f28 6473 7349 6e73  .__init__(dssIns
+00000510: 7461 6e63 6529 0a20 2020 2020 2020 2073  tance).        s
+00000520: 656c 662e 5f4e 756d 5769 6e64 696e 6773  elf._NumWindings
+00000530: 203d 2064 7373 496e 7374 616e 6365 2e54   = dssInstance.T
+00000540: 7261 6e73 666f 726d 6572 732e 4e75 6d57  ransformers.NumW
+00000550: 696e 6469 6e67 7328 290a 2020 2020 2020  indings().      
+00000560: 2020 7365 6c66 2e5f 6473 7349 6e73 7461    self._dssInsta
+00000570: 6e63 6520 3d20 6473 7349 6e73 7461 6e63  nce = dssInstanc
+00000580: 650a 0a20 2020 2040 7072 6f70 6572 7479  e..    @property
+00000590: 0a20 2020 2064 6566 204e 756d 5769 6e64  .    def NumWind
+000005a0: 696e 6773 2873 656c 6629 3a0a 2020 2020  ings(self):.    
+000005b0: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
+000005c0: 5f4e 756d 5769 6e64 696e 6773 0a0a 2020  _NumWindings..  
+000005d0: 2020 4073 7461 7469 636d 6574 686f 640a    @staticmethod.
+000005e0: 2020 2020 6465 6620 6368 756e 6b5f 6c69      def chunk_li
+000005f0: 7374 2876 616c 7565 732c 206e 4c69 7374  st(values, nList
+00000600: 7329 3a0a 2020 2020 2020 2020 7265 7475  s):.        retu
+00000610: 726e 205b 7661 6c75 6573 5b69 202a 206e  rn [values[i * n
+00000620: 4c69 7374 733a 2869 202b 2031 2920 2a20  Lists:(i + 1) * 
+00000630: 6e4c 6973 7473 5d20 666f 7220 6920 696e  nLists] for i in
+00000640: 2072 616e 6765 2828 6c65 6e28 7661 6c75   range((len(valu
+00000650: 6573 2920 2b20 6e4c 6973 7473 202d 2031  es) + nLists - 1
+00000660: 2920 2f2f 206e 4c69 7374 7329 5d0a 0a20  ) // nLists)].. 
+00000670: 2020 2064 6566 2047 6574 5661 6c75 6528     def GetValue(
+00000680: 7365 6c66 2c20 5661 724e 616d 652c 2063  self, VarName, c
+00000690: 6f6e 7665 7274 3d46 616c 7365 293a 0a20  onvert=False):. 
+000006a0: 2020 2020 2020 2069 6620 5661 724e 616d         if VarNam
+000006b0: 6520 696e 2073 656c 662e 5f56 6172 6961  e in self._Varia
+000006c0: 626c 6573 3a0a 2020 2020 2020 2020 2020  bles:.          
+000006d0: 2020 5661 7256 616c 7565 203d 2073 656c    VarValue = sel
+000006e0: 662e 4765 7456 6172 6961 626c 6528 5661  f.GetVariable(Va
+000006f0: 724e 616d 652c 2063 6f6e 7665 7274 3d63  rName, convert=c
+00000700: 6f6e 7665 7274 290a 2020 2020 2020 2020  onvert).        
+00000710: 656c 6966 2056 6172 4e61 6d65 2069 6e20  elif VarName in 
+00000720: 7365 6c66 2e5f 5061 7261 6d65 7465 7273  self._Parameters
+00000730: 3a0a 2020 2020 2020 2020 2020 2020 5661  :.            Va
+00000740: 7256 616c 7565 203d 2073 656c 662e 4765  rValue = self.Ge
+00000750: 7450 6172 616d 6574 6572 2856 6172 4e61  tParameter(VarNa
+00000760: 6d65 290a 2020 2020 2020 2020 2020 2020  me).            
+00000770: 6966 2063 6f6e 7665 7274 3a0a 2020 2020  if convert:.    
+00000780: 2020 2020 2020 2020 2020 2020 6966 2056              if V
+00000790: 6172 4e61 6d65 2069 6e20 7365 6c66 2e56  arName in self.V
+000007a0: 4152 4941 424c 455f 4f55 5450 5554 535f  ARIABLE_OUTPUTS_
+000007b0: 4259 5f4c 4953 543a 0a20 2020 2020 2020  BY_LIST:.       
+000007c0: 2020 2020 2020 2020 2020 2020 2056 6172               Var
+000007d0: 5661 6c75 6520 3d20 5661 7256 616c 7565  Value = VarValue
+000007e0: 5b3a 7365 6c66 2e4e 756d 5769 6e64 696e  [:self.NumWindin
+000007f0: 6773 5d0a 2020 2020 2020 2020 2020 2020  gs].            
+00000800: 2020 2020 2020 2020 5661 7256 616c 7565          VarValue
+00000810: 203d 2056 616c 7565 4279 4c69 7374 280a   = ValueByList(.
+00000820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000830: 2020 2020 2020 2020 7365 6c66 2e5f 4675          self._Fu
+00000840: 6c6c 4e61 6d65 2c20 5661 724e 616d 652c  llName, VarName,
+00000850: 2056 6172 5661 6c75 652c 205b 2777 6467   VarValue, ['wdg
+00000860: 7b7d 272e 666f 726d 6174 2869 2b31 2920  {}'.format(i+1) 
+00000870: 666f 7220 6920 696e 2072 616e 6765 2873  for i in range(s
+00000880: 656c 662e 4e75 6d57 696e 6469 6e67 7329  elf.NumWindings)
+00000890: 5d0a 2020 2020 2020 2020 2020 2020 2020  ].              
+000008a0: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
+000008b0: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+000008c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000008d0: 2020 5661 7256 616c 7565 203d 2056 616c    VarValue = Val
+000008e0: 7565 4279 4e75 6d62 6572 2873 656c 662e  ueByNumber(self.
+000008f0: 5f46 756c 6c4e 616d 652c 2056 6172 4e61  _FullName, VarNa
+00000900: 6d65 2c20 5661 7256 616c 7565 290a 0a20  me, VarValue).. 
+00000910: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
+00000920: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
+00000930: 4e6f 6e65 0a20 2020 2020 2020 2072 6574  None.        ret
+00000940: 7572 6e20 5661 7256 616c 7565 0a         urn VarValue.
```

### Comparing `nrel_pydss-3.1.3/src/pydss/element_fields.py` & `nrel_pydss-3.1.4/src/pydss/element_fields.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,159 +1,159 @@
-ELEMENT_FIELDS = {
-    "Lines": [
-        {
-            "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
-            "options": [
-                "phase_terminal",
-            ],
-        },
-        {
-            "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
-            "options": [
-                "phase_terminal", "mag_ang"
-            ],
-        },
-        {
-            "names": ["Losses", "NormalAmps"],
-            "options": [
-            ],
-        },
-    ],
-    "Transformers": [
-        {
-            "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
-            "options": [
-                "phase_terminal",
-            ],
-        },
-        {
-            "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
-            "options": [
-                "phase_terminal", "mag_ang"
-            ],
-        },
-        {
-            "names": ["taps", "maxtap", 'mintap', 'numtaps', 'kv', 'kva'],
-            "options": [
-                "wdg",
-            ],
-        },
-        {
-            "names": ["Losses", "NormalAmps", "tap"],
-            "options": [
-            ],
-        },
-    ],
-    "Generators": [
-            {
-                "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
-                "options": [
-                    "phase_terminal",
-                ],
-            },
-            {
-                "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
-                "options": [
-                    "phase_terminal", "mag_ang"
-                ],
-            },
-        ],
-    "PVSystems": [
-                {
-                    "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
-                    "options": [
-                        "phase_terminal",
-                    ],
-                },
-                {
-                    "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
-                    "options": [
-                        "phase_terminal", "mag_ang"
-                    ],
-                },
-                {
-                    "names": ["Pmpp"],
-                    "options": [
-                    ],
-                },
-            ],
-    "Loads": [
-                {
-                    "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
-                    "options": [
-                        "phase_terminal",
-                    ],
-                },
-                {
-                    "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
-                    "options": [
-                        "phase_terminal", "mag_ang"
-                    ],
-                },
-            ],
-    "Storages": [
-        {
-            "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
-            "options": [
-                "phase_terminal",
-            ],
-        },
-        {
-            "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
-            "options": [
-                "phase_terminal", "mag_ang"
-            ],
-        },
-    ],
-    "Capacitors": [
-            {
-                "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
-                "options": [
-                    "phase_terminal",
-                ],
-            },
-            {
-                "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
-                "options": [
-                    "phase_terminal", "mag_ang"
-                ],
-            },
-{
-                "names": ["states", 'open', 'close'],
-                "options": [
-                ],
-            },
-        ],
-    "Meters": [
-                {
-                    "names": ["AllocFactors"],
-                    "options": [
-                        "phase_terminal",
-                    ],
-                }
-            ],
-    "Buses": [
-        {
-            "names": ["CplxSeqVoltages", "Voc", 'Voltages', 'puVLL', 'PuVoltage'],
-            "options": [
-                "phase_terminal",
-            ],
-        },
-        {
-            "names": ["SeqVoltages", "VMagAngle", 'SeqCurrents', 'puVmagAngle'],
-            "options": [
-                "phase_terminal", "mag_ang"
-            ],
-        },
-        {
-            "names": ["Distance"],
-            "options": [
-            ],
-        },
-    ],
-    "Circuits": [
-                {
-                    "names": ["TotalPower", "LineLosses", "Losses", "SubstationLosses"],
-                    "options": [],
-                }
-            ],
-}
+ELEMENT_FIELDS = {
+    "Lines": [
+        {
+            "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
+            "options": [
+                "phase_terminal",
+            ],
+        },
+        {
+            "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
+            "options": [
+                "phase_terminal", "mag_ang"
+            ],
+        },
+        {
+            "names": ["Losses", "NormalAmps"],
+            "options": [
+            ],
+        },
+    ],
+    "Transformers": [
+        {
+            "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
+            "options": [
+                "phase_terminal",
+            ],
+        },
+        {
+            "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
+            "options": [
+                "phase_terminal", "mag_ang"
+            ],
+        },
+        {
+            "names": ["taps", "maxtap", 'mintap', 'numtaps', 'kv', 'kva'],
+            "options": [
+                "wdg",
+            ],
+        },
+        {
+            "names": ["Losses", "NormalAmps", "tap"],
+            "options": [
+            ],
+        },
+    ],
+    "Generators": [
+            {
+                "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
+                "options": [
+                    "phase_terminal",
+                ],
+            },
+            {
+                "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
+                "options": [
+                    "phase_terminal", "mag_ang"
+                ],
+            },
+        ],
+    "PVSystems": [
+                {
+                    "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
+                    "options": [
+                        "phase_terminal",
+                    ],
+                },
+                {
+                    "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
+                    "options": [
+                        "phase_terminal", "mag_ang"
+                    ],
+                },
+                {
+                    "names": ["Pmpp"],
+                    "options": [
+                    ],
+                },
+            ],
+    "Loads": [
+                {
+                    "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
+                    "options": [
+                        "phase_terminal",
+                    ],
+                },
+                {
+                    "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
+                    "options": [
+                        "phase_terminal", "mag_ang"
+                    ],
+                },
+            ],
+    "Storages": [
+        {
+            "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
+            "options": [
+                "phase_terminal",
+            ],
+        },
+        {
+            "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
+            "options": [
+                "phase_terminal", "mag_ang"
+            ],
+        },
+    ],
+    "Capacitors": [
+            {
+                "names": ["Currents", "Powers", 'Voltages', 'PhaseLosses'],
+                "options": [
+                    "phase_terminal",
+                ],
+            },
+            {
+                "names": ["SeqVoltages", "SeqPowers", 'SeqCurrents', 'VoltagesMagAng', 'CurrentsMagAng'],
+                "options": [
+                    "phase_terminal", "mag_ang"
+                ],
+            },
+{
+                "names": ["states", 'open', 'close'],
+                "options": [
+                ],
+            },
+        ],
+    "Meters": [
+                {
+                    "names": ["AllocFactors"],
+                    "options": [
+                        "phase_terminal",
+                    ],
+                }
+            ],
+    "Buses": [
+        {
+            "names": ["CplxSeqVoltages", "Voc", 'Voltages', 'puVLL', 'PuVoltage'],
+            "options": [
+                "phase_terminal",
+            ],
+        },
+        {
+            "names": ["SeqVoltages", "VMagAngle", 'SeqCurrents', 'puVmagAngle'],
+            "options": [
+                "phase_terminal", "mag_ang"
+            ],
+        },
+        {
+            "names": ["Distance"],
+            "options": [
+            ],
+        },
+    ],
+    "Circuits": [
+                {
+                    "names": ["TotalPower", "LineLosses", "Losses", "SubstationLosses"],
+                    "options": [],
+                }
+            ],
+}
```

### Comparing `nrel_pydss-3.1.3/src/pydss/element_options.py` & `nrel_pydss-3.1.4/src/pydss/element_options.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-"""Stores the options available for element classes and properties."""
-
-from loguru import logger
-
-from pydss.element_fields import ELEMENT_FIELDS
-
-class ElementOptions:
-    """Stores the options available for element classes and properties."""
-    def __init__(self, data=None):
-        if data is None:
-            data = ELEMENT_FIELDS
-        self._element_classes = {}
-        for elem_class, option_combos in data.items():
-            options = {}
-            for option_combo in option_combos:
-                for prop in option_combo["names"]:
-                    assert prop not in options
-                    options[prop] = option_combo["options"]
-            self._element_classes[elem_class] = options
-
-    def is_option_valid(self, element_class, prop, option):
-        """Returns True if the option is valid for the class and property.
-
-        Returns
-        -------
-        True
-
-        """
-        return option in self.list_options(element_class, prop)
-
-    def list_options(self, element_class, prop):
-        """List the options available for a class and property.
-
-        Returns
-        -------
-        list
-
-        """
-        if element_class not in self._element_classes:
-            logger.debug("class=%s is not stored", element_class)
-            return []
-        if prop not in self._element_classes[element_class]:
-            logger.debug("class=%s property=%s is not stored", element_class, prop)
-            return []
-        return self._element_classes[element_class][prop][:]
+"""Stores the options available for element classes and properties."""
+
+from loguru import logger
+
+from pydss.element_fields import ELEMENT_FIELDS
+
+class ElementOptions:
+    """Stores the options available for element classes and properties."""
+    def __init__(self, data=None):
+        if data is None:
+            data = ELEMENT_FIELDS
+        self._element_classes = {}
+        for elem_class, option_combos in data.items():
+            options = {}
+            for option_combo in option_combos:
+                for prop in option_combo["names"]:
+                    assert prop not in options
+                    options[prop] = option_combo["options"]
+            self._element_classes[elem_class] = options
+
+    def is_option_valid(self, element_class, prop, option):
+        """Returns True if the option is valid for the class and property.
+
+        Returns
+        -------
+        True
+
+        """
+        return option in self.list_options(element_class, prop)
+
+    def list_options(self, element_class, prop):
+        """List the options available for a class and property.
+
+        Returns
+        -------
+        list
+
+        """
+        if element_class not in self._element_classes:
+            logger.debug("class=%s is not stored", element_class)
+            return []
+        if prop not in self._element_classes[element_class]:
+            logger.debug("class=%s property=%s is not stored", element_class, prop)
+            return []
+        return self._element_classes[element_class][prop][:]
```

### Comparing `nrel_pydss-3.1.3/src/pydss/export_list_reader.py` & `nrel_pydss-3.1.4/src/pydss/export_list_reader.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,477 +1,477 @@
-
-
-from collections import defaultdict
-from pathlib import Path
-import os
-import re
-
-from loguru import logger
-
-
-from pydss.common import DataConversion, LimitsFilter, StoreValuesType, \
-    DatasetPropertyType, MinMax
-from pydss.pyContrReader import pyExportReader
-from pydss.utils.utils import load_data
-from pydss.exceptions import InvalidConfiguration, InvalidParameter
-from pydss.metrics import (
-    NodeVoltageMetric, TrackCapacitorChangeCounts,
-    TrackRegControlTapNumberChanges, ExportLoadingsMetric, OverloadsMetricInMemory,
-    ExportPowersMetric, FeederHeadMetrics
-)
-
-
-CUSTOM_METRICS = {
-    "Capacitors.TrackStateChanges": TrackCapacitorChangeCounts,
-    "CktElement.ExportLoadingsMetric": ExportLoadingsMetric,
-    "CktElement.OverloadsMetricInMemory": OverloadsMetricInMemory,
-    "CktElement.ExportPowersMetric": ExportPowersMetric,
-    "FeederHead.FeederHeadMetrics": FeederHeadMetrics,
-    #"Lines.LoadingPercent": LineLoadingPercent,
-    "Nodes.VoltageMetric": NodeVoltageMetric,
-    "RegControls.TrackTapNumberChanges": TrackRegControlTapNumberChanges,
-    #"Transformers.LoadingPercent": TransformerLoadingPercent,
-}
-
-
-class ExportListProperty:
-    """Contains export options for an element property."""
-    def __init__(self, elem_class, data):
-        self.elem_class = elem_class
-        self._opendss_classes = data.get("opendss_classes", [])
-        self.name = data["property"]
-        self.publish = data.get("publish", False)
-        self._data_conversion = DataConversion(data.get("data_conversion", "none"))
-        self._sum_elements = data.get("sum_elements", False)
-        self._sum_groups = data.get("sum_groups", [])
-        sum_groups_file = data.get("sum_groups_file")
-        self._limits = self._parse_limits(data, "limits")
-        self._limits_filter = LimitsFilter(data.get("limits_filter", "outside"))
-        self._limits_b = self._parse_limits(data, "limits_b")
-        self._limits_filter_b = LimitsFilter(data.get("limits_filter_b", "outside"))
-        self._store_values_type = StoreValuesType(data.get("store_values_type", "all"))
-        self._names, self._are_names_regex, self._are_names_filtered = self._parse_names(data)
-        self._sample_interval = data.get("sample_interval", 1)
-        self._window_size = data.get("window_size", 100)
-        self._window_sizes = data.get("window_sizes", {})
-        custom_prop = f"{elem_class}.{self.name}"
-        self._custom_metric = CUSTOM_METRICS.get(custom_prop)
-
-        if self._sum_groups or sum_groups_file:
-            self._check_sum_groups(sum_groups_file)
-
-        # Note to devs: any field added here needs to be handled in serialize()
-
-        if self._sum_elements and self._store_values_type not in \
-                (StoreValuesType.ALL, StoreValuesType.SUM):
-            raise InvalidConfiguration(
-                "sum_elements requires store_values_types = all or sum"
-            )
-
-        if self._is_max() and self._limits is not None:
-            raise InvalidConfiguration("limits are not allowed with max types")
-
-        requires_opendss_classes = (
-            "ExportLoadingsMetric",
-            "OverloadsMetricInMemory",
-            "ExportPowersMetric",
-        )
-        if elem_class == "CktElement" and self.name in requires_opendss_classes and not self._opendss_classes:
-            raise InvalidConfiguration(
-                f"Exporting {elem_class}.{self.name} requires that opendss_classes be specifed"
-            )
-
-    def _check_sum_groups(self, sum_groups_file):
-        if sum_groups_file is not None:
-            if self._sum_groups:
-                raise InvalidConfiguration(f"Cannot set both sum_groups and sum_groups_file")
-
-            # This path needs to be relative to the current directory, not the Exports.toml.
-            # This might need to be changed.
-            if not Path(sum_groups_file).exists():
-                raise InvalidConfiguration(
-                    f"{sum_groups_file} does not exist. The path must be relative to the current directory."
-                )
-            self._sum_groups = load_data(sum_groups_file)["sum_groups"]
-
-        self._sum_elements = True  # Ignore the user setting. This must be true.
-        # Ensure that there are no duplicate names.
-        orig_length = 0
-        all_names = set()
-        for group in self._sum_groups:
-            orig_length += len(group["elements"])
-            group["elements"] = set(group["elements"])
-            all_names = all_names.union(group["elements"])
-        if orig_length != len(all_names):
-            tag = f"{self.elem_class}/{self.name}"
-            raise InvalidConfiguration(f"{tag} has duplicate element names in sum_groups")
-
-    def _is_max(self):
-        return self._store_values_type in (
-            StoreValuesType.MAX, StoreValuesType.MOVING_AVERAGE_MAX,
-        )
-
-    @property
-    def are_names_filtered(self):
-        return self._are_names_filtered
-
-    def is_moving_average(self):
-        return self._store_values_type in (
-            StoreValuesType.MOVING_AVERAGE, StoreValuesType.MOVING_AVERAGE_MAX,
-        )
-
-    @staticmethod
-    def _parse_limits(data, field_name):
-        limits = data.get(field_name)
-        if limits is None:
-            return None
-
-        if not isinstance(limits, list) or len(limits) != 2:
-            raise InvalidConfiguration(f"invalid limits format: {limits}")
-
-        return MinMax(limits[0], limits[1])
-
-    @staticmethod
-    def _parse_names(data):
-        names = data.get("names")
-        name_regexes = data.get("name_regexes")
-
-        if names and name_regexes:
-            raise InvalidConfiguration("names and name_regexes cannot both be set")
-        for obj in (names, name_regexes):
-            if obj is None:
-                continue
-            if not isinstance(obj, list) or not isinstance(obj[0], str):
-                raise InvalidConfiguration(f"invalid name format: {obj}")
-
-        if names:
-            return set(names), False, True
-        if name_regexes:
-            return [re.compile(r"{}".format(x)) for x in name_regexes], True, True
-
-        return None, False, False
-
-    def _is_inside_limits(self, value):
-        """Return True if the value is between min and max, inclusive."""
-        return value >= self._limits.min and value <= self._limits.max
-
-    def _is_outside_limits(self, value):
-        """Return True if the value is lower than min or greater than max."""
-        return value < self._limits.min or value > self._limits.max
-
-    def append_opendss_classes(self, opendss_classes):
-        """Append all opendss_classes from a list."""
-        self._opendss_classes[:] = list(set(self._opendss_classes + opendss_classes))
-
-    @property
-    def custom_metric(self):
-        """Return the custom_metric attribute.
-
-        Returns
-        -------
-        Metric | None
-
-        """
-        return self._custom_metric
-
-    @property
-    def data_conversion(self):
-        """Return the data_conversion attribute
-
-        Returns
-        -------
-        DataConversion
-
-        """
-        return self._data_conversion
-
-    def get_dataset_property_type(self):
-        """Return the encoding to use for the dataset backing these values.
-
-        Returns
-        -------
-        DatasetPropertyType
-
-        """
-        if self._limits is not None:
-            return DatasetPropertyType.FILTERED
-        if self._store_values_type in \
-                (StoreValuesType.SUM, StoreValuesType.MAX,
-                 StoreValuesType.MIN,
-                 StoreValuesType.MOVING_AVERAGE_MAX,
-                 StoreValuesType.CHANGE_COUNT):
-            return DatasetPropertyType.VALUE
-        return DatasetPropertyType.PER_TIME_POINT
-
-    def get_max_size(self, num_steps):
-        """Return the max number of items that could be stored."""
-        singles = (
-            StoreValuesType.CHANGE_COUNT, StoreValuesType.MAX,
-            StoreValuesType.MIN, StoreValuesType.MOVING_AVERAGE_MAX,
-            StoreValuesType.SUM,
-        )
-        if self._store_values_type in singles:
-            return 1
-        num_samples = num_steps / self._sample_interval
-        return int(num_samples)
-
-    @property
-    def limits(self):
-        """Return the limits for the export property.
-
-        Returns
-        -------
-        MinMax
-
-        """
-        return self._limits
-
-    @property
-    def limits_b(self):
-        """Return the limits_b for the export property.
-
-        Returns
-        -------
-        MinMax
-
-        """
-        return self._limits_b
-
-    @property
-    def opendss_classes(self):
-        """Return the element classes to be used with the property.
-
-        Returns
-        -------
-        list
-
-        """
-        return self._opendss_classes[:]
-
-    def serialize(self):
-        """Serialize object to a dictionary."""
-        if self._are_names_regex:
-            #raise InvalidConfiguration("cannot serialize when names are regex")
-            logger.warning("cannot serialize when names are regex")
-            names = None
-        else:
-            names = self._names
-        data = {
-            "property": self.name,
-            "data_conversion": self._data_conversion.value,
-            "opendss_classes": self._opendss_classes,
-            "sample_interval": self._sample_interval,
-            "names": names,
-            "publish": self.publish,
-            "store_values_type": self.store_values_type.value,
-            "sum_elements": self.sum_elements,
-            "sum_groups": self.sum_groups,
-        }
-        if self._limits is not None:
-            data["limits"] = [self._limits.min, self._limits.max]
-            data["limits_filter"] = self._limits_filter.value
-        if self._limits_b is not None:
-            data["limits_b"] = [self._limits_b.min, self._limits_b.max]
-            data["limits_filter_b"] = self._limits_filter_b.value
-        if self.is_moving_average():
-            if self.window_sizes:
-                data["window_sizes"] = self._window_sizes
-                if not self._opendss_classes:
-                    raise InvalidConfiguration(
-                        f"window_sizes requires opendss_classes: {self.name}"
-                    )
-            else:
-                data["window_size"] = self._window_size
-
-        return data
-
-    def should_store_name(self, name):
-        """Return True if name matches the input criteria."""
-        if self._names is None:
-            return True
-
-        if self._are_names_regex:
-            for regex in self._names:
-                if regex.search(name):
-                    return True
-            return False
-
-        return name in self._names
-
-    def should_sample_value(self, step_number):
-        """Return True if it's time to read a new value."""
-        return step_number % self._sample_interval == 0
-
-    def should_store_value(self, value):
-        """Return True if the value meets the input criteria."""
-        if self._limits is None:
-            return True
-
-        if self._limits_filter == LimitsFilter.OUTSIDE:
-            return self._is_outside_limits(value)
-        return self._is_inside_limits(value)
-
-    def should_store_time_step(self):
-        """Return True if the time step should be stored with the value."""
-        return self.limits is not None
-
-    @property
-    def storage_name(self):
-        if self._store_values_type in (StoreValuesType.ALL, StoreValuesType.CHANGE_COUNT):
-            return self.name
-        if self._store_values_type == StoreValuesType.MOVING_AVERAGE:
-            return self.name + "Avg"
-        if self._store_values_type == StoreValuesType.MOVING_AVERAGE_MAX:
-            return self.name + "AvgMax"
-        if self._store_values_type == StoreValuesType.MAX:
-            return self.name + "Max"
-        if self._store_values_type == StoreValuesType.MIN:
-            return self.name + "Min"
-        if self._store_values_type == StoreValuesType.SUM:
-            return self.name + "Sum"
-        assert False
-
-    @property
-    def store_values_type(self):
-        """Return the type of storage for this property."""
-        return self._store_values_type
-
-    @property
-    def sum_elements(self):
-        """Return True if the value is the sum of all elements."""
-        return self._sum_elements
-
-    @property
-    def sum_groups(self):
-        """Return the groups of element names to sum.
-
-        Returns
-        -------
-        list
-
-        """
-        return self._sum_groups
-
-    @property
-    def window_size(self):
-        """Return the window size for moving averages.
-
-        Returns
-        -------
-        int
-
-        """
-        return self._window_size
-
-    @property
-    def window_sizes(self):
-        """Return window sizes for moving averages. Tied to opendss_classes.
-
-        Returns
-        -------
-        dict
-
-        """
-        return self._window_sizes
-
-
-class ExportListReader:
-    """Reads export files and provides access to export properties."""
-    def __init__(self, filename):
-        self._elem_classes = defaultdict(list)
-        legacy_files = ("ExportMode-byClass.toml", "ExportMode-byElement.toml")
-        if os.path.basename(filename) in legacy_files:
-            parser = self._parse_legacy_file
-        else:
-            parser = self._parse_file
-
-        for elem_class, data in parser(filename):
-            self._elem_classes[elem_class].append(ExportListProperty(
-                elem_class, data
-            ))
-
-        # TODO: verify that multiple instances of the same property have
-        # the same names.
-
-    @staticmethod
-    def _parse_file(filename):
-        data = load_data(filename)
-        for elem_class, prop_info in data.items():
-            if isinstance(prop_info, list):
-                for prop in prop_info:
-                    yield elem_class, prop
-            else:
-                assert isinstance(prop_info, dict)
-                for prop, values in prop_info.items():
-                    new_data = {"property": prop, **values}
-                    yield elem_class, new_data
-
-    @staticmethod
-    def _parse_legacy_file(filename):
-        reader = pyExportReader(filename)
-        publications = {tuple(x.split()) for x in reader.publicationList}
-        for elem_class, props in reader.pyControllers.items():
-            for prop in props:
-                publish = (elem_class, prop) in publications
-                yield elem_class, {"property": prop, "publish": publish}
-
-    def append_property(self, elem_class, prop_data):
-        self._elem_classes[elem_class].append(ExportListProperty(elem_class, prop_data))
-
-    def get_element_properties(self, elem_class, prop):
-        if elem_class not in self._elem_classes:
-            raise InvalidParameter(f"{elem_class} is not stored")
-        return [x for x in self._elem_classes[elem_class] if x.name == prop]
-
-    def iter_export_properties(self, elem_class=None):
-        """Returns a generator over the ExportListProperty objects.
-
-        Yields
-        ------
-        ExportListProperty
-
-        """
-        if elem_class is None:
-            for props in self._elem_classes.values():
-                for prop in props:
-                    yield prop
-        elif elem_class not in self._elem_classes:
-            raise InvalidParameter(f"{elem_class} is not stored")
-        else:
-            for prop in self._elem_classes[elem_class]:
-                yield prop
-
-    def list_element_classes(self):
-        return sorted(list(self._elem_classes.keys()))
-
-    def list_element_properties(self, elem_class):
-        if elem_class not in self._elem_classes:
-            return []
-        return self._elem_classes[elem_class]
-
-    def list_element_property_names(self, elem_class):
-        return sorted({x.name for x in self._elem_classes[elem_class]})
-
-    # This name needs to match the interface defined in pyExportReader.
-    @property
-    def publicationList(self):
-        """Return the properties to be published to HELICS.
-
-        Returns
-        -------
-        list
-            Format: ["ElementClass Property"]
-
-        """
-        return [
-            f"{x.elem_class} {x.name}" for x in self.iter_export_properties()
-            if x.publish
-        ]
-
-    def serialize(self):
-        """Serialize object to a dictionary."""
-        data = defaultdict(list)
-        for elem_class, props in self._elem_classes.items():
-            for prop in props:
-                data[elem_class].append(prop.serialize())
-
-        return data
+
+
+from collections import defaultdict
+from pathlib import Path
+import os
+import re
+
+from loguru import logger
+
+
+from pydss.common import DataConversion, LimitsFilter, StoreValuesType, \
+    DatasetPropertyType, MinMax
+from pydss.pyContrReader import pyExportReader
+from pydss.utils.utils import load_data
+from pydss.exceptions import InvalidConfiguration, InvalidParameter
+from pydss.metrics import (
+    NodeVoltageMetric, TrackCapacitorChangeCounts,
+    TrackRegControlTapNumberChanges, ExportLoadingsMetric, OverloadsMetricInMemory,
+    ExportPowersMetric, FeederHeadMetrics
+)
+
+
+CUSTOM_METRICS = {
+    "Capacitors.TrackStateChanges": TrackCapacitorChangeCounts,
+    "CktElement.ExportLoadingsMetric": ExportLoadingsMetric,
+    "CktElement.OverloadsMetricInMemory": OverloadsMetricInMemory,
+    "CktElement.ExportPowersMetric": ExportPowersMetric,
+    "FeederHead.FeederHeadMetrics": FeederHeadMetrics,
+    #"Lines.LoadingPercent": LineLoadingPercent,
+    "Nodes.VoltageMetric": NodeVoltageMetric,
+    "RegControls.TrackTapNumberChanges": TrackRegControlTapNumberChanges,
+    #"Transformers.LoadingPercent": TransformerLoadingPercent,
+}
+
+
+class ExportListProperty:
+    """Contains export options for an element property."""
+    def __init__(self, elem_class, data):
+        self.elem_class = elem_class
+        self._opendss_classes = data.get("opendss_classes", [])
+        self.name = data["property"]
+        self.publish = data.get("publish", False)
+        self._data_conversion = DataConversion(data.get("data_conversion", "none"))
+        self._sum_elements = data.get("sum_elements", False)
+        self._sum_groups = data.get("sum_groups", [])
+        sum_groups_file = data.get("sum_groups_file")
+        self._limits = self._parse_limits(data, "limits")
+        self._limits_filter = LimitsFilter(data.get("limits_filter", "outside"))
+        self._limits_b = self._parse_limits(data, "limits_b")
+        self._limits_filter_b = LimitsFilter(data.get("limits_filter_b", "outside"))
+        self._store_values_type = StoreValuesType(data.get("store_values_type", "all"))
+        self._names, self._are_names_regex, self._are_names_filtered = self._parse_names(data)
+        self._sample_interval = data.get("sample_interval", 1)
+        self._window_size = data.get("window_size", 100)
+        self._window_sizes = data.get("window_sizes", {})
+        custom_prop = f"{elem_class}.{self.name}"
+        self._custom_metric = CUSTOM_METRICS.get(custom_prop)
+
+        if self._sum_groups or sum_groups_file:
+            self._check_sum_groups(sum_groups_file)
+
+        # Note to devs: any field added here needs to be handled in serialize()
+
+        if self._sum_elements and self._store_values_type not in \
+                (StoreValuesType.ALL, StoreValuesType.SUM):
+            raise InvalidConfiguration(
+                "sum_elements requires store_values_types = all or sum"
+            )
+
+        if self._is_max() and self._limits is not None:
+            raise InvalidConfiguration("limits are not allowed with max types")
+
+        requires_opendss_classes = (
+            "ExportLoadingsMetric",
+            "OverloadsMetricInMemory",
+            "ExportPowersMetric",
+        )
+        if elem_class == "CktElement" and self.name in requires_opendss_classes and not self._opendss_classes:
+            raise InvalidConfiguration(
+                f"Exporting {elem_class}.{self.name} requires that opendss_classes be specifed"
+            )
+
+    def _check_sum_groups(self, sum_groups_file):
+        if sum_groups_file is not None:
+            if self._sum_groups:
+                raise InvalidConfiguration(f"Cannot set both sum_groups and sum_groups_file")
+
+            # This path needs to be relative to the current directory, not the Exports.toml.
+            # This might need to be changed.
+            if not Path(sum_groups_file).exists():
+                raise InvalidConfiguration(
+                    f"{sum_groups_file} does not exist. The path must be relative to the current directory."
+                )
+            self._sum_groups = load_data(sum_groups_file)["sum_groups"]
+
+        self._sum_elements = True  # Ignore the user setting. This must be true.
+        # Ensure that there are no duplicate names.
+        orig_length = 0
+        all_names = set()
+        for group in self._sum_groups:
+            orig_length += len(group["elements"])
+            group["elements"] = set(group["elements"])
+            all_names = all_names.union(group["elements"])
+        if orig_length != len(all_names):
+            tag = f"{self.elem_class}/{self.name}"
+            raise InvalidConfiguration(f"{tag} has duplicate element names in sum_groups")
+
+    def _is_max(self):
+        return self._store_values_type in (
+            StoreValuesType.MAX, StoreValuesType.MOVING_AVERAGE_MAX,
+        )
+
+    @property
+    def are_names_filtered(self):
+        return self._are_names_filtered
+
+    def is_moving_average(self):
+        return self._store_values_type in (
+            StoreValuesType.MOVING_AVERAGE, StoreValuesType.MOVING_AVERAGE_MAX,
+        )
+
+    @staticmethod
+    def _parse_limits(data, field_name):
+        limits = data.get(field_name)
+        if limits is None:
+            return None
+
+        if not isinstance(limits, list) or len(limits) != 2:
+            raise InvalidConfiguration(f"invalid limits format: {limits}")
+
+        return MinMax(limits[0], limits[1])
+
+    @staticmethod
+    def _parse_names(data):
+        names = data.get("names")
+        name_regexes = data.get("name_regexes")
+
+        if names and name_regexes:
+            raise InvalidConfiguration("names and name_regexes cannot both be set")
+        for obj in (names, name_regexes):
+            if obj is None:
+                continue
+            if not isinstance(obj, list) or not isinstance(obj[0], str):
+                raise InvalidConfiguration(f"invalid name format: {obj}")
+
+        if names:
+            return set(names), False, True
+        if name_regexes:
+            return [re.compile(r"{}".format(x)) for x in name_regexes], True, True
+
+        return None, False, False
+
+    def _is_inside_limits(self, value):
+        """Return True if the value is between min and max, inclusive."""
+        return value >= self._limits.min and value <= self._limits.max
+
+    def _is_outside_limits(self, value):
+        """Return True if the value is lower than min or greater than max."""
+        return value < self._limits.min or value > self._limits.max
+
+    def append_opendss_classes(self, opendss_classes):
+        """Append all opendss_classes from a list."""
+        self._opendss_classes[:] = list(set(self._opendss_classes + opendss_classes))
+
+    @property
+    def custom_metric(self):
+        """Return the custom_metric attribute.
+
+        Returns
+        -------
+        Metric | None
+
+        """
+        return self._custom_metric
+
+    @property
+    def data_conversion(self):
+        """Return the data_conversion attribute
+
+        Returns
+        -------
+        DataConversion
+
+        """
+        return self._data_conversion
+
+    def get_dataset_property_type(self):
+        """Return the encoding to use for the dataset backing these values.
+
+        Returns
+        -------
+        DatasetPropertyType
+
+        """
+        if self._limits is not None:
+            return DatasetPropertyType.FILTERED
+        if self._store_values_type in \
+                (StoreValuesType.SUM, StoreValuesType.MAX,
+                 StoreValuesType.MIN,
+                 StoreValuesType.MOVING_AVERAGE_MAX,
+                 StoreValuesType.CHANGE_COUNT):
+            return DatasetPropertyType.VALUE
+        return DatasetPropertyType.PER_TIME_POINT
+
+    def get_max_size(self, num_steps):
+        """Return the max number of items that could be stored."""
+        singles = (
+            StoreValuesType.CHANGE_COUNT, StoreValuesType.MAX,
+            StoreValuesType.MIN, StoreValuesType.MOVING_AVERAGE_MAX,
+            StoreValuesType.SUM,
+        )
+        if self._store_values_type in singles:
+            return 1
+        num_samples = num_steps / self._sample_interval
+        return int(num_samples)
+
+    @property
+    def limits(self):
+        """Return the limits for the export property.
+
+        Returns
+        -------
+        MinMax
+
+        """
+        return self._limits
+
+    @property
+    def limits_b(self):
+        """Return the limits_b for the export property.
+
+        Returns
+        -------
+        MinMax
+
+        """
+        return self._limits_b
+
+    @property
+    def opendss_classes(self):
+        """Return the element classes to be used with the property.
+
+        Returns
+        -------
+        list
+
+        """
+        return self._opendss_classes[:]
+
+    def serialize(self):
+        """Serialize object to a dictionary."""
+        if self._are_names_regex:
+            #raise InvalidConfiguration("cannot serialize when names are regex")
+            logger.warning("cannot serialize when names are regex")
+            names = None
+        else:
+            names = self._names
+        data = {
+            "property": self.name,
+            "data_conversion": self._data_conversion.value,
+            "opendss_classes": self._opendss_classes,
+            "sample_interval": self._sample_interval,
+            "names": names,
+            "publish": self.publish,
+            "store_values_type": self.store_values_type.value,
+            "sum_elements": self.sum_elements,
+            "sum_groups": self.sum_groups,
+        }
+        if self._limits is not None:
+            data["limits"] = [self._limits.min, self._limits.max]
+            data["limits_filter"] = self._limits_filter.value
+        if self._limits_b is not None:
+            data["limits_b"] = [self._limits_b.min, self._limits_b.max]
+            data["limits_filter_b"] = self._limits_filter_b.value
+        if self.is_moving_average():
+            if self.window_sizes:
+                data["window_sizes"] = self._window_sizes
+                if not self._opendss_classes:
+                    raise InvalidConfiguration(
+                        f"window_sizes requires opendss_classes: {self.name}"
+                    )
+            else:
+                data["window_size"] = self._window_size
+
+        return data
+
+    def should_store_name(self, name):
+        """Return True if name matches the input criteria."""
+        if self._names is None:
+            return True
+
+        if self._are_names_regex:
+            for regex in self._names:
+                if regex.search(name):
+                    return True
+            return False
+
+        return name in self._names
+
+    def should_sample_value(self, step_number):
+        """Return True if it's time to read a new value."""
+        return step_number % self._sample_interval == 0
+
+    def should_store_value(self, value):
+        """Return True if the value meets the input criteria."""
+        if self._limits is None:
+            return True
+
+        if self._limits_filter == LimitsFilter.OUTSIDE:
+            return self._is_outside_limits(value)
+        return self._is_inside_limits(value)
+
+    def should_store_time_step(self):
+        """Return True if the time step should be stored with the value."""
+        return self.limits is not None
+
+    @property
+    def storage_name(self):
+        if self._store_values_type in (StoreValuesType.ALL, StoreValuesType.CHANGE_COUNT):
+            return self.name
+        if self._store_values_type == StoreValuesType.MOVING_AVERAGE:
+            return self.name + "Avg"
+        if self._store_values_type == StoreValuesType.MOVING_AVERAGE_MAX:
+            return self.name + "AvgMax"
+        if self._store_values_type == StoreValuesType.MAX:
+            return self.name + "Max"
+        if self._store_values_type == StoreValuesType.MIN:
+            return self.name + "Min"
+        if self._store_values_type == StoreValuesType.SUM:
+            return self.name + "Sum"
+        assert False
+
+    @property
+    def store_values_type(self):
+        """Return the type of storage for this property."""
+        return self._store_values_type
+
+    @property
+    def sum_elements(self):
+        """Return True if the value is the sum of all elements."""
+        return self._sum_elements
+
+    @property
+    def sum_groups(self):
+        """Return the groups of element names to sum.
+
+        Returns
+        -------
+        list
+
+        """
+        return self._sum_groups
+
+    @property
+    def window_size(self):
+        """Return the window size for moving averages.
+
+        Returns
+        -------
+        int
+
+        """
+        return self._window_size
+
+    @property
+    def window_sizes(self):
+        """Return window sizes for moving averages. Tied to opendss_classes.
+
+        Returns
+        -------
+        dict
+
+        """
+        return self._window_sizes
+
+
+class ExportListReader:
+    """Reads export files and provides access to export properties."""
+    def __init__(self, filename):
+        self._elem_classes = defaultdict(list)
+        legacy_files = ("ExportMode-byClass.toml", "ExportMode-byElement.toml")
+        if os.path.basename(filename) in legacy_files:
+            parser = self._parse_legacy_file
+        else:
+            parser = self._parse_file
+
+        for elem_class, data in parser(filename):
+            self._elem_classes[elem_class].append(ExportListProperty(
+                elem_class, data
+            ))
+
+        # TODO: verify that multiple instances of the same property have
+        # the same names.
+
+    @staticmethod
+    def _parse_file(filename):
+        data = load_data(filename)
+        for elem_class, prop_info in data.items():
+            if isinstance(prop_info, list):
+                for prop in prop_info:
+                    yield elem_class, prop
+            else:
+                assert isinstance(prop_info, dict)
+                for prop, values in prop_info.items():
+                    new_data = {"property": prop, **values}
+                    yield elem_class, new_data
+
+    @staticmethod
+    def _parse_legacy_file(filename):
+        reader = pyExportReader(filename)
+        publications = {tuple(x.split()) for x in reader.publicationList}
+        for elem_class, props in reader.pyControllers.items():
+            for prop in props:
+                publish = (elem_class, prop) in publications
+                yield elem_class, {"property": prop, "publish": publish}
+
+    def append_property(self, elem_class, prop_data):
+        self._elem_classes[elem_class].append(ExportListProperty(elem_class, prop_data))
+
+    def get_element_properties(self, elem_class, prop):
+        if elem_class not in self._elem_classes:
+            raise InvalidParameter(f"{elem_class} is not stored")
+        return [x for x in self._elem_classes[elem_class] if x.name == prop]
+
+    def iter_export_properties(self, elem_class=None):
+        """Returns a generator over the ExportListProperty objects.
+
+        Yields
+        ------
+        ExportListProperty
+
+        """
+        if elem_class is None:
+            for props in self._elem_classes.values():
+                for prop in props:
+                    yield prop
+        elif elem_class not in self._elem_classes:
+            raise InvalidParameter(f"{elem_class} is not stored")
+        else:
+            for prop in self._elem_classes[elem_class]:
+                yield prop
+
+    def list_element_classes(self):
+        return sorted(list(self._elem_classes.keys()))
+
+    def list_element_properties(self, elem_class):
+        if elem_class not in self._elem_classes:
+            return []
+        return self._elem_classes[elem_class]
+
+    def list_element_property_names(self, elem_class):
+        return sorted({x.name for x in self._elem_classes[elem_class]})
+
+    # This name needs to match the interface defined in pyExportReader.
+    @property
+    def publicationList(self):
+        """Return the properties to be published to HELICS.
+
+        Returns
+        -------
+        list
+            Format: ["ElementClass Property"]
+
+        """
+        return [
+            f"{x.elem_class} {x.name}" for x in self.iter_export_properties()
+            if x.publish
+        ]
+
+    def serialize(self):
+        """Serialize object to a dictionary."""
+        data = defaultdict(list)
+        for elem_class, props in self._elem_classes.items():
+            for prop in props:
+                data[elem_class].append(prop.serialize())
+
+        return data
```

### Comparing `nrel_pydss-3.1.3/src/pydss/get_snapshot_timepoints.py` & `nrel_pydss-3.1.4/src/pydss/get_snapshot_timepoints.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,95 +1,95 @@
-"""Logic to determine snapshot time point by mode"""
-
-import os
-
-from loguru import logger
-
-import opendssdirect as dss
-import pandas as pd
-import numpy as np
-
-from pydss.common import SnapshotTimePointSelectionMode
-from pydss.utils.simulation_utils import create_loadshape_pmult_dataframe_for_simulation
-from pydss.utils.utils import dump_data
-from pydss.reports.reports import logger
-from pydss.simulation_input_models import SimulationSettingsModel
-
-
-def get_snapshot_timepoint(settings: SimulationSettingsModel, mode: SnapshotTimePointSelectionMode):
-    pv_systems = dss.PVsystems.AllNames()
-    if not pv_systems:
-        logger.info("No PVSystems are present.")
-        if mode != SnapshotTimePointSelectionMode.MAX_LOAD:
-            mode = SnapshotTimePointSelectionMode.MAX_LOAD
-            logger.info("Changed mode to %s", SnapshotTimePointSelectionMode.MAX_LOAD.value)
-    if mode == SnapshotTimePointSelectionMode.MAX_LOAD:
-        column = "Max Load"
-    elif mode == SnapshotTimePointSelectionMode.MAX_PV_LOAD_RATIO:
-        column = "Max PV to Load Ratio"
-    elif mode == SnapshotTimePointSelectionMode.DAYTIME_MIN_LOAD:
-        column = "Min Daytime Load"
-    elif mode == SnapshotTimePointSelectionMode.MAX_PV_MINUS_LOAD:
-        column = "Max PV minus Load"
-    else:
-        assert False, f"{mode} is not supported"
-
-    temp_filename = settings.project.project_path / settings.project.active_project / "Exports" / ".snapshot_time_points.json"
-    final_filename = settings.project.project_path / settings.project.active_project / "Exports" / "snapshot_time_points.json"
-    if temp_filename.exists():
-        timepoints = pd.read_json(temp_filename)
-        if settings.project.active_scenario == settings.project.scenarios[-1].name:
-            os.rename(temp_filename, final_filename)
-        return pd.to_datetime(timepoints[column].iloc[0]).to_pydatetime()
-    pv_generation_hours = {'start_time': '8:00', 'end_time': '17:00'}
-    aggregate_profiles = pd.DataFrame(columns=['Load', 'PV'])
-    pv_shapes = {}
-    for pv_name in pv_systems:
-        dss.PVsystems.Name(pv_name)
-        pmpp = float(dss.Properties.Value('Pmpp'))
-        profile_name = dss.Properties.Value('yearly')
-        dss.LoadShape.Name(profile_name)
-        if profile_name not in pv_shapes.keys():
-            pv_shapes[profile_name] = create_loadshape_pmult_dataframe_for_simulation(settings)
-        if len(aggregate_profiles) == 0:
-            aggregate_profiles['PV'] = (pv_shapes[profile_name] * pmpp)[0]
-            aggregate_profiles = aggregate_profiles.replace(np.nan, 0)
-        else:
-            aggregate_profiles['PV'] = aggregate_profiles['PV'] + (pv_shapes[profile_name] * pmpp)[0]
-    del pv_shapes
-    loads = dss.Loads.AllNames()
-    if not loads:
-        logger.info("No Loads are present")
-    load_shapes = {}
-    for load_name in loads:
-        dss.Loads.Name(load_name)
-        kw = float(dss.Properties.Value('kW'))
-        profile_name = dss.Properties.Value('yearly')
-        dss.LoadShape.Name(profile_name)
-        if profile_name not in load_shapes.keys():
-            load_shapes[profile_name] = create_loadshape_pmult_dataframe_for_simulation(settings)
-        if len(aggregate_profiles) == 0:
-            aggregate_profiles['Load'] = (load_shapes[profile_name] * kw)[0]
-        else:
-            aggregate_profiles['Load'] = aggregate_profiles['Load'] + (load_shapes[profile_name] * kw)[0]
-    del load_shapes
-    if pv_systems:
-        aggregate_profiles['PV to Load Ratio'] = aggregate_profiles['PV'] / aggregate_profiles['Load']
-        aggregate_profiles['PV minus Load'] = aggregate_profiles['PV'] - aggregate_profiles['Load']
-
-    timepoints = pd.DataFrame(columns=['Timepoints'])
-    timepoints.loc['Max Load'] = aggregate_profiles['Load'].idxmax()
-    if pv_systems:
-        timepoints.loc['Max PV to Load Ratio'] = aggregate_profiles.between_time(pv_generation_hours['start_time'],
-                                                                                 pv_generation_hours['end_time'])['PV to Load Ratio'].idxmax()
-        timepoints.loc['Max PV minus Load'] = aggregate_profiles.between_time(pv_generation_hours['start_time'],
-                                                                              pv_generation_hours['end_time'])['PV minus Load'].idxmax()
-        timepoints.loc['Max PV'] = aggregate_profiles.between_time(pv_generation_hours['start_time'],
-                                                                   pv_generation_hours['end_time'])['PV'].idxmax()
-    timepoints.loc['Min Load'] = aggregate_profiles['Load'].idxmin()
-    timepoints.loc['Min Daytime Load'] = aggregate_profiles.between_time(pv_generation_hours['start_time'],
-                                                                         pv_generation_hours['end_time'])['Load'].idxmin()
-    logger.info("Time points: %s", {k: str(v) for k, v in timepoints.to_records()})
-    dump_data(timepoints.astype(str).to_dict(orient='index'), temp_filename, indent=2)
-    if settings.project.active_scenario == settings.project.scenarios[-1].name:
-        os.rename(temp_filename, final_filename)
-    return timepoints.loc[column].iloc[0].to_pydatetime()
+"""Logic to determine snapshot time point by mode"""
+
+import os
+
+from loguru import logger
+
+import opendssdirect as dss
+import pandas as pd
+import numpy as np
+
+from pydss.common import SnapshotTimePointSelectionMode
+from pydss.utils.simulation_utils import create_loadshape_pmult_dataframe_for_simulation
+from pydss.utils.utils import dump_data
+from pydss.reports.reports import logger
+from pydss.simulation_input_models import SimulationSettingsModel
+
+
+def get_snapshot_timepoint(settings: SimulationSettingsModel, mode: SnapshotTimePointSelectionMode):
+    pv_systems = dss.PVsystems.AllNames()
+    if not pv_systems:
+        logger.info("No PVSystems are present.")
+        if mode != SnapshotTimePointSelectionMode.MAX_LOAD:
+            mode = SnapshotTimePointSelectionMode.MAX_LOAD
+            logger.info("Changed mode to %s", SnapshotTimePointSelectionMode.MAX_LOAD.value)
+    if mode == SnapshotTimePointSelectionMode.MAX_LOAD:
+        column = "Max Load"
+    elif mode == SnapshotTimePointSelectionMode.MAX_PV_LOAD_RATIO:
+        column = "Max PV to Load Ratio"
+    elif mode == SnapshotTimePointSelectionMode.DAYTIME_MIN_LOAD:
+        column = "Min Daytime Load"
+    elif mode == SnapshotTimePointSelectionMode.MAX_PV_MINUS_LOAD:
+        column = "Max PV minus Load"
+    else:
+        assert False, f"{mode} is not supported"
+
+    temp_filename = settings.project.project_path / settings.project.active_project / "Exports" / ".snapshot_time_points.json"
+    final_filename = settings.project.project_path / settings.project.active_project / "Exports" / "snapshot_time_points.json"
+    if temp_filename.exists():
+        timepoints = pd.read_json(temp_filename)
+        if settings.project.active_scenario == settings.project.scenarios[-1].name:
+            os.rename(temp_filename, final_filename)
+        return pd.to_datetime(timepoints[column].iloc[0]).to_pydatetime()
+    pv_generation_hours = {'start_time': '8:00', 'end_time': '17:00'}
+    aggregate_profiles = pd.DataFrame(columns=['Load', 'PV'])
+    pv_shapes = {}
+    for pv_name in pv_systems:
+        dss.PVsystems.Name(pv_name)
+        pmpp = float(dss.Properties.Value('Pmpp'))
+        profile_name = dss.Properties.Value('yearly')
+        dss.LoadShape.Name(profile_name)
+        if profile_name not in pv_shapes.keys():
+            pv_shapes[profile_name] = create_loadshape_pmult_dataframe_for_simulation(settings)
+        if len(aggregate_profiles) == 0:
+            aggregate_profiles['PV'] = (pv_shapes[profile_name] * pmpp)[0]
+            aggregate_profiles = aggregate_profiles.replace(np.nan, 0)
+        else:
+            aggregate_profiles['PV'] = aggregate_profiles['PV'] + (pv_shapes[profile_name] * pmpp)[0]
+    del pv_shapes
+    loads = dss.Loads.AllNames()
+    if not loads:
+        logger.info("No Loads are present")
+    load_shapes = {}
+    for load_name in loads:
+        dss.Loads.Name(load_name)
+        kw = float(dss.Properties.Value('kW'))
+        profile_name = dss.Properties.Value('yearly')
+        dss.LoadShape.Name(profile_name)
+        if profile_name not in load_shapes.keys():
+            load_shapes[profile_name] = create_loadshape_pmult_dataframe_for_simulation(settings)
+        if len(aggregate_profiles) == 0:
+            aggregate_profiles['Load'] = (load_shapes[profile_name] * kw)[0]
+        else:
+            aggregate_profiles['Load'] = aggregate_profiles['Load'] + (load_shapes[profile_name] * kw)[0]
+    del load_shapes
+    if pv_systems:
+        aggregate_profiles['PV to Load Ratio'] = aggregate_profiles['PV'] / aggregate_profiles['Load']
+        aggregate_profiles['PV minus Load'] = aggregate_profiles['PV'] - aggregate_profiles['Load']
+
+    timepoints = pd.DataFrame(columns=['Timepoints'])
+    timepoints.loc['Max Load'] = aggregate_profiles['Load'].idxmax()
+    if pv_systems:
+        timepoints.loc['Max PV to Load Ratio'] = aggregate_profiles.between_time(pv_generation_hours['start_time'],
+                                                                                 pv_generation_hours['end_time'])['PV to Load Ratio'].idxmax()
+        timepoints.loc['Max PV minus Load'] = aggregate_profiles.between_time(pv_generation_hours['start_time'],
+                                                                              pv_generation_hours['end_time'])['PV minus Load'].idxmax()
+        timepoints.loc['Max PV'] = aggregate_profiles.between_time(pv_generation_hours['start_time'],
+                                                                   pv_generation_hours['end_time'])['PV'].idxmax()
+    timepoints.loc['Min Load'] = aggregate_profiles['Load'].idxmin()
+    timepoints.loc['Min Daytime Load'] = aggregate_profiles.between_time(pv_generation_hours['start_time'],
+                                                                         pv_generation_hours['end_time'])['Load'].idxmin()
+    logger.info("Time points: %s", {k: str(v) for k, v in timepoints.to_records()})
+    dump_data(timepoints.astype(str).to_dict(orient='index'), temp_filename, indent=2)
+    if settings.project.active_scenario == settings.project.scenarios[-1].name:
+        os.rename(temp_filename, final_filename)
+    return timepoints.loc[column].iloc[0].to_pydatetime()
```

### Comparing `nrel_pydss-3.1.3/src/pydss/helics_interface.py` & `nrel_pydss-3.1.4/src/pydss/helics_interface.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,368 +1,368 @@
-from pydantic import ConfigDict, BaseModel, model_validator
-from typing import List, Optional, Any, Union, Dict
-from enum import Enum
-import helics
-import os
-import re
-
-from loguru import logger
-
-from pydss.simulation_input_models import SimulationSettingsModel
-from pydss.common import SUBSCRIPTIONS_FILENAME, ExportMode
-from pydss.utils.utils import load_data
-
-TYPE_INFO = {
-        'CurrentsMagAng': 'vector',
-        'Currents': 'vector',
-        'RatedCurrent': 'double',
-        'EmergAmps': 'double',
-        'NormalAmps': 'double',
-        'normamps': 'double',
-        'Losses': 'vector',
-        'PhaseLosses': 'vector',
-        'Powers': 'vector',
-        'TotalPower': 'vector',
-        'LineLosses': 'vector',
-        'SubstationLosses': 'vector',
-        'kV': 'double',
-        'kVARated': 'double',
-        'kvar': 'double',
-        'kW': 'double',
-        'kVABase': 'double',
-        'kWh': 'double',
-        'puVmagAngle': 'vector',
-        'VoltagesMagAng': 'vector',
-        'VMagAngle': 'vector',
-        'Voltages': 'vector',
-        'Vmaxpu': 'double',
-        'Vminpu': 'double',
-        'Frequency': 'double',
-        'Taps': 'vector',
-        '%stored': 'double',
-        'Distance': 'double'
-    }
-
-class DataType(Enum):
-    DOUBLE = "double"
-    VECTOR = "vector"
-    STRING = "string"
-    BOOLEAN = "boolean"
-    INTEGER = "integer"
-
-class Subscription(BaseModel):
-    model: str
-    property: str
-    id: str
-    unit: Optional[str] = None 
-    subscribe: bool = True
-    data_type: DataType
-    multiplier: float = 1.0
-    object: Any = None
-    states: List[Union[float, int, bool]] = [0.0, 0.0, 0.0, 0.0, 0.0]
-    sub: Any = None
-    model_config = ConfigDict(arbitrary_types_allowed=True)
-
-class Publication(BaseModel):
-    model: str
-    property: str
-    id: str
-    object: Any = None
-    pub: Any = None
-    data_type: DataType
-    
-class Subscriptions(BaseModel):
-    federate: Any = None
-    opendss_models: Dict
-    subscriptions: List[Subscription]
-
-    @model_validator(mode='after')
-    def is_in_opendss_model(self)-> 'Subscriptions':
-        for subscription in self.subscriptions:
-            if subscription.model not in self.opendss_models:
-                raise AssertionError(f"The loaded OpenDSS model does not have an element define with the name {subscription}")
-            
-            if subscription.subscribe:
-                subscription.object = self.opendss_models[subscription.model]
-                subscription.sub = helics.helicsFederateRegisterSubscription(
-                    self.federate,
-                    subscription.id,
-                    subscription.unit
-                )
-        return self
-    
-class Publications(BaseModel):
-    
-    federate: Any = None
-    federate_name: str
-    opendss_models: Dict
-    publications: List[Publication] = []
-    legacy_input: Dict = {}
-    input: Dict = {}
-    
-    @model_validator(mode='after')
-    def build_from_legacy(self)-> 'Publications':
-        publications = []
-        for object_type, k in self.legacy_input.items():
-            if object_type in self.opendss_models:
-                models =  self.opendss_models[object_type]    
-                for model in models:
-                    for ppty in k['Publish']:
-                        name = '{}.{}.{}'.format(self.federate_name, model, ppty)
-                        pub_dict = {
-                            "model" : model,
-                            "object" :  models[model],
-                            "id" : name,
-                            "property" : ppty,
-                            "data_type" : TYPE_INFO[ppty],
-                            "pub" :  helics.helicsFederateRegisterGlobalTypePublication(
-                                self.federate,
-                                name,
-                                TYPE_INFO[ppty],
-                                ''
-                            )
-                        }
-                        publications.append(Publication.model_validate(pub_dict))
-        self.publications = publications
-        return self
-
-    @model_validator(mode='after')
-    def build_from_export(self)-> 'Publications':
-        publications = []
-        for object_type, export_properties in self.input.items():
-            if object_type in self.opendss_models:
-                models =  self.opendss_models[object_type]
-                for export_property in export_properties:
-                    filtered_models = {}
-                    if export_property['publish']:
-                        if 'names' in export_property and export_property['names']:
-                            for k, v in models.items():
-                                if k in export_property['names']:
-                                    filtered_models[k] = models[k]
-                        elif 'name_regexes' in export_property and export_property['name_regexes']:
-                            for regex_expression in export_property['name_regexes']:
-                                r = re.compile(regex_expression)
-                                matches = list(filter(r.match, models.keys()))
-                                for match in matches:
-                                    filtered_models[match] = models[match]
-                        else:
-                            filtered_models = models
-                    
-                        for model_name, model_obj in filtered_models.items():
-                            if object_type == "Buses":
-                                name = '{}.Bus.{}.{}'.format(self.federate_name, model_name, export_property["property"])
-                            else:
-                                name = '{}.{}.{}'.format(self.federate_name, model_name, export_property["property"])
-                            pub_dict = {
-                                "model" : model_name,
-                                "object" :  model_obj,
-                                "id" : name,
-                                "property" : export_property["property"],
-                                "data_type" : TYPE_INFO[export_property["property"]],
-                                "pub" :  helics.helicsFederateRegisterGlobalTypePublication(
-                                    self.federate,
-                                    name,
-                                    TYPE_INFO[export_property["property"]],
-                                    ''
-                                )
-                            }
-                            publications.append(Publication.model_validate(pub_dict))
-        self.publications = publications        
-        return self
-
-class helics_interface:
-    n_states = 5
-    init_state = 1
-    
-    def __init__(self, dss_solver, objects_by_name, objects_by_class, settings: SimulationSettingsModel, system_paths, default=True):
-        self.itr = 0
-        self.c_seconds = 0
-        self.c_seconds_old = -1
-        self._settings = settings
-        self._co_convergance_error_tolerance = settings.helics.error_tolerance
-        self._co_convergance_max_iterations = self._settings.helics.max_co_iterations
-        self._publications = {}
-        self._system_paths = system_paths
-        self._objects_by_element = objects_by_name
-        self._objects_by_class = objects_by_class
-        self._create_helics_federate()
-        self._dss_solver = dss_solver
-        if default:
-            self.registerPubSubTags()
-
-    def registerPubSubTags(self, pubs=None, subs=None):
-
-        self._registerFederateSubscriptions(subs)
-        self._registerFederatePublications(pubs)
-
-        helics.helicsFederateEnterExecutingModeIterative(
-            self._federate,
-            helics.helics_iteration_request_iterate_if_needed
-        )
-        logger.info('Entered HELICS execution mode')
-
-    def _create_helics_federate(self):
-        self.fedinfo = helics.helicsCreateFederateInfo()
-        helics.helicsFederateInfoSetCoreName(self.fedinfo, self._settings.helics.federate_name)
-        helics.helicsFederateInfoSetCoreTypeFromString(self.fedinfo, self._settings.helics.core_type)
-        helics.helicsFederateInfoSetCoreInitString(self.fedinfo, f"--federates=1")
-        IP = self._settings.helics.broker
-        Port = self._settings.helics.broker_port
-        logger.info("Connecting to broker @ {}".format(f"{IP}:{Port}" if Port else IP))
-        if self._settings.helics.broker:
-            helics.helicsFederateInfoSetBroker(self.fedinfo, str(self._settings.helics.broker))
-        if self._settings.helics.broker_port:
-            helics.helicsFederateInfoSetBrokerPort(self.fedinfo, self._settings.helics.broker_port)
-        helics.helicsFederateInfoSetTimeProperty(self.fedinfo, helics.helics_property_time_delta,
-                                                 self._settings.helics.time_delta)
-        helics.helicsFederateInfoSetIntegerProperty(self.fedinfo, helics.helics_property_int_log_level,
-                                                    self._settings.helics.logging_level)
-        helics.helicsFederateInfoSetIntegerProperty(self.fedinfo, helics.helics_property_int_max_iterations,
-                                                    self._settings.helics.max_co_iterations)
-        self._federate = helics.helicsCreateValueFederate(self._settings.helics.federate_name, self.fedinfo)
-        return
-
-
-    def _registerFederateSubscriptions(self, subscriptions:Subscriptions = None):
-        """
-        :param subs:
-        :return:
-        """
-        if subscriptions is None:
-            subscription_file = os.path.join(
-                self._system_paths["ExportLists"],
-                SUBSCRIPTIONS_FILENAME,
-            )
-            assert os.path.exists(subscription_file), f"The following file does not exist: {subscription_file}"
-            file_data = load_data(subscription_file)
-            file_data["opendss_models"] = self._objects_by_element
-            file_data["federate"] = self._federate
-         
-            self.subscriptions = Subscriptions.model_validate(file_data)
-        else:
-            self.subscriptions = subscriptions
-        logger.info(str(self.subscriptions.subscriptions))
-        for subscription in self.subscriptions.subscriptions:
-            logger.info(f"subscription created: {subscription}")
-        return
-
-    def updateHelicsSubscriptions(self):
-        for subscription in self.subscriptions.subscriptions:
-            if subscription.subscribe:
-                value = None
-                if subscription.data_type == DataType.DOUBLE:
-                    value = helics.helicsInputGetDouble(subscription.sub)
-                elif subscription.data_type == DataType.VECTOR:
-                    value = helics.helicsInputGetVector(subscription.sub)
-                elif subscription.data_type == DataType.STRING:
-                    value = helics.helicsInputGetString(subscription.sub)
-                elif subscription.data_type == DataType.BOOLEAN:
-                    value = helics.helicsInputGetBoolean(subscription.sub)
-                elif subscription.data_type == DataType.INTEGER:
-                    value = helics.helicsInputGetInteger(subscription.sub)
-                    
-                if value and value != 0:
-                    if value > 1e6 or value < -1e6:
-                        value = 1.0 
-
-                value = value * subscription.multiplier
-                subscription.object.SetParameter(subscription.property, value) 
-                logger.info('Value for "{}.{}" changed to "{}"'.format(
-                        subscription.model,
-                        subscription.property,
-                        value
-                    ))
-
-                if self._settings.helics.iterative_mode:
-                    if self.c_seconds != self.c_seconds_old:
-                        subscription.states = [self.init_state] * self.n_states
-                    else:
-                        subscription.states.insert(0, subscription.states.pop())
-                    subscription.states[0] = value
-
-        self.c_seconds_old = self.c_seconds
-  
-    def _registerFederatePublications(self, publications:Publications = None):
-        if publications:
-            self.publicatiuons = publications
-        else:
-            legacy_export_file = os.path.join(
-                self._system_paths ["ExportLists"],
-                ExportMode.BY_CLASS.value + ".toml",
-            )
-            export_file = os.path.join(
-                self._system_paths ["ExportLists"],
-                ExportMode.EXPORTS.value  + ".toml",
-            )
-            
-            publication_dict = {
-                "opendss_models" : self._objects_by_class,
-                "federate_name" : self._settings.helics.federate_name,
-                "federate" : self._federate,
-            }
-                  
-            if os.path.exists(export_file):
-                export_data = load_data(export_file)
-                publication_dict["input"] = export_data     
-            elif os.path.exists(legacy_export_file):
-                legacy_data = load_data(legacy_export_file)
-                publication_dict["legacy_input"] = legacy_data               
-            else:
-                raise FileNotFoundError("No valid export settings found for the current scenario")
-            
-            self.publications = Publications.model_validate(publication_dict)
-            logger.info(str(self.publications.publications))
-            for publication in self.publications.publications:
-                logger.info(f"pubscription created: {publication}")
-        return
-
-    def updateHelicsPublications(self):
-        
-        for publication in self.publications.publications:
-            value = publication.object.GetValue(publication.property)
-            
-            if publication.data_type == DataType.VECTOR:
-                helics.helicsPublicationPublishVector(publication.pub, value)
-            elif publication.data_type == DataType.DOUBLE:
-                helics.helicsPublicationPublishDouble(publication.pub, value)
-            elif publication.data_type == DataType.STRING:
-                helics.helicsPublicationPublishString(publication.pub, value)
-            elif publication.data_type == DataType.BOOLEAN:
-                helics.helicsPublicationPublishBoolean(publication.pub, value)
-            elif publication.data_type == DataType.INTEGER:
-                helics.helicsPublicationPublishInteger(publication.pub, value)
-            else:
-                raise ValueError("Unsupported data type forr teh HELICS interface")
-            logger.info(f"{publication} - {value}")
-        return
-
-    def request_time_increment(self):
-        error = sum([abs(sub.states[0] - sub.states[1]) for sub in self.subscriptions.subscriptions])
-        r_seconds = self._dss_solver.GetTotalSeconds() #- self._dss_solver.GetStepResolutionSeconds()
-        if not self._settings.helics.iterative_mode:
-            while self.c_seconds < r_seconds:
-                self.c_seconds = helics.helicsFederateRequestTime(self._federate, r_seconds)
-            logger.info('Time requested: {} - time granted: {} '.format(r_seconds, self.c_seconds))
-            return True, self.c_seconds
-        else:
-
-            self.c_seconds, iteration_state = helics.helicsFederateRequestTimeIterative(
-                self._federate,
-                r_seconds,
-                helics.helics_iteration_request_iterate_if_needed
-            )
-
-            logger.info('Time requested: {} - time granted: {} error: {} it: {}'.format(
-                r_seconds, self.c_seconds, error, self.itr))
-            if error > -1 and self.itr < self._co_convergance_max_iterations - 1:
-                self.itr += 1
-                return False, self.c_seconds
-            else:
-                self.itr = 0
-                return True, self.c_seconds
-
-    def __del__(self):
-        helics.helicsFederateDisconnect(self._federate)
-        state = helics.helicsFederateGetState(self._federate)
-        helics.helicsFederateInfoFree(self.fedinfo)
-        helics.helicsFederateFree(self._federate)
-        logger.info('HELICS federate for pydss destroyed')
+from pydantic import ConfigDict, BaseModel, model_validator
+from typing import List, Optional, Any, Union, Dict
+from enum import Enum
+import helics
+import os
+import re
+
+from loguru import logger
+
+from pydss.simulation_input_models import SimulationSettingsModel
+from pydss.common import SUBSCRIPTIONS_FILENAME, ExportMode
+from pydss.utils.utils import load_data
+
+TYPE_INFO = {
+        'CurrentsMagAng': 'vector',
+        'Currents': 'vector',
+        'RatedCurrent': 'double',
+        'EmergAmps': 'double',
+        'NormalAmps': 'double',
+        'normamps': 'double',
+        'Losses': 'vector',
+        'PhaseLosses': 'vector',
+        'Powers': 'vector',
+        'TotalPower': 'vector',
+        'LineLosses': 'vector',
+        'SubstationLosses': 'vector',
+        'kV': 'double',
+        'kVARated': 'double',
+        'kvar': 'double',
+        'kW': 'double',
+        'kVABase': 'double',
+        'kWh': 'double',
+        'puVmagAngle': 'vector',
+        'VoltagesMagAng': 'vector',
+        'VMagAngle': 'vector',
+        'Voltages': 'vector',
+        'Vmaxpu': 'double',
+        'Vminpu': 'double',
+        'Frequency': 'double',
+        'Taps': 'vector',
+        '%stored': 'double',
+        'Distance': 'double'
+    }
+
+class DataType(Enum):
+    DOUBLE = "double"
+    VECTOR = "vector"
+    STRING = "string"
+    BOOLEAN = "boolean"
+    INTEGER = "integer"
+
+class Subscription(BaseModel):
+    model: str
+    property: str
+    id: str
+    unit: Optional[str] = None 
+    subscribe: bool = True
+    data_type: DataType
+    multiplier: float = 1.0
+    object: Any = None
+    states: List[Union[float, int, bool]] = [0.0, 0.0, 0.0, 0.0, 0.0]
+    sub: Any = None
+    model_config = ConfigDict(arbitrary_types_allowed=True)
+
+class Publication(BaseModel):
+    model: str
+    property: str
+    id: str
+    object: Any = None
+    pub: Any = None
+    data_type: DataType
+    
+class Subscriptions(BaseModel):
+    federate: Any = None
+    opendss_models: Dict
+    subscriptions: List[Subscription]
+
+    @model_validator(mode='after')
+    def is_in_opendss_model(self)-> 'Subscriptions':
+        for subscription in self.subscriptions:
+            if subscription.model not in self.opendss_models:
+                raise AssertionError(f"The loaded OpenDSS model does not have an element define with the name {subscription}")
+            
+            if subscription.subscribe:
+                subscription.object = self.opendss_models[subscription.model]
+                subscription.sub = helics.helicsFederateRegisterSubscription(
+                    self.federate,
+                    subscription.id,
+                    subscription.unit
+                )
+        return self
+    
+class Publications(BaseModel):
+    
+    federate: Any = None
+    federate_name: str
+    opendss_models: Dict
+    publications: List[Publication] = []
+    legacy_input: Dict = {}
+    input: Dict = {}
+    
+    @model_validator(mode='after')
+    def build_from_legacy(self)-> 'Publications':
+        publications = []
+        for object_type, k in self.legacy_input.items():
+            if object_type in self.opendss_models:
+                models =  self.opendss_models[object_type]    
+                for model in models:
+                    for ppty in k['Publish']:
+                        name = '{}.{}.{}'.format(self.federate_name, model, ppty)
+                        pub_dict = {
+                            "model" : model,
+                            "object" :  models[model],
+                            "id" : name,
+                            "property" : ppty,
+                            "data_type" : TYPE_INFO[ppty],
+                            "pub" :  helics.helicsFederateRegisterGlobalTypePublication(
+                                self.federate,
+                                name,
+                                TYPE_INFO[ppty],
+                                ''
+                            )
+                        }
+                        publications.append(Publication.model_validate(pub_dict))
+        self.publications = publications
+        return self
+
+    @model_validator(mode='after')
+    def build_from_export(self)-> 'Publications':
+        publications = []
+        for object_type, export_properties in self.input.items():
+            if object_type in self.opendss_models:
+                models =  self.opendss_models[object_type]
+                for export_property in export_properties:
+                    filtered_models = {}
+                    if export_property['publish']:
+                        if 'names' in export_property and export_property['names']:
+                            for k, v in models.items():
+                                if k in export_property['names']:
+                                    filtered_models[k] = models[k]
+                        elif 'name_regexes' in export_property and export_property['name_regexes']:
+                            for regex_expression in export_property['name_regexes']:
+                                r = re.compile(regex_expression)
+                                matches = list(filter(r.match, models.keys()))
+                                for match in matches:
+                                    filtered_models[match] = models[match]
+                        else:
+                            filtered_models = models
+                    
+                        for model_name, model_obj in filtered_models.items():
+                            if object_type == "Buses":
+                                name = '{}.Bus.{}.{}'.format(self.federate_name, model_name, export_property["property"])
+                            else:
+                                name = '{}.{}.{}'.format(self.federate_name, model_name, export_property["property"])
+                            pub_dict = {
+                                "model" : model_name,
+                                "object" :  model_obj,
+                                "id" : name,
+                                "property" : export_property["property"],
+                                "data_type" : TYPE_INFO[export_property["property"]],
+                                "pub" :  helics.helicsFederateRegisterGlobalTypePublication(
+                                    self.federate,
+                                    name,
+                                    TYPE_INFO[export_property["property"]],
+                                    ''
+                                )
+                            }
+                            publications.append(Publication.model_validate(pub_dict))
+        self.publications = publications        
+        return self
+
+class helics_interface:
+    n_states = 5
+    init_state = 1
+    
+    def __init__(self, dss_solver, objects_by_name, objects_by_class, settings: SimulationSettingsModel, system_paths, default=True):
+        self.itr = 0
+        self.c_seconds = 0
+        self.c_seconds_old = -1
+        self._settings = settings
+        self._co_convergance_error_tolerance = settings.helics.error_tolerance
+        self._co_convergance_max_iterations = self._settings.helics.max_co_iterations
+        self._publications = {}
+        self._system_paths = system_paths
+        self._objects_by_element = objects_by_name
+        self._objects_by_class = objects_by_class
+        self._create_helics_federate()
+        self._dss_solver = dss_solver
+        if default:
+            self.registerPubSubTags()
+
+    def registerPubSubTags(self, pubs=None, subs=None):
+
+        self._registerFederateSubscriptions(subs)
+        self._registerFederatePublications(pubs)
+
+        helics.helicsFederateEnterExecutingModeIterative(
+            self._federate,
+            helics.helics_iteration_request_iterate_if_needed
+        )
+        logger.info('Entered HELICS execution mode')
+
+    def _create_helics_federate(self):
+        self.fedinfo = helics.helicsCreateFederateInfo()
+        helics.helicsFederateInfoSetCoreName(self.fedinfo, self._settings.helics.federate_name)
+        helics.helicsFederateInfoSetCoreTypeFromString(self.fedinfo, self._settings.helics.core_type)
+        helics.helicsFederateInfoSetCoreInitString(self.fedinfo, f"--federates=1")
+        IP = self._settings.helics.broker
+        Port = self._settings.helics.broker_port
+        logger.info("Connecting to broker @ {}".format(f"{IP}:{Port}" if Port else IP))
+        if self._settings.helics.broker:
+            helics.helicsFederateInfoSetBroker(self.fedinfo, str(self._settings.helics.broker))
+        if self._settings.helics.broker_port:
+            helics.helicsFederateInfoSetBrokerPort(self.fedinfo, self._settings.helics.broker_port)
+        helics.helicsFederateInfoSetTimeProperty(self.fedinfo, helics.helics_property_time_delta,
+                                                 self._settings.helics.time_delta)
+        helics.helicsFederateInfoSetIntegerProperty(self.fedinfo, helics.helics_property_int_log_level,
+                                                    self._settings.helics.logging_level)
+        helics.helicsFederateInfoSetIntegerProperty(self.fedinfo, helics.helics_property_int_max_iterations,
+                                                    self._settings.helics.max_co_iterations)
+        self._federate = helics.helicsCreateValueFederate(self._settings.helics.federate_name, self.fedinfo)
+        return
+
+
+    def _registerFederateSubscriptions(self, subscriptions:Subscriptions = None):
+        """
+        :param subs:
+        :return:
+        """
+        if subscriptions is None:
+            subscription_file = os.path.join(
+                self._system_paths["ExportLists"],
+                SUBSCRIPTIONS_FILENAME,
+            )
+            assert os.path.exists(subscription_file), f"The following file does not exist: {subscription_file}"
+            file_data = load_data(subscription_file)
+            file_data["opendss_models"] = self._objects_by_element
+            file_data["federate"] = self._federate
+         
+            self.subscriptions = Subscriptions.model_validate(file_data)
+        else:
+            self.subscriptions = subscriptions
+        logger.info(str(self.subscriptions.subscriptions))
+        for subscription in self.subscriptions.subscriptions:
+            logger.info(f"subscription created: {subscription}")
+        return
+
+    def updateHelicsSubscriptions(self):
+        for subscription in self.subscriptions.subscriptions:
+            if subscription.subscribe:
+                value = None
+                if subscription.data_type == DataType.DOUBLE:
+                    value = helics.helicsInputGetDouble(subscription.sub)
+                elif subscription.data_type == DataType.VECTOR:
+                    value = helics.helicsInputGetVector(subscription.sub)
+                elif subscription.data_type == DataType.STRING:
+                    value = helics.helicsInputGetString(subscription.sub)
+                elif subscription.data_type == DataType.BOOLEAN:
+                    value = helics.helicsInputGetBoolean(subscription.sub)
+                elif subscription.data_type == DataType.INTEGER:
+                    value = helics.helicsInputGetInteger(subscription.sub)
+                    
+                if value and value != 0:
+                    if value > 1e6 or value < -1e6:
+                        value = 1.0 
+
+                value = value * subscription.multiplier
+                subscription.object.SetParameter(subscription.property, value) 
+                logger.info('Value for "{}.{}" changed to "{}"'.format(
+                        subscription.model,
+                        subscription.property,
+                        value
+                    ))
+
+                if self._settings.helics.iterative_mode:
+                    if self.c_seconds != self.c_seconds_old:
+                        subscription.states = [self.init_state] * self.n_states
+                    else:
+                        subscription.states.insert(0, subscription.states.pop())
+                    subscription.states[0] = value
+
+        self.c_seconds_old = self.c_seconds
+  
+    def _registerFederatePublications(self, publications:Publications = None):
+        if publications:
+            self.publicatiuons = publications
+        else:
+            legacy_export_file = os.path.join(
+                self._system_paths ["ExportLists"],
+                ExportMode.BY_CLASS.value + ".toml",
+            )
+            export_file = os.path.join(
+                self._system_paths ["ExportLists"],
+                ExportMode.EXPORTS.value  + ".toml",
+            )
+            
+            publication_dict = {
+                "opendss_models" : self._objects_by_class,
+                "federate_name" : self._settings.helics.federate_name,
+                "federate" : self._federate,
+            }
+                  
+            if os.path.exists(export_file):
+                export_data = load_data(export_file)
+                publication_dict["input"] = export_data     
+            elif os.path.exists(legacy_export_file):
+                legacy_data = load_data(legacy_export_file)
+                publication_dict["legacy_input"] = legacy_data               
+            else:
+                raise FileNotFoundError("No valid export settings found for the current scenario")
+            
+            self.publications = Publications.model_validate(publication_dict)
+            logger.info(str(self.publications.publications))
+            for publication in self.publications.publications:
+                logger.info(f"pubscription created: {publication}")
+        return
+
+    def updateHelicsPublications(self):
+        
+        for publication in self.publications.publications:
+            value = publication.object.GetValue(publication.property)
+            
+            if publication.data_type == DataType.VECTOR:
+                helics.helicsPublicationPublishVector(publication.pub, value)
+            elif publication.data_type == DataType.DOUBLE:
+                helics.helicsPublicationPublishDouble(publication.pub, value)
+            elif publication.data_type == DataType.STRING:
+                helics.helicsPublicationPublishString(publication.pub, value)
+            elif publication.data_type == DataType.BOOLEAN:
+                helics.helicsPublicationPublishBoolean(publication.pub, value)
+            elif publication.data_type == DataType.INTEGER:
+                helics.helicsPublicationPublishInteger(publication.pub, value)
+            else:
+                raise ValueError("Unsupported data type forr teh HELICS interface")
+            logger.info(f"{publication} - {value}")
+        return
+
+    def request_time_increment(self):
+        error = sum([abs(sub.states[0] - sub.states[1]) for sub in self.subscriptions.subscriptions])
+        r_seconds = self._dss_solver.GetTotalSeconds() #- self._dss_solver.GetStepResolutionSeconds()
+        if not self._settings.helics.iterative_mode:
+            while self.c_seconds < r_seconds:
+                self.c_seconds = helics.helicsFederateRequestTime(self._federate, r_seconds)
+            logger.info('Time requested: {} - time granted: {} '.format(r_seconds, self.c_seconds))
+            return True, self.c_seconds
+        else:
+
+            self.c_seconds, iteration_state = helics.helicsFederateRequestTimeIterative(
+                self._federate,
+                r_seconds,
+                helics.helics_iteration_request_iterate_if_needed
+            )
+
+            logger.info('Time requested: {} - time granted: {} error: {} it: {}'.format(
+                r_seconds, self.c_seconds, error, self.itr))
+            if error > -1 and self.itr < self._co_convergance_max_iterations - 1:
+                self.itr += 1
+                return False, self.c_seconds
+            else:
+                self.itr = 0
+                return True, self.c_seconds
+
+    def __del__(self):
+        helics.helicsFederateDisconnect(self._federate)
+        state = helics.helicsFederateGetState(self._federate)
+        helics.helicsFederateInfoFree(self.fedinfo)
+        helics.helicsFederateFree(self._federate)
+        logger.info('HELICS federate for pydss destroyed')
```

### Comparing `nrel_pydss-3.1.3/src/pydss/metrics.py` & `nrel_pydss-3.1.4/src/pydss/metrics.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,1221 +1,1221 @@
-import abc
-import copy
-import os
-import shutil
-import tempfile
-from collections import namedtuple
-from datetime import timedelta
-from pathlib import Path
-
-from loguru import logger
-import pandas as pd
-import opendssdirect as dss
-
-from pydss.common import DataConversion, StoreValuesType
-from pydss.exceptions import InvalidConfiguration, InvalidParameter
-from pydss.reports.reports import ReportBase
-from pydss.storage_filters import STORAGE_TYPE_MAP, StorageFilterBase
-from pydss.value_storage import ValueByNumber, ValueStorageBase
-from pydss.node_voltage_metrics import NodeVoltageMetrics
-from pydss.simulation_input_models import SimulationSettingsModel
-from pydss.thermal_metrics import ThermalMetrics
-from pydss.utils.simulation_utils import get_start_time, get_simulation_resolution
-
-class MetricBase(abc.ABC):
-    """Base class for all metrics"""
-
-    def __init__(self, prop, dss_objs, settings: SimulationSettingsModel):
-        self._name = prop.name
-        self._base_path = None
-        self._hdf_store = None
-        self._max_chunk_bytes = settings.exports.hdf_max_chunk_bytes
-        self._num_steps = None
-        self._properties = {}  # StoreValuesType to ExportListProperty
-        self._dss_objs = dss_objs
-        self._name_to_dss_obj = {x.Name: x for x in dss_objs}
-        self._elem_class = _OPEN_DSS_CLASS_FOR_ITERATION.get(dss_objs[0]._Class)
-        self._settings = settings
-        self._are_names_filtered = prop.are_names_filtered
-
-        self.add_property(prop)
-
-    def add_property(self, prop):
-        """Add an instance of ExportListProperty for tracking."""
-        if prop.are_names_filtered != self._are_names_filtered:
-            raise InvalidConfiguration(f"All properties for shared elements must have the same filters: "
-                f"{self._elem_class.__name__} / {prop.name}.")
-        existing = self._properties.get(prop.store_values_type)
-        if existing is None:
-            self._properties[prop.store_values_type] = prop
-        elif prop != existing:
-            raise InvalidParameter(f"{prop.store_values_type} is already stored")
-
-    @abc.abstractmethod
-    def append_values(self, time_step, store_nan=False):
-        """Get the values for all elements at the current time step."""
-
-    def close(self):
-        """Perform any final writes to the container."""
-        for container in self.iter_containers():
-            if container is not None:
-                container.close()
-
-        # Write an empty dataset to the .h5 file if no data was collected.
-        for prop in self.iter_empty_containers():
-            assert not isinstance(self, SummedElementsOpenDssPropertyMetric)
-            path = f"{self._base_path}/{prop.elem_class}/ElementProperties/{prop.storage_name}"
-            self.make_empty_storage_container(path, prop)
-
-    def flush_data(self):
-        """Flush any data in memory to storage."""
-        for container in self.iter_containers():
-            container.flush_data()
-
-    def initialize_data_store(self, hdf_store, base_path, num_steps):
-        """Initialize data store values."""
-        self._hdf_store = hdf_store
-        self._base_path = base_path
-        self._num_steps = num_steps
-
-    @staticmethod
-    def is_circuit_wide():
-        """Return True if this metric should be used once for a circuit."""
-        return False
-
-    @abc.abstractmethod
-    def iter_containers(self):
-        """Return an iterator over the StorageFilterBase containers."""
-
-    def iter_empty_containers(self):
-        """Return an iterator over empty containers."""
-        return
-        yield
-
-    def label(self):
-        """Return a label for the metric.
-
-        Returns
-        -------
-        str
-
-        """
-        prop = next(iter(self._properties.values()))
-        return f"{prop.elem_class}.{prop.name}"
-
-    def _make_elem_names(self):
-        return [x.FullName for x in self._dss_objs]
-
-    def make_empty_storage_container(self, path, prop):
-        """Make an empty storage container."""
-        if prop.store_values_type not in STORAGE_TYPE_MAP:
-            raise InvalidConfiguration(f"unsupported {prop.store_values_type}")
-        elem_names = self._make_elem_names()
-        cls = STORAGE_TYPE_MAP[prop.store_values_type]
-        values = [ValueByNumber(x.FullName, self.label(), 0.0) for x in self._dss_objs]
-        container = cls(
-            self._hdf_store, path, prop, 1, self._max_chunk_bytes, values, elem_names
-        )
-        return container
-
-    def make_storage_container(
-        self, path, prop, num_steps, max_chunk_bytes, values, **kwargs
-    ):
-        """Make a storage container.
-
-        Returns
-        -------
-        StorageFilterBase
-
-        """
-        if prop.store_values_type not in STORAGE_TYPE_MAP:
-            raise InvalidConfiguration(f"unsupported {prop.store_values_type}")
-        elem_names = self._make_elem_names()
-        cls = STORAGE_TYPE_MAP[prop.store_values_type]
-        container = cls(
-            self._hdf_store,
-            path,
-            prop,
-            num_steps,
-            max_chunk_bytes,
-            values,
-            elem_names,
-            **kwargs,
-        )
-        return container
-
-    def max_num_bytes(self):
-        """Return the maximum number of bytes the containers could hold.
-
-        Returns
-        -------
-        int
-
-        """
-        total = 0
-        for container in self.iter_containers():
-            if container is not None:
-                total += container.max_num_bytes()
-        return total
-
-    def _can_use_native_iteration(self):
-        return self._elem_class is not None and not self._are_names_filtered
-
-
-class ChangeCountMetricBase(MetricBase, abc.ABC):
-    """Base class for any metric that only tracks number of changes."""
-
-    def __init__(self, prop, dss_objs, settings: SimulationSettingsModel):
-        super().__init__(prop, dss_objs, settings)
-        self._container = None
-        self._last_values = {x.FullName: None for x in dss_objs}
-        self._change_counts = {x.FullName: 0 for x in dss_objs}
-
-    def append_values(self, time_step, store_nan=False):
-        pass
-
-    def close(self):
-        assert len(self._properties) == 1
-        prop = next(iter(self._properties.values()))
-        path = (
-            f"{self._base_path}/{prop.elem_class}/ElementProperties/{prop.storage_name}"
-        )
-        values = [
-            ValueByNumber(x, prop.name, y) for x, y in self._change_counts.items()
-        ]
-        # This class creates an instance of ValueContainer directly because
-        # these metrics can only store one type, and so don't need an instance
-        # of StorageFilterBase.
-        self._container = StorageFilterBase.make_container(
-            self._hdf_store,
-            path,
-            prop,
-            self._num_steps,
-            self._max_chunk_bytes,
-            values,
-            [x.FullName for x in self._dss_objs],
-        )
-        self._container.append(values)
-        self._container.flush_data()
-
-    def iter_containers(self):
-        yield self._container
-
-
-class MultiValueTypeMetricBase(MetricBase, abc.ABC):
-    """Stores a property with multiple values of StoreValueType.
-
-    For example, a user might want to store a moving average as well as the
-    max of all instantaneous values.
-
-    """
-
-    def __init__(self, prop, dss_objs, settings):
-        super().__init__(prop, dss_objs, settings)
-        self._containers = {}  # StoreValuesType to StorageFilterBase
-        self._name_order = []  # Ensures that name-value ordering will always be consistent.
-
-    def _iter_dss_objs(self):
-        if self._can_use_native_iteration():
-            flag = self._elem_class.First()
-            while flag > 0:
-                name = self._elem_class.Name()
-                dss_obj = self._name_to_dss_obj[name]
-                yield dss_obj
-                flag = self._elem_class.Next()
-        else:
-            yield from self._dss_objs
-
-    @abc.abstractmethod
-    def _get_value(self, dss_obj, time_step):
-        """Get a value at the current time step.
-
-        Parameters
-        ----------
-        dss_obj : dssObjBase
-        time_step : int
-
-        """
-
-    def _initialize_containers(self, values):
-        prop_name = None
-        for prop in self._properties.values():
-            if prop_name is None:
-                prop_name = prop.name
-            else:
-                assert prop.name == prop_name, f"{prop.name} {prop_name}"
-            if prop.data_conversion != DataConversion.NONE:
-                vals = [
-                    convert_data(x, prop_name, y, prop.data_conversion)
-                    for x, y in zip(self._name_order, values)
-                ]
-            else:
-                vals = values
-            path = f"{self._base_path}/{prop.elem_class}/ElementProperties/{prop.storage_name}"
-            self._containers[prop.store_values_type] = self.make_storage_container(
-                path,
-                prop,
-                self._num_steps,
-                self._max_chunk_bytes,
-                vals,
-            )
-
-    def append_values(self, time_step, store_nan=False):
-        if not self._name_order:
-            self._name_order[:] = [x.FullName for x in self._iter_dss_objs()]
-    
-        #values = [self._get_value(x, time_step) for x in self._dss_objs]
-        values = []
-        objects_changed = False
-        for dss_obj, expected_name in zip(self._iter_dss_objs(), self._name_order):
-            if dss_obj.FullName != expected_name:
-                # This can happen if an element is disabled. OpenDSS won't deliver it in .Next()
-                # Need to access the element directly, which breaks the iteration.
-                objects_changed = True
-                break
-            values.append(self._get_value(dss_obj, time_step))
-
-        if objects_changed or not values:
-            values = [self._get_value(x, time_step) for x in self._dss_objs]
-
-        assert len(values) == len(self._dss_objs)
-
-        if not self._containers:
-            self._initialize_containers(values)
-
-        if store_nan:
-            for val in values:
-                val.set_nan()
-                
-        for value_type, container in self._containers.items():
-            prop = self._properties[value_type]
-            if prop.data_conversion != DataConversion.NONE:
-                assert len(self._name_order) == len(values)
-                vals = [
-                    convert_data(x, prop.name, y, prop.data_conversion)
-                    for x, y in zip(self._name_order, values)
-                ]
-            else:
-                vals = values
-            container.append_values(vals, time_step)
-
-        return vals
-
-    def iter_containers(self):
-        return self._containers.values()
-
-    def iter_empty_containers(self):
-        for prop in self._properties.values():
-            if prop.store_values_type not in self._containers:
-                yield prop
-
-
-class OpenDssPropertyMetric(MultiValueTypeMetricBase):
-    """Stores metrics for any OpenDSS element property."""
-
-    def _get_value(self, dss_obj, _time_step):
-        return dss_obj.UpdateValue(self._name)
-
-    def append_values(self, time_step, store_nan=False):
-        curr_data = {}
-        values = super().append_values(time_step, store_nan=store_nan)
-        for _, value in zip(self._dss_objs, values):
-            if len(value.make_columns()) > 1:
-                for column, val in zip(value.make_columns(), value.value):
-                    curr_data[column] = val
-            else:
-                curr_data[value.make_columns()[0]] = value.value
-
-        return curr_data
-
-
-# These next two might work but are untested.
-
-#class LineLoadingPercent(MultiValueTypeMetricBase):
-#    """Calculates line loading percent at every time point."""
-#
-#    def __init__(self, prop, dss_objs, settings):
-#        super().__init__(prop, dss_objs, settings)
-#        self._normal_amps = {}  # Name to normal_amps value
-#
-#    def _get_value(self, dss_obj, _time_step):
-#        line = dss_obj
-#        normal_amps = self._normal_amps.get(line.Name)
-#        if normal_amps is None:
-#            normal_amps = line.GetValue("NormalAmps", convert=True).value
-#            self._normal_amps[line.Name] = normal_amps
-#
-#        currents = line.UpdateValue("Currents").value
-#        current = max([abs(x) for x in currents])
-#        loading = current / normal_amps * 100
-#        return ValueByNumber(line.Name, "LineLoading", loading)
-#
-#
-#class TransformerLoadingPercent(MultiValueTypeMetricBase):
-#    """Calculates transformer loading percent at every time point."""
-#
-#    def __init__(self, prop, dss_objs, settings):
-#        super().__init__(prop, dss_objs, settings)
-#        self._normal_amps = {}  # Name to normal_amps value
-#
-#    def _get_value(self, dss_obj, _time_step):
-#        transformer = dss_obj
-#        normal_amps = self._normal_amps.get(transformer.Name)
-#        if normal_amps is None:
-#            normal_amps = transformer.GetValue("NormalAmps", convert=True).value
-#            self._normal_amps[transformer.Name] = normal_amps
-#
-#        currents = transformer.UpdateValue("Currents").value
-#        current = max([abs(x) for x in currents])
-#        loading = current / normal_amps * 100
-#        return ValueByNumber(transformer.Name, "TransformerLoading", loading)
-
-
-FeederHeadValues = namedtuple("FeederHeadValues", ["load_kvar", "load_kw", "loading", "reverse_power_flow"])
-
-
-class FeederHeadMetrics(MetricBase):
-    """Calculates loading at the feeder head at each time point"""
-
-    def __init__(self, prop, dss_objs, settings):
-        super().__init__(prop, dss_objs, settings)
-        # dss_objs contains the Circuit, but we won't use it.
-        assert len(dss_objs) == 1, dss_objs
-        self._prop = prop
-        self._containers = {}
-        self._feeder_head_line = None
-        self._values = {}
-
-    def _initialize_containers(self):
-        assert len(self._properties) == 1, self._properties
-        self._feeder_head_line = self._find_feeder_head_line()
-        values = self._get_values()
-        self._values = {
-            "load_kvar": ValueByNumber("FeederHead", "load_kvar", values.load_kvar),
-            "load_kw": ValueByNumber("FeederHead", "load_kw", values.load_kw),
-            "loading": ValueByNumber("FeederHead", "loading", values.loading),
-            "reverse_power_flow": ValueByNumber("FeederHead", "reverse_power_flow", values.reverse_power_flow),
-        }
-
-        for name in self._values:
-            path = f"{self._base_path}/{self._prop.elem_class}/ElementProperties/{name}"
-            self._containers[name] = self.make_storage_container(
-                path,
-                self._prop,
-                self._num_steps,
-                self._max_chunk_bytes,
-                [self._values[name]],
-            )
-
-    @staticmethod
-    def is_circuit_wide():
-        return True
-
-    @staticmethod
-    def _find_feeder_head_line():
-        feeder_head_line = None
-        flag = dss.Topology.First()
-        while flag > 0:
-            if "line" in dss.Topology.BranchName().lower():
-                feeder_head_line = dss.Topology.BranchName()
-                break
-            flag = dss.Topology.Next()
-
-        assert feeder_head_line is not None
-        return feeder_head_line
-
-    def _get_values(self):
-        total_power = dss.Circuit.TotalPower()
-        feeder_head_values = FeederHeadValues(
-            load_kvar=total_power[1],
-            load_kw=total_power[0],
-            loading=self._get_feeder_head_loading(),
-            reverse_power_flow=self._reverse_power_flow(),
-        )
-        return feeder_head_values
-
-    def _get_feeder_head_loading(self):
-        flag = dss.Circuit.SetActiveElement(self._feeder_head_line)
-        if not flag > 0:
-            raise Exception("Failed to set the feeder head line")
-        n_phases = dss.CktElement.NumPhases()
-        max_amps = dss.CktElement.NormalAmps()
-        currents = dss.CktElement.CurrentsMagAng()[:2*n_phases]
-        current_magnitude = currents[::2]
-
-        max_flow = max(max(current_magnitude), 1e-10)
-        loading = max_flow / max_amps
-        return loading
-
-    @staticmethod
-    def _reverse_power_flow():
-        # total substation power is an injection(-) or a consumption(+)
-        reverse_pf = dss.Circuit.TotalPower()[0] > 0
-        # Storing NaN with bools is not working correctly.
-        return int(reverse_pf)
-
-    def append_values(self, time_step, store_nan=False):
-        if not self._containers:
-            self._initialize_containers()
-
-        if store_nan:
-            for val in self._values.values():
-                val.set_nan()
-        else:
-            values = self._get_values()
-            for name, value in self._values.items():
-                value.set_value(getattr(values, name))
-
-        vals = []
-        for name, container in self._containers.items():
-            value = self._values[name]
-            vals.append(value)
-            container.append_values([value], time_step)
-
-        return vals
-
-    def iter_containers(self):
-        for container in self._containers.values():
-            yield container
-
-
-class SummedElementsOpenDssPropertyMetric(MetricBase):
-    """Sums all elements' values for a given property at each time point."""
-
-    def __init__(self, prop, dss_objs, settings):
-        super().__init__(prop, dss_objs, settings)
-        self._container = None
-        self._data_conversion = prop.data_conversion
-
-    def _get_value(self, obj):
-        value = obj.UpdateValue(self._name)
-        if self._data_conversion != DataConversion.NONE:
-            value = convert_data(
-                "Total",
-                next(iter(self._properties.values())).name,
-                value,
-                self._data_conversion,
-            )
-        return value
-
-    def append_values(self, time_step, store_nan=False):
-        if store_nan:
-            if self._can_use_native_iteration():
-                self._elem_class.First()
-            total = self._get_value(self._dss_objs[0])
-            total.set_nan()
-        else:
-            total = None
-            if self._can_use_native_iteration():
-                iterations = 0
-                flag = self._elem_class.First()
-                while flag > 0:
-                    dss_obj = self._name_to_dss_obj[self._elem_class.Name()]
-                    value = self._get_value(dss_obj)
-                    if total is None:
-                        total = value
-                    else:
-                        total += value
-                    iterations += 1
-                    flag = self._elem_class.Next()
-                assert iterations == len(self._dss_objs)
-            else:
-                for dss_obj in self._dss_objs:
-                    value = self._get_value(dss_obj)
-                    if total is None:
-                        total = value
-                    else:
-                        total += value
-
-        if self._container is None:
-            assert len(self._properties) == 1
-            prop = next(iter(self._properties.values()))
-            assert prop.store_values_type in (StoreValuesType.ALL, StoreValuesType.SUM)
-            total.set_name("Total")
-            path = f"{self._base_path}/{prop.elem_class}/SummedElementProperties/{prop.storage_name}"
-            self._container = self.make_storage_container(
-                path,
-                prop,
-                self._num_steps,
-                self._max_chunk_bytes,
-                [total],
-            )
-        self._container.append_values([total], time_step)
-
-    @staticmethod
-    def is_circuit_wide():
-        return True
-
-    def iter_containers(self):
-        yield self._container
-
-
-class SummedElementsByGroupOpenDssPropertyMetric(MetricBase):
-    """Sums all elements' values for a given property at each time point.
-    Elements are separated into groups by name.
-
-    """
-    def __init__(self, prop, dss_objs, settings):
-        super().__init__(prop, dss_objs, settings)
-        self._containers = {}
-        self._name_to_group = {}
-
-        # This allows names to be in the group that aren't in the circuit
-        # in order to reduce having to duplicate the sum_group files many times
-        # in cases where there are many projects/scenarios.
-        elements = {x.Name for x in dss_objs}
-        for group in prop.sum_groups:
-            group_elems = elements.intersection(set(group["elements"]))
-            if group_elems:
-                self._containers[group["name"]] = None
-                for element_name in group_elems:
-                    self._name_to_group[element_name] = group["name"]
-
-        self._data_conversion = prop.data_conversion
-
-    def _get_value(self, obj):
-        value = obj.UpdateValue(self._name)
-        if self._data_conversion != DataConversion.NONE:
-            value = convert_data(
-                "Total",
-                next(iter(self._properties.values())).name,
-                value,
-                self._data_conversion,
-            )
-        return value
-
-    def append_values(self, time_step, store_nan=False):
-        total_by_group = {x: None for x in self._containers}
-        if store_nan:
-            for group in self._containers:
-                if self._can_use_native_iteration():
-                    self._elem_class.First()
-                total_by_group[group] = self._get_value(self._dss_objs[0])
-                total_by_group[group].set_nan()
-        else:
-            if self._can_use_native_iteration():
-                iterations = 0
-                flag = self._elem_class.First()
-                while flag > 0:
-                    name = self._elem_class.Name()
-                    group = self._name_to_group[name]
-                    dss_obj = self._name_to_dss_obj[name]
-                    value = self._get_value(dss_obj)
-                    if total_by_group[group] is None:
-                        total_by_group[group] = value
-                    else:
-                        total_by_group[group] += value
-                    iterations += 1
-                    flag = self._elem_class.Next()
-                assert iterations == len(self._dss_objs)
-            else:
-                for dss_obj in self._dss_objs:
-                    value = self._get_value(dss_obj)
-                    if total_by_group[group] is None:
-                        total_by_group[group] = value
-                    else:
-                        total_by_group[group] += value
-
-        if next(iter(self._containers.values())) is None:
-            assert len(self._properties) == 1
-            prop = next(iter(self._properties.values()))
-            assert prop.store_values_type in (StoreValuesType.ALL, StoreValuesType.SUM)
-            for group in self._containers:
-                total_by_group[group].set_name("Total")
-                key = ValueStorageBase.DELIMITER.join((prop.storage_name, group))
-                path = f"{self._base_path}/{prop.elem_class}/SummedElementProperties/{key}"
-                self._containers[group] = self.make_storage_container(
-                    path,
-                    prop,
-                    self._num_steps,
-                    self._max_chunk_bytes,
-                    [total_by_group[group]],
-                )
-        for group in self._containers:
-            self._containers[group].append_values([total_by_group[group]], time_step)
-
-    @staticmethod
-    def is_circuit_wide():
-        return True
-
-    def iter_containers(self):
-        return self._containers.values()
-
-
-class NodeVoltageMetric(MetricBase):
-    """Stores metrics for node voltages."""
-
-    PRIMARY_BUS_THRESHOLD_KV = 1.0
-
-    def __init__(self, prop, dss_obj, settings):
-        super().__init__(prop, dss_obj, settings)
-        props = list(self._properties)
-        assert len(props) == 1
-        self._prop = props[0]
-        # Indices for node names are tied to indices for node voltages.
-        self._node_names = None
-        self._voltages = None
-        start_time = get_start_time(settings)
-        sim_resolution = get_simulation_resolution(settings)
-        inputs = ReportBase.get_inputs_from_defaults(settings, "Voltage Metrics")
-        window_size = max(1, int(
-            timedelta(minutes=inputs["window_size_minutes"]) / sim_resolution
-        ))
-        self._voltage_metrics = NodeVoltageMetrics(
-            prop, start_time, sim_resolution, window_size, inputs["store_per_element_data"]
-        )
-        self._primary_node_names = []
-        self._primary_indices = []
-        self._secondary_node_names = []
-        self._secondary_indices = []
-
-    def _make_elem_names(self):
-        return self._node_names
-
-    def _identify_primary_v_secondary(self):
-        for i, name in enumerate(self._node_names):
-            dss.Circuit.SetActiveBus(name)
-            kv_base = dss.Bus.kVBase()
-            if kv_base > self.PRIMARY_BUS_THRESHOLD_KV:
-                self._primary_node_names.append(name)
-                self._primary_indices.append(i)
-            else:
-                self._secondary_node_names.append(name)
-                self._secondary_indices.append(i)
-
-    def append_values(self, time_step, store_nan=False):
-        voltages = dss.Circuit.AllBusMagPu()
-        if self._voltages is None:
-            # TODO: limit to objects that have been added
-            self._node_names = dss.Circuit.AllNodeNames()
-            self._identify_primary_v_secondary()
-            self._voltages = [
-                ValueByNumber(x, "Voltage", y)
-                for x, y in zip(self._node_names, voltages)
-            ]
-            self._voltage_metrics.set_node_info(
-                self._primary_node_names,
-                self._primary_indices,
-                self._secondary_node_names,
-                self._secondary_indices,
-            )
-        else:
-            for i, voltage in enumerate(voltages):
-                self._voltages[i].set_value_from_raw(voltage)
-
-        if not store_nan:
-            self._voltage_metrics.update(time_step, self._voltages)
-
-        self._voltage_metrics.increment_steps()
-
-    def close(self):
-        path = os.path.join(
-            str(self._settings.project.active_project_path),
-            "Exports",
-            self._settings.project.active_scenario,
-        )
-        self._voltage_metrics.generate_report(path)
-
-    @staticmethod
-    def is_circuit_wide():
-        return True
-
-    def iter_containers(self):
-        return
-        yield
-
-
-class TrackCapacitorChangeCounts(ChangeCountMetricBase):
-    """Store the number of changes for a capacitor."""
-
-    def append_values(self, _time_step, store_nan=False):
-        if store_nan:
-            return
-
-        iterations = 0
-        flag = dss.Capacitors.First()
-        while flag > 0:
-            capacitor = self._name_to_dss_obj[dss.Capacitors.Name()]
-            self._update_counts(capacitor)
-            iterations += 1
-            flag = dss.Capacitors.Next()
-        assert iterations == len(self._dss_objs)
-
-    def _update_counts(self, capacitor):
-        states = dss.Capacitors.States()
-        if states == -1:
-            raise Exception(f"failed to get Capacitors.States() for {capacitor.Name}")
-
-        if len(states) != 1:
-            raise Exception(f"length of states greater than 1 is not supported: {states}")
-        cur_value = sum(states)
-        last_value = self._last_values[capacitor.FullName]
-        if last_value is not None and cur_value != last_value:
-            logger.debug(
-                "%s changed state old=%s new=%s", capacitor.Name, last_value, cur_value
-            )
-            self._change_counts[capacitor.FullName] += 1
-
-        self._last_values[capacitor.FullName] = cur_value
-
-
-class TrackRegControlTapNumberChanges(ChangeCountMetricBase):
-    """Store the number of tap number changes for a RegControl."""
-
-    def append_values(self, _time_step, store_nan=False):
-        if store_nan:
-            return
-
-        iterations = 0
-        flag = dss.RegControls.First()
-        while flag > 0:
-            reg_control = self._name_to_dss_obj[dss.RegControls.Name()]
-            self._update_counts(reg_control)
-            iterations += 1
-            flag = dss.RegControls.Next()
-        assert iterations == len(self._dss_objs)
-
-    def _update_counts(self, reg_control):
-        tap_number = dss.RegControls.TapNumber()
-        last_value = self._last_values[reg_control.FullName]
-        if last_value is not None and last_value != tap_number:
-            self._change_counts[reg_control.FullName] += 1
-            logger.debug(
-                "%s changed count from %s to %s count=%s",
-                reg_control.Name,
-                last_value,
-                tap_number,
-                self._change_counts[reg_control.FullName],
-            )
-
-        self._last_values[reg_control.FullName] = tap_number
-
-
-class OpenDssExportMetric(MetricBase):
-    def __init__(self, prop, dss_objs, settings):
-        super().__init__(prop, dss_objs, settings)
-        self._tmp_dir = tempfile.mkdtemp()
-        self._containers = {}
-        self._sum_elements = prop.sum_elements
-        if self._sum_elements:
-            self._append_func = self._append_summed_values
-        else:
-            self._append_func = self._append_values
-        self._check_output()
-
-        # Some OpenDSS files upper-case the name.
-        # Make a mapping for fast matching and lookup.
-        self._names = {}
-        self._values = []
-        for i, dss_obj in enumerate(dss_objs):
-            elem_type, name = dss_obj.FullName.split(".")
-            if self.requires_upper_case():
-                self._names[f"{elem_type}.{name.upper()}"] = i
-            else:
-                self._names[f"{elem_type}.{name}"] = i
-            if self._sum_elements and not self._values:
-                self._values.append(ValueByNumber("Total", self.label(), 0.0))
-                break
-            self._values.append(ValueByNumber(dss_obj.FullName, self.label(), 0.0))
-
-    def __del__(self):
-        shutil.rmtree(self._tmp_dir)
-
-    def _run_command(self):
-        cmd = f"{self.export_command()}"
-        result = dss.utils.run_command(cmd)
-        if not result:
-            raise Exception(f"{cmd} failed")
-        return result
-
-    def append_values(self, time_step, store_nan=False):
-        filename = self._run_command()
-        self.parse_file(filename)
-        self._append_func(time_step, store_nan=store_nan)
-
-    def _append_values(self, time_step, store_nan=False):
-        if not self._containers:
-            for prop in self._properties.values():
-                if prop.window_sizes and prop.is_moving_average():
-                    # This is somewhat ugly code to allow different
-                    # sizes for different element types collected in the same
-                    # OpenDSS report.
-                    window_sizes = self._get_window_size_by_name_index(prop)
-                else:
-                    window_sizes = None
-                path = f"{self._base_path}/{self.element_class()}/ElementProperties/{prop.storage_name}"
-                self._containers[prop.store_values_type] = self.make_storage_container(
-                    path,
-                    prop,
-                    self._num_steps,
-                    self._max_chunk_bytes,
-                    self._values,
-                    window_sizes=window_sizes,
-                )
-
-        if store_nan:
-            for i in range(len(self._values)):
-                self._values[i].set_nan()
-
-        for sv_type, prop in self._properties.items():
-            self._containers[sv_type].append_values(self._values, time_step)
-
-    def _append_summed_values(self, time_step, store_nan=False):
-        if store_nan:
-            return
-
-        self._values[0].set_value(sum([x.value for x in self._values]))
-
-        prop = next(iter(self._properties.values()))
-        if not self._containers:
-            if len(self._properties) > 1:
-                raise InvalidConfiguration(
-                    "summing elements only supports one Property"
-                )
-            assert len(self._properties) == 1
-            assert prop.store_values_type in (StoreValuesType.ALL, StoreValuesType.SUM)
-            path = f"{self._base_path}/{prop.elem_class}/SummedElementProperties/{prop.storage_name}"
-            self._containers[prop.store_values_type] = self.make_storage_container(
-                path,
-                prop,
-                self._num_steps,
-                self._max_chunk_bytes,
-                self._values,
-            )
-
-        self._containers[prop.store_values_type].append_values(self._values, time_step)
-
-    def _check_output(self):
-        filename = self._run_command()
-        df = pd.read_csv(filename)
-        for index, val in self.expected_column_headers().items():
-            if df.columns[index].strip() != val:
-                raise Exception(
-                    f"Unexpected format in export file file: {index} {val} {df.columns}"
-                )
-
-    @staticmethod
-    def _get_name_from_line(fields):
-        return fields[0].strip()[1:-1]
-
-    def _get_window_size_by_name_index(self, prop):
-        """Returns a list of window sizes per element name corresponding to self._names."""
-        if not prop.opendss_classes:
-            raise InvalidConfiguration(
-                f"window_sizes requires opendss_classes: {prop.name}"
-            )
-
-        window_sizes = [None] * len(self._names)
-        for opendss_class, window_size in prop.window_sizes.items():
-            if opendss_class not in prop.opendss_classes:
-                raise InvalidConfiguration(
-                    f"{opendss_class} is not defined in opendss_classes: {prop.name}"
-                )
-
-        # Note: names have singluar class names, such as Line.line1.
-        # opendss_classes are plural, such as Lines
-        mapping = {}
-        for i, name in enumerate(self._names):
-            opendss_class_singular = name.split(".")[0]
-            size = mapping.get(opendss_class_singular)
-            if size is None:
-                for opendss_class in prop.opendss_classes:
-                    if opendss_class.startswith(opendss_class_singular):
-                        size = prop.window_sizes[opendss_class]
-                        mapping[opendss_class_singular] = size
-            if size is None:
-                raise InvalidConfiguration(f"Failed to find window_size for {name}")
-            window_sizes[i] = size
-
-        return window_sizes
-
-    @staticmethod
-    @abc.abstractmethod
-    def element_class():
-        """Return the element class."""
-
-    @staticmethod
-    @abc.abstractmethod
-    def expected_column_headers():
-        """Return the expected column headers in the CSV file."""
-
-    @abc.abstractmethod
-    def export_command(self):
-        """Return the command to run in OpenDSS."""
-
-    @staticmethod
-    @abc.abstractmethod
-    def label():
-        """Return the label to use in dataframe columns."""
-
-    @staticmethod
-    def is_circuit_wide():
-        return True
-
-    def iter_containers(self):
-        for sv_type in self._properties:
-            if sv_type in self._containers:
-                yield self._containers[sv_type]
-
-    def iter_empty_containers(self):
-        for prop in self._properties.values():
-            if prop.store_values_type not in self._containers:
-                yield prop
-
-    @abc.abstractmethod
-    def parse_file(self, filename):
-        """Parse data in filename."""
-
-    @staticmethod
-    @abc.abstractmethod
-    def requires_upper_case():
-        """Return True if the names are upper case."""
-
-
-class ExportLoadingsMetric(OpenDssExportMetric):
-    """Stores line and transformer loading percentages in HDF5."""
-    @staticmethod
-    def element_class():
-        return "CktElement"
-
-    @staticmethod
-    def expected_column_headers():
-        return {0: "Name", 2: "%normal"}
-
-    def export_command(self):
-        filename = Path(self._tmp_dir) / "opendss_loading.csv"
-        return f"export capacity {filename}"
-
-    @staticmethod
-    def label():
-        return "Loading"
-
-    @staticmethod
-    def _get_name_from_line(fields):
-        return fields[0].strip()
-
-    def parse_file(self, filename):
-        count = 0
-        with open(filename) as f_in:
-            # Skip the header.
-            next(f_in)
-            for line in f_in:
-                fields = line.split(",")
-                name = self._get_name_from_line(fields)
-                if name in self._names:
-                    index = self._names[name]
-                    val = float(fields[2].strip())
-                    self._values[index].set_value_from_raw(val)
-                    count += 1
-                else:
-                    # There may be other element types that are not being tracked.
-                    continue
-        assert count == len(self._names), f"count={count} num_names={len(self._names)}"
-
-    @staticmethod
-    def requires_upper_case():
-        return True
-
-
-class ExportPowersMetric(OpenDssExportMetric):
-    """Stores power values in HDF5."""
-    @staticmethod
-    def element_class():
-        return "CktElement"
-
-    def export_command(self):
-        filename = Path(self._tmp_dir) / "opendss_powers.csv"
-        return f"export powers {filename}"
-
-    @staticmethod
-    def expected_column_headers():
-        return {0: "Element", 1: "Terminal", 2: "P(kW)"}
-
-    @staticmethod
-    def label():
-        return "Powers"
-
-    def parse_file(self, filename):
-        data = {}
-        with open(filename) as f_in:
-            # Skip the header.
-            next(f_in)
-            for line in f_in:
-                fields = line.split(",")
-                name = self._get_name_from_line(fields)
-                if name in self._names:
-                    terminal = int(fields[1])
-                    if terminal == 1:
-                        val = abs(float(fields[2].strip()))
-                        data[name] = val
-
-        for name, val in data.items():
-            index = self._names[name]
-            self._values[index].set_value_from_raw(val)
-
-    @staticmethod
-    def requires_upper_case():
-        return True
-
-
-class OverloadsMetricInMemory(OpenDssExportMetric):
-    """Stores line and transformer loading percentages in memory."""
-    def __init__(self, prop, dss_objs, settings):
-        super().__init__(prop, dss_objs, settings)
-        # Indices for node names are tied to indices for node voltages.
-        self._transformer_index = None
-        self._discovered_elements = False
-        start_time = get_start_time(settings)
-        sim_resolution = get_simulation_resolution(settings)
-        inputs = ReportBase.get_inputs_from_defaults(settings, "Thermal Metrics")
-        line_window_size, transformer_window_size = self._get_window_sizes(inputs, sim_resolution)
-        self._thermal_metrics = ThermalMetrics(
-            prop,
-            start_time,
-            sim_resolution,
-            line_window_size_hours=inputs["line_window_size_hours"],
-            line_window_size=line_window_size,
-            transformer_window_size_hours=inputs["transformer_window_size_hours"],
-            transformer_window_size=transformer_window_size,
-            line_loading_percent_threshold=inputs["line_loading_percent_threshold"],
-            line_loading_percent_moving_average_threshold=inputs["line_loading_percent_moving_average_threshold"],
-            transformer_loading_percent_threshold=inputs["transformer_loading_percent_threshold"],
-            transformer_loading_percent_moving_average_threshold=inputs["transformer_loading_percent_moving_average_threshold"],
-            store_per_element_data=inputs["store_per_element_data"],
-        )
-
-    def _append_values(self, time_step, store_nan=False):
-        if not self._discovered_elements:
-            line_names = []
-            transformer_names = []
-            for i, val in enumerate(self._values):
-                if val.name.startswith("Line"):
-                    line_names.append(val.name)
-                elif val.name.startswith("Transformer"):
-                    assert i != 0
-                    transformer_names.append(val.name)
-                    if self._transformer_index is None:
-                        self._transformer_index = i
-                else:
-                    assert False, val.name
-            self._thermal_metrics.line_names = line_names
-            self._thermal_metrics.transformer_names = transformer_names
-            self._discovered_elements = True
-
-        if self._transformer_index is None:
-            # There are no transformers.
-            line_loadings = self._values[:]
-            transformer_loadings = []
-        else:
-            line_loadings = self._values[:self._transformer_index]
-            transformer_loadings = self._values[self._transformer_index:]
-
-        if not store_nan:
-            self._thermal_metrics.update(time_step, line_loadings, transformer_loadings)
-
-        self._thermal_metrics.increment_steps()
-
-    @staticmethod
-    def _get_window_sizes(inputs, resolution):
-        line_window_size = timedelta(hours=inputs["line_window_size_hours"])
-        if line_window_size % resolution != timedelta(0):
-            raise InvalidConfiguration(
-                f"line_window_size={line_window_size} must be a multiple of {resolution}"
-            )
-        transformer_window_size = timedelta(hours=inputs["transformer_window_size_hours"])
-        if transformer_window_size % resolution != timedelta(0):
-            raise InvalidConfiguration(
-                f"transformer_window_size={transformer_window_size} must be a multiple of {resolution}"
-            )
-        line_window_size = int(line_window_size / resolution)
-        transformer_window_size = int(transformer_window_size / resolution)
-        return line_window_size, transformer_window_size
-
-    def close(self):
-        path = os.path.join(
-            str(self._settings.project.active_project_path),
-            "Exports",
-            self._settings.project.active_scenario,
-        )
-        self._thermal_metrics.generate_report(path)
-
-    @staticmethod
-    def element_class():
-        return "CktElement"
-
-    @staticmethod
-    def expected_column_headers():
-        return {0: "Name", 2: "%normal"}
-
-    def export_command(self):
-        filename = Path(self._tmp_dir) / "opendss_capacity.csv"
-        return f"export capacity {filename}"
-
-    @staticmethod
-    def _get_name_from_line(fields):
-        return fields[0].strip()
-
-    @staticmethod
-    def label():
-        return "Overloads"
-
-    def parse_file(self, filename):
-        with open(filename) as f_in:
-            # Skip the header.
-            next(f_in)
-            for line in f_in:
-                fields = line.split(",")
-                name = self._get_name_from_line(fields)
-                if name in self._names:
-                    index = self._names[name]
-                    val = float(fields[2].strip())
-                    self._values[index].set_value_from_raw(val)
-
-    @staticmethod
-    def requires_upper_case():
-        return True
-
-
-def convert_data(name, prop_name, value, conversion):
-    if conversion == DataConversion.ABS:
-        converted = copy.deepcopy(value)
-        if isinstance(value.value, list):
-            converted.set_value([abs(x) for x in value.value])
-        else:
-            converted.set_value(abs(value.value))
-    elif conversion == DataConversion.SUM:
-        converted = ValueByNumber(name, prop_name, sum(value.value))
-    elif conversion == DataConversion.ABS_SUM:
-        converted = ValueByNumber(name, prop_name, abs(sum(value.value)))
-    elif conversion == DataConversion.SUM_REAL:
-        converted = ValueByNumber(
-            name, prop_name, sum((x.real for x in value.value))
-        )
-    elif conversion == DataConversion.SUM_ABS_REAL:
-        converted = ValueByNumber(
-            name, prop_name, sum((abs(x.real) for x in value.value))
-        )
-    else:
-        converted = value
-
-    return converted
-
-
-# Bus and Circuit are excluded.
-_OPEN_DSS_CLASS_FOR_ITERATION = {
-    "Capacitor": dss.Capacitors,
-    "Fuse": dss.Fuses,
-    "Generator": dss.Generators,
-    "Isource": dss.Isource,
-    "Line": dss.Lines,
-    "Load": dss.Loads,
-    "Meter": dss.Meters,
-    "Monitor": dss.Monitors,
-    "PVSystem": dss.PVsystems,
-    "Recloser": dss.Reclosers,
-    "RegControl": dss.RegControls,
-    "Relay": dss.Relays,
-    "Sensor": dss.Sensors,
-    "Transformer": dss.Transformers,
-    "Vsource": dss.Vsources,
-    "XYCurve": dss.XYCurves,
-}
+import abc
+import copy
+import os
+import shutil
+import tempfile
+from collections import namedtuple
+from datetime import timedelta
+from pathlib import Path
+
+from loguru import logger
+import pandas as pd
+import opendssdirect as dss
+
+from pydss.common import DataConversion, StoreValuesType
+from pydss.exceptions import InvalidConfiguration, InvalidParameter
+from pydss.reports.reports import ReportBase
+from pydss.storage_filters import STORAGE_TYPE_MAP, StorageFilterBase
+from pydss.value_storage import ValueByNumber, ValueStorageBase
+from pydss.node_voltage_metrics import NodeVoltageMetrics
+from pydss.simulation_input_models import SimulationSettingsModel
+from pydss.thermal_metrics import ThermalMetrics
+from pydss.utils.simulation_utils import get_start_time, get_simulation_resolution
+
+class MetricBase(abc.ABC):
+    """Base class for all metrics"""
+
+    def __init__(self, prop, dss_objs, settings: SimulationSettingsModel):
+        self._name = prop.name
+        self._base_path = None
+        self._hdf_store = None
+        self._max_chunk_bytes = settings.exports.hdf_max_chunk_bytes
+        self._num_steps = None
+        self._properties = {}  # StoreValuesType to ExportListProperty
+        self._dss_objs = dss_objs
+        self._name_to_dss_obj = {x.Name: x for x in dss_objs}
+        self._elem_class = _OPEN_DSS_CLASS_FOR_ITERATION.get(dss_objs[0]._Class)
+        self._settings = settings
+        self._are_names_filtered = prop.are_names_filtered
+
+        self.add_property(prop)
+
+    def add_property(self, prop):
+        """Add an instance of ExportListProperty for tracking."""
+        if prop.are_names_filtered != self._are_names_filtered:
+            raise InvalidConfiguration(f"All properties for shared elements must have the same filters: "
+                f"{self._elem_class.__name__} / {prop.name}.")
+        existing = self._properties.get(prop.store_values_type)
+        if existing is None:
+            self._properties[prop.store_values_type] = prop
+        elif prop != existing:
+            raise InvalidParameter(f"{prop.store_values_type} is already stored")
+
+    @abc.abstractmethod
+    def append_values(self, time_step, store_nan=False):
+        """Get the values for all elements at the current time step."""
+
+    def close(self):
+        """Perform any final writes to the container."""
+        for container in self.iter_containers():
+            if container is not None:
+                container.close()
+
+        # Write an empty dataset to the .h5 file if no data was collected.
+        for prop in self.iter_empty_containers():
+            assert not isinstance(self, SummedElementsOpenDssPropertyMetric)
+            path = f"{self._base_path}/{prop.elem_class}/ElementProperties/{prop.storage_name}"
+            self.make_empty_storage_container(path, prop)
+
+    def flush_data(self):
+        """Flush any data in memory to storage."""
+        for container in self.iter_containers():
+            container.flush_data()
+
+    def initialize_data_store(self, hdf_store, base_path, num_steps):
+        """Initialize data store values."""
+        self._hdf_store = hdf_store
+        self._base_path = base_path
+        self._num_steps = num_steps
+
+    @staticmethod
+    def is_circuit_wide():
+        """Return True if this metric should be used once for a circuit."""
+        return False
+
+    @abc.abstractmethod
+    def iter_containers(self):
+        """Return an iterator over the StorageFilterBase containers."""
+
+    def iter_empty_containers(self):
+        """Return an iterator over empty containers."""
+        return
+        yield
+
+    def label(self):
+        """Return a label for the metric.
+
+        Returns
+        -------
+        str
+
+        """
+        prop = next(iter(self._properties.values()))
+        return f"{prop.elem_class}.{prop.name}"
+
+    def _make_elem_names(self):
+        return [x.FullName for x in self._dss_objs]
+
+    def make_empty_storage_container(self, path, prop):
+        """Make an empty storage container."""
+        if prop.store_values_type not in STORAGE_TYPE_MAP:
+            raise InvalidConfiguration(f"unsupported {prop.store_values_type}")
+        elem_names = self._make_elem_names()
+        cls = STORAGE_TYPE_MAP[prop.store_values_type]
+        values = [ValueByNumber(x.FullName, self.label(), 0.0) for x in self._dss_objs]
+        container = cls(
+            self._hdf_store, path, prop, 1, self._max_chunk_bytes, values, elem_names
+        )
+        return container
+
+    def make_storage_container(
+        self, path, prop, num_steps, max_chunk_bytes, values, **kwargs
+    ):
+        """Make a storage container.
+
+        Returns
+        -------
+        StorageFilterBase
+
+        """
+        if prop.store_values_type not in STORAGE_TYPE_MAP:
+            raise InvalidConfiguration(f"unsupported {prop.store_values_type}")
+        elem_names = self._make_elem_names()
+        cls = STORAGE_TYPE_MAP[prop.store_values_type]
+        container = cls(
+            self._hdf_store,
+            path,
+            prop,
+            num_steps,
+            max_chunk_bytes,
+            values,
+            elem_names,
+            **kwargs,
+        )
+        return container
+
+    def max_num_bytes(self):
+        """Return the maximum number of bytes the containers could hold.
+
+        Returns
+        -------
+        int
+
+        """
+        total = 0
+        for container in self.iter_containers():
+            if container is not None:
+                total += container.max_num_bytes()
+        return total
+
+    def _can_use_native_iteration(self):
+        return self._elem_class is not None and not self._are_names_filtered
+
+
+class ChangeCountMetricBase(MetricBase, abc.ABC):
+    """Base class for any metric that only tracks number of changes."""
+
+    def __init__(self, prop, dss_objs, settings: SimulationSettingsModel):
+        super().__init__(prop, dss_objs, settings)
+        self._container = None
+        self._last_values = {x.FullName: None for x in dss_objs}
+        self._change_counts = {x.FullName: 0 for x in dss_objs}
+
+    def append_values(self, time_step, store_nan=False):
+        pass
+
+    def close(self):
+        assert len(self._properties) == 1
+        prop = next(iter(self._properties.values()))
+        path = (
+            f"{self._base_path}/{prop.elem_class}/ElementProperties/{prop.storage_name}"
+        )
+        values = [
+            ValueByNumber(x, prop.name, y) for x, y in self._change_counts.items()
+        ]
+        # This class creates an instance of ValueContainer directly because
+        # these metrics can only store one type, and so don't need an instance
+        # of StorageFilterBase.
+        self._container = StorageFilterBase.make_container(
+            self._hdf_store,
+            path,
+            prop,
+            self._num_steps,
+            self._max_chunk_bytes,
+            values,
+            [x.FullName for x in self._dss_objs],
+        )
+        self._container.append(values)
+        self._container.flush_data()
+
+    def iter_containers(self):
+        yield self._container
+
+
+class MultiValueTypeMetricBase(MetricBase, abc.ABC):
+    """Stores a property with multiple values of StoreValueType.
+
+    For example, a user might want to store a moving average as well as the
+    max of all instantaneous values.
+
+    """
+
+    def __init__(self, prop, dss_objs, settings):
+        super().__init__(prop, dss_objs, settings)
+        self._containers = {}  # StoreValuesType to StorageFilterBase
+        self._name_order = []  # Ensures that name-value ordering will always be consistent.
+
+    def _iter_dss_objs(self):
+        if self._can_use_native_iteration():
+            flag = self._elem_class.First()
+            while flag > 0:
+                name = self._elem_class.Name()
+                dss_obj = self._name_to_dss_obj[name]
+                yield dss_obj
+                flag = self._elem_class.Next()
+        else:
+            yield from self._dss_objs
+
+    @abc.abstractmethod
+    def _get_value(self, dss_obj, time_step):
+        """Get a value at the current time step.
+
+        Parameters
+        ----------
+        dss_obj : dssObjBase
+        time_step : int
+
+        """
+
+    def _initialize_containers(self, values):
+        prop_name = None
+        for prop in self._properties.values():
+            if prop_name is None:
+                prop_name = prop.name
+            else:
+                assert prop.name == prop_name, f"{prop.name} {prop_name}"
+            if prop.data_conversion != DataConversion.NONE:
+                vals = [
+                    convert_data(x, prop_name, y, prop.data_conversion)
+                    for x, y in zip(self._name_order, values)
+                ]
+            else:
+                vals = values
+            path = f"{self._base_path}/{prop.elem_class}/ElementProperties/{prop.storage_name}"
+            self._containers[prop.store_values_type] = self.make_storage_container(
+                path,
+                prop,
+                self._num_steps,
+                self._max_chunk_bytes,
+                vals,
+            )
+
+    def append_values(self, time_step, store_nan=False):
+        if not self._name_order:
+            self._name_order[:] = [x.FullName for x in self._iter_dss_objs()]
+    
+        #values = [self._get_value(x, time_step) for x in self._dss_objs]
+        values = []
+        objects_changed = False
+        for dss_obj, expected_name in zip(self._iter_dss_objs(), self._name_order):
+            if dss_obj.FullName != expected_name:
+                # This can happen if an element is disabled. OpenDSS won't deliver it in .Next()
+                # Need to access the element directly, which breaks the iteration.
+                objects_changed = True
+                break
+            values.append(self._get_value(dss_obj, time_step))
+
+        if objects_changed or not values:
+            values = [self._get_value(x, time_step) for x in self._dss_objs]
+
+        assert len(values) == len(self._dss_objs)
+
+        if not self._containers:
+            self._initialize_containers(values)
+
+        if store_nan:
+            for val in values:
+                val.set_nan()
+                
+        for value_type, container in self._containers.items():
+            prop = self._properties[value_type]
+            if prop.data_conversion != DataConversion.NONE:
+                assert len(self._name_order) == len(values)
+                vals = [
+                    convert_data(x, prop.name, y, prop.data_conversion)
+                    for x, y in zip(self._name_order, values)
+                ]
+            else:
+                vals = values
+            container.append_values(vals, time_step)
+
+        return vals
+
+    def iter_containers(self):
+        return self._containers.values()
+
+    def iter_empty_containers(self):
+        for prop in self._properties.values():
+            if prop.store_values_type not in self._containers:
+                yield prop
+
+
+class OpenDssPropertyMetric(MultiValueTypeMetricBase):
+    """Stores metrics for any OpenDSS element property."""
+
+    def _get_value(self, dss_obj, _time_step):
+        return dss_obj.UpdateValue(self._name)
+
+    def append_values(self, time_step, store_nan=False):
+        curr_data = {}
+        values = super().append_values(time_step, store_nan=store_nan)
+        for _, value in zip(self._dss_objs, values):
+            if len(value.make_columns()) > 1:
+                for column, val in zip(value.make_columns(), value.value):
+                    curr_data[column] = val
+            else:
+                curr_data[value.make_columns()[0]] = value.value
+
+        return curr_data
+
+
+# These next two might work but are untested.
+
+#class LineLoadingPercent(MultiValueTypeMetricBase):
+#    """Calculates line loading percent at every time point."""
+#
+#    def __init__(self, prop, dss_objs, settings):
+#        super().__init__(prop, dss_objs, settings)
+#        self._normal_amps = {}  # Name to normal_amps value
+#
+#    def _get_value(self, dss_obj, _time_step):
+#        line = dss_obj
+#        normal_amps = self._normal_amps.get(line.Name)
+#        if normal_amps is None:
+#            normal_amps = line.GetValue("NormalAmps", convert=True).value
+#            self._normal_amps[line.Name] = normal_amps
+#
+#        currents = line.UpdateValue("Currents").value
+#        current = max([abs(x) for x in currents])
+#        loading = current / normal_amps * 100
+#        return ValueByNumber(line.Name, "LineLoading", loading)
+#
+#
+#class TransformerLoadingPercent(MultiValueTypeMetricBase):
+#    """Calculates transformer loading percent at every time point."""
+#
+#    def __init__(self, prop, dss_objs, settings):
+#        super().__init__(prop, dss_objs, settings)
+#        self._normal_amps = {}  # Name to normal_amps value
+#
+#    def _get_value(self, dss_obj, _time_step):
+#        transformer = dss_obj
+#        normal_amps = self._normal_amps.get(transformer.Name)
+#        if normal_amps is None:
+#            normal_amps = transformer.GetValue("NormalAmps", convert=True).value
+#            self._normal_amps[transformer.Name] = normal_amps
+#
+#        currents = transformer.UpdateValue("Currents").value
+#        current = max([abs(x) for x in currents])
+#        loading = current / normal_amps * 100
+#        return ValueByNumber(transformer.Name, "TransformerLoading", loading)
+
+
+FeederHeadValues = namedtuple("FeederHeadValues", ["load_kvar", "load_kw", "loading", "reverse_power_flow"])
+
+
+class FeederHeadMetrics(MetricBase):
+    """Calculates loading at the feeder head at each time point"""
+
+    def __init__(self, prop, dss_objs, settings):
+        super().__init__(prop, dss_objs, settings)
+        # dss_objs contains the Circuit, but we won't use it.
+        assert len(dss_objs) == 1, dss_objs
+        self._prop = prop
+        self._containers = {}
+        self._feeder_head_line = None
+        self._values = {}
+
+    def _initialize_containers(self):
+        assert len(self._properties) == 1, self._properties
+        self._feeder_head_line = self._find_feeder_head_line()
+        values = self._get_values()
+        self._values = {
+            "load_kvar": ValueByNumber("FeederHead", "load_kvar", values.load_kvar),
+            "load_kw": ValueByNumber("FeederHead", "load_kw", values.load_kw),
+            "loading": ValueByNumber("FeederHead", "loading", values.loading),
+            "reverse_power_flow": ValueByNumber("FeederHead", "reverse_power_flow", values.reverse_power_flow),
+        }
+
+        for name in self._values:
+            path = f"{self._base_path}/{self._prop.elem_class}/ElementProperties/{name}"
+            self._containers[name] = self.make_storage_container(
+                path,
+                self._prop,
+                self._num_steps,
+                self._max_chunk_bytes,
+                [self._values[name]],
+            )
+
+    @staticmethod
+    def is_circuit_wide():
+        return True
+
+    @staticmethod
+    def _find_feeder_head_line():
+        feeder_head_line = None
+        flag = dss.Topology.First()
+        while flag > 0:
+            if "line" in dss.Topology.BranchName().lower():
+                feeder_head_line = dss.Topology.BranchName()
+                break
+            flag = dss.Topology.Next()
+
+        assert feeder_head_line is not None
+        return feeder_head_line
+
+    def _get_values(self):
+        total_power = dss.Circuit.TotalPower()
+        feeder_head_values = FeederHeadValues(
+            load_kvar=total_power[1],
+            load_kw=total_power[0],
+            loading=self._get_feeder_head_loading(),
+            reverse_power_flow=self._reverse_power_flow(),
+        )
+        return feeder_head_values
+
+    def _get_feeder_head_loading(self):
+        flag = dss.Circuit.SetActiveElement(self._feeder_head_line)
+        if not flag > 0:
+            raise Exception("Failed to set the feeder head line")
+        n_phases = dss.CktElement.NumPhases()
+        max_amps = dss.CktElement.NormalAmps()
+        currents = dss.CktElement.CurrentsMagAng()[:2*n_phases]
+        current_magnitude = currents[::2]
+
+        max_flow = max(max(current_magnitude), 1e-10)
+        loading = max_flow / max_amps
+        return loading
+
+    @staticmethod
+    def _reverse_power_flow():
+        # total substation power is an injection(-) or a consumption(+)
+        reverse_pf = dss.Circuit.TotalPower()[0] > 0
+        # Storing NaN with bools is not working correctly.
+        return int(reverse_pf)
+
+    def append_values(self, time_step, store_nan=False):
+        if not self._containers:
+            self._initialize_containers()
+
+        if store_nan:
+            for val in self._values.values():
+                val.set_nan()
+        else:
+            values = self._get_values()
+            for name, value in self._values.items():
+                value.set_value(getattr(values, name))
+
+        vals = []
+        for name, container in self._containers.items():
+            value = self._values[name]
+            vals.append(value)
+            container.append_values([value], time_step)
+
+        return vals
+
+    def iter_containers(self):
+        for container in self._containers.values():
+            yield container
+
+
+class SummedElementsOpenDssPropertyMetric(MetricBase):
+    """Sums all elements' values for a given property at each time point."""
+
+    def __init__(self, prop, dss_objs, settings):
+        super().__init__(prop, dss_objs, settings)
+        self._container = None
+        self._data_conversion = prop.data_conversion
+
+    def _get_value(self, obj):
+        value = obj.UpdateValue(self._name)
+        if self._data_conversion != DataConversion.NONE:
+            value = convert_data(
+                "Total",
+                next(iter(self._properties.values())).name,
+                value,
+                self._data_conversion,
+            )
+        return value
+
+    def append_values(self, time_step, store_nan=False):
+        if store_nan:
+            if self._can_use_native_iteration():
+                self._elem_class.First()
+            total = self._get_value(self._dss_objs[0])
+            total.set_nan()
+        else:
+            total = None
+            if self._can_use_native_iteration():
+                iterations = 0
+                flag = self._elem_class.First()
+                while flag > 0:
+                    dss_obj = self._name_to_dss_obj[self._elem_class.Name()]
+                    value = self._get_value(dss_obj)
+                    if total is None:
+                        total = value
+                    else:
+                        total += value
+                    iterations += 1
+                    flag = self._elem_class.Next()
+                assert iterations == len(self._dss_objs)
+            else:
+                for dss_obj in self._dss_objs:
+                    value = self._get_value(dss_obj)
+                    if total is None:
+                        total = value
+                    else:
+                        total += value
+
+        if self._container is None:
+            assert len(self._properties) == 1
+            prop = next(iter(self._properties.values()))
+            assert prop.store_values_type in (StoreValuesType.ALL, StoreValuesType.SUM)
+            total.set_name("Total")
+            path = f"{self._base_path}/{prop.elem_class}/SummedElementProperties/{prop.storage_name}"
+            self._container = self.make_storage_container(
+                path,
+                prop,
+                self._num_steps,
+                self._max_chunk_bytes,
+                [total],
+            )
+        self._container.append_values([total], time_step)
+
+    @staticmethod
+    def is_circuit_wide():
+        return True
+
+    def iter_containers(self):
+        yield self._container
+
+
+class SummedElementsByGroupOpenDssPropertyMetric(MetricBase):
+    """Sums all elements' values for a given property at each time point.
+    Elements are separated into groups by name.
+
+    """
+    def __init__(self, prop, dss_objs, settings):
+        super().__init__(prop, dss_objs, settings)
+        self._containers = {}
+        self._name_to_group = {}
+
+        # This allows names to be in the group that aren't in the circuit
+        # in order to reduce having to duplicate the sum_group files many times
+        # in cases where there are many projects/scenarios.
+        elements = {x.Name for x in dss_objs}
+        for group in prop.sum_groups:
+            group_elems = elements.intersection(set(group["elements"]))
+            if group_elems:
+                self._containers[group["name"]] = None
+                for element_name in group_elems:
+                    self._name_to_group[element_name] = group["name"]
+
+        self._data_conversion = prop.data_conversion
+
+    def _get_value(self, obj):
+        value = obj.UpdateValue(self._name)
+        if self._data_conversion != DataConversion.NONE:
+            value = convert_data(
+                "Total",
+                next(iter(self._properties.values())).name,
+                value,
+                self._data_conversion,
+            )
+        return value
+
+    def append_values(self, time_step, store_nan=False):
+        total_by_group = {x: None for x in self._containers}
+        if store_nan:
+            for group in self._containers:
+                if self._can_use_native_iteration():
+                    self._elem_class.First()
+                total_by_group[group] = self._get_value(self._dss_objs[0])
+                total_by_group[group].set_nan()
+        else:
+            if self._can_use_native_iteration():
+                iterations = 0
+                flag = self._elem_class.First()
+                while flag > 0:
+                    name = self._elem_class.Name()
+                    group = self._name_to_group[name]
+                    dss_obj = self._name_to_dss_obj[name]
+                    value = self._get_value(dss_obj)
+                    if total_by_group[group] is None:
+                        total_by_group[group] = value
+                    else:
+                        total_by_group[group] += value
+                    iterations += 1
+                    flag = self._elem_class.Next()
+                assert iterations == len(self._dss_objs)
+            else:
+                for dss_obj in self._dss_objs:
+                    value = self._get_value(dss_obj)
+                    if total_by_group[group] is None:
+                        total_by_group[group] = value
+                    else:
+                        total_by_group[group] += value
+
+        if next(iter(self._containers.values())) is None:
+            assert len(self._properties) == 1
+            prop = next(iter(self._properties.values()))
+            assert prop.store_values_type in (StoreValuesType.ALL, StoreValuesType.SUM)
+            for group in self._containers:
+                total_by_group[group].set_name("Total")
+                key = ValueStorageBase.DELIMITER.join((prop.storage_name, group))
+                path = f"{self._base_path}/{prop.elem_class}/SummedElementProperties/{key}"
+                self._containers[group] = self.make_storage_container(
+                    path,
+                    prop,
+                    self._num_steps,
+                    self._max_chunk_bytes,
+                    [total_by_group[group]],
+                )
+        for group in self._containers:
+            self._containers[group].append_values([total_by_group[group]], time_step)
+
+    @staticmethod
+    def is_circuit_wide():
+        return True
+
+    def iter_containers(self):
+        return self._containers.values()
+
+
+class NodeVoltageMetric(MetricBase):
+    """Stores metrics for node voltages."""
+
+    PRIMARY_BUS_THRESHOLD_KV = 1.0
+
+    def __init__(self, prop, dss_obj, settings):
+        super().__init__(prop, dss_obj, settings)
+        props = list(self._properties)
+        assert len(props) == 1
+        self._prop = props[0]
+        # Indices for node names are tied to indices for node voltages.
+        self._node_names = None
+        self._voltages = None
+        start_time = get_start_time(settings)
+        sim_resolution = get_simulation_resolution(settings)
+        inputs = ReportBase.get_inputs_from_defaults(settings, "Voltage Metrics")
+        window_size = max(1, int(
+            timedelta(minutes=inputs["window_size_minutes"]) / sim_resolution
+        ))
+        self._voltage_metrics = NodeVoltageMetrics(
+            prop, start_time, sim_resolution, window_size, inputs["store_per_element_data"]
+        )
+        self._primary_node_names = []
+        self._primary_indices = []
+        self._secondary_node_names = []
+        self._secondary_indices = []
+
+    def _make_elem_names(self):
+        return self._node_names
+
+    def _identify_primary_v_secondary(self):
+        for i, name in enumerate(self._node_names):
+            dss.Circuit.SetActiveBus(name)
+            kv_base = dss.Bus.kVBase()
+            if kv_base > self.PRIMARY_BUS_THRESHOLD_KV:
+                self._primary_node_names.append(name)
+                self._primary_indices.append(i)
+            else:
+                self._secondary_node_names.append(name)
+                self._secondary_indices.append(i)
+
+    def append_values(self, time_step, store_nan=False):
+        voltages = dss.Circuit.AllBusMagPu()
+        if self._voltages is None:
+            # TODO: limit to objects that have been added
+            self._node_names = dss.Circuit.AllNodeNames()
+            self._identify_primary_v_secondary()
+            self._voltages = [
+                ValueByNumber(x, "Voltage", y)
+                for x, y in zip(self._node_names, voltages)
+            ]
+            self._voltage_metrics.set_node_info(
+                self._primary_node_names,
+                self._primary_indices,
+                self._secondary_node_names,
+                self._secondary_indices,
+            )
+        else:
+            for i, voltage in enumerate(voltages):
+                self._voltages[i].set_value_from_raw(voltage)
+
+        if not store_nan:
+            self._voltage_metrics.update(time_step, self._voltages)
+
+        self._voltage_metrics.increment_steps()
+
+    def close(self):
+        path = os.path.join(
+            str(self._settings.project.active_project_path),
+            "Exports",
+            self._settings.project.active_scenario,
+        )
+        self._voltage_metrics.generate_report(path)
+
+    @staticmethod
+    def is_circuit_wide():
+        return True
+
+    def iter_containers(self):
+        return
+        yield
+
+
+class TrackCapacitorChangeCounts(ChangeCountMetricBase):
+    """Store the number of changes for a capacitor."""
+
+    def append_values(self, _time_step, store_nan=False):
+        if store_nan:
+            return
+
+        iterations = 0
+        flag = dss.Capacitors.First()
+        while flag > 0:
+            capacitor = self._name_to_dss_obj[dss.Capacitors.Name()]
+            self._update_counts(capacitor)
+            iterations += 1
+            flag = dss.Capacitors.Next()
+        assert iterations == len(self._dss_objs)
+
+    def _update_counts(self, capacitor):
+        states = dss.Capacitors.States()
+        if states == -1:
+            raise Exception(f"failed to get Capacitors.States() for {capacitor.Name}")
+
+        if len(states) != 1:
+            raise Exception(f"length of states greater than 1 is not supported: {states}")
+        cur_value = sum(states)
+        last_value = self._last_values[capacitor.FullName]
+        if last_value is not None and cur_value != last_value:
+            logger.debug(
+                "%s changed state old=%s new=%s", capacitor.Name, last_value, cur_value
+            )
+            self._change_counts[capacitor.FullName] += 1
+
+        self._last_values[capacitor.FullName] = cur_value
+
+
+class TrackRegControlTapNumberChanges(ChangeCountMetricBase):
+    """Store the number of tap number changes for a RegControl."""
+
+    def append_values(self, _time_step, store_nan=False):
+        if store_nan:
+            return
+
+        iterations = 0
+        flag = dss.RegControls.First()
+        while flag > 0:
+            reg_control = self._name_to_dss_obj[dss.RegControls.Name()]
+            self._update_counts(reg_control)
+            iterations += 1
+            flag = dss.RegControls.Next()
+        assert iterations == len(self._dss_objs)
+
+    def _update_counts(self, reg_control):
+        tap_number = dss.RegControls.TapNumber()
+        last_value = self._last_values[reg_control.FullName]
+        if last_value is not None and last_value != tap_number:
+            self._change_counts[reg_control.FullName] += 1
+            logger.debug(
+                "%s changed count from %s to %s count=%s",
+                reg_control.Name,
+                last_value,
+                tap_number,
+                self._change_counts[reg_control.FullName],
+            )
+
+        self._last_values[reg_control.FullName] = tap_number
+
+
+class OpenDssExportMetric(MetricBase):
+    def __init__(self, prop, dss_objs, settings):
+        super().__init__(prop, dss_objs, settings)
+        self._tmp_dir = tempfile.mkdtemp()
+        self._containers = {}
+        self._sum_elements = prop.sum_elements
+        if self._sum_elements:
+            self._append_func = self._append_summed_values
+        else:
+            self._append_func = self._append_values
+        self._check_output()
+
+        # Some OpenDSS files upper-case the name.
+        # Make a mapping for fast matching and lookup.
+        self._names = {}
+        self._values = []
+        for i, dss_obj in enumerate(dss_objs):
+            elem_type, name = dss_obj.FullName.split(".")
+            if self.requires_upper_case():
+                self._names[f"{elem_type}.{name.upper()}"] = i
+            else:
+                self._names[f"{elem_type}.{name}"] = i
+            if self._sum_elements and not self._values:
+                self._values.append(ValueByNumber("Total", self.label(), 0.0))
+                break
+            self._values.append(ValueByNumber(dss_obj.FullName, self.label(), 0.0))
+
+    def __del__(self):
+        shutil.rmtree(self._tmp_dir)
+
+    def _run_command(self):
+        cmd = f"{self.export_command()}"
+        result = dss.utils.run_command(cmd)
+        if not result:
+            raise Exception(f"{cmd} failed")
+        return result
+
+    def append_values(self, time_step, store_nan=False):
+        filename = self._run_command()
+        self.parse_file(filename)
+        self._append_func(time_step, store_nan=store_nan)
+
+    def _append_values(self, time_step, store_nan=False):
+        if not self._containers:
+            for prop in self._properties.values():
+                if prop.window_sizes and prop.is_moving_average():
+                    # This is somewhat ugly code to allow different
+                    # sizes for different element types collected in the same
+                    # OpenDSS report.
+                    window_sizes = self._get_window_size_by_name_index(prop)
+                else:
+                    window_sizes = None
+                path = f"{self._base_path}/{self.element_class()}/ElementProperties/{prop.storage_name}"
+                self._containers[prop.store_values_type] = self.make_storage_container(
+                    path,
+                    prop,
+                    self._num_steps,
+                    self._max_chunk_bytes,
+                    self._values,
+                    window_sizes=window_sizes,
+                )
+
+        if store_nan:
+            for i in range(len(self._values)):
+                self._values[i].set_nan()
+
+        for sv_type, prop in self._properties.items():
+            self._containers[sv_type].append_values(self._values, time_step)
+
+    def _append_summed_values(self, time_step, store_nan=False):
+        if store_nan:
+            return
+
+        self._values[0].set_value(sum([x.value for x in self._values]))
+
+        prop = next(iter(self._properties.values()))
+        if not self._containers:
+            if len(self._properties) > 1:
+                raise InvalidConfiguration(
+                    "summing elements only supports one Property"
+                )
+            assert len(self._properties) == 1
+            assert prop.store_values_type in (StoreValuesType.ALL, StoreValuesType.SUM)
+            path = f"{self._base_path}/{prop.elem_class}/SummedElementProperties/{prop.storage_name}"
+            self._containers[prop.store_values_type] = self.make_storage_container(
+                path,
+                prop,
+                self._num_steps,
+                self._max_chunk_bytes,
+                self._values,
+            )
+
+        self._containers[prop.store_values_type].append_values(self._values, time_step)
+
+    def _check_output(self):
+        filename = self._run_command()
+        df = pd.read_csv(filename)
+        for index, val in self.expected_column_headers().items():
+            if df.columns[index].strip() != val:
+                raise Exception(
+                    f"Unexpected format in export file file: {index} {val} {df.columns}"
+                )
+
+    @staticmethod
+    def _get_name_from_line(fields):
+        return fields[0].strip()[1:-1]
+
+    def _get_window_size_by_name_index(self, prop):
+        """Returns a list of window sizes per element name corresponding to self._names."""
+        if not prop.opendss_classes:
+            raise InvalidConfiguration(
+                f"window_sizes requires opendss_classes: {prop.name}"
+            )
+
+        window_sizes = [None] * len(self._names)
+        for opendss_class, window_size in prop.window_sizes.items():
+            if opendss_class not in prop.opendss_classes:
+                raise InvalidConfiguration(
+                    f"{opendss_class} is not defined in opendss_classes: {prop.name}"
+                )
+
+        # Note: names have singluar class names, such as Line.line1.
+        # opendss_classes are plural, such as Lines
+        mapping = {}
+        for i, name in enumerate(self._names):
+            opendss_class_singular = name.split(".")[0]
+            size = mapping.get(opendss_class_singular)
+            if size is None:
+                for opendss_class in prop.opendss_classes:
+                    if opendss_class.startswith(opendss_class_singular):
+                        size = prop.window_sizes[opendss_class]
+                        mapping[opendss_class_singular] = size
+            if size is None:
+                raise InvalidConfiguration(f"Failed to find window_size for {name}")
+            window_sizes[i] = size
+
+        return window_sizes
+
+    @staticmethod
+    @abc.abstractmethod
+    def element_class():
+        """Return the element class."""
+
+    @staticmethod
+    @abc.abstractmethod
+    def expected_column_headers():
+        """Return the expected column headers in the CSV file."""
+
+    @abc.abstractmethod
+    def export_command(self):
+        """Return the command to run in OpenDSS."""
+
+    @staticmethod
+    @abc.abstractmethod
+    def label():
+        """Return the label to use in dataframe columns."""
+
+    @staticmethod
+    def is_circuit_wide():
+        return True
+
+    def iter_containers(self):
+        for sv_type in self._properties:
+            if sv_type in self._containers:
+                yield self._containers[sv_type]
+
+    def iter_empty_containers(self):
+        for prop in self._properties.values():
+            if prop.store_values_type not in self._containers:
+                yield prop
+
+    @abc.abstractmethod
+    def parse_file(self, filename):
+        """Parse data in filename."""
+
+    @staticmethod
+    @abc.abstractmethod
+    def requires_upper_case():
+        """Return True if the names are upper case."""
+
+
+class ExportLoadingsMetric(OpenDssExportMetric):
+    """Stores line and transformer loading percentages in HDF5."""
+    @staticmethod
+    def element_class():
+        return "CktElement"
+
+    @staticmethod
+    def expected_column_headers():
+        return {0: "Name", 2: "%normal"}
+
+    def export_command(self):
+        filename = Path(self._tmp_dir) / "opendss_loading.csv"
+        return f"export capacity {filename}"
+
+    @staticmethod
+    def label():
+        return "Loading"
+
+    @staticmethod
+    def _get_name_from_line(fields):
+        return fields[0].strip()
+
+    def parse_file(self, filename):
+        count = 0
+        with open(filename) as f_in:
+            # Skip the header.
+            next(f_in)
+            for line in f_in:
+                fields = line.split(",")
+                name = self._get_name_from_line(fields)
+                if name in self._names:
+                    index = self._names[name]
+                    val = float(fields[2].strip())
+                    self._values[index].set_value_from_raw(val)
+                    count += 1
+                else:
+                    # There may be other element types that are not being tracked.
+                    continue
+        assert count == len(self._names), f"count={count} num_names={len(self._names)}"
+
+    @staticmethod
+    def requires_upper_case():
+        return True
+
+
+class ExportPowersMetric(OpenDssExportMetric):
+    """Stores power values in HDF5."""
+    @staticmethod
+    def element_class():
+        return "CktElement"
+
+    def export_command(self):
+        filename = Path(self._tmp_dir) / "opendss_powers.csv"
+        return f"export powers {filename}"
+
+    @staticmethod
+    def expected_column_headers():
+        return {0: "Element", 1: "Terminal", 2: "P(kW)"}
+
+    @staticmethod
+    def label():
+        return "Powers"
+
+    def parse_file(self, filename):
+        data = {}
+        with open(filename) as f_in:
+            # Skip the header.
+            next(f_in)
+            for line in f_in:
+                fields = line.split(",")
+                name = self._get_name_from_line(fields)
+                if name in self._names:
+                    terminal = int(fields[1])
+                    if terminal == 1:
+                        val = abs(float(fields[2].strip()))
+                        data[name] = val
+
+        for name, val in data.items():
+            index = self._names[name]
+            self._values[index].set_value_from_raw(val)
+
+    @staticmethod
+    def requires_upper_case():
+        return True
+
+
+class OverloadsMetricInMemory(OpenDssExportMetric):
+    """Stores line and transformer loading percentages in memory."""
+    def __init__(self, prop, dss_objs, settings):
+        super().__init__(prop, dss_objs, settings)
+        # Indices for node names are tied to indices for node voltages.
+        self._transformer_index = None
+        self._discovered_elements = False
+        start_time = get_start_time(settings)
+        sim_resolution = get_simulation_resolution(settings)
+        inputs = ReportBase.get_inputs_from_defaults(settings, "Thermal Metrics")
+        line_window_size, transformer_window_size = self._get_window_sizes(inputs, sim_resolution)
+        self._thermal_metrics = ThermalMetrics(
+            prop,
+            start_time,
+            sim_resolution,
+            line_window_size_hours=inputs["line_window_size_hours"],
+            line_window_size=line_window_size,
+            transformer_window_size_hours=inputs["transformer_window_size_hours"],
+            transformer_window_size=transformer_window_size,
+            line_loading_percent_threshold=inputs["line_loading_percent_threshold"],
+            line_loading_percent_moving_average_threshold=inputs["line_loading_percent_moving_average_threshold"],
+            transformer_loading_percent_threshold=inputs["transformer_loading_percent_threshold"],
+            transformer_loading_percent_moving_average_threshold=inputs["transformer_loading_percent_moving_average_threshold"],
+            store_per_element_data=inputs["store_per_element_data"],
+        )
+
+    def _append_values(self, time_step, store_nan=False):
+        if not self._discovered_elements:
+            line_names = []
+            transformer_names = []
+            for i, val in enumerate(self._values):
+                if val.name.startswith("Line"):
+                    line_names.append(val.name)
+                elif val.name.startswith("Transformer"):
+                    assert i != 0
+                    transformer_names.append(val.name)
+                    if self._transformer_index is None:
+                        self._transformer_index = i
+                else:
+                    assert False, val.name
+            self._thermal_metrics.line_names = line_names
+            self._thermal_metrics.transformer_names = transformer_names
+            self._discovered_elements = True
+
+        if self._transformer_index is None:
+            # There are no transformers.
+            line_loadings = self._values[:]
+            transformer_loadings = []
+        else:
+            line_loadings = self._values[:self._transformer_index]
+            transformer_loadings = self._values[self._transformer_index:]
+
+        if not store_nan:
+            self._thermal_metrics.update(time_step, line_loadings, transformer_loadings)
+
+        self._thermal_metrics.increment_steps()
+
+    @staticmethod
+    def _get_window_sizes(inputs, resolution):
+        line_window_size = timedelta(hours=inputs["line_window_size_hours"])
+        if line_window_size % resolution != timedelta(0):
+            raise InvalidConfiguration(
+                f"line_window_size={line_window_size} must be a multiple of {resolution}"
+            )
+        transformer_window_size = timedelta(hours=inputs["transformer_window_size_hours"])
+        if transformer_window_size % resolution != timedelta(0):
+            raise InvalidConfiguration(
+                f"transformer_window_size={transformer_window_size} must be a multiple of {resolution}"
+            )
+        line_window_size = int(line_window_size / resolution)
+        transformer_window_size = int(transformer_window_size / resolution)
+        return line_window_size, transformer_window_size
+
+    def close(self):
+        path = os.path.join(
+            str(self._settings.project.active_project_path),
+            "Exports",
+            self._settings.project.active_scenario,
+        )
+        self._thermal_metrics.generate_report(path)
+
+    @staticmethod
+    def element_class():
+        return "CktElement"
+
+    @staticmethod
+    def expected_column_headers():
+        return {0: "Name", 2: "%normal"}
+
+    def export_command(self):
+        filename = Path(self._tmp_dir) / "opendss_capacity.csv"
+        return f"export capacity {filename}"
+
+    @staticmethod
+    def _get_name_from_line(fields):
+        return fields[0].strip()
+
+    @staticmethod
+    def label():
+        return "Overloads"
+
+    def parse_file(self, filename):
+        with open(filename) as f_in:
+            # Skip the header.
+            next(f_in)
+            for line in f_in:
+                fields = line.split(",")
+                name = self._get_name_from_line(fields)
+                if name in self._names:
+                    index = self._names[name]
+                    val = float(fields[2].strip())
+                    self._values[index].set_value_from_raw(val)
+
+    @staticmethod
+    def requires_upper_case():
+        return True
+
+
+def convert_data(name, prop_name, value, conversion):
+    if conversion == DataConversion.ABS:
+        converted = copy.deepcopy(value)
+        if isinstance(value.value, list):
+            converted.set_value([abs(x) for x in value.value])
+        else:
+            converted.set_value(abs(value.value))
+    elif conversion == DataConversion.SUM:
+        converted = ValueByNumber(name, prop_name, sum(value.value))
+    elif conversion == DataConversion.ABS_SUM:
+        converted = ValueByNumber(name, prop_name, abs(sum(value.value)))
+    elif conversion == DataConversion.SUM_REAL:
+        converted = ValueByNumber(
+            name, prop_name, sum((x.real for x in value.value))
+        )
+    elif conversion == DataConversion.SUM_ABS_REAL:
+        converted = ValueByNumber(
+            name, prop_name, sum((abs(x.real) for x in value.value))
+        )
+    else:
+        converted = value
+
+    return converted
+
+
+# Bus and Circuit are excluded.
+_OPEN_DSS_CLASS_FOR_ITERATION = {
+    "Capacitor": dss.Capacitors,
+    "Fuse": dss.Fuses,
+    "Generator": dss.Generators,
+    "Isource": dss.Isource,
+    "Line": dss.Lines,
+    "Load": dss.Loads,
+    "Meter": dss.Meters,
+    "Monitor": dss.Monitors,
+    "PVSystem": dss.PVsystems,
+    "Recloser": dss.Reclosers,
+    "RegControl": dss.RegControls,
+    "Relay": dss.Relays,
+    "Sensor": dss.Sensors,
+    "Transformer": dss.Transformers,
+    "Vsource": dss.Vsources,
+    "XYCurve": dss.XYCurves,
+}
```

### Comparing `nrel_pydss-3.1.3/src/pydss/naerm.py` & `nrel_pydss-3.1.4/src/pydss/naerm.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,130 +1,130 @@
-# Kapil Duwadi
-PYDSS_DICT = {
-        'CurrentsMagAng': 'vector',
-        'Currents': 'vector',
-        'RatedCurrent': 'double',
-        'EmergAmps': 'double',
-        'NormalAmps': 'double',
-        'normamps': 'double',
-        'Losses': 'vector',
-        'PhaseLosses': 'vector',
-        'Powers': 'vector',
-        'TotalPower': 'vector',
-        'LineLosses': 'vector',
-        'SubstationLosses': 'vector',
-        'kV': 'double',
-        'kVARated': 'double',
-        'kvar': 'double',
-        'kW': 'double',
-        'kVABase': 'double',
-        'kWh': 'double',
-        'puVmagAngle': 'vector',
-        'VoltagesMagAng': 'vector',
-        'VMagAngle': 'vector',
-        'Voltages': 'vector',
-        'Vmaxpu': 'double',
-        'Vminpu': 'double',
-        'Frequency': 'double',
-        'Taps': 'vector',
-        '%stored': 'double',
-        'Distance': 'double'
-    }
-
-
-ASSETTYPEMAPDICT = {
-    'Buses' : 'bus',
-    'Lines' : 'line',
-    'Loads' : 'load',
-    'PVSystems' : 'pv',
-    'Transformers': 'trans',
-    'Faults' : 'fault',
-    'Storages': 'storage',
-    'Circuit': 'circuit'
-
-}
-
-PROPERTYMAPDICT = {
-    'CurrentMagAng' : ['current double', 'current_angle double'],
-    'TotalPower' : ['power_real double', 'power_imag double'],
-    
-}
-
-PARAMETERNAME2INDEX = {
-    'power_real' : 0,
-    'power_imag': 1
-}
-
-
-def isnaerm(name):
-
-    try: 
-        if len(name.split('/')) == 5:
-            return True
-        else:
-            return False
-        
-    except Exception as e:
-        return False
-
-
-
-def get_naerm_value(value, naerm_name):
-
-    federate_name, asset_type, asset_name, parameter_name, parameter_unit = naerm_name.split('/')
-    if isinstance(value, list):
-        try:
-            value = value[PARAMETERNAME2INDEX[parameter_name]]
-            return value
-        except Exception as e:
-            return 'Failed'
-        
-    else:
-        return value
-
-def pydss_to_naerm(pydssname):
-
-    try:
-        fed_name, class_name, object_name, ppty_name = pydssname.split('.')
-        asset_type = ASSETTYPEMAPDICT[class_name]
-        parameter_name = PROPERTYMAPDICT[ppty_name]
-        if isinstance(parameter_name, list):
-            naerm_name = [f"{fed_name}/{asset_type}/{object_name}/{param.split(' ')[0]}/{param.split(' ')[1]}" for param in parameter_name]
-    
-        else:
-            naerm_name = f"{fed_name}/{asset_type}/{object_name}/{parameter_name.split(' ')[0]}/{parameter_name.split(' ')[1]}"
-        return naerm_name
-
-    except Exception as e:
-        return 'Failed'
-
-
-def naerm_to_pydss(naerm_name):
-
-    try:
-        federate_name, asset_type, asset_name, parameter_name, parameter_unit = naerm_name.split('/')
-        for keys, values in ASSETTYPEMAPDICT.items():
-            if values == asset_type:
-                class_name = keys
-                break
-        
-        for keys, values in PROPERTYMAPDICT.items():
-
-            if isinstance(values, list):
-                if parameter_name + ' ' + parameter_unit in values:
-                    ppty_name = keys
-                    break
-            else:
-                if parameter_name == values:
-                    pptty_name = keys
-                    break
-
-        return f"{federate_name}.{class_name}.{asset_name}.{ppty_name}"
-
-    except Exception as e:
-        return 'Failed'
-
-
-if __name__ == '__main__':
-
-    name = 'pydss_x/circuit/heco19021/power_real/double'
+# Kapil Duwadi
+PYDSS_DICT = {
+        'CurrentsMagAng': 'vector',
+        'Currents': 'vector',
+        'RatedCurrent': 'double',
+        'EmergAmps': 'double',
+        'NormalAmps': 'double',
+        'normamps': 'double',
+        'Losses': 'vector',
+        'PhaseLosses': 'vector',
+        'Powers': 'vector',
+        'TotalPower': 'vector',
+        'LineLosses': 'vector',
+        'SubstationLosses': 'vector',
+        'kV': 'double',
+        'kVARated': 'double',
+        'kvar': 'double',
+        'kW': 'double',
+        'kVABase': 'double',
+        'kWh': 'double',
+        'puVmagAngle': 'vector',
+        'VoltagesMagAng': 'vector',
+        'VMagAngle': 'vector',
+        'Voltages': 'vector',
+        'Vmaxpu': 'double',
+        'Vminpu': 'double',
+        'Frequency': 'double',
+        'Taps': 'vector',
+        '%stored': 'double',
+        'Distance': 'double'
+    }
+
+
+ASSETTYPEMAPDICT = {
+    'Buses' : 'bus',
+    'Lines' : 'line',
+    'Loads' : 'load',
+    'PVSystems' : 'pv',
+    'Transformers': 'trans',
+    'Faults' : 'fault',
+    'Storages': 'storage',
+    'Circuit': 'circuit'
+
+}
+
+PROPERTYMAPDICT = {
+    'CurrentMagAng' : ['current double', 'current_angle double'],
+    'TotalPower' : ['power_real double', 'power_imag double'],
+    
+}
+
+PARAMETERNAME2INDEX = {
+    'power_real' : 0,
+    'power_imag': 1
+}
+
+
+def isnaerm(name):
+
+    try: 
+        if len(name.split('/')) == 5:
+            return True
+        else:
+            return False
+        
+    except Exception as e:
+        return False
+
+
+
+def get_naerm_value(value, naerm_name):
+
+    federate_name, asset_type, asset_name, parameter_name, parameter_unit = naerm_name.split('/')
+    if isinstance(value, list):
+        try:
+            value = value[PARAMETERNAME2INDEX[parameter_name]]
+            return value
+        except Exception as e:
+            return 'Failed'
+        
+    else:
+        return value
+
+def pydss_to_naerm(pydssname):
+
+    try:
+        fed_name, class_name, object_name, ppty_name = pydssname.split('.')
+        asset_type = ASSETTYPEMAPDICT[class_name]
+        parameter_name = PROPERTYMAPDICT[ppty_name]
+        if isinstance(parameter_name, list):
+            naerm_name = [f"{fed_name}/{asset_type}/{object_name}/{param.split(' ')[0]}/{param.split(' ')[1]}" for param in parameter_name]
+    
+        else:
+            naerm_name = f"{fed_name}/{asset_type}/{object_name}/{parameter_name.split(' ')[0]}/{parameter_name.split(' ')[1]}"
+        return naerm_name
+
+    except Exception as e:
+        return 'Failed'
+
+
+def naerm_to_pydss(naerm_name):
+
+    try:
+        federate_name, asset_type, asset_name, parameter_name, parameter_unit = naerm_name.split('/')
+        for keys, values in ASSETTYPEMAPDICT.items():
+            if values == asset_type:
+                class_name = keys
+                break
+        
+        for keys, values in PROPERTYMAPDICT.items():
+
+            if isinstance(values, list):
+                if parameter_name + ' ' + parameter_unit in values:
+                    ppty_name = keys
+                    break
+            else:
+                if parameter_name == values:
+                    pptty_name = keys
+                    break
+
+        return f"{federate_name}.{class_name}.{asset_name}.{ppty_name}"
+
+    except Exception as e:
+        return 'Failed'
+
+
+if __name__ == '__main__':
+
+    name = 'pydss_x/circuit/heco19021/power_real/double'
     #print(get_naerm_value([34,45],'pydss_x/circuit/heco19021/power_imag/double' ))
```

### Comparing `nrel_pydss-3.1.3/src/pydss/node_voltage_metrics.py` & `nrel_pydss-3.1.4/src/pydss/node_voltage_metrics.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,576 +1,576 @@
-from typing import Dict, List, Union, Annotated
-from datetime import datetime, timedelta
-from pathlib import Path
-
-from loguru import logger
-from pydantic import BaseModel, Field
-from pydantic import ConfigDict
-
-from pydss.utils.simulation_utils import CircularBufferHelper
-
-class VoltageMetricsBaseModel(BaseModel):
-    model_config = ConfigDict(title="VoltageMetricsBaseModel", str_strip_whitespace=True, validate_assignment=True, validate_default=True, extra="forbid", use_enum_values=False)
-
-
-class VoltageMetric1(VoltageMetricsBaseModel):
-    time_points: Annotated[
-        List[datetime],
-        Field(
-            None,
-            title="time_points",
-            description="time points that contain voltages between ANSI A and ANSI B thresholds",
-        )]
-    duration: Annotated[
-        timedelta,
-        Field(
-            None,
-            title="duration",
-            description="amount of time where metric 1 existed (len(time_points) * resolution)",
-        )]
-
-
-class VoltageMetric2(VoltageMetricsBaseModel):
-    duration: Annotated[
-        timedelta,
-        Field(
-            title="duration",
-            description="amount of time where a node experienced ANSI A violations",
-        )]
-    duration_percentage: Annotated[
-        float,
-        Field(
-            title="duration_percentage",
-            description="percentage of overall time",
-        )]
-
-
-class VoltageMetric3(VoltageMetricsBaseModel):
-    time_points: Annotated[
-        List[datetime],
-        Field(
-            title="time_points",
-            description="time points where moving average voltages violated ANSI A thresholds",
-        )]
-    duration:Annotated[
-        timedelta,
-        Field(
-            title="duration",
-            description="amount of time where metric 3 existed (len(time_points) * resolution)",
-        )]
-
-
-class VoltageMetric4(VoltageMetricsBaseModel):
-    percent_node_ansi_a_violations: Annotated[
-        List[List],
-        Field(
-            title="percent_node_ansi_a_violations",
-            description="percent of nodes with ANSI A violations at time points. Excludes time "
-            "points with no violations. Inner list is [timestamp, percent].",
-        )]
-
-
-class VoltageMetric5(VoltageMetricsBaseModel):
-    min_voltages: Annotated[
-        Dict,
-        Field(
-            title="min_voltage_by_node",
-            description="Mapping of node name to minimum voltage",
-        )]
-    max_voltages: Annotated[
-        Dict,
-        Field(
-            title="max_voltage_by_node",
-            description="Mapping of node name to maximum voltage",
-        )]
-
-
-class VoltageMetric6(VoltageMetricsBaseModel):
-    num_time_points: Annotated[
-        int,
-        Field(
-            title="num_time_points",
-            description="number of time points that violate ANSI B thresholds",
-        )]
-    percent_time_points: Annotated[
-        float,
-        Field(
-            title="percent_time_points",
-            description="percentage of time points that violate ANSI B thresholds",
-        )]
-    duration: Annotated[
-        timedelta,
-        Field(
-            title="duration",
-            description="amount of time where metric 6 existed (len(num_time_points) * resolution)",
-        )]
-
-
-class VoltageMetricsSummaryModel(VoltageMetricsBaseModel):
-    voltage_duration_between_ansi_a_and_b_minutes: Annotated[
-        int,
-        Field(
-            title="voltage_duration_between_ansi_a_and_b_minutes",
-            description="time in minutes that contain voltages between ANSI A and ANSI B thresholds",
-        )]
-    max_per_node_voltage_duration_outside_ansi_a_minutes: Annotated[
-        float,
-            Field(
-            title="max_per_node_voltage_duration_outside_ansi_a_minutes",
-            description="maximum time in minutes that a node was outside ANSI A thresholds",
-        )]
-    moving_average_voltage_duration_outside_ansi_a_minutes: Annotated[
-        float,
-        Field(
-            title="moving_average_voltage_duration_outside_ansi_a_minutes",
-            description="time in minutes the moving average voltage was outside ANSI A",
-        )]
-    moving_window_minutes: Annotated[
-        int,
-        Field(
-            title="moving_window_minutes",
-            description="window size in minutes for moving average metrics",
-        )]
-    max_voltage: Annotated[
-        float,
-        Field(
-            title="max_voltage",
-            description="maximum voltage seen on any node",
-        )]
-    min_voltage: Annotated[
-        float,
-        Field(
-            title="min_voltage",
-            description="minimum voltage seen on any node",
-        )]
-    num_nodes_always_inside_ansi_a: Annotated[
-        int,
-            Field(
-            title="num_nodes_always_inside_ansi_a",
-            description="number of nodes always inside ANSI A thresholds",
-        )]
-    num_nodes_any_outside_ansi_a_always_inside_ansi_b: Annotated[
-        int,
-        Field(
-            title="num_nodes_any_outside_ansi_a_always_inside_ansi_b",
-            description="number of nodes with some ANSI A violations but no ANSI B violations",
-        )]
-    num_nodes_any_outside_ansi_b: Annotated[
-        int,
-        Field(
-            title="num_nodes_always_outside_ansi_b",
-            description="number of nodes with some ANSI B violations",
-        )]
-    num_time_points_with_ansi_b_violations: Annotated[
-        int,
-        Field(
-            title="num_time_points_with_ansi_b_violations",
-            description="number of time points with ANSI B violations",
-        )]
-    total_num_time_points: Annotated[
-        int,
-        Field(
-            title="total_num_time_points",
-            description="number of time points in the simulation",
-        )]
-    total_simulation_duration: Annotated[
-        timedelta,
-        Field(
-            title="total_simulation_duration",
-            description="total length of time of the simulation",
-        )]
-
-
-VOLTAGE_METRIC_FIELDS_TO_INCLUDE_AS_PASS_CRITERIA = (
-    "voltage_duration_between_ansi_a_and_b_minutes",
-    "max_per_node_voltage_duration_outside_ansi_a_minutes",
-    "moving_average_voltage_duration_outside_ansi_a_minutes",
-    "num_nodes_always_inside_ansi_a",
-    "num_nodes_any_outside_ansi_a_always_inside_ansi_b",
-    "num_nodes_any_outside_ansi_b",
-    "num_time_points_with_ansi_b_violations",
-    "min_voltage",
-    "max_voltage",
-)
-
-
-class VoltageMetricsModel(VoltageMetricsBaseModel):
-    metric_1: Annotated[
-        VoltageMetric1,
-        Field(
-            title="metric_1",
-            description="metric 1",
-        )]
-    metric_2: Annotated[
-        Dict[str, VoltageMetric2],
-        Field(
-            title="metric_2",
-            description="metric 2",
-        )]
-    metric_3: Annotated[
-        VoltageMetric3,
-        Field(
-            title="metric_3",
-            description="metric 3",
-        )]
-    metric_4: Annotated[
-        VoltageMetric4,
-        Field(
-            title="metric_4",
-            description="metric 4",
-        )]
-    metric_5: Annotated[
-        VoltageMetric5,
-        Field(
-            title="metric_5",
-            description="metric 5",
-        )]
-    metric_6: Annotated[
-        VoltageMetric6,
-        Field(
-            title="metric_6",
-            description="metric 6",
-        )]
-    summary: Annotated[
-        Union[VoltageMetricsSummaryModel, None],
-        Field(
-            title="summary",
-            description="summary of metrics",
-        )]
-
-
-class VoltageMetricsByBusTypeModel(VoltageMetricsBaseModel):
-    """Metrics separated by bus type"""
-    primaries: Annotated[
-        VoltageMetricsModel,
-        Field(
-            title="primaries",
-            description="metrics for primary buses",
-        )]
-    secondaries: Annotated[
-        VoltageMetricsModel,
-        Field(
-            title="secondaries",
-            description="metrics for secondary buses",
-        )]
-
-
-class SimulationVoltageMetricsModel(VoltageMetricsBaseModel):
-    scenarios: Annotated[
-        Dict[str, VoltageMetricsByBusTypeModel],
-        Field(
-            title="scenarios",
-            description="voltage metrics by pydss scenario name",
-        )]
-
-
-def compare_voltage_metrics(metrics1: VoltageMetricsByBusTypeModel, metrics2: VoltageMetricsByBusTypeModel):
-    """Compares the values of two instances of VoltageMetricsModel.
-
-    Returns
-    -------
-    bool
-        Return True if they match.
-
-    """
-    match = True
-    fields = (
-        "voltage_duration_between_ansi_a_and_b_minutes",
-        "max_per_node_voltage_duration_outside_ansi_a_minutes",
-        "moving_average_voltage_duration_outside_ansi_a_minutes",
-        "moving_window_minutes",
-        "max_voltage",
-        "min_voltage",
-        "num_nodes_always_inside_ansi_a",
-        "num_nodes_any_outside_ansi_a_always_inside_ansi_b",
-        "num_nodes_any_outside_ansi_b",
-        "num_time_points_with_ansi_b_violations",
-        "total_num_time_points",
-        "total_simulation_duration",
-    )
-    for node_type in ("primaries", "secondaries"):
-        _metrics1 = getattr(metrics1, node_type)
-        _metrics2 = getattr(metrics2, node_type)
-        for field in fields:
-            val1 = getattr(_metrics1.summary, field)
-            val2 = getattr(_metrics2.summary, field)
-            if val1 != val2:
-                logger.error("field=%s mismatch %s != %s", field, val1, val2)
-                match = False
-
-        for field in ("metric_1", "metric_2", "metric_3", "metric_4", "metric_5", "metric_6"):
-            val1 = getattr(_metrics1, field)
-            val2 = getattr(_metrics2, field)
-            if val1 != val2:
-                logger.error("%s mismatch: val1=%s val2=%s", field, val1, val2)
-                match = False
-
-    return match
-
-
-class NodeVoltageMetricsByType:
-    def __init__(self, prop, start_time, resolution, window_size):
-        self._start_time = start_time
-        self._resolution = resolution
-        self._range_a_limits = prop.limits
-        self._range_b_limits = prop.limits_b
-        self._window_size = window_size
-        self._node_names = None
-        self._node_indices = None
-        self._metric_1_time_steps = []
-        self._metric_2_violation_counts = []
-        self._metric_3_time_steps = []
-        self._metric_4_violations = []
-        self._metric_5_min_violations = []
-        self._metric_5_max_violations = []
-        self._num_metric_6_time_points_outside_range_b = 0
-        self._bufs = None
-        self._num_time_points = 0
-
-    @staticmethod
-    def create_summary(metric_1, metric_2, metric_3, metric_5, metric_6, node_names,
-                       num_time_points, resolution, range_a_limits, range_b_limits,
-                       moving_window_minutes):
-        if not node_names:
-            # There may not be any secondary nodes.
-            return None
-
-        max_pnvdoaa = max((x.duration for x in metric_2.values())).total_seconds()
-        vdbaab = metric_1.duration.total_seconds()
-        mmavdoa = metric_3.duration.total_seconds()
-        max_voltage_overall = max(metric_5.max_voltages.values())
-        min_voltage_overall = min(metric_5.min_voltages.values())
-
-        num_nodes_always_inside_range_a = 0
-        num_nodes_any_outside_range_a_no_b = 0
-        num_nodes_any_outside_range_b = 0
-        for node in node_names:
-            min_voltage = metric_5.min_voltages[node]
-            max_voltage = metric_5.max_voltages[node]
-            if (
-                min_voltage < range_b_limits.min
-                or max_voltage > range_b_limits.max
-            ):
-                num_nodes_any_outside_range_b += 1
-            elif (
-                min_voltage < range_a_limits.min
-                or max_voltage > range_a_limits.max
-            ):
-                num_nodes_any_outside_range_a_no_b += 1
-            else:
-                num_nodes_always_inside_range_a += 1
-
-        return VoltageMetricsSummaryModel(
-            voltage_duration_between_ansi_a_and_b_minutes=vdbaab / 60,
-            max_per_node_voltage_duration_outside_ansi_a_minutes=max_pnvdoaa / 60,
-            moving_average_voltage_duration_outside_ansi_a_minutes=mmavdoa / 60,
-            moving_window_minutes=moving_window_minutes,
-            max_voltage=max_voltage_overall,
-            min_voltage=min_voltage_overall,
-            num_nodes_always_inside_ansi_a=num_nodes_always_inside_range_a,
-            num_nodes_any_outside_ansi_a_always_inside_ansi_b=num_nodes_any_outside_range_a_no_b,
-            num_nodes_any_outside_ansi_b=num_nodes_any_outside_range_b,
-            num_time_points_with_ansi_b_violations=metric_6.num_time_points,
-            total_num_time_points=num_time_points,
-            total_simulation_duration=num_time_points * resolution,
-        )
-
-    def generate(self, store_per_element_data):
-        if self._num_time_points == 0:
-            logger.error("Cannot generate report with no time points")
-            return
-
-        metric_1 = VoltageMetric1(
-            time_points=self._metric_1_time_steps,
-            duration=len(self._metric_1_time_steps) * self._resolution,
-        )
-        metric_2 = {
-            self._node_names[i]: VoltageMetric2(
-                duration=x * self._resolution,
-                duration_percentage=x / self._num_time_points * 100,
-            )
-            for i, x in enumerate(self._metric_2_violation_counts)
-        }
-        metric_3 = VoltageMetric3(
-            time_points=self._metric_3_time_steps,
-            duration=len(self._metric_3_time_steps) * self._resolution,
-        )
-        metric_4 = VoltageMetric4(
-            percent_node_ansi_a_violations=self._metric_4_violations,
-        )
-        metric_5 = VoltageMetric5(
-            min_voltages={
-                self._node_names[i]: x
-                for i, x in enumerate(self._metric_5_min_violations)
-            },
-            max_voltages={
-                self._node_names[i]: x
-                for i, x in enumerate(self._metric_5_max_violations)
-            },
-        )
-        metric_6 = VoltageMetric6(
-            num_time_points=self._num_metric_6_time_points_outside_range_b,
-            percent_time_points=self._num_metric_6_time_points_outside_range_b
-                / self._num_time_points
-                * 100,
-            duration=self._num_metric_6_time_points_outside_range_b * self._resolution,
-        )
-        moving_window_minutes = int(
-            (self._window_size * self._resolution).total_seconds() / 60
-        )
-
-        metrics = VoltageMetricsModel(
-            metric_1=metric_1,
-            metric_2=metric_2,
-            metric_3=metric_3,
-            metric_4=metric_4,
-            metric_5=metric_5,
-            metric_6=metric_6,
-            summary=self.create_summary(
-                metric_1, metric_2, metric_3, metric_5, metric_6, self._node_names,
-                self._num_time_points, self._resolution, self._range_a_limits, self._range_b_limits,
-                moving_window_minutes,
-            )
-        )
-
-        if not store_per_element_data:
-            metrics.metric_1.time_points.clear()
-            metrics.metric_2.clear()
-            metrics.metric_3.time_points.clear()
-            metrics.metric_4.percent_node_ansi_a_violations.clear()
-            metrics.metric_5.min_voltages.clear()
-            metrics.metric_5.max_voltages.clear()
-
-        return metrics
-
-    def has_data(self):
-        return self._bufs is not None
-
-    def update(self, time_step, voltages):
-        cur_time = self._start_time + self._resolution * time_step
-        if self._bufs is None:
-            self._bufs = [CircularBufferHelper(self._window_size) for _ in range(len(self._node_names))]
-            self._metric_2_violation_counts = [0] * len(self._node_names)
-            self._metric_5_min_violations = [None] * len(self._node_names)
-            self._metric_5_max_violations = [None] * len(self._node_names)
-
-        count_outside_range_a = 0
-        any_outside_range_b = False
-        any_moving_avg_violates_range_a = False
-        # The voltages passed include all nodes. self._node_indices has the ones
-        # being tracked here.
-        for i, node_index in enumerate(self._node_indices):
-            voltage = voltages[node_index]
-            buf = self._bufs[i]
-            buf.append(voltage.value)
-            if not any_moving_avg_violates_range_a and self._is_outside_range_a(buf.average()):
-                any_moving_avg_violates_range_a = True
-
-            if self._is_outside_range_a(voltage.value):
-                count_outside_range_a += 1
-                self._metric_2_violation_counts[i] += 1
-            if self._is_outside_range_b(voltage.value):
-                any_outside_range_b = True
-            if self._metric_5_min_violations[i] is None:
-                self._metric_5_min_violations[i] = voltage.value
-                self._metric_5_max_violations[i] = voltage.value
-            elif voltage.value < self._metric_5_min_violations[i]:
-                self._metric_5_min_violations[i] = voltage.value
-            elif voltage.value > self._metric_5_max_violations[i]:
-                self._metric_5_max_violations[i] = voltage.value
-
-        if count_outside_range_a > 0:
-            if not any_outside_range_b:
-                self._metric_1_time_steps.append(cur_time)
-
-            percent_violations = count_outside_range_a / len(self._node_names) * 100
-            self._metric_4_violations.append((cur_time, percent_violations))
-
-        if any_moving_avg_violates_range_a:
-            self._metric_3_time_steps.append(cur_time)
-
-        if any_outside_range_b:
-            self._num_metric_6_time_points_outside_range_b += 1
-
-    def _is_outside_range_a(self, value):
-        return value < self._range_a_limits.min or value > self._range_a_limits.max
-
-    def _is_outside_range_b(self, value):
-        return value < self._range_b_limits.min or value > self._range_b_limits.max
-
-    def increment_steps(self):
-        self._num_time_points += 1
-
-    def set_node_info(self, node_names, node_indices):
-        self._node_names = node_names
-        self._node_indices = node_indices
-
-
-class NodeVoltageMetrics:
-    """Stores node voltage metrics in memory.
-
-    The metrics are defined in this paper:
-    https://www.sciencedirect.com/science/article/pii/S0306261920311351
-
-    """
-
-    FILENAME = "voltage_metrics.json"
-
-    def __init__(self, prop, start_time, resolution, window_size, store_per_element_data):
-        self._start_time = start_time
-        self._resolution = resolution
-        self._window_size = window_size
-        self._num_time_points = 0
-        self._metrics = {
-            "primary": NodeVoltageMetricsByType(prop, start_time, resolution, window_size),
-            "secondary": NodeVoltageMetricsByType(prop, start_time, resolution, window_size),
-        }
-        self._store_per_element_data = store_per_element_data
-
-    def generate_report(self, path):
-        """Create a summary file containing all metrics.
-
-        Parameters
-        ----------
-        path : str
-
-        Returns
-        -------
-        str
-            report filename
-
-        """
-        if not self._metrics["primary"].has_data():
-            logger.error("Cannot generate report with no data")
-            return
-
-        metrics = VoltageMetricsByBusTypeModel(
-            primaries=self._metrics["primary"].generate(self._store_per_element_data),
-            secondaries=self._metrics["secondary"].generate(self._store_per_element_data),
-        )
-
-        filename = Path(path) / self.FILENAME
-        with open(filename, "w") as f_out:
-            f_out.write(metrics.model_dump_json())
-            f_out.write("\n")
-
-    def increment_steps(self):
-        """Increment the time step counter."""
-        for metric in self._metrics.values():
-            metric.increment_steps()
-
-    def set_node_info(self, primary_names, primary_indices, secondary_names, secondary_indices):
-        self._metrics["primary"].set_node_info(primary_names, primary_indices)
-        self._metrics["secondary"].set_node_info(secondary_names, secondary_indices)
-
-    def update(self, time_step, voltages):
-        """Update the metrics for the time step.
-
-        Parameters
-        ----------
-        time_step : int
-        voltages : list
-            list of ValueStorageBase
-
-        """
-        for metric in self._metrics.values():
-            metric.update(time_step, voltages)
+from typing import Dict, List, Union, Annotated
+from datetime import datetime, timedelta
+from pathlib import Path
+
+from loguru import logger
+from pydantic import BaseModel, Field
+from pydantic import ConfigDict
+
+from pydss.utils.simulation_utils import CircularBufferHelper
+
+class VoltageMetricsBaseModel(BaseModel):
+    model_config = ConfigDict(title="VoltageMetricsBaseModel", str_strip_whitespace=True, validate_assignment=True, validate_default=True, extra="forbid", use_enum_values=False)
+
+
+class VoltageMetric1(VoltageMetricsBaseModel):
+    time_points: Annotated[
+        List[datetime],
+        Field(
+            None,
+            title="time_points",
+            description="time points that contain voltages between ANSI A and ANSI B thresholds",
+        )]
+    duration: Annotated[
+        timedelta,
+        Field(
+            None,
+            title="duration",
+            description="amount of time where metric 1 existed (len(time_points) * resolution)",
+        )]
+
+
+class VoltageMetric2(VoltageMetricsBaseModel):
+    duration: Annotated[
+        timedelta,
+        Field(
+            title="duration",
+            description="amount of time where a node experienced ANSI A violations",
+        )]
+    duration_percentage: Annotated[
+        float,
+        Field(
+            title="duration_percentage",
+            description="percentage of overall time",
+        )]
+
+
+class VoltageMetric3(VoltageMetricsBaseModel):
+    time_points: Annotated[
+        List[datetime],
+        Field(
+            title="time_points",
+            description="time points where moving average voltages violated ANSI A thresholds",
+        )]
+    duration:Annotated[
+        timedelta,
+        Field(
+            title="duration",
+            description="amount of time where metric 3 existed (len(time_points) * resolution)",
+        )]
+
+
+class VoltageMetric4(VoltageMetricsBaseModel):
+    percent_node_ansi_a_violations: Annotated[
+        List[List],
+        Field(
+            title="percent_node_ansi_a_violations",
+            description="percent of nodes with ANSI A violations at time points. Excludes time "
+            "points with no violations. Inner list is [timestamp, percent].",
+        )]
+
+
+class VoltageMetric5(VoltageMetricsBaseModel):
+    min_voltages: Annotated[
+        Dict,
+        Field(
+            title="min_voltage_by_node",
+            description="Mapping of node name to minimum voltage",
+        )]
+    max_voltages: Annotated[
+        Dict,
+        Field(
+            title="max_voltage_by_node",
+            description="Mapping of node name to maximum voltage",
+        )]
+
+
+class VoltageMetric6(VoltageMetricsBaseModel):
+    num_time_points: Annotated[
+        int,
+        Field(
+            title="num_time_points",
+            description="number of time points that violate ANSI B thresholds",
+        )]
+    percent_time_points: Annotated[
+        float,
+        Field(
+            title="percent_time_points",
+            description="percentage of time points that violate ANSI B thresholds",
+        )]
+    duration: Annotated[
+        timedelta,
+        Field(
+            title="duration",
+            description="amount of time where metric 6 existed (len(num_time_points) * resolution)",
+        )]
+
+
+class VoltageMetricsSummaryModel(VoltageMetricsBaseModel):
+    voltage_duration_between_ansi_a_and_b_minutes: Annotated[
+        int,
+        Field(
+            title="voltage_duration_between_ansi_a_and_b_minutes",
+            description="time in minutes that contain voltages between ANSI A and ANSI B thresholds",
+        )]
+    max_per_node_voltage_duration_outside_ansi_a_minutes: Annotated[
+        float,
+            Field(
+            title="max_per_node_voltage_duration_outside_ansi_a_minutes",
+            description="maximum time in minutes that a node was outside ANSI A thresholds",
+        )]
+    moving_average_voltage_duration_outside_ansi_a_minutes: Annotated[
+        float,
+        Field(
+            title="moving_average_voltage_duration_outside_ansi_a_minutes",
+            description="time in minutes the moving average voltage was outside ANSI A",
+        )]
+    moving_window_minutes: Annotated[
+        int,
+        Field(
+            title="moving_window_minutes",
+            description="window size in minutes for moving average metrics",
+        )]
+    max_voltage: Annotated[
+        float,
+        Field(
+            title="max_voltage",
+            description="maximum voltage seen on any node",
+        )]
+    min_voltage: Annotated[
+        float,
+        Field(
+            title="min_voltage",
+            description="minimum voltage seen on any node",
+        )]
+    num_nodes_always_inside_ansi_a: Annotated[
+        int,
+            Field(
+            title="num_nodes_always_inside_ansi_a",
+            description="number of nodes always inside ANSI A thresholds",
+        )]
+    num_nodes_any_outside_ansi_a_always_inside_ansi_b: Annotated[
+        int,
+        Field(
+            title="num_nodes_any_outside_ansi_a_always_inside_ansi_b",
+            description="number of nodes with some ANSI A violations but no ANSI B violations",
+        )]
+    num_nodes_any_outside_ansi_b: Annotated[
+        int,
+        Field(
+            title="num_nodes_always_outside_ansi_b",
+            description="number of nodes with some ANSI B violations",
+        )]
+    num_time_points_with_ansi_b_violations: Annotated[
+        int,
+        Field(
+            title="num_time_points_with_ansi_b_violations",
+            description="number of time points with ANSI B violations",
+        )]
+    total_num_time_points: Annotated[
+        int,
+        Field(
+            title="total_num_time_points",
+            description="number of time points in the simulation",
+        )]
+    total_simulation_duration: Annotated[
+        timedelta,
+        Field(
+            title="total_simulation_duration",
+            description="total length of time of the simulation",
+        )]
+
+
+VOLTAGE_METRIC_FIELDS_TO_INCLUDE_AS_PASS_CRITERIA = (
+    "voltage_duration_between_ansi_a_and_b_minutes",
+    "max_per_node_voltage_duration_outside_ansi_a_minutes",
+    "moving_average_voltage_duration_outside_ansi_a_minutes",
+    "num_nodes_always_inside_ansi_a",
+    "num_nodes_any_outside_ansi_a_always_inside_ansi_b",
+    "num_nodes_any_outside_ansi_b",
+    "num_time_points_with_ansi_b_violations",
+    "min_voltage",
+    "max_voltage",
+)
+
+
+class VoltageMetricsModel(VoltageMetricsBaseModel):
+    metric_1: Annotated[
+        VoltageMetric1,
+        Field(
+            title="metric_1",
+            description="metric 1",
+        )]
+    metric_2: Annotated[
+        Dict[str, VoltageMetric2],
+        Field(
+            title="metric_2",
+            description="metric 2",
+        )]
+    metric_3: Annotated[
+        VoltageMetric3,
+        Field(
+            title="metric_3",
+            description="metric 3",
+        )]
+    metric_4: Annotated[
+        VoltageMetric4,
+        Field(
+            title="metric_4",
+            description="metric 4",
+        )]
+    metric_5: Annotated[
+        VoltageMetric5,
+        Field(
+            title="metric_5",
+            description="metric 5",
+        )]
+    metric_6: Annotated[
+        VoltageMetric6,
+        Field(
+            title="metric_6",
+            description="metric 6",
+        )]
+    summary: Annotated[
+        Union[VoltageMetricsSummaryModel, None],
+        Field(
+            title="summary",
+            description="summary of metrics",
+        )]
+
+
+class VoltageMetricsByBusTypeModel(VoltageMetricsBaseModel):
+    """Metrics separated by bus type"""
+    primaries: Annotated[
+        VoltageMetricsModel,
+        Field(
+            title="primaries",
+            description="metrics for primary buses",
+        )]
+    secondaries: Annotated[
+        VoltageMetricsModel,
+        Field(
+            title="secondaries",
+            description="metrics for secondary buses",
+        )]
+
+
+class SimulationVoltageMetricsModel(VoltageMetricsBaseModel):
+    scenarios: Annotated[
+        Dict[str, VoltageMetricsByBusTypeModel],
+        Field(
+            title="scenarios",
+            description="voltage metrics by pydss scenario name",
+        )]
+
+
+def compare_voltage_metrics(metrics1: VoltageMetricsByBusTypeModel, metrics2: VoltageMetricsByBusTypeModel):
+    """Compares the values of two instances of VoltageMetricsModel.
+
+    Returns
+    -------
+    bool
+        Return True if they match.
+
+    """
+    match = True
+    fields = (
+        "voltage_duration_between_ansi_a_and_b_minutes",
+        "max_per_node_voltage_duration_outside_ansi_a_minutes",
+        "moving_average_voltage_duration_outside_ansi_a_minutes",
+        "moving_window_minutes",
+        "max_voltage",
+        "min_voltage",
+        "num_nodes_always_inside_ansi_a",
+        "num_nodes_any_outside_ansi_a_always_inside_ansi_b",
+        "num_nodes_any_outside_ansi_b",
+        "num_time_points_with_ansi_b_violations",
+        "total_num_time_points",
+        "total_simulation_duration",
+    )
+    for node_type in ("primaries", "secondaries"):
+        _metrics1 = getattr(metrics1, node_type)
+        _metrics2 = getattr(metrics2, node_type)
+        for field in fields:
+            val1 = getattr(_metrics1.summary, field)
+            val2 = getattr(_metrics2.summary, field)
+            if val1 != val2:
+                logger.error("field=%s mismatch %s != %s", field, val1, val2)
+                match = False
+
+        for field in ("metric_1", "metric_2", "metric_3", "metric_4", "metric_5", "metric_6"):
+            val1 = getattr(_metrics1, field)
+            val2 = getattr(_metrics2, field)
+            if val1 != val2:
+                logger.error("%s mismatch: val1=%s val2=%s", field, val1, val2)
+                match = False
+
+    return match
+
+
+class NodeVoltageMetricsByType:
+    def __init__(self, prop, start_time, resolution, window_size):
+        self._start_time = start_time
+        self._resolution = resolution
+        self._range_a_limits = prop.limits
+        self._range_b_limits = prop.limits_b
+        self._window_size = window_size
+        self._node_names = None
+        self._node_indices = None
+        self._metric_1_time_steps = []
+        self._metric_2_violation_counts = []
+        self._metric_3_time_steps = []
+        self._metric_4_violations = []
+        self._metric_5_min_violations = []
+        self._metric_5_max_violations = []
+        self._num_metric_6_time_points_outside_range_b = 0
+        self._bufs = None
+        self._num_time_points = 0
+
+    @staticmethod
+    def create_summary(metric_1, metric_2, metric_3, metric_5, metric_6, node_names,
+                       num_time_points, resolution, range_a_limits, range_b_limits,
+                       moving_window_minutes):
+        if not node_names:
+            # There may not be any secondary nodes.
+            return None
+
+        max_pnvdoaa = max((x.duration for x in metric_2.values())).total_seconds()
+        vdbaab = metric_1.duration.total_seconds()
+        mmavdoa = metric_3.duration.total_seconds()
+        max_voltage_overall = max(metric_5.max_voltages.values())
+        min_voltage_overall = min(metric_5.min_voltages.values())
+
+        num_nodes_always_inside_range_a = 0
+        num_nodes_any_outside_range_a_no_b = 0
+        num_nodes_any_outside_range_b = 0
+        for node in node_names:
+            min_voltage = metric_5.min_voltages[node]
+            max_voltage = metric_5.max_voltages[node]
+            if (
+                min_voltage < range_b_limits.min
+                or max_voltage > range_b_limits.max
+            ):
+                num_nodes_any_outside_range_b += 1
+            elif (
+                min_voltage < range_a_limits.min
+                or max_voltage > range_a_limits.max
+            ):
+                num_nodes_any_outside_range_a_no_b += 1
+            else:
+                num_nodes_always_inside_range_a += 1
+
+        return VoltageMetricsSummaryModel(
+            voltage_duration_between_ansi_a_and_b_minutes=vdbaab / 60,
+            max_per_node_voltage_duration_outside_ansi_a_minutes=max_pnvdoaa / 60,
+            moving_average_voltage_duration_outside_ansi_a_minutes=mmavdoa / 60,
+            moving_window_minutes=moving_window_minutes,
+            max_voltage=max_voltage_overall,
+            min_voltage=min_voltage_overall,
+            num_nodes_always_inside_ansi_a=num_nodes_always_inside_range_a,
+            num_nodes_any_outside_ansi_a_always_inside_ansi_b=num_nodes_any_outside_range_a_no_b,
+            num_nodes_any_outside_ansi_b=num_nodes_any_outside_range_b,
+            num_time_points_with_ansi_b_violations=metric_6.num_time_points,
+            total_num_time_points=num_time_points,
+            total_simulation_duration=num_time_points * resolution,
+        )
+
+    def generate(self, store_per_element_data):
+        if self._num_time_points == 0:
+            logger.error("Cannot generate report with no time points")
+            return
+
+        metric_1 = VoltageMetric1(
+            time_points=self._metric_1_time_steps,
+            duration=len(self._metric_1_time_steps) * self._resolution,
+        )
+        metric_2 = {
+            self._node_names[i]: VoltageMetric2(
+                duration=x * self._resolution,
+                duration_percentage=x / self._num_time_points * 100,
+            )
+            for i, x in enumerate(self._metric_2_violation_counts)
+        }
+        metric_3 = VoltageMetric3(
+            time_points=self._metric_3_time_steps,
+            duration=len(self._metric_3_time_steps) * self._resolution,
+        )
+        metric_4 = VoltageMetric4(
+            percent_node_ansi_a_violations=self._metric_4_violations,
+        )
+        metric_5 = VoltageMetric5(
+            min_voltages={
+                self._node_names[i]: x
+                for i, x in enumerate(self._metric_5_min_violations)
+            },
+            max_voltages={
+                self._node_names[i]: x
+                for i, x in enumerate(self._metric_5_max_violations)
+            },
+        )
+        metric_6 = VoltageMetric6(
+            num_time_points=self._num_metric_6_time_points_outside_range_b,
+            percent_time_points=self._num_metric_6_time_points_outside_range_b
+                / self._num_time_points
+                * 100,
+            duration=self._num_metric_6_time_points_outside_range_b * self._resolution,
+        )
+        moving_window_minutes = int(
+            (self._window_size * self._resolution).total_seconds() / 60
+        )
+
+        metrics = VoltageMetricsModel(
+            metric_1=metric_1,
+            metric_2=metric_2,
+            metric_3=metric_3,
+            metric_4=metric_4,
+            metric_5=metric_5,
+            metric_6=metric_6,
+            summary=self.create_summary(
+                metric_1, metric_2, metric_3, metric_5, metric_6, self._node_names,
+                self._num_time_points, self._resolution, self._range_a_limits, self._range_b_limits,
+                moving_window_minutes,
+            )
+        )
+
+        if not store_per_element_data:
+            metrics.metric_1.time_points.clear()
+            metrics.metric_2.clear()
+            metrics.metric_3.time_points.clear()
+            metrics.metric_4.percent_node_ansi_a_violations.clear()
+            metrics.metric_5.min_voltages.clear()
+            metrics.metric_5.max_voltages.clear()
+
+        return metrics
+
+    def has_data(self):
+        return self._bufs is not None
+
+    def update(self, time_step, voltages):
+        cur_time = self._start_time + self._resolution * time_step
+        if self._bufs is None:
+            self._bufs = [CircularBufferHelper(self._window_size) for _ in range(len(self._node_names))]
+            self._metric_2_violation_counts = [0] * len(self._node_names)
+            self._metric_5_min_violations = [None] * len(self._node_names)
+            self._metric_5_max_violations = [None] * len(self._node_names)
+
+        count_outside_range_a = 0
+        any_outside_range_b = False
+        any_moving_avg_violates_range_a = False
+        # The voltages passed include all nodes. self._node_indices has the ones
+        # being tracked here.
+        for i, node_index in enumerate(self._node_indices):
+            voltage = voltages[node_index]
+            buf = self._bufs[i]
+            buf.append(voltage.value)
+            if not any_moving_avg_violates_range_a and self._is_outside_range_a(buf.average()):
+                any_moving_avg_violates_range_a = True
+
+            if self._is_outside_range_a(voltage.value):
+                count_outside_range_a += 1
+                self._metric_2_violation_counts[i] += 1
+            if self._is_outside_range_b(voltage.value):
+                any_outside_range_b = True
+            if self._metric_5_min_violations[i] is None:
+                self._metric_5_min_violations[i] = voltage.value
+                self._metric_5_max_violations[i] = voltage.value
+            elif voltage.value < self._metric_5_min_violations[i]:
+                self._metric_5_min_violations[i] = voltage.value
+            elif voltage.value > self._metric_5_max_violations[i]:
+                self._metric_5_max_violations[i] = voltage.value
+
+        if count_outside_range_a > 0:
+            if not any_outside_range_b:
+                self._metric_1_time_steps.append(cur_time)
+
+            percent_violations = count_outside_range_a / len(self._node_names) * 100
+            self._metric_4_violations.append((cur_time, percent_violations))
+
+        if any_moving_avg_violates_range_a:
+            self._metric_3_time_steps.append(cur_time)
+
+        if any_outside_range_b:
+            self._num_metric_6_time_points_outside_range_b += 1
+
+    def _is_outside_range_a(self, value):
+        return value < self._range_a_limits.min or value > self._range_a_limits.max
+
+    def _is_outside_range_b(self, value):
+        return value < self._range_b_limits.min or value > self._range_b_limits.max
+
+    def increment_steps(self):
+        self._num_time_points += 1
+
+    def set_node_info(self, node_names, node_indices):
+        self._node_names = node_names
+        self._node_indices = node_indices
+
+
+class NodeVoltageMetrics:
+    """Stores node voltage metrics in memory.
+
+    The metrics are defined in this paper:
+    https://www.sciencedirect.com/science/article/pii/S0306261920311351
+
+    """
+
+    FILENAME = "voltage_metrics.json"
+
+    def __init__(self, prop, start_time, resolution, window_size, store_per_element_data):
+        self._start_time = start_time
+        self._resolution = resolution
+        self._window_size = window_size
+        self._num_time_points = 0
+        self._metrics = {
+            "primary": NodeVoltageMetricsByType(prop, start_time, resolution, window_size),
+            "secondary": NodeVoltageMetricsByType(prop, start_time, resolution, window_size),
+        }
+        self._store_per_element_data = store_per_element_data
+
+    def generate_report(self, path):
+        """Create a summary file containing all metrics.
+
+        Parameters
+        ----------
+        path : str
+
+        Returns
+        -------
+        str
+            report filename
+
+        """
+        if not self._metrics["primary"].has_data():
+            logger.error("Cannot generate report with no data")
+            return
+
+        metrics = VoltageMetricsByBusTypeModel(
+            primaries=self._metrics["primary"].generate(self._store_per_element_data),
+            secondaries=self._metrics["secondary"].generate(self._store_per_element_data),
+        )
+
+        filename = Path(path) / self.FILENAME
+        with open(filename, "w") as f_out:
+            f_out.write(metrics.model_dump_json())
+            f_out.write("\n")
+
+    def increment_steps(self):
+        """Increment the time step counter."""
+        for metric in self._metrics.values():
+            metric.increment_steps()
+
+    def set_node_info(self, primary_names, primary_indices, secondary_names, secondary_indices):
+        self._metrics["primary"].set_node_info(primary_names, primary_indices)
+        self._metrics["secondary"].set_node_info(secondary_names, secondary_indices)
+
+    def update(self, time_step, voltages):
+        """Update the metrics for the time step.
+
+        Parameters
+        ----------
+        time_step : int
+        voltages : list
+            list of ValueStorageBase
+
+        """
+        for metric in self._metrics.values():
+            metric.update(time_step, voltages)
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyContrReader.py` & `nrel_pydss-3.1.4/src/pydss/pyContrReader.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,94 +1,94 @@
-from collections import defaultdict
-
-import pandas as pd
-import numpy as np
-import os
-
-import toml
-
-from pydss.config_data import convert_config_data_to_toml
-from pydss.registry import Registry
-from pydss.utils.utils import load_data
-from pydss.exceptions import InvalidConfiguration, InvalidParameter
-
-class pyContrReader:
-    def __init__(self, Path):
-        self.pyControllers = {}
-        filenames = os.listdir(Path)
-        found_config_file = False
-        found_excel_file = False
-        for filename in filenames:
-            pyControllerType, ext = os.path.splitext(filename)
-            if filename.startswith('~$'):
-                continue
-            elif ext == '.xlsx':
-                filename = convert_config_data_to_toml(filename)
-            elif ext != ".toml":
-                continue
-            if pyControllerType not in self.pyControllers:
-                self.pyControllers[pyControllerType] = {}
-            filepath = os.path.join(Path, filename)
-            assert (os.path.exists(filepath)), 'path: "{}" does not exist!'.format(filepath)
-            for name, controller in load_data(filepath).items():
-                if name in self.pyControllers[pyControllerType]:
-                    raise InvalidParameter(
-                        f"Multiple pydss controller definitions for a single OpenDSS element not allowed: {name}"
-                    )
-                self.pyControllers[pyControllerType][name] = controller
-
-
-def read_controller_settings_from_registry(path):
-    registry = Registry()
-    controllers = defaultdict(dict)
-    controller_settings = defaultdict(dict)
-    filenames = os.listdir(path)
-    for filename in filenames:
-        controller_type, ext = os.path.splitext(filename)
-        # This file contains a mapping of controller to an array of names.
-        # The controller settings must be stored in the pydss registry.
-
-        controller_to_name = load_data(os.path.join(path, filename))
-        
-        for controller, names in controller_to_name.items():
-            settings = controller_settings[controller_type].get(controller)
-
-            if settings is None:
-                if not registry.is_controller_registered(controller_type, controller):
-                    raise InvalidConfiguration(
-                        f"{controller_type} / {controller} is not registered"
-                    )
-                settings = registry.read_controller_settings(controller_type, controller)
-                controller_settings[controller_type][controller] = settings
-            for name in names:
-                controllers[controller_type][name] = settings
-
-    return controllers
-
-
-class pySubscriptionReader:
-    def __init__(self, filePath):
-        self.SubscriptionList = {}
-        if not os.path.exists(filePath):
-            raise FileNotFoundError('path: "{}" does not exist!'.format(filePath))
-
-        for elem, elem_data in load_data(filePath).items():
-            if elem_data["Subscribe"]:
-                self.SubscriptionList[elem] = elem_data
-
-
-class pyExportReader:
-    def __init__(self, filePath):
-        self.pyControllers = {}
-        self.publicationList = []
-        xlsx_filename = os.path.splitext(filePath)[0] + '.xlsx'
-        if not os.path.exists(filePath) and os.path.exists(xlsx_filename):
-            convert_config_data_to_toml(xlsx_filename)
-
-        if not os.path.exists(filePath):
-            raise FileNotFoundError('path: "{}" does not exist!'.format(filePath))
-
-        for elem, elem_data in load_data(filePath).items():
-            self.pyControllers[elem] = elem_data["Publish"][:]
-            self.pyControllers[elem] += elem_data["NoPublish"]
-            for item in elem_data["Publish"]:
-                self.publicationList.append(f"{elem} {item}")
+from collections import defaultdict
+
+import pandas as pd
+import numpy as np
+import os
+
+import toml
+
+from pydss.config_data import convert_config_data_to_toml
+from pydss.registry import Registry
+from pydss.utils.utils import load_data
+from pydss.exceptions import InvalidConfiguration, InvalidParameter
+
+class pyContrReader:
+    def __init__(self, Path):
+        self.pyControllers = {}
+        filenames = os.listdir(Path)
+        found_config_file = False
+        found_excel_file = False
+        for filename in filenames:
+            pyControllerType, ext = os.path.splitext(filename)
+            if filename.startswith('~$'):
+                continue
+            elif ext == '.xlsx':
+                filename = convert_config_data_to_toml(filename)
+            elif ext != ".toml":
+                continue
+            if pyControllerType not in self.pyControllers:
+                self.pyControllers[pyControllerType] = {}
+            filepath = os.path.join(Path, filename)
+            assert (os.path.exists(filepath)), 'path: "{}" does not exist!'.format(filepath)
+            for name, controller in load_data(filepath).items():
+                if name in self.pyControllers[pyControllerType]:
+                    raise InvalidParameter(
+                        f"Multiple pydss controller definitions for a single OpenDSS element not allowed: {name}"
+                    )
+                self.pyControllers[pyControllerType][name] = controller
+
+
+def read_controller_settings_from_registry(path):
+    registry = Registry()
+    controllers = defaultdict(dict)
+    controller_settings = defaultdict(dict)
+    filenames = os.listdir(path)
+    for filename in filenames:
+        controller_type, ext = os.path.splitext(filename)
+        # This file contains a mapping of controller to an array of names.
+        # The controller settings must be stored in the pydss registry.
+
+        controller_to_name = load_data(os.path.join(path, filename))
+        
+        for controller, names in controller_to_name.items():
+            settings = controller_settings[controller_type].get(controller)
+
+            if settings is None:
+                if not registry.is_controller_registered(controller_type, controller):
+                    raise InvalidConfiguration(
+                        f"{controller_type} / {controller} is not registered"
+                    )
+                settings = registry.read_controller_settings(controller_type, controller)
+                controller_settings[controller_type][controller] = settings
+            for name in names:
+                controllers[controller_type][name] = settings
+
+    return controllers
+
+
+class pySubscriptionReader:
+    def __init__(self, filePath):
+        self.SubscriptionList = {}
+        if not os.path.exists(filePath):
+            raise FileNotFoundError('path: "{}" does not exist!'.format(filePath))
+
+        for elem, elem_data in load_data(filePath).items():
+            if elem_data["Subscribe"]:
+                self.SubscriptionList[elem] = elem_data
+
+
+class pyExportReader:
+    def __init__(self, filePath):
+        self.pyControllers = {}
+        self.publicationList = []
+        xlsx_filename = os.path.splitext(filePath)[0] + '.xlsx'
+        if not os.path.exists(filePath) and os.path.exists(xlsx_filename):
+            convert_config_data_to_toml(xlsx_filename)
+
+        if not os.path.exists(filePath):
+            raise FileNotFoundError('path: "{}" does not exist!'.format(filePath))
+
+        for elem, elem_data in load_data(filePath).items():
+            self.pyControllers[elem] = elem_data["Publish"][:]
+            self.pyControllers[elem] += elem_data["NoPublish"]
+            for item in elem_data["Publish"]:
+                self.publicationList.append(f"{elem} {item}")
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyDSS.py` & `nrel_pydss-3.1.4/src/pydss/pyDSS.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,66 +1,66 @@
-import pydss
-import os
-
-from loguru import logger
-
-from pydss.common import RUN_SIMULATION_FILENAME
-from pydss.exceptions import InvalidConfiguration
-from pydss.dssInstance import OpenDSS
-from pydss.simulation_input_models import SimulationSettingsModel, dump_settings
-from pydss.utils.utils import dump_data, load_data
-
-PYDSS_BASE_DIR = os.path.join(os.path.dirname(getattr(pydss, "__path__")[0]), "pydss")
-
-class instance(object):
-
-    def __init__(self):
-        self._estimated_space = None
-
-    def run(self, settings: SimulationSettingsModel, project, scenario, dry_run=False):
-        
-        self.run_scenario(
-            project,
-            scenario,
-            settings,
-            dry_run=dry_run,
-        )
-        return
-
-    def create_dss_instance(self, dss_args):
-        return OpenDSS(dss_args)
-
-    def run_scenario(self, project, scenario, settings: SimulationSettingsModel, dry_run=False):
-        if dry_run:
-            dss = OpenDSS(settings)
-            self._dump_scenario_simulation_settings(settings)
-            #dss.init(dss_args)
-            logger.info('Dry run scenario: %s', settings.project.active_scenario)
-            if settings.monte_carlo.num_scenarios > 0:
-                raise InvalidConfiguration("Dry run does not support MonteCarlo simulation.")
-            else:
-                self._estimated_space = dss.DryRunSimulation(project, scenario)
-            return None, None
-
-        opendss = OpenDSS(settings)
-        self._dump_scenario_simulation_settings(settings)
-        logger.info('Running scenario: %s', settings.project.active_scenario)
-        if settings.monte_carlo.num_scenarios > 0:
-            opendss.RunMCsimulation(project, scenario, samples=settings.monte_carlo.num_scenarios)
-        else:
-            for is_complete, _, _, _ in opendss.RunSimulation(project, scenario):
-                if is_complete:
-                    break
-
-    def get_estimated_space(self):
-        return self._estimated_space
-
-    def _dump_scenario_simulation_settings(self, settings: SimulationSettingsModel):
-        # Various settings may have been updated. Write the actual settings to a file.
-        scenario_simulation_filename = os.path.join(
-            settings.project.project_path,
-            settings.project.active_project,
-            "Scenarios",
-            settings.project.active_scenario,
-            RUN_SIMULATION_FILENAME,
-        )
-        dump_settings(settings, scenario_simulation_filename)
+import pydss
+import os
+
+from loguru import logger
+
+from pydss.common import RUN_SIMULATION_FILENAME
+from pydss.exceptions import InvalidConfiguration
+from pydss.dssInstance import OpenDSS
+from pydss.simulation_input_models import SimulationSettingsModel, dump_settings
+from pydss.utils.utils import dump_data, load_data
+
+PYDSS_BASE_DIR = os.path.join(os.path.dirname(getattr(pydss, "__path__")[0]), "pydss")
+
+class instance(object):
+
+    def __init__(self):
+        self._estimated_space = None
+
+    def run(self, settings: SimulationSettingsModel, project, scenario, dry_run=False):
+        
+        self.run_scenario(
+            project,
+            scenario,
+            settings,
+            dry_run=dry_run,
+        )
+        return
+
+    def create_dss_instance(self, dss_args):
+        return OpenDSS(dss_args)
+
+    def run_scenario(self, project, scenario, settings: SimulationSettingsModel, dry_run=False):
+        if dry_run:
+            dss = OpenDSS(settings)
+            self._dump_scenario_simulation_settings(settings)
+            #dss.init(dss_args)
+            logger.info('Dry run scenario: %s', settings.project.active_scenario)
+            if settings.monte_carlo.num_scenarios > 0:
+                raise InvalidConfiguration("Dry run does not support MonteCarlo simulation.")
+            else:
+                self._estimated_space = dss.DryRunSimulation(project, scenario)
+            return None, None
+
+        opendss = OpenDSS(settings)
+        self._dump_scenario_simulation_settings(settings)
+        logger.info('Running scenario: %s', settings.project.active_scenario)
+        if settings.monte_carlo.num_scenarios > 0:
+            opendss.RunMCsimulation(project, scenario, samples=settings.monte_carlo.num_scenarios)
+        else:
+            for is_complete, _, _, _ in opendss.RunSimulation(project, scenario):
+                if is_complete:
+                    break
+
+    def get_estimated_space(self):
+        return self._estimated_space
+
+    def _dump_scenario_simulation_settings(self, settings: SimulationSettingsModel):
+        # Various settings may have been updated. Write the actual settings to a file.
+        scenario_simulation_filename = os.path.join(
+            settings.project.project_path,
+            settings.project.active_project,
+            "Scenarios",
+            settings.project.active_scenario,
+            RUN_SIMULATION_FILENAME,
+        )
+        dump_settings(settings, scenario_simulation_filename)
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyResults.py` & `nrel_pydss-3.1.4/src/pydss/pyResults.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,86 +1,86 @@
-import pandas as pd
-import numpy as np
-import os
-
-class pyContrReader:
-    def __init__(self, Path):
-        self.pyControllers = {}
-        filenames = os.listdir(Path)
-        for filename in filenames:
-            if filename.endswith('.xlsx') and not filename.startswith('~$'):
-                pyControllerType  = filename.split('.')[0]
-                filepath = os.path.join(Path, filename)
-                assert (os.path.exists(filepath)), 'path: "{}" does not exist!'.format(filepath)
-                ControllerDataset = pd.read_excel(filepath, skiprows=[0,], index_col=[0])
-                pyControllerNames = ControllerDataset.index.tolist()
-                pyController = {}
-                for pyControllerName in pyControllerNames:
-                    pyControllerData = ControllerDataset.loc[pyControllerName]
-                    assert len(pyControllerData == 1), 'Multiple pydss controller definitions for a single OpenDSS ' +\
-                                                       'element not allowed'
-                    pyControllerDict = pyControllerData.to_dict()
-                    pyController[pyControllerName] = pyControllerDict
-                self.pyControllers[pyControllerType] = pyController
-
-
-class pySubscriptionReader:
-    def __init__(self, filePath):
-        self.SubscriptionDict = {}
-        assert (os.path.exists(filePath)), 'path: "{}" does not exist!'.format(filePath)
-        SubscriptionData = pd.read_excel(filePath, skiprows=[0,], index_col=[0])
-        requiredColumns = {'Property', 'Subscription ID', 'Unit', 'Subscribe', 'Data type'}
-        fileColumns = set(SubscriptionData.columns)
-        diff  = requiredColumns.difference(fileColumns)
-
-        assert (len(diff) == 0), 'Missing column in the subscriptions file.\nRequired columns: {}'.format(
-            requiredColumns
-        )
-        Subscribe = SubscriptionData['Subscribe']
-        assert (Subscribe.dtype == bool), 'The subscribe column can only have boolean values.'
-        self.SubscriptionDict = SubscriptionData.T.to_dict()
-
-
-
-
-class pyExportReader:
-    def __init__(self, filePath):
-        self.pyControllers = {}
-        self.publicationList = []
-        assert (os.path.exists(filePath)), 'path: "{}" does not exist!'.format(filePath)
-        ControllerDataset = pd.read_excel(filePath, skiprows=[0,], index_col=[0])
-        assert (ControllerDataset.columns[0] == 'Publish'), 'First column after class declarations in the ' +\
-                                                            'export defination files  should have column ' +\
-                                                            'name "Publish"'
-
-        Publish = ControllerDataset['Publish']
-        assert (Publish.dtype == bool), 'The publish column can only have boolean values.'
-        ControllerDatasetFiltered = ControllerDataset[ControllerDataset.columns[1:]]
-
-        pyControllerNames = ControllerDatasetFiltered.index.tolist()
-        pyController = {}
-        for pyControllerName, doPublish in zip(pyControllerNames, Publish.values):
-            pyControllerData = ControllerDatasetFiltered.loc[pyControllerName]
-            pulishdata = ControllerDataset['Publish'].loc[pyControllerName]
-            if isinstance(pulishdata, np.bool_):
-                pulishdata = [pulishdata]
-            else:
-                pulishdata = pulishdata.dropna().values
-            Data = pyControllerData.copy()
-            Data.index = range(len(Data))
-            for i, publish in enumerate(pulishdata):
-                if publish:
-                    if isinstance(Data, pd.core.frame.DataFrame):
-                        properties = Data.loc[i].dropna()
-                    else:
-                        properties = Data.dropna().values
-                    for property in properties:
-                        self.publicationList.append("{} {}".format(pyControllerName, property))
-
-            if len(pyControllerData) > 1:
-                pyControllerData = pd.Series(pyControllerData.values.flatten())
-                pyControllerDict = pyControllerData.dropna().to_dict()
-            else:
-                pyControllerDict = pyControllerData.dropna().to_dict()
-
-            self.pyControllers[pyControllerName] = pyControllerDict
+import pandas as pd
+import numpy as np
+import os
+
+class pyContrReader:
+    def __init__(self, Path):
+        self.pyControllers = {}
+        filenames = os.listdir(Path)
+        for filename in filenames:
+            if filename.endswith('.xlsx') and not filename.startswith('~$'):
+                pyControllerType  = filename.split('.')[0]
+                filepath = os.path.join(Path, filename)
+                assert (os.path.exists(filepath)), 'path: "{}" does not exist!'.format(filepath)
+                ControllerDataset = pd.read_excel(filepath, skiprows=[0,], index_col=[0])
+                pyControllerNames = ControllerDataset.index.tolist()
+                pyController = {}
+                for pyControllerName in pyControllerNames:
+                    pyControllerData = ControllerDataset.loc[pyControllerName]
+                    assert len(pyControllerData == 1), 'Multiple pydss controller definitions for a single OpenDSS ' +\
+                                                       'element not allowed'
+                    pyControllerDict = pyControllerData.to_dict()
+                    pyController[pyControllerName] = pyControllerDict
+                self.pyControllers[pyControllerType] = pyController
+
+
+class pySubscriptionReader:
+    def __init__(self, filePath):
+        self.SubscriptionDict = {}
+        assert (os.path.exists(filePath)), 'path: "{}" does not exist!'.format(filePath)
+        SubscriptionData = pd.read_excel(filePath, skiprows=[0,], index_col=[0])
+        requiredColumns = {'Property', 'Subscription ID', 'Unit', 'Subscribe', 'Data type'}
+        fileColumns = set(SubscriptionData.columns)
+        diff  = requiredColumns.difference(fileColumns)
+
+        assert (len(diff) == 0), 'Missing column in the subscriptions file.\nRequired columns: {}'.format(
+            requiredColumns
+        )
+        Subscribe = SubscriptionData['Subscribe']
+        assert (Subscribe.dtype == bool), 'The subscribe column can only have boolean values.'
+        self.SubscriptionDict = SubscriptionData.T.to_dict()
+
+
+
+
+class pyExportReader:
+    def __init__(self, filePath):
+        self.pyControllers = {}
+        self.publicationList = []
+        assert (os.path.exists(filePath)), 'path: "{}" does not exist!'.format(filePath)
+        ControllerDataset = pd.read_excel(filePath, skiprows=[0,], index_col=[0])
+        assert (ControllerDataset.columns[0] == 'Publish'), 'First column after class declarations in the ' +\
+                                                            'export defination files  should have column ' +\
+                                                            'name "Publish"'
+
+        Publish = ControllerDataset['Publish']
+        assert (Publish.dtype == bool), 'The publish column can only have boolean values.'
+        ControllerDatasetFiltered = ControllerDataset[ControllerDataset.columns[1:]]
+
+        pyControllerNames = ControllerDatasetFiltered.index.tolist()
+        pyController = {}
+        for pyControllerName, doPublish in zip(pyControllerNames, Publish.values):
+            pyControllerData = ControllerDatasetFiltered.loc[pyControllerName]
+            pulishdata = ControllerDataset['Publish'].loc[pyControllerName]
+            if isinstance(pulishdata, np.bool_):
+                pulishdata = [pulishdata]
+            else:
+                pulishdata = pulishdata.dropna().values
+            Data = pyControllerData.copy()
+            Data.index = range(len(Data))
+            for i, publish in enumerate(pulishdata):
+                if publish:
+                    if isinstance(Data, pd.core.frame.DataFrame):
+                        properties = Data.loc[i].dropna()
+                    else:
+                        properties = Data.dropna().values
+                    for property in properties:
+                        self.publicationList.append("{} {}".format(pyControllerName, property))
+
+            if len(pyControllerData) > 1:
+                pyControllerData = pd.Series(pyControllerData.values.flatten())
+                pyControllerDict = pyControllerData.dropna().to_dict()
+            else:
+                pyControllerDict = pyControllerData.dropna().to_dict()
+
+            self.pyControllers[pyControllerName] = pyControllerDict
         self.publicationList = list(set(self.publicationList))
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pydss_fs_interface.py` & `nrel_pydss-3.1.4/src/pydss/pydss_fs_interface.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,419 +1,419 @@
-"""Interface to read pydss files on differing filesystem structures."""
-
-import abc
-import io
-import json
-import os
-import sys
-import tarfile
-import zipfile
-
-from loguru import logger
-import pandas as pd
-import toml
-
-from pydss.common import PLOTS_FILENAME, PROJECT_TAR, PROJECT_ZIP, \
-    ControllerType, ExportMode, SIMULATION_SETTINGS_FILENAME
-from pydss.exceptions import InvalidConfiguration
-from pydss.simulation_input_models import SimulationSettingsModel, load_simulation_settings
-from pydss.utils.utils import load_data
-
-
-STORE_FILENAME = "store.h5"
-SCENARIOS = "Scenarios"
-PROJECT_DIRECTORIES = ("DSSfiles", "Exports", "Logs", "Scenarios")
-
-class PyDssFileSystemInterface(abc.ABC):
-    """Interface to read pydss files on differing filesystem structures."""
-
-    @abc.abstractmethod
-    def exists(self, filename):
-        """Return True if the filename exists.
-
-        Parameters
-        ----------
-        filename : str
-
-        Returns
-        -------
-        bool
-
-        """
-
-    @abc.abstractmethod
-    def read_file(self, path):
-        """Return the contents of file.
-
-        Parameters
-        ----------
-        path : str
-
-        Returns
-        -------
-        str
-
-        """
-
-    @abc.abstractmethod
-    def read_csv(self, path):
-        """Return a pandas DataFrame from the CSV file.
-
-        Parameters
-        ----------
-        path : str
-
-        Returns
-        -------
-        str
-
-        """
-
-    def read_controller_config(self, scenario):
-        """Read the controller config for a scenario from disk.
-
-        Parameters
-        ----------
-        scenario : str
-            scenario name
-
-        Returns
-        -------
-        dict
-
-        """
-
-    @abc.abstractmethod
-    def read_export_config(self, scenario):
-        """Read the export config for a scenario from disk.
-
-        Parameters
-        ----------
-        scenario : str
-            scenario name
-
-        Returns
-        -------
-        dict
-
-        """
-
-    @abc.abstractmethod
-    def read_plot_config(self, scenario):
-        """Read the export config for a scenario from disk.
-
-        Parameters
-        ----------
-        scenario : str
-            scenario name
-
-        Returns
-        -------
-        dict
-
-        """
-
-    @abc.abstractmethod
-    def read_scenario_export_metadata(self, scenario_name):
-        """Return the metadata for a scenario's exported data.
-
-        Parameters
-        ----------
-        scenario_name : str
-
-        Returns
-        -------
-        dict
-
-        """
-
-    @abc.abstractmethod
-    def read_scenario_pv_profiles(self, scenario_name):
-        """Return the PV profiles for a scenario.
-
-        Parameters
-        ----------
-        scenario_name : str
-
-        Returns
-        -------
-        dict
-
-        """
-
-
-    @property
-    def scenario_names(self):
-        """Return the scenario names in the project.
-
-        Returns
-        -------
-        list
-
-        """
-        return [x.name for x in self._settings.project.scenarios]
-
-    @property
-    @abc.abstractmethod
-    def simulation_config(self):
-        """Return the config from the simulation settings file.
-
-        Returns
-        -------
-        dict
-
-        """
-
-    def _check_scenarios(self):
-        scenarios = self._list_scenario_names()
-
-        if scenarios is None:
-            return
-
-        exp_scenarios = self.scenario_names
-        exp_scenarios.sort()
-
-        for scenario in exp_scenarios:
-            if scenario not in scenarios:
-                raise InvalidConfiguration(
-                    f"{scenario} is not a valid scenario. Valid scenarios: {scenarios}"
-                )
-
-    @abc.abstractmethod
-    def _list_scenario_names(self):
-        """Return the scenario names on disk.
-
-        Returns
-        -------
-        list
-
-        """
-
-
-class PyDssFileSystemInterface(PyDssFileSystemInterface):
-    """Reads pydss files when the project is expanded into directories."""
-    def __init__(self, project_dir, simulation_file):
-        self._project_dir = project_dir
-        self._scenarios_dir = os.path.join(self._project_dir, SCENARIOS)
-        self._dss_dir = os.path.join(self._project_dir, "DSSfiles")
-
-        self._settings = load_simulation_settings(
-            os.path.join(self._project_dir, simulation_file)
-        )
-
-        self._check_scenarios()
-
-    def _get_full_path(self, path):
-        return os.path.join(self._project_dir, path)
-
-    def exists(self, filename):
-        return os.path.exists(filename)
-
-    def read_file(self, path):
-        with open(self._get_full_path(path)) as f_in:
-            return f_in.read()
-
-    def read_csv(self, path):
-        return pd.read_csv(self._get_full_path(path))
-
-    def _list_scenario_names(self):
-        scenarios = [
-            x for x in os.listdir(self._scenarios_dir)
-            if os.path.isdir(os.path.join(self._scenarios_dir, x))
-        ]
-        scenarios.sort()
-        return scenarios
-
-    def read_controller_config(self, scenario):
-        controllers = {}
-        path = os.path.join(self._project_dir, SCENARIOS, scenario, "pyControllerList")
-        if not os.path.exists(path):
-            return controllers
-        for filename in os.listdir(path):
-            base, ext = os.path.splitext(filename)
-            if ext == ".toml":
-                controller_type = ControllerType(base)
-                controllers[controller_type] = load_data(os.path.join(path, filename))
-
-        return controllers
-
-    def read_export_config(self, scenario):
-        exports = {}
-        path = os.path.join(self._project_dir, SCENARIOS, scenario, "ExportLists")
-        for filename in os.listdir(path):
-            base, ext = os.path.splitext(filename)
-            if ext == ".toml":
-                export_mode = ExportMode(base)
-                exports[export_mode] = load_data(os.path.join(path, filename))
-
-        return exports
-
-    def read_plot_config(self, scenario):
-        path = os.path.join(self._project_dir, SCENARIOS, scenario, "pyPlotList", PLOTS_FILENAME)
-        return load_data(path)
-
-    def read_scenario_export_metadata(self, scenario_name):
-        filename = os.path.join(
-            self._project_dir,
-            "Exports",
-            scenario_name,
-            "metadata.json",
-        )
-        if not self.exists(filename):
-            return {}
-        return load_data(filename)
-
-    def read_scenario_pv_profiles(self, scenario_name):
-        filename = os.path.join(
-            self._project_dir,
-            "Exports",
-            scenario_name,
-            "pv_profiles.json",
-        )
-        if not os.path.exists(filename):
-            return {}
-        return load_data(filename)
-
-    @property
-    def simulation_config(self):
-        return self._settings
-
-
-class PyDssArchiveFileInterfaceBase(PyDssFileSystemInterface):
-    """Base class for archive types."""
-    def __init__(self, project_dir):
-        self._project_dir = project_dir
-        data = self._load_data(SIMULATION_SETTINGS_FILENAME)
-        self._settings = SimulationSettingsModel(**data)
-        self._check_scenarios()
-
-    def _load_data(self, path):
-        ext = os.path.splitext(path)[1]
-        if ext == ".json":
-            return self._read_json(path)
-        if ext == ".toml":
-            return self._read_toml(path)
-
-        raise Exception(f"unsupported extension {ext}")
-
-    def _read_json(self, path):
-        return json.loads(self.read_file(path))
-
-    def _read_toml(self, path):
-        return toml.loads(self.read_file(path))
-
-    def _list_scenario_names(self):
-        store_filename = os.path.join(self._project_dir, STORE_FILENAME)
-        if not os.path.exists(store_filename):
-            return None
-
-        scenarios = None
-        with pd.HDFStore(store_filename, "r") as store:
-            for (path, subgroups, _) in store.walk():
-                if path == "/Exports":
-                    scenarios = subgroups
-                    break
-
-        # scenarios will be None if no exports were defined.
-        if scenarios:
-            scenarios.sort()
-        return scenarios
-
-    @staticmethod
-    def normalize_path(path):
-        return os.path.normpath(path).replace("\\", "/")
-
-    @property
-    def simulation_config(self):
-        return self._settings
-
-    def read_controller_config(self, scenario):
-        # Not currently needed for reading projects.
-        pass
-
-    def read_csv(self, path):
-        assert False, "Not implemented"
-
-    def read_file(self, path):
-        assert False, "Not implemented"
-
-    def read_export_config(self, scenario):
-        # Not currently needed for reading projects.
-        pass
-
-    def read_plot_config(self, scenario):
-        # Not currently needed for reading projects.
-        pass
-
-    def read_scenario_export_metadata(self, scenario_name):
-        filename = os.path.join(
-            "Exports",
-            scenario_name,
-            "metadata.json",
-        )
-        if not self.exists(self.normalize_path(filename)):
-            return {}
-        return self._load_data(filename)
-
-    def read_scenario_pv_profiles(self, scenario_name):
-        filename = os.path.join(
-            "Exports",
-            scenario_name,
-            "pv_profiles.json",
-        )
-        try:
-            return self._load_data(filename)
-        except KeyError:
-            # The file isn't stored in the archive.
-            return {}
-
-
-class PyDssTarFileInterface(PyDssArchiveFileInterfaceBase):
-    """Reads pydss files when the project is archived in tar file."""
-    def __init__(self, project_dir):
-        tar_path = os.path.join(project_dir, PROJECT_TAR)
-        self._tar = tarfile.open(tar_path)
-        super(PyDssTarFileInterface, self).__init__(project_dir)
-
-    def __del__(self):
-        if not self._tar.closed:
-            self._tar.close()
-
-    def exists(self, filename):
-        return filename in self._tar.getnames()
-
-    def read_file(self, path):
-        if sys.platform == "win32":
-            path = self.normalize_path(path)
-        return self._tar.extractfile(path).read().decode("utf-8")
-
-    def read_csv(self, path):
-        return pd.read_csv(self._tar.extractfile(os.path.normpath(path).replace("\\", "/")))
-
-
-
-
-class PyDssZipFileInterface(PyDssArchiveFileInterfaceBase):
-    """Reads pydss files when the project is archived in zip file."""
-    def __init__(self, project_dir):
-        self._zip = zipfile.ZipFile(os.path.join(project_dir, PROJECT_ZIP))
-        super(PyDssZipFileInterface, self).__init__(project_dir)
-
-    def __del__(self):
-        self._zip.close()
-
-    def exists(self, filename):
-        return filename in self._zip.namelist()
-
-    def read_file(self, path):
-        if sys.platform == "win32":
-            path = self.normalize_path(path)
-        data = self._zip.read(path)
-        ext = os.path.splitext(path)[1]
-        if ext != ".h5":
-            data = data.decode("utf-8")
-        return data
-
-    def read_csv(self, path):
-        if sys.platform == "win32":
-            path = self.normalize_path(path)
-        return pd.read_csv(io.BytesIO(self._zip.read(path)))
+"""Interface to read pydss files on differing filesystem structures."""
+
+import abc
+import io
+import json
+import os
+import sys
+import tarfile
+import zipfile
+
+from loguru import logger
+import pandas as pd
+import toml
+
+from pydss.common import PLOTS_FILENAME, PROJECT_TAR, PROJECT_ZIP, \
+    ControllerType, ExportMode, SIMULATION_SETTINGS_FILENAME
+from pydss.exceptions import InvalidConfiguration
+from pydss.simulation_input_models import SimulationSettingsModel, load_simulation_settings
+from pydss.utils.utils import load_data
+
+
+STORE_FILENAME = "store.h5"
+SCENARIOS = "Scenarios"
+PROJECT_DIRECTORIES = ("DSSfiles", "Exports", "Logs", "Scenarios")
+
+class PyDssFileSystemInterface(abc.ABC):
+    """Interface to read pydss files on differing filesystem structures."""
+
+    @abc.abstractmethod
+    def exists(self, filename):
+        """Return True if the filename exists.
+
+        Parameters
+        ----------
+        filename : str
+
+        Returns
+        -------
+        bool
+
+        """
+
+    @abc.abstractmethod
+    def read_file(self, path):
+        """Return the contents of file.
+
+        Parameters
+        ----------
+        path : str
+
+        Returns
+        -------
+        str
+
+        """
+
+    @abc.abstractmethod
+    def read_csv(self, path):
+        """Return a pandas DataFrame from the CSV file.
+
+        Parameters
+        ----------
+        path : str
+
+        Returns
+        -------
+        str
+
+        """
+
+    def read_controller_config(self, scenario):
+        """Read the controller config for a scenario from disk.
+
+        Parameters
+        ----------
+        scenario : str
+            scenario name
+
+        Returns
+        -------
+        dict
+
+        """
+
+    @abc.abstractmethod
+    def read_export_config(self, scenario):
+        """Read the export config for a scenario from disk.
+
+        Parameters
+        ----------
+        scenario : str
+            scenario name
+
+        Returns
+        -------
+        dict
+
+        """
+
+    @abc.abstractmethod
+    def read_plot_config(self, scenario):
+        """Read the export config for a scenario from disk.
+
+        Parameters
+        ----------
+        scenario : str
+            scenario name
+
+        Returns
+        -------
+        dict
+
+        """
+
+    @abc.abstractmethod
+    def read_scenario_export_metadata(self, scenario_name):
+        """Return the metadata for a scenario's exported data.
+
+        Parameters
+        ----------
+        scenario_name : str
+
+        Returns
+        -------
+        dict
+
+        """
+
+    @abc.abstractmethod
+    def read_scenario_pv_profiles(self, scenario_name):
+        """Return the PV profiles for a scenario.
+
+        Parameters
+        ----------
+        scenario_name : str
+
+        Returns
+        -------
+        dict
+
+        """
+
+
+    @property
+    def scenario_names(self):
+        """Return the scenario names in the project.
+
+        Returns
+        -------
+        list
+
+        """
+        return [x.name for x in self._settings.project.scenarios]
+
+    @property
+    @abc.abstractmethod
+    def simulation_config(self):
+        """Return the config from the simulation settings file.
+
+        Returns
+        -------
+        dict
+
+        """
+
+    def _check_scenarios(self):
+        scenarios = self._list_scenario_names()
+
+        if scenarios is None:
+            return
+
+        exp_scenarios = self.scenario_names
+        exp_scenarios.sort()
+
+        for scenario in exp_scenarios:
+            if scenario not in scenarios:
+                raise InvalidConfiguration(
+                    f"{scenario} is not a valid scenario. Valid scenarios: {scenarios}"
+                )
+
+    @abc.abstractmethod
+    def _list_scenario_names(self):
+        """Return the scenario names on disk.
+
+        Returns
+        -------
+        list
+
+        """
+
+
+class PyDssFileSystemInterface(PyDssFileSystemInterface):
+    """Reads pydss files when the project is expanded into directories."""
+    def __init__(self, project_dir, simulation_file):
+        self._project_dir = project_dir
+        self._scenarios_dir = os.path.join(self._project_dir, SCENARIOS)
+        self._dss_dir = os.path.join(self._project_dir, "DSSfiles")
+
+        self._settings = load_simulation_settings(
+            os.path.join(self._project_dir, simulation_file)
+        )
+
+        self._check_scenarios()
+
+    def _get_full_path(self, path):
+        return os.path.join(self._project_dir, path)
+
+    def exists(self, filename):
+        return os.path.exists(filename)
+
+    def read_file(self, path):
+        with open(self._get_full_path(path)) as f_in:
+            return f_in.read()
+
+    def read_csv(self, path):
+        return pd.read_csv(self._get_full_path(path))
+
+    def _list_scenario_names(self):
+        scenarios = [
+            x for x in os.listdir(self._scenarios_dir)
+            if os.path.isdir(os.path.join(self._scenarios_dir, x))
+        ]
+        scenarios.sort()
+        return scenarios
+
+    def read_controller_config(self, scenario):
+        controllers = {}
+        path = os.path.join(self._project_dir, SCENARIOS, scenario, "pyControllerList")
+        if not os.path.exists(path):
+            return controllers
+        for filename in os.listdir(path):
+            base, ext = os.path.splitext(filename)
+            if ext == ".toml":
+                controller_type = ControllerType(base)
+                controllers[controller_type] = load_data(os.path.join(path, filename))
+
+        return controllers
+
+    def read_export_config(self, scenario):
+        exports = {}
+        path = os.path.join(self._project_dir, SCENARIOS, scenario, "ExportLists")
+        for filename in os.listdir(path):
+            base, ext = os.path.splitext(filename)
+            if ext == ".toml":
+                export_mode = ExportMode(base)
+                exports[export_mode] = load_data(os.path.join(path, filename))
+
+        return exports
+
+    def read_plot_config(self, scenario):
+        path = os.path.join(self._project_dir, SCENARIOS, scenario, "pyPlotList", PLOTS_FILENAME)
+        return load_data(path)
+
+    def read_scenario_export_metadata(self, scenario_name):
+        filename = os.path.join(
+            self._project_dir,
+            "Exports",
+            scenario_name,
+            "metadata.json",
+        )
+        if not self.exists(filename):
+            return {}
+        return load_data(filename)
+
+    def read_scenario_pv_profiles(self, scenario_name):
+        filename = os.path.join(
+            self._project_dir,
+            "Exports",
+            scenario_name,
+            "pv_profiles.json",
+        )
+        if not os.path.exists(filename):
+            return {}
+        return load_data(filename)
+
+    @property
+    def simulation_config(self):
+        return self._settings
+
+
+class PyDssArchiveFileInterfaceBase(PyDssFileSystemInterface):
+    """Base class for archive types."""
+    def __init__(self, project_dir):
+        self._project_dir = project_dir
+        data = self._load_data(SIMULATION_SETTINGS_FILENAME)
+        self._settings = SimulationSettingsModel(**data)
+        self._check_scenarios()
+
+    def _load_data(self, path):
+        ext = os.path.splitext(path)[1]
+        if ext == ".json":
+            return self._read_json(path)
+        if ext == ".toml":
+            return self._read_toml(path)
+
+        raise Exception(f"unsupported extension {ext}")
+
+    def _read_json(self, path):
+        return json.loads(self.read_file(path))
+
+    def _read_toml(self, path):
+        return toml.loads(self.read_file(path))
+
+    def _list_scenario_names(self):
+        store_filename = os.path.join(self._project_dir, STORE_FILENAME)
+        if not os.path.exists(store_filename):
+            return None
+
+        scenarios = None
+        with pd.HDFStore(store_filename, "r") as store:
+            for (path, subgroups, _) in store.walk():
+                if path == "/Exports":
+                    scenarios = subgroups
+                    break
+
+        # scenarios will be None if no exports were defined.
+        if scenarios:
+            scenarios.sort()
+        return scenarios
+
+    @staticmethod
+    def normalize_path(path):
+        return os.path.normpath(path).replace("\\", "/")
+
+    @property
+    def simulation_config(self):
+        return self._settings
+
+    def read_controller_config(self, scenario):
+        # Not currently needed for reading projects.
+        pass
+
+    def read_csv(self, path):
+        assert False, "Not implemented"
+
+    def read_file(self, path):
+        assert False, "Not implemented"
+
+    def read_export_config(self, scenario):
+        # Not currently needed for reading projects.
+        pass
+
+    def read_plot_config(self, scenario):
+        # Not currently needed for reading projects.
+        pass
+
+    def read_scenario_export_metadata(self, scenario_name):
+        filename = os.path.join(
+            "Exports",
+            scenario_name,
+            "metadata.json",
+        )
+        if not self.exists(self.normalize_path(filename)):
+            return {}
+        return self._load_data(filename)
+
+    def read_scenario_pv_profiles(self, scenario_name):
+        filename = os.path.join(
+            "Exports",
+            scenario_name,
+            "pv_profiles.json",
+        )
+        try:
+            return self._load_data(filename)
+        except KeyError:
+            # The file isn't stored in the archive.
+            return {}
+
+
+class PyDssTarFileInterface(PyDssArchiveFileInterfaceBase):
+    """Reads pydss files when the project is archived in tar file."""
+    def __init__(self, project_dir):
+        tar_path = os.path.join(project_dir, PROJECT_TAR)
+        self._tar = tarfile.open(tar_path)
+        super(PyDssTarFileInterface, self).__init__(project_dir)
+
+    def __del__(self):
+        if not self._tar.closed:
+            self._tar.close()
+
+    def exists(self, filename):
+        return filename in self._tar.getnames()
+
+    def read_file(self, path):
+        if sys.platform == "win32":
+            path = self.normalize_path(path)
+        return self._tar.extractfile(path).read().decode("utf-8")
+
+    def read_csv(self, path):
+        return pd.read_csv(self._tar.extractfile(os.path.normpath(path).replace("\\", "/")))
+
+
+
+
+class PyDssZipFileInterface(PyDssArchiveFileInterfaceBase):
+    """Reads pydss files when the project is archived in zip file."""
+    def __init__(self, project_dir):
+        self._zip = zipfile.ZipFile(os.path.join(project_dir, PROJECT_ZIP))
+        super(PyDssZipFileInterface, self).__init__(project_dir)
+
+    def __del__(self):
+        self._zip.close()
+
+    def exists(self, filename):
+        return filename in self._zip.namelist()
+
+    def read_file(self, path):
+        if sys.platform == "win32":
+            path = self.normalize_path(path)
+        data = self._zip.read(path)
+        ext = os.path.splitext(path)[1]
+        if ext != ".h5":
+            data = data.decode("utf-8")
+        return data
+
+    def read_csv(self, path):
+        if sys.platform == "win32":
+            path = self.normalize_path(path)
+        return pd.read_csv(io.BytesIO(self._zip.read(path)))
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pydss_project.py` & `nrel_pydss-3.1.4/src/pydss/pydss_project.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,889 +1,889 @@
-"""Contains functionality to configure pydss simulations."""
-import os
-import shutil
-import sys
-import tarfile
-import tempfile
-import zipfile
-from pathlib import Path
-
-import h5py
-
-from loguru import logger
-
-import pydss
-from pydss.common import PROJECT_TAR, PROJECT_ZIP, CONTROLLER_TYPES, \
-    SIMULATION_SETTINGS_FILENAME, DEFAULT_SIMULATION_SETTINGS_FILE, \
-    ControllerType, ExportMode, SnapshotTimePointSelectionMode, MONTE_CARLO_SETTINGS_FILENAME,\
-    filename_from_enum, DEFAULT_MONTE_CARLO_SETTINGS_FILE,\
-    SUBSCRIPTIONS_FILENAME, DEFAULT_SUBSCRIPTIONS_FILE, OPENDSS_MASTER_FILENAME, \
-    RUN_SIMULATION_FILENAME
-from pydss.exceptions import InvalidParameter, InvalidConfiguration
-from pydss.pyDSS import instance
-from pydss.pydss_fs_interface import PyDssFileSystemInterface, \
-    PyDssArchiveFileInterfaceBase, PyDssTarFileInterface, \
-    PyDssZipFileInterface, PROJECT_DIRECTORIES, \
-    SCENARIOS, STORE_FILENAME
-from pydss.reports.reports import REPORTS_DIR
-from pydss.registry import Registry
-from pydss.simulation_input_models import (
-    ScenarioModel,
-    ScenarioPostProcessModel,
-    SimulationSettingsModel,
-    SnapshotTimePointSelectionConfigModel,
-    create_simulation_settings,
-    load_simulation_settings,
-    dump_settings,
-)
-from pydss.utils.dss_utils import read_pv_systems_from_dss_file
-from pydss.utils.utils import dump_data, load_data
-
-from distutils.dir_util import copy_tree
-
-
-DATA_FORMAT_VERSION = "1.0.2"
-
-READ_CONTROLLER_FUNCTIONS = {
-    ControllerType.PV_CONTROLLER.value: read_pv_systems_from_dss_file,
-}
-
-
-class PyDssProject:
-    """Represents the project options for a pydss simulation."""
-
-    _SKIP_ARCHIVE = (PROJECT_ZIP, PROJECT_TAR, STORE_FILENAME, REPORTS_DIR)
-
-    def __init__(self, path, name, scenarios, settings: SimulationSettingsModel, fs_intf=None,
-                 simulation_file=SIMULATION_SETTINGS_FILENAME):
-        self._name = name
-        self._scenarios = scenarios
-        self._settings = settings
-        self._project_dir = os.path.join(path, self._name)
-        if simulation_file is None:
-            self._simulation_file = os.path.join(self._project_dir, SIMULATION_SETTINGS_FILENAME)
-        else:
-            self._simulation_file = simulation_file
-        self._scenarios_dir = os.path.join(self._project_dir, SCENARIOS)
-        self._fs_intf = fs_intf  # Only needed for reading a project that was
-                                 # already executed.
-        self._hdf_store = None
-        self._estimated_space = {}
-
-    @property
-    def dss_files_path(self):
-        """Return the path containing OpenDSS files.
-
-        Returns
-        -------
-        str
-
-        """
-        return os.path.join(self._project_dir, "DSSfiles")
-
-    def export_path(self, scenario):
-        """Return the path containing export data.
-
-        Parameters
-        ----------
-        scenario : str
-
-        Returns
-        -------
-        str
-
-        """
-        return os.path.join(self._project_dir, "Exports", scenario)
-
-    @property
-    def hdf_store(self):
-        """Return the HDFStore
-
-        Returns
-        -------
-        pd.HDFStore
-
-        """
-        if self._hdf_store is None:
-            raise InvalidConfiguration("hdf_store is not defined")
-        return self._hdf_store
-
-    @property
-    def fs_interface(self):
-        """Return the interface object used to read files.
-
-        Returns
-        -------
-        PyDssFileSystemInterface
-
-        """
-        if self._fs_intf is None:
-            raise InvalidConfiguration("fs interface is not defined")
-        return self._fs_intf
-
-    def get_hdf_store_filename(self):
-        """Return the HDFStore filename.
-
-        Returns
-        -------
-        str
-            Path to the HDFStore.
-
-        Raises
-        ------
-        InvalidConfiguration
-            Raised if no store exists.
-
-        """
-        filename = os.path.join(self._project_dir, STORE_FILENAME)
-        if not os.path.exists(filename):
-            raise InvalidConfiguration(f"HDFStore does not exist: {filename}")
-
-        return filename
-
-    def get_post_process_directory(self, scenario_name):
-        """Return the post-process output directory for scenario_name.
-
-        Parameters
-        ----------
-        scenario_name : str
-
-        Returns
-        -------
-        str
-
-        """
-        # Make sure the scenario exists. This will throw if not.
-        self.get_scenario(scenario_name)
-        return os.path.join(
-            self._project_dir, "Scenarios", scenario_name, "PostProcess"
-        )
-
-    def get_scenario(self, name):
-        """Return the scenario with name.
-
-        Parameters
-        ----------
-        name : str
-
-        Returns
-        -------
-        PyDssScenario
-
-        """
-        for scenario in self._scenarios:
-            if scenario.name == name:
-                return scenario
-
-        raise InvalidParameter(f"{name} is not a valid scenario")
-
-    @property
-    def name(self):
-        """Return the project name.
-
-        Returns
-        -------
-        str
-
-        """
-        return self._name
-
-    @property
-    def project_path(self):
-        """Return the path to the project.
-
-        Returns
-        -------
-        str
-
-        """
-        return self._project_dir
-
-    @property
-    def scenarios(self):
-        """Return the project scenarios.
-
-        Returns
-        -------
-        list
-            list of PyDssScenario
-
-        """
-        return self._scenarios
-
-    @property
-    def simulation_config(self):
-        """Return the simulation configuration
-
-        Returns
-        -------
-        dict
-
-        """
-        return self._settings
-
-    @property
-    def estimated_space(self):
-        """Return the estimated space in bytes.
-        
-        Returns
-        -------
-        int
-        """
-        return self._estimated_space
-
-    def serialize(self, opendss_project_folder):
-        """Create the project on the filesystem."""
-        os.makedirs(self._project_dir, exist_ok=True)
-        for name in PROJECT_DIRECTORIES:
-            os.makedirs(os.path.join(self._project_dir, name), exist_ok=True)
-        if opendss_project_folder:
-            dest = os.path.join(self._project_dir, PROJECT_DIRECTORIES[0])
-            copy_tree(opendss_project_folder, dest)
-        self._serialize_scenarios()
-        dump_settings(
-            self._settings,
-            os.path.join(self._project_dir, self._simulation_file),
-        )
-        logger.info("Initialized directories in %s", self._project_dir)
-
-    @classmethod
-
-    def create_project(cls, path, name, scenarios, simulation_config=None, options=None,
-                       simulation_file=SIMULATION_SETTINGS_FILENAME, opendss_project_folder=None,
-                       master_dss_file=OPENDSS_MASTER_FILENAME, force=False):
-        """Create a new PyDssProject on the filesystem.
-
-        Parameters
-        ----------
-        path : str
-            path in which to create directories
-        name : str
-            project name
-        scenarios : list
-            list of PyDssScenario objects
-        simulation_config : str
-            simulation config file; if None, use default
-
-        """
-        if simulation_config is None:
-            scenario_names = [x.name for x in scenarios]
-            simulation_config = create_simulation_settings(path, name, scenario_names, force=force)
-        simulation_config = load_data(simulation_config)
-        if options is not None:
-            for category, category_options in options.items():
-                simulation_config[category].update(category_options)
-        if master_dss_file:
-            simulation_config["project"]["dss_file"] = master_dss_file
-        simulation_config["project"]["project_path"] = path
-        simulation_config["project"]["active_project"] = name
-        settings = SimulationSettingsModel(**simulation_config)
-        project = cls(
-            path=path,
-            name=name,
-            scenarios=scenarios,
-            settings=settings,
-            simulation_file=simulation_file,
-        )
-        project.serialize(opendss_project_folder=opendss_project_folder)
-        sc_names = project.list_scenario_names()
-        logger.info("Created project=%s with scenarios=%s at %s", name,
-                    sc_names, path)
-        return project
-
-    def read_scenario_export_metadata(self, scenario_name):
-        """Return the metadata for a scenario's exported data.
-
-        Parameters
-        ----------
-        scenario_name : str
-
-        Returns
-        -------
-        dict
-
-        """
-        if self._fs_intf is None:
-            raise InvalidConfiguration("pydss fs interface is not defined")
-
-        if scenario_name not in self.list_scenario_names():
-            raise InvalidParameter(f"invalid scenario: {scenario_name}")
-
-        return self._fs_intf.read_scenario_export_metadata(scenario_name)
-
-    def list_scenario_names(self):
-        return [x.name for x in self.scenarios]
-
-    def run(self, logging_configured=True, tar_project=False, zip_project=False, dry_run=False):
-        """Run all scenarios in the project."""
-        if isinstance(self._fs_intf, PyDssArchiveFileInterfaceBase):
-            raise InvalidConfiguration("cannot run from an archived project")
-        if tar_project and zip_project:
-            raise InvalidParameter("tar_project and zip_project cannot both be True")
-        if self._settings.project.dss_file == "":
-            raise InvalidConfiguration("a valid opendss file needs to be passed")
-
-        inst = instance()
-        if not logging_configured:
-            if self._settings.logging.enable_console:
-                console_level = "INFO"
-            else:
-                console_level = "ERROR"
-            if self._settings.logging.enable_file:
-                filename = os.path.join(self._project_dir, "Logs", "pydss.log")
-            else:
-                filename = None
-            file_level = "INFO"
-            logger.level(console_level)
-            if filename:
-                logger.add(filename)
-            
-        if dry_run:
-            store_filename = os.path.join(tempfile.gettempdir(), STORE_FILENAME)
-        else:
-            store_filename = os.path.join(self._project_dir, STORE_FILENAME)
-            self._dump_simulation_settings()
-
-        driver = None
-        if self._settings.exports.export_data_in_memory:
-            driver = "core"
-        if os.path.exists(store_filename):
-            os.remove(store_filename)
-
-        try:
-            # This ensures that all datasets are flushed and closed after each
-            # scenario. If there is an unexpected crash in a later scenario then
-            # the file will still be valid for completed scenarios.
-            for scenario in self._scenarios:
-                with h5py.File(store_filename, mode="a", driver=driver) as hdf_store:
-                    self._hdf_store = hdf_store
-                    self._hdf_store.attrs["version"] = DATA_FORMAT_VERSION
-                    self._settings.project.active_scenario = scenario.name
-                    inst.run(self._settings, self, scenario, dry_run=dry_run)
-                    self._estimated_space[scenario.name] = inst.get_estimated_space()
-
-            export_tables = self._settings.exports.export_data_tables
-            generate_reports = bool(self._settings.reports)
-            if not dry_run and (export_tables or generate_reports):
-                # Hack. Have to import here. Need to re-organize to fix.
-                from pydss.pydss_results import PyDssResults
-                results = PyDssResults(self._project_dir)
-                if export_tables:
-                    for scenario in results.scenarios:
-                        scenario.export_data()
-
-                if generate_reports:
-                    results.generate_reports()
-
-        except Exception:
-            logger.exception("Simulation failed")
-            raise
-
-        finally:
-            logger.remove()
-            if tar_project:
-                self._tar_project_files()
-            elif zip_project:
-                self._zip_project_files()
-
-            if dry_run and os.path.exists(store_filename):
-                os.remove(store_filename)
-
-    def _dump_simulation_settings(self):
-        # Various settings may have been updated. Write the actual settings to a file.
-        filename = os.path.join( self._project_dir, RUN_SIMULATION_FILENAME)
-        dump_settings(self._settings, filename)
-
-    def _serialize_scenarios(self):
-        scenarios = []
-        for scenario in self._scenarios:
-            cfg = scenario.snapshot_time_point_selection_config or SnapshotTimePointSelectionConfigModel()
-            model = ScenarioModel(
-                name=scenario.name,
-                post_process_infos=[],
-                snapshot_time_point_selection_config=cfg,
-            )
-            model.post_process_infos = scenario.post_process_infos
-            scenarios.append(model)
-            scenario.serialize(
-                os.path.join(self._scenarios_dir, scenario.name)
-            )
-
-        self._settings.project.scenarios = scenarios
-
-    def _tar_project_files(self, delete=True):
-        orig = os.getcwd()
-        os.chdir(self._project_dir)
-        skip_names = (PROJECT_ZIP, STORE_FILENAME, REPORTS_DIR)
-        try:
-            filename = PROJECT_TAR
-            to_delete = []
-            with tarfile.open(filename, "w") as tar:
-                for name in os.listdir("."):
-                    if name in self._SKIP_ARCHIVE:
-                        continue
-                    tar.add(name)
-                    if delete:
-                        to_delete.append(name)
-
-            for name in to_delete:
-                if os.path.isfile(name):
-                    os.remove(name)
-                else:
-                    shutil.rmtree(name)
-
-            path = os.path.join(self._project_dir, filename)
-        finally:
-            os.chdir(orig)
-
-    def _zip_project_files(self, delete=True):
-        orig = os.getcwd()
-        os.chdir(self._project_dir)
-        try:
-            filename = PROJECT_ZIP
-            to_delete = []
-            with zipfile.ZipFile(filename, "w") as zipf:
-                for root, dirs, files in os.walk("."):
-                    if delete and root == ".":
-                        to_delete += dirs
-                    for filename in files:
-                        if root == "." and filename in self._SKIP_ARCHIVE:
-                            continue
-                        path = os.path.join(root, filename)
-                        zipf.write(path)
-                        # We delete files and directories at the root only.
-                        if delete and root == ".":
-                            to_delete.append(path)
-
-            for name in to_delete:
-                if os.path.isfile(name):
-                    os.remove(name)
-                else:
-                    shutil.rmtree(name)
-
-            path = os.path.join(self._project_dir, filename)
-        finally:
-            os.chdir(orig)
-
-    @staticmethod
-    def load_simulation_settings(project_path, simulations_file):
-        """Return the simulation settings for a project, using defaults if the
-        file is not defined.
-
-        Parameters
-        ----------
-        project_path : Path
-
-        Returns
-        -------
-        SimulationSettingsModel
-
-        """
-        filename = project_path / simulations_file
-        if not filename.exists():
-            filename = project_path / DEFAULT_SIMULATION_SETTINGS_FILE
-            assert filename.exists()
-        return load_simulation_settings(filename)
-
-    @classmethod
-    def load_project(
-        cls,
-        path,
-        options=None,
-        in_memory=False,
-        simulation_file=SIMULATION_SETTINGS_FILENAME,
-    ):
-        """Load a PyDssProject from directory.
-
-        Parameters
-        ----------
-        path : str
-            full path to existing project
-        options : dict
-            options that override the config file
-        in_memory : bool
-            If True, load all exported data into memory.
-
-        """
-        name = os.path.basename(path)
-        #if simulation_file is None:
-            #simulation_file = SIMULATION_SETTINGS_FILENAME
-
-        if os.path.exists(os.path.join(path, PROJECT_TAR)):
-            fs_intf = PyDssTarFileInterface(path)
-        elif os.path.exists(os.path.join(path, PROJECT_ZIP)):
-            fs_intf = PyDssZipFileInterface(path)
-        else:
-            fs_intf = PyDssFileSystemInterface(path, simulation_file)
-
-        simulation_config = fs_intf.simulation_config.dict(by_alias=False)
-        if options is not None:
-            for category, params in options.items():
-                if category not in simulation_config:
-                    simulation_config[category] = {}
-                simulation_config[category].update(params)
-            logger.info("Overrode config options: %s", options)
-
-        settings = SimulationSettingsModel(**simulation_config)
-        scenarios = [
-            PyDssScenario.deserialize(
-                fs_intf,
-                x.name,
-                post_process_infos=x.post_process_infos,
-            )
-            for x in settings.project.scenarios
-        ]
-
-        return PyDssProject(
-            os.path.dirname(path),
-            name,
-            scenarios,
-            settings,
-            fs_intf=fs_intf,
-        )
-
-    @classmethod
-    def run_project(cls, path, options=None, tar_project=False, zip_project=False, simulation_file=None, dry_run=False):
-
-        """Load a PyDssProject from directory and run all scenarios.
-
-        Parameters
-        ----------
-        path : str
-            full path to existing project
-        options : dict
-            options that override the config file
-        tar_project : bool
-            tar project files after successful execution
-        zip_project : bool
-            zip project files after successful execution
-        dry_run: bool
-            dry run for getting estimated space.
-        """
-
-        project = cls.load_project(path, options=options, simulation_file=simulation_file)
-        return project.run(tar_project=tar_project, zip_project=zip_project, dry_run=dry_run)
-
-    def read_scenario_settings(self, scenario):
-        """Read the simulation settings file for the scenario.
-
-        Parameters
-        ----------
-        scenario : str
-            Scenario name
-
-        Returns
-        -------
-        SimulationSettingsModel
-
-        """
-        scenario_path = Path(self._project_dir) / "Scenarios" / scenario
-        if not scenario_path.exists():
-            raise InvalidParameter(f"scenario={scenario} is not present")
-
-        settings_file = scenario_path / RUN_SIMULATION_FILENAME
-        if not settings_file.exists():
-            raise InvalidConfiguration(f"{RUN_SIMULATION_FILENAME} does not exist. Was the scenario run?")
-
-        return load_simulation_settings(settings_file)
-
-    def read_scenario_time_settings(self, scenario):
-        """Return the simulation time-related settings for the scenario.
-
-        Parameters
-        ----------
-        scenario : str
-            Scenario name
-
-        Returns
-        -------
-        dict
-
-        """
-        settings = self.read_scenario_settings(scenario).project
-        data = {}
-        for key in (
-            "start_time",
-            "simulation_duration_min",
-            "loadshape_start_time",
-            "step_resolution_sec",
-        ):
-            data[key] = getattr(settings, key)
-        return data
-
-
-class PyDssScenario:
-    """Represents a pydss Scenario."""
-
-    DEFAULT_CONTROLLER_TYPES = (ControllerType.PV_CONTROLLER,)
-    DEFAULT_EXPORT_MODE = ExportMode.EXPORTS
-    _SCENARIO_DIRECTORIES = (
-        "ExportLists",
-        "pyControllerList",
-        "pyPlotList",
-        "PostProcess",
-        'Monte_Carlo'
-    )
-    REQUIRED_POST_PROCESS_FIELDS = ("script", "config_file")
-
-    def __init__(self, name, controller_types=None, controllers=None,
-                 export_modes=None, exports=None,
-                 post_process_infos=None, 
-                 snapshot_time_point_selection_config=None):
-        self.name = name
-        self.post_process_infos = []
-        self.snapshot_time_point_selection_config = None
-
-        if (controller_types is None and controllers is None):
-            self.controllers = {}
-        elif controller_types is not None:
-            self.controllers = {
-                x: self.load_controller_config_from_type(x)
-                for x in controller_types
-            }
-        elif isinstance(controllers, str):
-            basename = os.path.splitext(os.path.basename(controllers))[0]
-            controller_type = ControllerType(basename)
-            self.controllers = {controller_type: load_data(controllers)}
-        else:
-            assert isinstance(controllers, dict)
-            self.controllers = controllers
-
-        if export_modes is not None and exports is not None:
-            raise InvalidParameter(
-                "export_modes and exports cannot both be set"
-            )
-        if (export_modes is None and exports is None):
-            mode = PyDssScenario.DEFAULT_EXPORT_MODE
-            self.exports = {mode: self.load_export_config_from_mode(mode)}
-        elif export_modes is not None:
-            self.exports = {
-                x: self.load_export_config_from_mode(x) for x in export_modes
-            }
-        elif isinstance(exports, str):
-            mode = ExportMode(os.path.splitext(os.path.basename(exports))[0])
-            self.exports = {mode: load_data(exports)}
-        else:
-            assert isinstance(exports, dict)
-            self.exports = exports
-
-        if post_process_infos is not None:
-            for pp_info in post_process_infos:
-                if isinstance(pp_info, dict):
-                    pp_info = ScenarioPostProcessModel(**pp_info)
-                self.add_post_process(pp_info)
-
-        if snapshot_time_point_selection_config is not None:
-            # Ensure the mode is valid.
-            SnapshotTimePointSelectionMode(snapshot_time_point_selection_config["mode"])
-            self.snapshot_time_point_selection_config = snapshot_time_point_selection_config
-
-    @classmethod
-    def deserialize(cls, fs_intf, name, post_process_infos):
-        """Deserialize a PyDssScenario from a path.
-
-        Parameters
-        ----------
-        fs_intf : PyDssFileSystemInterface
-            object to read on-disk information
-        name : str
-            scenario name
-        post_process_infos : list
-            list of post_process_info dictionaries
-
-        Returns
-        -------
-        PyDssScenario
-
-        """
-        controllers = fs_intf.read_controller_config(name)
-        exports = fs_intf.read_export_config(name)
-
-        return cls(
-            name,
-            controllers=controllers,
-            exports=exports,
-            post_process_infos=post_process_infos,
-        )
-
-    def serialize(self, path):
-        """Serialize a PyDssScenario to a directory.
-
-        Parameters
-        ----------
-        path : str
-            full path to scenario
-
-        """
-        os.makedirs(path, exist_ok=True)
-        for name in self._SCENARIO_DIRECTORIES:
-            os.makedirs(os.path.join(path, name), exist_ok=True)
-
-        for controller_type, controllers in self.controllers.items():
-            filename = os.path.join(
-                path, "pyControllerList", filename_from_enum(controller_type)
-            )
-            dump_data(controllers, filename)
-
-        for mode, exports in self.exports.items():
-            dump_data(
-                exports,
-                os.path.join(path, "ExportLists", filename_from_enum(mode))
-            )
-
-        dump_data(
-            load_data(DEFAULT_MONTE_CARLO_SETTINGS_FILE),
-            os.path.join(path, "Monte_Carlo", MONTE_CARLO_SETTINGS_FILENAME)
-        )
-
-        dump_data(
-            load_data(DEFAULT_SUBSCRIPTIONS_FILE),
-            os.path.join(path, "ExportLists", SUBSCRIPTIONS_FILENAME)
-        )
-
-    @staticmethod
-    def load_controller_config_from_type(controller_type):
-        """Load a default controller config from a type.
-
-        Parameters
-        ----------
-        controller_type : ControllerType
-
-        Returns
-        -------
-        dict
-
-        """
-
-        path = os.path.join(
-            os.path.dirname(getattr(pydss, "__path__")[0]),
-            "pydss",
-            "defaults",
-            "pyControllerList",
-            filename_from_enum(controller_type),
-        )
-
-        return load_data(path)
-
-    @staticmethod
-    def load_export_config_from_mode(export_mode):
-        """Load a default export config from a type.
-
-        Parameters
-        ----------
-        export_mode : ExportMode
-
-        Returns
-        -------
-        dict
-
-        """
-        path = os.path.join(
-            os.path.dirname(getattr(pydss, "__path__")[0]),
-            "pydss",
-            "defaults",
-            "ExportLists",
-            filename_from_enum(export_mode),
-        )
-
-        return load_data(path)
-
-    def add_post_process(self, post_process_info):
-        """Add a post-process script to a scenario.
-
-        Parameters
-        ----------
-        post_process_info : dict
-            Must define all fields in PyDssScenario.REQUIRED_POST_PROCESS_FIELDS
-
-        """
-        config_file = post_process_info.config_file
-        if config_file and not os.path.exists(config_file):
-            raise InvalidParameter(f"{config_file} does not exist")
-
-        self.post_process_infos.append(post_process_info)
-
-
-def load_config(path):
-    """Return a configuration from files.
-
-    Parameters
-    ----------
-    path : str
-
-    Returns
-    -------
-    dict
-
-    """
-    files = [os.path.join(path, x) for x in os.listdir(path) \
-             if os.path.splitext(x)[1] == ".toml"]
-    assert len(files) == 1, "only 1 .toml file is currently supported"
-    return load_data(files[0])
-
-
-def update_pydss_controllers(project_path, scenario, controller_type, 
-                             controller, dss_file):
-    """Update a scenario's controllers from an OpenDSS file.
-
-    Parameters
-    ----------
-    project_path : str
-        pydss project path.
-    scenario : str
-        pydss scenario name in project.
-    controller_type : str
-        A type of pydss controler
-    controller : str
-        The controller name
-    dss_file : str
-        A DSS file path
-    """
-    if controller_type not in READ_CONTROLLER_FUNCTIONS:
-        supported_types = list(READ_CONTROLLER_FUNCTIONS.keys())
-        print(f"Invalid controller_type={controller_type}, supported: {supported_types}")
-        sys.exit(1)
-
-    sim_file = os.path.join(project_path, SIMULATION_SETTINGS_FILENAME)
-    settings = load_simulation_settings(sim_file)
-    if not settings.project.use_controller_registry:
-        print(f"'Use Controller Registry' must be set to true in {sim_file}")
-        sys.exit(1)
-
-    registry = Registry()
-    if not registry.is_controller_registered(controller_type, controller):
-        print(f"{controller_type} / {controller} is not registered")
-        sys.exit(1)
-
-    data = {}
-    filename = f"{project_path}/Scenarios/{scenario}/pyControllerList/{controller_type}.toml"
-    if os.path.exists(filename):
-        data = load_data(filename)
-        for val in data.values():
-            if not isinstance(val, list):
-                print(f"{filename} has an invalid format")
-                sys.exit(1)
-
-    element_names = READ_CONTROLLER_FUNCTIONS[controller_type](dss_file)
-    num_added = 0
-    if controller in data:
-        existing = set(data[controller])
-        final = list(existing.union(set(element_names)))
-        data[controller] = final
-        num_added = len(final) - len(existing)
-    else:
-        data[controller] = element_names
-        num_added = len(element_names)
-
-    # Remove element_names from any other controllers.
-    set_names = set(element_names)
-    for _controller, values in data.items():
-        if _controller != controller:
-            final = set(values).difference_update(set_names)
-            if final is None:
-                final_list = None
-            else:
-                final_list = list(final)
-            data[_controller] = final_list
-
-    dump_data(data, filename)
-    print(f"Added {num_added} names to {filename}")
+"""Contains functionality to configure pydss simulations."""
+import os
+import shutil
+import sys
+import tarfile
+import tempfile
+import zipfile
+from pathlib import Path
+
+import h5py
+
+from loguru import logger
+
+import pydss
+from pydss.common import PROJECT_TAR, PROJECT_ZIP, CONTROLLER_TYPES, \
+    SIMULATION_SETTINGS_FILENAME, DEFAULT_SIMULATION_SETTINGS_FILE, \
+    ControllerType, ExportMode, SnapshotTimePointSelectionMode, MONTE_CARLO_SETTINGS_FILENAME,\
+    filename_from_enum, DEFAULT_MONTE_CARLO_SETTINGS_FILE,\
+    SUBSCRIPTIONS_FILENAME, DEFAULT_SUBSCRIPTIONS_FILE, OPENDSS_MASTER_FILENAME, \
+    RUN_SIMULATION_FILENAME
+from pydss.exceptions import InvalidParameter, InvalidConfiguration
+from pydss.pyDSS import instance
+from pydss.pydss_fs_interface import PyDssFileSystemInterface, \
+    PyDssArchiveFileInterfaceBase, PyDssTarFileInterface, \
+    PyDssZipFileInterface, PROJECT_DIRECTORIES, \
+    SCENARIOS, STORE_FILENAME
+from pydss.reports.reports import REPORTS_DIR
+from pydss.registry import Registry
+from pydss.simulation_input_models import (
+    ScenarioModel,
+    ScenarioPostProcessModel,
+    SimulationSettingsModel,
+    SnapshotTimePointSelectionConfigModel,
+    create_simulation_settings,
+    load_simulation_settings,
+    dump_settings,
+)
+from pydss.utils.dss_utils import read_pv_systems_from_dss_file
+from pydss.utils.utils import dump_data, load_data
+
+from distutils.dir_util import copy_tree
+
+
+DATA_FORMAT_VERSION = "1.0.2"
+
+READ_CONTROLLER_FUNCTIONS = {
+    ControllerType.PV_CONTROLLER.value: read_pv_systems_from_dss_file,
+}
+
+
+class PyDssProject:
+    """Represents the project options for a pydss simulation."""
+
+    _SKIP_ARCHIVE = (PROJECT_ZIP, PROJECT_TAR, STORE_FILENAME, REPORTS_DIR)
+
+    def __init__(self, path, name, scenarios, settings: SimulationSettingsModel, fs_intf=None,
+                 simulation_file=SIMULATION_SETTINGS_FILENAME):
+        self._name = name
+        self._scenarios = scenarios
+        self._settings = settings
+        self._project_dir = os.path.join(path, self._name)
+        if simulation_file is None:
+            self._simulation_file = os.path.join(self._project_dir, SIMULATION_SETTINGS_FILENAME)
+        else:
+            self._simulation_file = simulation_file
+        self._scenarios_dir = os.path.join(self._project_dir, SCENARIOS)
+        self._fs_intf = fs_intf  # Only needed for reading a project that was
+                                 # already executed.
+        self._hdf_store = None
+        self._estimated_space = {}
+
+    @property
+    def dss_files_path(self):
+        """Return the path containing OpenDSS files.
+
+        Returns
+        -------
+        str
+
+        """
+        return os.path.join(self._project_dir, "DSSfiles")
+
+    def export_path(self, scenario):
+        """Return the path containing export data.
+
+        Parameters
+        ----------
+        scenario : str
+
+        Returns
+        -------
+        str
+
+        """
+        return os.path.join(self._project_dir, "Exports", scenario)
+
+    @property
+    def hdf_store(self):
+        """Return the HDFStore
+
+        Returns
+        -------
+        pd.HDFStore
+
+        """
+        if self._hdf_store is None:
+            raise InvalidConfiguration("hdf_store is not defined")
+        return self._hdf_store
+
+    @property
+    def fs_interface(self):
+        """Return the interface object used to read files.
+
+        Returns
+        -------
+        PyDssFileSystemInterface
+
+        """
+        if self._fs_intf is None:
+            raise InvalidConfiguration("fs interface is not defined")
+        return self._fs_intf
+
+    def get_hdf_store_filename(self):
+        """Return the HDFStore filename.
+
+        Returns
+        -------
+        str
+            Path to the HDFStore.
+
+        Raises
+        ------
+        InvalidConfiguration
+            Raised if no store exists.
+
+        """
+        filename = os.path.join(self._project_dir, STORE_FILENAME)
+        if not os.path.exists(filename):
+            raise InvalidConfiguration(f"HDFStore does not exist: {filename}")
+
+        return filename
+
+    def get_post_process_directory(self, scenario_name):
+        """Return the post-process output directory for scenario_name.
+
+        Parameters
+        ----------
+        scenario_name : str
+
+        Returns
+        -------
+        str
+
+        """
+        # Make sure the scenario exists. This will throw if not.
+        self.get_scenario(scenario_name)
+        return os.path.join(
+            self._project_dir, "Scenarios", scenario_name, "PostProcess"
+        )
+
+    def get_scenario(self, name):
+        """Return the scenario with name.
+
+        Parameters
+        ----------
+        name : str
+
+        Returns
+        -------
+        PyDssScenario
+
+        """
+        for scenario in self._scenarios:
+            if scenario.name == name:
+                return scenario
+
+        raise InvalidParameter(f"{name} is not a valid scenario")
+
+    @property
+    def name(self):
+        """Return the project name.
+
+        Returns
+        -------
+        str
+
+        """
+        return self._name
+
+    @property
+    def project_path(self):
+        """Return the path to the project.
+
+        Returns
+        -------
+        str
+
+        """
+        return self._project_dir
+
+    @property
+    def scenarios(self):
+        """Return the project scenarios.
+
+        Returns
+        -------
+        list
+            list of PyDssScenario
+
+        """
+        return self._scenarios
+
+    @property
+    def simulation_config(self):
+        """Return the simulation configuration
+
+        Returns
+        -------
+        dict
+
+        """
+        return self._settings
+
+    @property
+    def estimated_space(self):
+        """Return the estimated space in bytes.
+        
+        Returns
+        -------
+        int
+        """
+        return self._estimated_space
+
+    def serialize(self, opendss_project_folder):
+        """Create the project on the filesystem."""
+        os.makedirs(self._project_dir, exist_ok=True)
+        for name in PROJECT_DIRECTORIES:
+            os.makedirs(os.path.join(self._project_dir, name), exist_ok=True)
+        if opendss_project_folder:
+            dest = os.path.join(self._project_dir, PROJECT_DIRECTORIES[0])
+            copy_tree(opendss_project_folder, dest)
+        self._serialize_scenarios()
+        dump_settings(
+            self._settings,
+            os.path.join(self._project_dir, self._simulation_file),
+        )
+        logger.info("Initialized directories in %s", self._project_dir)
+
+    @classmethod
+
+    def create_project(cls, path, name, scenarios, simulation_config=None, options=None,
+                       simulation_file=SIMULATION_SETTINGS_FILENAME, opendss_project_folder=None,
+                       master_dss_file=OPENDSS_MASTER_FILENAME, force=False):
+        """Create a new PyDssProject on the filesystem.
+
+        Parameters
+        ----------
+        path : str
+            path in which to create directories
+        name : str
+            project name
+        scenarios : list
+            list of PyDssScenario objects
+        simulation_config : str
+            simulation config file; if None, use default
+
+        """
+        if simulation_config is None:
+            scenario_names = [x.name for x in scenarios]
+            simulation_config = create_simulation_settings(path, name, scenario_names, force=force)
+        simulation_config = load_data(simulation_config)
+        if options is not None:
+            for category, category_options in options.items():
+                simulation_config[category].update(category_options)
+        if master_dss_file:
+            simulation_config["project"]["dss_file"] = master_dss_file
+        simulation_config["project"]["project_path"] = path
+        simulation_config["project"]["active_project"] = name
+        settings = SimulationSettingsModel(**simulation_config)
+        project = cls(
+            path=path,
+            name=name,
+            scenarios=scenarios,
+            settings=settings,
+            simulation_file=simulation_file,
+        )
+        project.serialize(opendss_project_folder=opendss_project_folder)
+        sc_names = project.list_scenario_names()
+        logger.info("Created project=%s with scenarios=%s at %s", name,
+                    sc_names, path)
+        return project
+
+    def read_scenario_export_metadata(self, scenario_name):
+        """Return the metadata for a scenario's exported data.
+
+        Parameters
+        ----------
+        scenario_name : str
+
+        Returns
+        -------
+        dict
+
+        """
+        if self._fs_intf is None:
+            raise InvalidConfiguration("pydss fs interface is not defined")
+
+        if scenario_name not in self.list_scenario_names():
+            raise InvalidParameter(f"invalid scenario: {scenario_name}")
+
+        return self._fs_intf.read_scenario_export_metadata(scenario_name)
+
+    def list_scenario_names(self):
+        return [x.name for x in self.scenarios]
+
+    def run(self, logging_configured=True, tar_project=False, zip_project=False, dry_run=False):
+        """Run all scenarios in the project."""
+        if isinstance(self._fs_intf, PyDssArchiveFileInterfaceBase):
+            raise InvalidConfiguration("cannot run from an archived project")
+        if tar_project and zip_project:
+            raise InvalidParameter("tar_project and zip_project cannot both be True")
+        if self._settings.project.dss_file == "":
+            raise InvalidConfiguration("a valid opendss file needs to be passed")
+
+        inst = instance()
+        if not logging_configured:
+            if self._settings.logging.enable_console:
+                console_level = "INFO"
+            else:
+                console_level = "ERROR"
+            if self._settings.logging.enable_file:
+                filename = os.path.join(self._project_dir, "Logs", "pydss.log")
+            else:
+                filename = None
+            file_level = "INFO"
+            logger.level(console_level)
+            if filename:
+                logger.add(filename)
+            
+        if dry_run:
+            store_filename = os.path.join(tempfile.gettempdir(), STORE_FILENAME)
+        else:
+            store_filename = os.path.join(self._project_dir, STORE_FILENAME)
+            self._dump_simulation_settings()
+
+        driver = None
+        if self._settings.exports.export_data_in_memory:
+            driver = "core"
+        if os.path.exists(store_filename):
+            os.remove(store_filename)
+
+        try:
+            # This ensures that all datasets are flushed and closed after each
+            # scenario. If there is an unexpected crash in a later scenario then
+            # the file will still be valid for completed scenarios.
+            for scenario in self._scenarios:
+                with h5py.File(store_filename, mode="a", driver=driver) as hdf_store:
+                    self._hdf_store = hdf_store
+                    self._hdf_store.attrs["version"] = DATA_FORMAT_VERSION
+                    self._settings.project.active_scenario = scenario.name
+                    inst.run(self._settings, self, scenario, dry_run=dry_run)
+                    self._estimated_space[scenario.name] = inst.get_estimated_space()
+
+            export_tables = self._settings.exports.export_data_tables
+            generate_reports = bool(self._settings.reports)
+            if not dry_run and (export_tables or generate_reports):
+                # Hack. Have to import here. Need to re-organize to fix.
+                from pydss.pydss_results import PyDssResults
+                results = PyDssResults(self._project_dir)
+                if export_tables:
+                    for scenario in results.scenarios:
+                        scenario.export_data()
+
+                if generate_reports:
+                    results.generate_reports()
+
+        except Exception:
+            logger.exception("Simulation failed")
+            raise
+
+        finally:
+            logger.remove()
+            if tar_project:
+                self._tar_project_files()
+            elif zip_project:
+                self._zip_project_files()
+
+            if dry_run and os.path.exists(store_filename):
+                os.remove(store_filename)
+
+    def _dump_simulation_settings(self):
+        # Various settings may have been updated. Write the actual settings to a file.
+        filename = os.path.join( self._project_dir, RUN_SIMULATION_FILENAME)
+        dump_settings(self._settings, filename)
+
+    def _serialize_scenarios(self):
+        scenarios = []
+        for scenario in self._scenarios:
+            cfg = scenario.snapshot_time_point_selection_config or SnapshotTimePointSelectionConfigModel()
+            model = ScenarioModel(
+                name=scenario.name,
+                post_process_infos=[],
+                snapshot_time_point_selection_config=cfg,
+            )
+            model.post_process_infos = scenario.post_process_infos
+            scenarios.append(model)
+            scenario.serialize(
+                os.path.join(self._scenarios_dir, scenario.name)
+            )
+
+        self._settings.project.scenarios = scenarios
+
+    def _tar_project_files(self, delete=True):
+        orig = os.getcwd()
+        os.chdir(self._project_dir)
+        skip_names = (PROJECT_ZIP, STORE_FILENAME, REPORTS_DIR)
+        try:
+            filename = PROJECT_TAR
+            to_delete = []
+            with tarfile.open(filename, "w") as tar:
+                for name in os.listdir("."):
+                    if name in self._SKIP_ARCHIVE:
+                        continue
+                    tar.add(name)
+                    if delete:
+                        to_delete.append(name)
+
+            for name in to_delete:
+                if os.path.isfile(name):
+                    os.remove(name)
+                else:
+                    shutil.rmtree(name)
+
+            path = os.path.join(self._project_dir, filename)
+        finally:
+            os.chdir(orig)
+
+    def _zip_project_files(self, delete=True):
+        orig = os.getcwd()
+        os.chdir(self._project_dir)
+        try:
+            filename = PROJECT_ZIP
+            to_delete = []
+            with zipfile.ZipFile(filename, "w") as zipf:
+                for root, dirs, files in os.walk("."):
+                    if delete and root == ".":
+                        to_delete += dirs
+                    for filename in files:
+                        if root == "." and filename in self._SKIP_ARCHIVE:
+                            continue
+                        path = os.path.join(root, filename)
+                        zipf.write(path)
+                        # We delete files and directories at the root only.
+                        if delete and root == ".":
+                            to_delete.append(path)
+
+            for name in to_delete:
+                if os.path.isfile(name):
+                    os.remove(name)
+                else:
+                    shutil.rmtree(name)
+
+            path = os.path.join(self._project_dir, filename)
+        finally:
+            os.chdir(orig)
+
+    @staticmethod
+    def load_simulation_settings(project_path, simulations_file):
+        """Return the simulation settings for a project, using defaults if the
+        file is not defined.
+
+        Parameters
+        ----------
+        project_path : Path
+
+        Returns
+        -------
+        SimulationSettingsModel
+
+        """
+        filename = project_path / simulations_file
+        if not filename.exists():
+            filename = project_path / DEFAULT_SIMULATION_SETTINGS_FILE
+            assert filename.exists()
+        return load_simulation_settings(filename)
+
+    @classmethod
+    def load_project(
+        cls,
+        path,
+        options=None,
+        in_memory=False,
+        simulation_file=SIMULATION_SETTINGS_FILENAME,
+    ):
+        """Load a PyDssProject from directory.
+
+        Parameters
+        ----------
+        path : str
+            full path to existing project
+        options : dict
+            options that override the config file
+        in_memory : bool
+            If True, load all exported data into memory.
+
+        """
+        name = os.path.basename(path)
+        #if simulation_file is None:
+            #simulation_file = SIMULATION_SETTINGS_FILENAME
+
+        if os.path.exists(os.path.join(path, PROJECT_TAR)):
+            fs_intf = PyDssTarFileInterface(path)
+        elif os.path.exists(os.path.join(path, PROJECT_ZIP)):
+            fs_intf = PyDssZipFileInterface(path)
+        else:
+            fs_intf = PyDssFileSystemInterface(path, simulation_file)
+
+        simulation_config = fs_intf.simulation_config.dict(by_alias=False)
+        if options is not None:
+            for category, params in options.items():
+                if category not in simulation_config:
+                    simulation_config[category] = {}
+                simulation_config[category].update(params)
+            logger.info("Overrode config options: %s", options)
+
+        settings = SimulationSettingsModel(**simulation_config)
+        scenarios = [
+            PyDssScenario.deserialize(
+                fs_intf,
+                x.name,
+                post_process_infos=x.post_process_infos,
+            )
+            for x in settings.project.scenarios
+        ]
+
+        return PyDssProject(
+            os.path.dirname(path),
+            name,
+            scenarios,
+            settings,
+            fs_intf=fs_intf,
+        )
+
+    @classmethod
+    def run_project(cls, path, options=None, tar_project=False, zip_project=False, simulation_file=None, dry_run=False):
+
+        """Load a PyDssProject from directory and run all scenarios.
+
+        Parameters
+        ----------
+        path : str
+            full path to existing project
+        options : dict
+            options that override the config file
+        tar_project : bool
+            tar project files after successful execution
+        zip_project : bool
+            zip project files after successful execution
+        dry_run: bool
+            dry run for getting estimated space.
+        """
+
+        project = cls.load_project(path, options=options, simulation_file=simulation_file)
+        return project.run(tar_project=tar_project, zip_project=zip_project, dry_run=dry_run)
+
+    def read_scenario_settings(self, scenario):
+        """Read the simulation settings file for the scenario.
+
+        Parameters
+        ----------
+        scenario : str
+            Scenario name
+
+        Returns
+        -------
+        SimulationSettingsModel
+
+        """
+        scenario_path = Path(self._project_dir) / "Scenarios" / scenario
+        if not scenario_path.exists():
+            raise InvalidParameter(f"scenario={scenario} is not present")
+
+        settings_file = scenario_path / RUN_SIMULATION_FILENAME
+        if not settings_file.exists():
+            raise InvalidConfiguration(f"{RUN_SIMULATION_FILENAME} does not exist. Was the scenario run?")
+
+        return load_simulation_settings(settings_file)
+
+    def read_scenario_time_settings(self, scenario):
+        """Return the simulation time-related settings for the scenario.
+
+        Parameters
+        ----------
+        scenario : str
+            Scenario name
+
+        Returns
+        -------
+        dict
+
+        """
+        settings = self.read_scenario_settings(scenario).project
+        data = {}
+        for key in (
+            "start_time",
+            "simulation_duration_min",
+            "loadshape_start_time",
+            "step_resolution_sec",
+        ):
+            data[key] = getattr(settings, key)
+        return data
+
+
+class PyDssScenario:
+    """Represents a pydss Scenario."""
+
+    DEFAULT_CONTROLLER_TYPES = (ControllerType.PV_CONTROLLER,)
+    DEFAULT_EXPORT_MODE = ExportMode.EXPORTS
+    _SCENARIO_DIRECTORIES = (
+        "ExportLists",
+        "pyControllerList",
+        "pyPlotList",
+        "PostProcess",
+        'Monte_Carlo'
+    )
+    REQUIRED_POST_PROCESS_FIELDS = ("script", "config_file")
+
+    def __init__(self, name, controller_types=None, controllers=None,
+                 export_modes=None, exports=None,
+                 post_process_infos=None, 
+                 snapshot_time_point_selection_config=None):
+        self.name = name
+        self.post_process_infos = []
+        self.snapshot_time_point_selection_config = None
+
+        if (controller_types is None and controllers is None):
+            self.controllers = {}
+        elif controller_types is not None:
+            self.controllers = {
+                x: self.load_controller_config_from_type(x)
+                for x in controller_types
+            }
+        elif isinstance(controllers, str):
+            basename = os.path.splitext(os.path.basename(controllers))[0]
+            controller_type = ControllerType(basename)
+            self.controllers = {controller_type: load_data(controllers)}
+        else:
+            assert isinstance(controllers, dict)
+            self.controllers = controllers
+
+        if export_modes is not None and exports is not None:
+            raise InvalidParameter(
+                "export_modes and exports cannot both be set"
+            )
+        if (export_modes is None and exports is None):
+            mode = PyDssScenario.DEFAULT_EXPORT_MODE
+            self.exports = {mode: self.load_export_config_from_mode(mode)}
+        elif export_modes is not None:
+            self.exports = {
+                x: self.load_export_config_from_mode(x) for x in export_modes
+            }
+        elif isinstance(exports, str):
+            mode = ExportMode(os.path.splitext(os.path.basename(exports))[0])
+            self.exports = {mode: load_data(exports)}
+        else:
+            assert isinstance(exports, dict)
+            self.exports = exports
+
+        if post_process_infos is not None:
+            for pp_info in post_process_infos:
+                if isinstance(pp_info, dict):
+                    pp_info = ScenarioPostProcessModel(**pp_info)
+                self.add_post_process(pp_info)
+
+        if snapshot_time_point_selection_config is not None:
+            # Ensure the mode is valid.
+            SnapshotTimePointSelectionMode(snapshot_time_point_selection_config["mode"])
+            self.snapshot_time_point_selection_config = snapshot_time_point_selection_config
+
+    @classmethod
+    def deserialize(cls, fs_intf, name, post_process_infos):
+        """Deserialize a PyDssScenario from a path.
+
+        Parameters
+        ----------
+        fs_intf : PyDssFileSystemInterface
+            object to read on-disk information
+        name : str
+            scenario name
+        post_process_infos : list
+            list of post_process_info dictionaries
+
+        Returns
+        -------
+        PyDssScenario
+
+        """
+        controllers = fs_intf.read_controller_config(name)
+        exports = fs_intf.read_export_config(name)
+
+        return cls(
+            name,
+            controllers=controllers,
+            exports=exports,
+            post_process_infos=post_process_infos,
+        )
+
+    def serialize(self, path):
+        """Serialize a PyDssScenario to a directory.
+
+        Parameters
+        ----------
+        path : str
+            full path to scenario
+
+        """
+        os.makedirs(path, exist_ok=True)
+        for name in self._SCENARIO_DIRECTORIES:
+            os.makedirs(os.path.join(path, name), exist_ok=True)
+
+        for controller_type, controllers in self.controllers.items():
+            filename = os.path.join(
+                path, "pyControllerList", filename_from_enum(controller_type)
+            )
+            dump_data(controllers, filename)
+
+        for mode, exports in self.exports.items():
+            dump_data(
+                exports,
+                os.path.join(path, "ExportLists", filename_from_enum(mode))
+            )
+
+        dump_data(
+            load_data(DEFAULT_MONTE_CARLO_SETTINGS_FILE),
+            os.path.join(path, "Monte_Carlo", MONTE_CARLO_SETTINGS_FILENAME)
+        )
+
+        dump_data(
+            load_data(DEFAULT_SUBSCRIPTIONS_FILE),
+            os.path.join(path, "ExportLists", SUBSCRIPTIONS_FILENAME)
+        )
+
+    @staticmethod
+    def load_controller_config_from_type(controller_type):
+        """Load a default controller config from a type.
+
+        Parameters
+        ----------
+        controller_type : ControllerType
+
+        Returns
+        -------
+        dict
+
+        """
+
+        path = os.path.join(
+            os.path.dirname(getattr(pydss, "__path__")[0]),
+            "pydss",
+            "defaults",
+            "pyControllerList",
+            filename_from_enum(controller_type),
+        )
+
+        return load_data(path)
+
+    @staticmethod
+    def load_export_config_from_mode(export_mode):
+        """Load a default export config from a type.
+
+        Parameters
+        ----------
+        export_mode : ExportMode
+
+        Returns
+        -------
+        dict
+
+        """
+        path = os.path.join(
+            os.path.dirname(getattr(pydss, "__path__")[0]),
+            "pydss",
+            "defaults",
+            "ExportLists",
+            filename_from_enum(export_mode),
+        )
+
+        return load_data(path)
+
+    def add_post_process(self, post_process_info):
+        """Add a post-process script to a scenario.
+
+        Parameters
+        ----------
+        post_process_info : dict
+            Must define all fields in PyDssScenario.REQUIRED_POST_PROCESS_FIELDS
+
+        """
+        config_file = post_process_info.config_file
+        if config_file and not os.path.exists(config_file):
+            raise InvalidParameter(f"{config_file} does not exist")
+
+        self.post_process_infos.append(post_process_info)
+
+
+def load_config(path):
+    """Return a configuration from files.
+
+    Parameters
+    ----------
+    path : str
+
+    Returns
+    -------
+    dict
+
+    """
+    files = [os.path.join(path, x) for x in os.listdir(path) \
+             if os.path.splitext(x)[1] == ".toml"]
+    assert len(files) == 1, "only 1 .toml file is currently supported"
+    return load_data(files[0])
+
+
+def update_pydss_controllers(project_path, scenario, controller_type, 
+                             controller, dss_file):
+    """Update a scenario's controllers from an OpenDSS file.
+
+    Parameters
+    ----------
+    project_path : str
+        pydss project path.
+    scenario : str
+        pydss scenario name in project.
+    controller_type : str
+        A type of pydss controler
+    controller : str
+        The controller name
+    dss_file : str
+        A DSS file path
+    """
+    if controller_type not in READ_CONTROLLER_FUNCTIONS:
+        supported_types = list(READ_CONTROLLER_FUNCTIONS.keys())
+        print(f"Invalid controller_type={controller_type}, supported: {supported_types}")
+        sys.exit(1)
+
+    sim_file = os.path.join(project_path, SIMULATION_SETTINGS_FILENAME)
+    settings = load_simulation_settings(sim_file)
+    if not settings.project.use_controller_registry:
+        print(f"'Use Controller Registry' must be set to true in {sim_file}")
+        sys.exit(1)
+
+    registry = Registry()
+    if not registry.is_controller_registered(controller_type, controller):
+        print(f"{controller_type} / {controller} is not registered")
+        sys.exit(1)
+
+    data = {}
+    filename = f"{project_path}/Scenarios/{scenario}/pyControllerList/{controller_type}.toml"
+    if os.path.exists(filename):
+        data = load_data(filename)
+        for val in data.values():
+            if not isinstance(val, list):
+                print(f"{filename} has an invalid format")
+                sys.exit(1)
+
+    element_names = READ_CONTROLLER_FUNCTIONS[controller_type](dss_file)
+    num_added = 0
+    if controller in data:
+        existing = set(data[controller])
+        final = list(existing.union(set(element_names)))
+        data[controller] = final
+        num_added = len(final) - len(existing)
+    else:
+        data[controller] = element_names
+        num_added = len(element_names)
+
+    # Remove element_names from any other controllers.
+    set_names = set(element_names)
+    for _controller, values in data.items():
+        if _controller != controller:
+            final = set(values).difference_update(set_names)
+            if final is None:
+                final_list = None
+            else:
+                final_list = list(final)
+            data[_controller] = final_list
+
+    dump_data(data, filename)
+    print(f"Added {num_added} names to {filename}")
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pydss_results.py` & `nrel_pydss-3.1.4/src/pydss/pydss_results.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,1116 +1,1116 @@
-"""Provides access to pydss result data."""
-from collections import defaultdict
-import json
-import os
-import re
-
-import h5py
-import numpy as np
-import pandas as pd
-from loguru import logger
-
-from pydss.common import  DatasetPropertyType
-from pydss.dataset_buffer import DatasetBuffer
-from pydss.element_options import ElementOptions
-from pydss.exceptions import InvalidParameter
-from pydss.pydss_project import PyDssProject, RUN_SIMULATION_FILENAME
-from pydss.reports.reports import Reports, REPORTS_DIR
-from pydss.utils.dataframe_utils import read_dataframe, write_dataframe
-from pydss.utils.utils import dump_data, load_data, make_json_serializable, \
-    make_timestamps
-from pydss.value_storage import ValueStorageBase, get_dataset_property_type, \
-    get_time_step_path
-
-class PyDssResults:
-    """Interface to perform analysis on pydss output data."""
-    def __init__(
-            self, project_path=None, project=None, in_memory=False,
-            frequency=False, mode=False
-        ):
-        """Constructs PyDssResults object.
-
-        Parameters
-        ----------
-        project_path : str | None
-            Load project from files in path
-        project : PyDssProject | None
-            Existing project object
-        in_memory : bool
-            If true, load all exported data into memory.
-        frequency : bool
-            If true, add frequency column to all dataframes.
-        mode : bool
-            If true, add mode column to all dataframes.
-
-        """
-        options = ElementOptions()
-        if project_path is not None:
-            # TODO: handle old version?
-            self._project = PyDssProject.load_project(
-                project_path,
-                simulation_file=RUN_SIMULATION_FILENAME,
-            )
-        elif project is None:
-            raise InvalidParameter("project_path or project must be set")
-        else:
-            self._project = project
-        self._fs_intf = self._project.fs_interface
-        self._scenarios = []
-        filename = self._project.get_hdf_store_filename()
-        driver = "core" if in_memory else None
-        self._hdf_store = h5py.File(filename, "r", driver=driver)
-
-        if self._project.simulation_config.exports.export_results:
-            for name in self._project.list_scenario_names():
-                metadata = self._project.read_scenario_export_metadata(name)
-                scenario_result = PyDssScenarioResults(
-                    name,
-                    self.project_path,
-                    self._hdf_store,
-                    self._fs_intf,
-                    metadata,
-                    options,
-                    frequency=frequency,
-                    mode=mode,
-                )
-                self._scenarios.append(scenario_result)
-
-    def __del__(self):
-        if hasattr(self, "_hdf_store"):
-            self._hdf_store.flush()
-            self._hdf_store.close()
-            logger.info("store closed sucessfully")
-    
-    def generate_reports(self):
-        """Generate all reports specified in the configuration.
-
-        Returns
-        -------
-        list
-            list of report filenames
-
-        """
-        return Reports.generate_reports(self)
-
-    def read_report(self, report_name):
-        """Return the report data.
-
-        Parameters
-        ----------
-        report_name : str
-
-        Returns
-        -------
-        str
-
-        """
-        all_reports = Reports.get_all_reports()
-        if report_name not in all_reports:
-            raise InvalidParameter(f"invalid report name {report_name}")
-        report_cls = all_reports[report_name]
-
-        # This bypasses self._fs_intf because reports are always extracted.
-        reports_dir = os.path.join(self._project.project_path, REPORTS_DIR)
-        for filename in os.listdir(reports_dir):
-            name, ext = os.path.splitext(filename)
-            if name == os.path.splitext(report_cls.FILENAME)[0]:
-                path = os.path.join(reports_dir, filename)
-                if ext in (".json", ".toml"):
-                    return load_data(path)
-                if ext in (".csv", ".h5"):
-                    return read_dataframe(path)
-
-        raise InvalidParameter(f"did not find report {report_name} in {reports_dir}")
-
-    @property
-    def project(self):
-        """Return the PyDssProject instance.
-
-        Returns
-        -------
-        PyDssProject
-
-        """
-        return self._project
-
-    @property
-    def scenarios(self):
-        """Return the PyDssScenarioResults instances for the project.
-
-        Returns
-        -------
-        list
-            list of PyDssScenarioResults
-
-        """
-        return self._scenarios
-
-    def get_scenario(self, name):
-        """Return the PyDssScenarioResults object for scenario with name.
-
-        Parameters
-        ----------
-        name : str
-            Scenario name
-
-        Results
-        -------
-        PyDssScenarioResults
-
-        Raises
-        ------
-        InvalidParameter
-            Raised if the scenario does not exist.
-
-        """
-        for scenario in self._scenarios:
-            if name == scenario.name:
-                return scenario
-
-        raise InvalidParameter(f"scenario {name} does not exist")
-
-    @property
-    def hdf_store(self):
-        """Return a handle to the HDF data store.
-
-        Returns
-        -------
-        h5py.File
-
-        """
-        return self._hdf_store
-
-    @property
-    def project_path(self):
-        """Return the path to the pydss project.
-
-        Returns
-        -------
-        str
-
-        """
-        return self._project.project_path
-
-    def read_file(self, path):
-        """Read a file from the pydss project.
-
-        Parameters
-        ----------
-        path : str
-            Path to the file relative from the project directory.
-
-        Returns
-        -------
-        str
-            Contents of the file
-
-        """
-        return self._fs_intf.read_file(path)
-
-    @property
-    def simulation_config(self):
-        """Return the simulation configuration
-
-        Returns
-        -------
-        dict
-
-        """
-        return self._project.simulation_config
-
-
-class PyDssScenarioResults:
-    """Contains results for one scenario."""
-    def __init__(
-            self, name, project_path, store, fs_intf, metadata, options,
-            frequency=False, mode=False
-        ):
-        self._name = name
-        self._project_path = project_path
-        self._hdf_store = store
-        self._metadata = metadata or {}
-        self._options = options
-        self._fs_intf = fs_intf
-        self._elems_by_class = defaultdict(set)
-        self._elem_data_by_prop = defaultdict(dict)
-        self._elem_values_by_prop = defaultdict(dict)
-        self._elem_indices_by_prop = defaultdict(dict)
-        self._props_by_class = defaultdict(list)
-        self._elem_props = defaultdict(list)
-        self._column_ranges_per_elem = defaultdict(dict)
-        self._summed_elem_props = defaultdict(dict)
-        self._summed_elem_timeseries_props = defaultdict(list)
-        self._indices_df = None
-        self._add_frequency = frequency
-        self._add_mode = mode
-        self._data_format_version = self._hdf_store.attrs["version"]
-        if name not in self._hdf_store["Exports"]:
-            self._group = None
-            return
-
-        self._group = self._hdf_store[f"Exports/{name}"]
-        self._elem_classes = [
-            x for x in self._group if isinstance(self._group[x], h5py.Group)
-        ]
-
-        self._parse_datasets()
-
-    def _parse_datasets(self):
-        for elem_class in self._elem_classes:
-            class_group = self._group[elem_class]
-            if "ElementProperties" in class_group:
-                prop_group = class_group["ElementProperties"]
-                for prop, dataset in prop_group.items():
-                    dataset_property_type = get_dataset_property_type(dataset)
-                    if dataset_property_type == DatasetPropertyType.TIME_STEP:
-                        continue
-                    if dataset_property_type == DatasetPropertyType.VALUE:
-                        self._elem_values_by_prop[elem_class][prop] = []
-                        prop_names = self._elem_values_by_prop
-                    elif dataset_property_type in (
-                            DatasetPropertyType.PER_TIME_POINT,
-                            DatasetPropertyType.FILTERED,
-                    ):
-                        self._elem_data_by_prop[elem_class][prop] = []
-                        prop_names = self._elem_data_by_prop
-                    else:
-                        continue
-
-                    self._props_by_class[elem_class].append(prop)
-                    self._elem_indices_by_prop[elem_class][prop] = {}
-                    names = DatasetBuffer.get_names(dataset)
-                    self._column_ranges_per_elem[elem_class][prop] = \
-                        DatasetBuffer.get_column_ranges(dataset)
-                    for i, name in enumerate(names):
-                        self._elems_by_class[elem_class].add(name)
-                        prop_names[elem_class][prop].append(name)
-                        self._elem_indices_by_prop[elem_class][prop][name] = i
-                        self._elem_props[name].append(prop)
-            else:
-                self._elems_by_class[elem_class] = set()
-
-            summed_elem_props = self._group[elem_class].get("SummedElementProperties", [])
-            for prop in summed_elem_props:
-                dataset = self._group[elem_class]["SummedElementProperties"][prop]
-                dataset_property_type = get_dataset_property_type(dataset)
-                if dataset_property_type == DatasetPropertyType.VALUE:
-                    df = DatasetBuffer.to_dataframe(dataset)
-                    assert len(df) == 1
-                    self._summed_elem_props[elem_class][prop] = {
-                        x: df[x].values[0] for x in df.columns
-                    }
-                elif dataset_property_type == DatasetPropertyType.PER_TIME_POINT:
-                    self._summed_elem_timeseries_props[elem_class].append(prop)
-
-    @staticmethod
-    def get_name_from_column(column):
-        """Return the element name from the dataframe column. The dataframe should have been
-        returned from this class.
-
-        Parameters
-        ----------
-        column : str
-
-        Returns
-        -------
-        str
-
-        """
-        fields = column.split(ValueStorageBase.DELIMITER)
-        assert len(fields) > 1
-        return fields[0]
-
-    @property
-    def name(self):
-        """Return the name of the scenario.
-
-        Returns
-        -------
-        str
-
-        """
-        return self._name
-
-    def export_data(self, path=None, fmt="csv", compress=False):
-        """Export data to path.
-
-        Parameters
-        ----------
-        path : str
-            Output directory; defaults to scenario exports path
-        fmt : str
-            Filer format type (csv, h5)
-        compress : bool
-            Compress data
-
-        """
-        if path is None:
-            path = os.path.join(self._project_path, "Exports", self._name)
-        os.makedirs(path, exist_ok=True)
-        self._export_element_timeseries(path, fmt, compress)
-        self._export_element_values(path, fmt, compress)
-        self._export_summed_element_timeseries(path, fmt, compress)
-        self._export_summed_element_values(path, fmt, compress)
-
-    def _export_element_timeseries(self, path, fmt, compress):
-        for elem_class in self.list_element_classes():
-            for prop in self.list_element_properties(elem_class):
-                dataset = self._group[f"{elem_class}/ElementProperties/{prop}"]
-                prop_type = get_dataset_property_type(dataset)
-                if prop_type == DatasetPropertyType.FILTERED:
-                    self._export_filtered_dataframes(elem_class, prop, path, fmt, compress)
-                else:
-                    df = self.get_full_dataframe(elem_class, prop)
-                    base = "__".join([elem_class, prop])
-                    filename = os.path.join(path, base + "." + fmt.replace(".", ""))
-                    write_dataframe(df, filename, compress=compress)
-
-    def _export_element_values(self, path, fmt, compress):
-        elem_prop_nums = defaultdict(dict)
-        for elem_class in self._elem_values_by_prop:
-            for prop in self._elem_values_by_prop[elem_class]:
-                dataset = self._group[f"{elem_class}/ElementProperties/{prop}"]
-                for name in self._elem_values_by_prop[elem_class][prop]:
-                    col_range = self._get_element_column_range(elem_class, prop, name)
-                    start = col_range[0]
-                    length = col_range[1]
-                    if length == 1:
-                        val = dataset[:][0][start]
-                    else:
-                        val = dataset[:][0][start: start + length]
-                    if prop not in elem_prop_nums[elem_class]:
-                        elem_prop_nums[elem_class][prop] = {}
-                    elem_prop_nums[elem_class][prop][name] = val
-        if elem_prop_nums:
-            filename = os.path.join(path, "element_property_values.json")
-            dump_data(elem_prop_nums, filename, indent=2, default=make_json_serializable)
-
-        logger.info("Exported data to %s", path)
-
-    def _export_filtered_dataframes(self, elem_class, prop, path, fmt, compress):
-        for name, df in self.get_filtered_dataframes(elem_class, prop).items():
-            if df.empty:
-                logger.debug("Skip empty dataframe %s %s %s", elem_class, prop, name)
-                continue
-            base = "__".join([elem_class, prop, name])
-            filename = os.path.join(path, base + "." + fmt.replace(".", ""))
-            write_dataframe(df, filename, compress=compress)
-
-    def _export_summed_element_timeseries(self, path, fmt, compress):
-        for elem_class in self._summed_elem_timeseries_props:
-            for prop in self._summed_elem_timeseries_props[elem_class]:
-                fields = prop.split(ValueStorageBase.DELIMITER)
-                if len(fields) == 1:
-                    base = ValueStorageBase.DELIMITER.join([elem_class, prop])
-                else:
-                    assert len(fields) == 2, fields
-                    # This will be <elem_class>__<prop>__<group>
-                    base = ValueStorageBase.DELIMITER.join([elem_class, prop])
-                filename = os.path.join(path, base + "." + fmt.replace(".", ""))
-                dataset = self._group[elem_class]["SummedElementProperties"][prop]
-                prop_type = get_dataset_property_type(dataset)
-                if prop_type == DatasetPropertyType.PER_TIME_POINT:
-                    df = DatasetBuffer.to_dataframe(dataset)
-                    self._finalize_dataframe(df, dataset)
-                    write_dataframe(df, filename, compress=compress)
-
-    def _export_summed_element_values(self, path, fmt, compress):
-        filename = os.path.join(path, "summed_element_property_values.json")
-        dump_data(self._summed_elem_props, filename, default=make_json_serializable)
-
-    def get_dataframe(self, element_class, prop, element_name, real_only=False, abs_val=False, **kwargs):
-        """Return the dataframe for an element.
-
-        Parameters
-        ----------
-        element_class : str
-        prop : str
-        element_name : str
-        real_only : bool
-            If dtype of any column is complex, drop the imaginary component.
-        abs_val : bool
-            If dtype of any column is complex, compute its absolute value.
-        kwargs
-            Filter on options; values can be strings or regular expressions.
-
-        Returns
-        -------
-        pd.DataFrame
-
-        Raises
-        ------
-        InvalidParameter
-            Raised if the element is not stored.
-
-        """
-        if element_name not in self._elem_props:
-            raise InvalidParameter(f"element {element_name} is not stored")
-
-        dataset = self._group[f"{element_class}/ElementProperties/{prop}"]
-        prop_type = get_dataset_property_type(dataset)
-        if prop_type == DatasetPropertyType.PER_TIME_POINT:
-            return self._get_elem_prop_dataframe(
-                element_class, prop, element_name, dataset, real_only=real_only,
-                abs_val=abs_val, **kwargs
-            )
-        elif prop_type == DatasetPropertyType.FILTERED:
-            return self._get_filtered_dataframe(
-                element_class, prop, element_name, dataset, real_only=real_only,
-                abs_val=abs_val, **kwargs
-            )
-        assert False, str(prop_type)
-
-    def get_filtered_dataframes(self, element_class, prop, real_only=False, abs_val=False):
-        """Return the dataframes for all elements.
-
-        Calling this is much more efficient than calling get_dataframe for each
-        element.
-
-        Parameters
-        ----------
-        element_class : str
-        prop : str
-        element_name : str
-        real_only : bool
-            If dtype of any column is complex, drop the imaginary component.
-        abs_val : bool
-            If dtype of any column is complex, compute its absolute value.
-
-        Returns
-        -------
-        dict
-            key = str (name), val = pd.DataFrame
-            The dict will be empty if no data was stored.
-
-        """
-        if prop not in self.list_element_properties(element_class):
-            logger.debug("%s/%s is not stored", element_class, prop)
-            return {}
-
-        dataset = self._group[f"{element_class}/ElementProperties/{prop}"]
-        columns = DatasetBuffer.get_columns(dataset)
-        names = DatasetBuffer.get_names(dataset)
-        length = dataset.attrs["length"]
-        indices_df = self._get_indices_df()
-        data_vals = dataset[:length]
-        elem_data = defaultdict(list)
-        elem_timestamps = defaultdict(list)
-
-        # The time_step_dataset has these columns:
-        # 1. time step index
-        # 2. element index
-        # Each row describes the source data in the dataset row.
-        path = dataset.attrs["time_step_path"]
-        assert length == self._hdf_store[path].attrs["length"]
-        time_step_data = self._hdf_store[path][:length]
-
-        for i in range(length):
-            ts_index = time_step_data[:, 0][i]
-            elem_index = time_step_data[:, 1][i]
-            # TODO DT: more than one column?
-            val = data_vals[i, 0]
-            if real_only:
-                val = val.real
-            elif abs_val:
-                val = abs(val)
-            elem_data[elem_index].append(val)
-            elem_timestamps[elem_index].append(indices_df.iloc[ts_index, 0])
-
-        dfs = {}
-        for elem_index, vals in elem_data.items():
-            elem_name = names[elem_index]
-            cols = self._fix_columns(elem_name, columns)
-            dfs[elem_name] = pd.DataFrame(
-                vals,
-                columns=cols,
-                index=elem_timestamps[elem_index],
-            )
-        return dfs
-
-    def get_full_dataframe(self, element_class, prop, real_only=False, abs_val=False, **kwargs):
-        """Return a dataframe containing all data.  The dataframe is copied.
-
-        Parameters
-        ----------
-        element_class : str
-        prop : str
-        real_only : bool
-            If dtype of any column is complex, drop the imaginary component.
-        abs_val : bool
-            If dtype of any column is complex, compute its absolute value.
-        kwargs
-            Filter on options; values can be strings or regular expressions.
-
-        Returns
-        -------
-        pd.DataFrame
-
-        """
-        if prop not in self.list_element_properties(element_class):
-            raise InvalidParameter(f"property {prop} is not stored")
-
-        dataset = self._group[f"{element_class}/ElementProperties/{prop}"]
-        df = DatasetBuffer.to_dataframe(dataset)
-        if kwargs:
-            options = self._check_options(element_class, prop, **kwargs)
-            names = self._elems_by_class.get(element_class, set())
-            columns = ValueStorageBase.get_columns(df, names, options, **kwargs)
-            columns = list(columns)
-            columns.sort()
-            df = df[columns]
-        self._finalize_dataframe(df, dataset, real_only=real_only, abs_val=abs_val)
-        return df
-
-    def get_summed_element_total(self, element_class, prop, group=None):
-        """Return the total value for a summed element property.
-
-        Parameters
-        ----------
-        element_class : str
-        prop : str
-        group : str | None
-            Specify a group name if sum_groups was assigned.
-
-        Returns
-        -------
-        dict
-
-        Raises
-        ------
-        InvalidParameter
-            Raised if the element class is not stored.
-
-        """
-        if group is not None:
-            prop = ValueStorageBase.DELIMITER.join((prop, group))
-        if element_class not in self._summed_elem_props:
-            raise InvalidParameter(f"{element_class} is not stored")
-        if prop not in self._summed_elem_props[element_class]:
-            raise InvalidParameter(f"{prop} is not stored")
-
-        return self._summed_elem_props[element_class][prop]
-
-    def get_element_property_value(self, element_class, prop, element_name):
-        """Return the number stored for the element property."""
-        if element_class not in self._elem_values_by_prop:
-            raise InvalidParameter(f"{element_class} is not stored")
-        if prop not in self._elem_values_by_prop[element_class]:
-            raise InvalidParameter(f"{prop} is not stored")
-        if element_name not in self._elem_values_by_prop[element_class][prop]:
-            raise InvalidParameter(f"{element_name} is not stored")
-        dataset = self._group[f"{element_class}/ElementProperties/{prop}"]
-        col_range = self._get_element_column_range(element_class, prop, element_name)
-        start = col_range[0]
-        length = col_range[1]
-        if length == 1:
-            return dataset[:][0][start]
-        return dataset[:][0][start: start + length]
-
-    def get_option_values(self, element_class, prop, element_name):
-        """Return the option values for the element property.
-
-        element_class : str
-        prop : str
-        element_name : str
-
-        Returns
-        -------
-        list
-
-        """
-        df = self.get_dataframe(element_class, prop, element_name)
-        return ValueStorageBase.get_option_values(df, element_name)
-
-    def get_summed_element_dataframe(self, element_class, prop, real_only=False, abs_val=False, group=None):
-        """Return the dataframe for a summed element property.
-
-        Parameters
-        ----------
-        element_class : str
-        prop : str
-        group : str | None
-            Specify a group name if sum_groups was assigned.
-        real_only : bool
-            If dtype of any column is complex, drop the imaginary component.
-        abs_val : bool
-            If dtype of any column is complex, compute its absolute value.
-
-        Returns
-        -------
-        pd.DataFrame
-
-        Raises
-        ------
-        InvalidParameter
-            Raised if the element class is not stored.
-
-        """
-        if group is not None:
-            prop = ValueStorageBase.DELIMITER.join((prop, group))
-        if element_class not in self._summed_elem_timeseries_props:
-            raise InvalidParameter(f"{element_class} is not stored")
-        if prop not in self._summed_elem_timeseries_props[element_class]:
-            raise InvalidParameter(f"{prop} is not stored")
-
-        elem_group = self._group[element_class]["SummedElementProperties"]
-        dataset = elem_group[prop]
-        df = DatasetBuffer.to_dataframe(dataset)
-        self._add_indices_to_dataframe(df)
-
-        if real_only:
-            for column in df.columns:
-                if df[column].dtype == complex:
-                    df[column] = np.real(df[column])
-        elif abs_val:
-            for column in df.columns:
-                if df[column].dtype == complex:
-                    df[column] = df[column].apply(np.absolute)
-
-        return df
-
-    def get_timestamps(self):
-        """Return the timestamps of the simulation in a pandas.Series."""
-        return self._get_indices_df()["Timestamp"]
-
-    def iterate_dataframes(self, element_class, prop, real_only=False, abs_val=False, **kwargs):
-        """Returns a generator over the dataframes by element name.
-
-        Parameters
-        ----------
-        element_class : str
-        prop : str
-        real_only : bool
-            If dtype of any column is complex, drop the imaginary component.
-        abs_val : bool
-            If dtype of any column is complex, compute its absolute value.
-        kwargs : dict
-            Filter on options; values can be strings or regular expressions.
-
-        Returns
-        -------
-        tuple
-            Tuple containing the name or property and a pd.DataFrame
-
-        """
-        for name in self.list_element_names(element_class):
-            if prop in self._elem_props[name]:
-                df = self.get_dataframe(
-                    element_class, prop, name, real_only=real_only, abs_val=abs_val, **kwargs
-                )
-                yield name, df
-
-    def iterate_element_property_values(self):
-        """Return a generator over all element properties stored as values.
-
-        Yields
-        ------
-        tuple
-            element_class, property, element_name, value
-
-        """
-        for elem_class in self._elem_values_by_prop:
-            for prop in self._elem_values_by_prop[elem_class]:
-                for name in self._elem_values_by_prop[elem_class][prop]:
-                    val = self.get_element_property_value(elem_class, prop, name)
-                    yield elem_class, prop, name, val
-
-    def list_element_classes(self):
-        """Return the element classes stored in the results.
-
-        Returns
-        -------
-        list
-
-        """
-        return self._elem_classes[:]
-
-    def list_element_names(self, element_class, prop=None):
-        """Return the element names for a property stored in the results.
-
-        Parameters
-        ----------
-        element_class : str
-        prop : str
-
-        Returns
-        -------
-        list
-
-        """
-        # TODO: prop is deprecated
-        return sorted(list(self._elems_by_class.get(element_class, [])))
-
-    def list_element_properties(self, element_class, element_name=None):
-        """Return the properties stored in the results for a class.
-
-        Parameters
-        ----------
-        element_class : str
-        element_name : str | None
-            If not None, list properties only for that name.
-
-        Returns
-        -------
-        list
-
-        """
-        if element_class not in self._props_by_class:
-            return []
-        if element_name is None:
-            return sorted(list(self._props_by_class[element_class]))
-        return self._elem_props.get(element_name, [])
-
-    def list_element_value_names(self, element_class, prop):
-        if element_class not in self._elem_values_by_prop:
-            raise InvalidParameter(f"{element_class} is not stored")
-        if prop not in self._elem_values_by_prop[element_class]:
-            raise InvalidParameter(f"{element_class} / {prop} is not stored")
-        return sorted(self._elem_values_by_prop[element_class][prop])
-
-    def list_element_property_values(self, element_name):
-        nums = []
-        for elem_class in self._elem_prop_nums:
-            for prop in self._elem_prop_nums[elem_class]:
-                for name in self._elem_prop_nums[elem_class][prop]:
-                    if name == element_name:
-                        nums.append(prop)
-        return nums
-
-    def list_element_property_options(self, element_class, prop):
-        """List the possible options for the element class and property.
-
-        Parameters
-        ----------
-        element_class : str
-        prop : str
-
-        Returns
-        -------
-        list
-
-        """
-        return self._options.list_options(element_class, prop)
-
-    def list_element_info_files(self):
-        """Return the files describing the OpenDSS element objects.
-
-        Returns
-        -------
-        list
-            list of filenames (str)
-
-        """
-        return self._metadata.get("element_info_files", [])
-
-    def list_summed_element_properties(self, element_class):
-        """Return the properties stored for a class where the values are a sum
-        of all elements.
-
-        Parameters
-        ----------
-        element_class : str
-
-        Returns
-        -------
-        list
-
-        Raises
-        ------
-        InvalidParameter
-            Raised if the element_class is not stored.
-
-        """
-        if element_class not in self._summed_elem_props:
-            raise InvalidParameter(f"class={element_class} is not stored")
-        return self._summed_elem_props[element_class]
-
-    def list_summed_element_time_series_properties(self, element_class):
-        """Return the properties stored for a class where the values are a sum
-        of all elements.
-
-        Parameters
-        ----------
-        element_class : str
-
-        Returns
-        -------
-        list
-
-        Raises
-        ------
-        InvalidParameter
-            Raised if the element_class is not stored.
-
-        """
-        if element_class not in self._summed_elem_timeseries_props:
-            raise InvalidParameter(f"class={element_class} is not stored")
-        return self._summed_elem_timeseries_props[element_class]
-
-    def read_element_info_file(self, filename):
-        """Return the contents of file describing an OpenDSS element object.
-
-        Parameters
-        ----------
-        filename : str
-            full path to a file (returned by list_element_info_files) or
-            an element class, like "Transformers"
-
-        Returns
-        -------
-        pd.DataFrame
-
-        """
-        if "." not in filename:
-            actual = None
-            for _file in self.list_element_info_files():
-                basename = os.path.splitext(os.path.basename(_file))[0]
-                if basename.replace("Info", "") == filename:
-                    actual = _file
-            if actual is None:
-                raise InvalidParameter(
-                    f"element info file for {filename} is not stored"
-                )
-            filename = actual
-
-        return self._fs_intf.read_csv(filename)
-
-    def read_capacitor_changes(self):
-        """Read the capacitor state changes from the OpenDSS event log.
-
-        Returns
-        -------
-        dict
-            Maps capacitor names to count of state changes.
-
-        """
-        text = self.read_file(self._metadata.get("event_log", ""))
-        return _read_capacitor_changes(text)
-
-    def read_event_log(self):
-        """Returns the event log for the scenario.
-
-        Returns
-        -------
-        list
-            list of dictionaries (one dict for each row in the file)
-
-        """
-        text = self.read_file(self._metadata.get("event_log", ""))
-        return _read_event_log(text)
-
-    def read_pv_profiles(self):
-        """Returns exported PV profiles for all PV systems.
-
-        Returns
-        -------
-        dict
-
-        """
-        return self._fs_intf.read_scenario_pv_profiles(self._name)
-
-    def _check_options(self, element_class, prop, **kwargs):
-        """Checks that kwargs are valid and returns available option names."""
-        for option in kwargs:
-            if not self._options.is_option_valid(element_class, prop, option):
-                raise InvalidParameter(
-                    f"class={element_class} property={prop} option={option} is invalid"
-                )
-
-        return self._options.list_options(element_class, prop)
-
-    def read_feeder_head_info(self):
-        """Read the feeder head information.
-
-        Returns
-        -------
-        dict
-
-        """
-        return json.loads(self.read_file(f"Exports/{self._name}/FeederHeadInfo.json"))
-
-    def read_file(self, path):
-        """Read a file from the pydss project.
-
-        Parameters
-        ----------
-        path : str
-            Path to the file relative from the project directory.
-
-        Returns
-        -------
-        str
-            Contents of the file
-
-        """
-        return self._fs_intf.read_file(path)
-
-    def _add_indices_to_dataframe(self, df):
-        indices_df = self._get_indices_df()
-        df["Timestamp"] = indices_df["Timestamp"]
-        if self._add_frequency:
-            df["Frequency"] = indices_df["Frequency"]
-        if self._add_mode:
-            df["Simulation Mode"] = indices_df["Simulation Mode"]
-        df.set_index("Timestamp", inplace=True)
-
-    def _finalize_dataframe(self, df, dataset, real_only=False, abs_val=False):
-        if df.empty:
-            return
-        dataset_property_type = get_dataset_property_type(dataset)
-        if dataset_property_type == DatasetPropertyType.FILTERED:
-            time_step_path = get_time_step_path(dataset)
-            time_step_dataset = self._hdf_store[time_step_path]
-            df["TimeStep"] = DatasetBuffer.to_datetime(time_step_dataset)
-            df.set_index("TimeStep", inplace=True)
-        else:
-            self._add_indices_to_dataframe(df)
-
-        if real_only:
-            for column in df.columns:
-                if df[column].dtype == complex:
-                    df[column] = np.real(df[column])
-        elif abs_val:
-            for column in df.columns:
-                if df[column].dtype == complex:
-                    df[column] = df[column].apply(np.absolute)
-
-    @staticmethod
-    def _fix_columns(name, columns):
-        cols = []
-        for column in columns:
-            fields = column.split(ValueStorageBase.DELIMITER)
-            fields[0] = name
-            cols.append(ValueStorageBase.DELIMITER.join(fields))
-        return cols
-
-    def _get_elem_prop_dataframe(self, elem_class, prop, name, dataset, real_only=False, abs_val=False, **kwargs):
-        col_range = self._get_element_column_range(elem_class, prop, name)
-        df = DatasetBuffer.to_dataframe(dataset, column_range=col_range)
-
-        if kwargs:
-            options = self._check_options(elem_class, prop, **kwargs)
-            columns = ValueStorageBase.get_columns(df, name, options, **kwargs)
-            df = df[columns]
-
-        self._finalize_dataframe(df, dataset, real_only=real_only, abs_val=abs_val)
-        return df
-
-    def _get_element_column_range(self, elem_class, prop, name):
-        elem_index = self._elem_indices_by_prop[elem_class][prop][name]
-        col_range = self._column_ranges_per_elem[elem_class][prop][elem_index]
-        return col_range
-
-    def _get_filtered_dataframe(self, elem_class, prop, name, dataset,
-                                real_only=False, abs_val=False, **kwargs):
-        indices_df = self._get_indices_df()
-        elem_index = self._elem_indices_by_prop[elem_class][prop][name]
-        length = dataset.attrs["length"]
-        data_vals = dataset[:length]
-
-        # The time_step_dataset has these columns:
-        # 1. time step index
-        # 2. element index
-        # Each row describes the source data in the dataset row.
-        path = dataset.attrs["time_step_path"]
-        time_step_data = self._hdf_store[path][:length]
-
-        assert length == self._hdf_store[path].attrs["length"]
-        data = []
-        timestamps = []
-        for i in range(length):
-            stored_elem_index = time_step_data[:, 1][i]
-            if stored_elem_index == elem_index:
-                ts_index = time_step_data[:, 0][i]
-                # TODO DT: more than one column?
-                val = data_vals[i, 0]
-                # TODO: profile this vs a df operation at end
-                if real_only:
-                    val = val.real
-                elif abs_val:
-                    val = abs(val)
-                data.append(val)
-                timestamps.append(indices_df.iloc[ts_index, 0])
-
-        columns = self._fix_columns(name, DatasetBuffer.get_columns(dataset))
-        return pd.DataFrame(data, columns=columns, index=timestamps)
-
-    def _get_indices_df(self):
-        if self._indices_df is None:
-            self._make_indices_df()
-        return self._indices_df
-
-    def _make_indices_df(self):
-        data = {
-            "Timestamp": make_timestamps(self._group["Timestamp"][:, 0])
-        }
-        if self._add_frequency:
-            data["Frequency"] = self._group["Frequency"][:, 0]
-        if self._add_mode:
-            data["Simulation Mode"] = self._group["Mode"][:, 0]
-        df = pd.DataFrame(data)
-        self._indices_df = df
-
-
-def _read_capacitor_changes(event_log_text):
-    """Read the capacitor state changes from an OpenDSS event log.
-
-    Parameters
-    ----------
-    event_log_text : str
-        Text of event log
-
-    Returns
-    -------
-    dict
-        Maps capacitor names to count of state changes.
-
-    """
-    capacitor_changes = {}
-    regex = re.compile(r"(Capacitor\.\w+)")
-
-    data = _read_event_log(event_log_text)
-    for row in data:
-        match = regex.search(row["Element"])
-        if match:
-            name = match.group(1)
-            if name not in capacitor_changes:
-                capacitor_changes[name] = 0
-            action = row["Action"].replace("*", "")
-            if action in ("OPENED", "CLOSED", "STEP UP"):
-                capacitor_changes[name] += 1
-
-    return capacitor_changes
-
-
-def _read_event_log(event_log_text):
-    """Return OpenDSS event log information.
-
-    Parameters
-    ----------
-    event_log_text : str
-        Text of event log
-
-
-    Returns
-    -------
-    list
-        list of dictionaries (one dict for each row in the file)
-
-    """
-    data = []
-    if not event_log_text:
-        return data
-
-    for line in event_log_text.split("\n"):
-        if line == "":
-            continue
-        tokens = [x.strip() for x in line.split(",")]
-        row = {}
-        for token in tokens:
-            name_and_value = [x.strip() for x in token.split("=")]
-            name = name_and_value[0]
-            value = name_and_value[1]
-            row[name] = value
-        data.append(row)
-
-    return data
+"""Provides access to pydss result data."""
+from collections import defaultdict
+import json
+import os
+import re
+
+import h5py
+import numpy as np
+import pandas as pd
+from loguru import logger
+
+from pydss.common import  DatasetPropertyType
+from pydss.dataset_buffer import DatasetBuffer
+from pydss.element_options import ElementOptions
+from pydss.exceptions import InvalidParameter
+from pydss.pydss_project import PyDssProject, RUN_SIMULATION_FILENAME
+from pydss.reports.reports import Reports, REPORTS_DIR
+from pydss.utils.dataframe_utils import read_dataframe, write_dataframe
+from pydss.utils.utils import dump_data, load_data, make_json_serializable, \
+    make_timestamps
+from pydss.value_storage import ValueStorageBase, get_dataset_property_type, \
+    get_time_step_path
+
+class PyDssResults:
+    """Interface to perform analysis on pydss output data."""
+    def __init__(
+            self, project_path=None, project=None, in_memory=False,
+            frequency=False, mode=False
+        ):
+        """Constructs PyDssResults object.
+
+        Parameters
+        ----------
+        project_path : str | None
+            Load project from files in path
+        project : PyDssProject | None
+            Existing project object
+        in_memory : bool
+            If true, load all exported data into memory.
+        frequency : bool
+            If true, add frequency column to all dataframes.
+        mode : bool
+            If true, add mode column to all dataframes.
+
+        """
+        options = ElementOptions()
+        if project_path is not None:
+            # TODO: handle old version?
+            self._project = PyDssProject.load_project(
+                project_path,
+                simulation_file=RUN_SIMULATION_FILENAME,
+            )
+        elif project is None:
+            raise InvalidParameter("project_path or project must be set")
+        else:
+            self._project = project
+        self._fs_intf = self._project.fs_interface
+        self._scenarios = []
+        filename = self._project.get_hdf_store_filename()
+        driver = "core" if in_memory else None
+        self._hdf_store = h5py.File(filename, "r", driver=driver)
+
+        if self._project.simulation_config.exports.export_results:
+            for name in self._project.list_scenario_names():
+                metadata = self._project.read_scenario_export_metadata(name)
+                scenario_result = PyDssScenarioResults(
+                    name,
+                    self.project_path,
+                    self._hdf_store,
+                    self._fs_intf,
+                    metadata,
+                    options,
+                    frequency=frequency,
+                    mode=mode,
+                )
+                self._scenarios.append(scenario_result)
+
+    def __del__(self):
+        if hasattr(self, "_hdf_store"):
+            self._hdf_store.flush()
+            self._hdf_store.close()
+            logger.info("store closed sucessfully")
+    
+    def generate_reports(self):
+        """Generate all reports specified in the configuration.
+
+        Returns
+        -------
+        list
+            list of report filenames
+
+        """
+        return Reports.generate_reports(self)
+
+    def read_report(self, report_name):
+        """Return the report data.
+
+        Parameters
+        ----------
+        report_name : str
+
+        Returns
+        -------
+        str
+
+        """
+        all_reports = Reports.get_all_reports()
+        if report_name not in all_reports:
+            raise InvalidParameter(f"invalid report name {report_name}")
+        report_cls = all_reports[report_name]
+
+        # This bypasses self._fs_intf because reports are always extracted.
+        reports_dir = os.path.join(self._project.project_path, REPORTS_DIR)
+        for filename in os.listdir(reports_dir):
+            name, ext = os.path.splitext(filename)
+            if name == os.path.splitext(report_cls.FILENAME)[0]:
+                path = os.path.join(reports_dir, filename)
+                if ext in (".json", ".toml"):
+                    return load_data(path)
+                if ext in (".csv", ".h5"):
+                    return read_dataframe(path)
+
+        raise InvalidParameter(f"did not find report {report_name} in {reports_dir}")
+
+    @property
+    def project(self):
+        """Return the PyDssProject instance.
+
+        Returns
+        -------
+        PyDssProject
+
+        """
+        return self._project
+
+    @property
+    def scenarios(self):
+        """Return the PyDssScenarioResults instances for the project.
+
+        Returns
+        -------
+        list
+            list of PyDssScenarioResults
+
+        """
+        return self._scenarios
+
+    def get_scenario(self, name):
+        """Return the PyDssScenarioResults object for scenario with name.
+
+        Parameters
+        ----------
+        name : str
+            Scenario name
+
+        Results
+        -------
+        PyDssScenarioResults
+
+        Raises
+        ------
+        InvalidParameter
+            Raised if the scenario does not exist.
+
+        """
+        for scenario in self._scenarios:
+            if name == scenario.name:
+                return scenario
+
+        raise InvalidParameter(f"scenario {name} does not exist")
+
+    @property
+    def hdf_store(self):
+        """Return a handle to the HDF data store.
+
+        Returns
+        -------
+        h5py.File
+
+        """
+        return self._hdf_store
+
+    @property
+    def project_path(self):
+        """Return the path to the pydss project.
+
+        Returns
+        -------
+        str
+
+        """
+        return self._project.project_path
+
+    def read_file(self, path):
+        """Read a file from the pydss project.
+
+        Parameters
+        ----------
+        path : str
+            Path to the file relative from the project directory.
+
+        Returns
+        -------
+        str
+            Contents of the file
+
+        """
+        return self._fs_intf.read_file(path)
+
+    @property
+    def simulation_config(self):
+        """Return the simulation configuration
+
+        Returns
+        -------
+        dict
+
+        """
+        return self._project.simulation_config
+
+
+class PyDssScenarioResults:
+    """Contains results for one scenario."""
+    def __init__(
+            self, name, project_path, store, fs_intf, metadata, options,
+            frequency=False, mode=False
+        ):
+        self._name = name
+        self._project_path = project_path
+        self._hdf_store = store
+        self._metadata = metadata or {}
+        self._options = options
+        self._fs_intf = fs_intf
+        self._elems_by_class = defaultdict(set)
+        self._elem_data_by_prop = defaultdict(dict)
+        self._elem_values_by_prop = defaultdict(dict)
+        self._elem_indices_by_prop = defaultdict(dict)
+        self._props_by_class = defaultdict(list)
+        self._elem_props = defaultdict(list)
+        self._column_ranges_per_elem = defaultdict(dict)
+        self._summed_elem_props = defaultdict(dict)
+        self._summed_elem_timeseries_props = defaultdict(list)
+        self._indices_df = None
+        self._add_frequency = frequency
+        self._add_mode = mode
+        self._data_format_version = self._hdf_store.attrs["version"]
+        if name not in self._hdf_store["Exports"]:
+            self._group = None
+            return
+
+        self._group = self._hdf_store[f"Exports/{name}"]
+        self._elem_classes = [
+            x for x in self._group if isinstance(self._group[x], h5py.Group)
+        ]
+
+        self._parse_datasets()
+
+    def _parse_datasets(self):
+        for elem_class in self._elem_classes:
+            class_group = self._group[elem_class]
+            if "ElementProperties" in class_group:
+                prop_group = class_group["ElementProperties"]
+                for prop, dataset in prop_group.items():
+                    dataset_property_type = get_dataset_property_type(dataset)
+                    if dataset_property_type == DatasetPropertyType.TIME_STEP:
+                        continue
+                    if dataset_property_type == DatasetPropertyType.VALUE:
+                        self._elem_values_by_prop[elem_class][prop] = []
+                        prop_names = self._elem_values_by_prop
+                    elif dataset_property_type in (
+                            DatasetPropertyType.PER_TIME_POINT,
+                            DatasetPropertyType.FILTERED,
+                    ):
+                        self._elem_data_by_prop[elem_class][prop] = []
+                        prop_names = self._elem_data_by_prop
+                    else:
+                        continue
+
+                    self._props_by_class[elem_class].append(prop)
+                    self._elem_indices_by_prop[elem_class][prop] = {}
+                    names = DatasetBuffer.get_names(dataset)
+                    self._column_ranges_per_elem[elem_class][prop] = \
+                        DatasetBuffer.get_column_ranges(dataset)
+                    for i, name in enumerate(names):
+                        self._elems_by_class[elem_class].add(name)
+                        prop_names[elem_class][prop].append(name)
+                        self._elem_indices_by_prop[elem_class][prop][name] = i
+                        self._elem_props[name].append(prop)
+            else:
+                self._elems_by_class[elem_class] = set()
+
+            summed_elem_props = self._group[elem_class].get("SummedElementProperties", [])
+            for prop in summed_elem_props:
+                dataset = self._group[elem_class]["SummedElementProperties"][prop]
+                dataset_property_type = get_dataset_property_type(dataset)
+                if dataset_property_type == DatasetPropertyType.VALUE:
+                    df = DatasetBuffer.to_dataframe(dataset)
+                    assert len(df) == 1
+                    self._summed_elem_props[elem_class][prop] = {
+                        x: df[x].values[0] for x in df.columns
+                    }
+                elif dataset_property_type == DatasetPropertyType.PER_TIME_POINT:
+                    self._summed_elem_timeseries_props[elem_class].append(prop)
+
+    @staticmethod
+    def get_name_from_column(column):
+        """Return the element name from the dataframe column. The dataframe should have been
+        returned from this class.
+
+        Parameters
+        ----------
+        column : str
+
+        Returns
+        -------
+        str
+
+        """
+        fields = column.split(ValueStorageBase.DELIMITER)
+        assert len(fields) > 1
+        return fields[0]
+
+    @property
+    def name(self):
+        """Return the name of the scenario.
+
+        Returns
+        -------
+        str
+
+        """
+        return self._name
+
+    def export_data(self, path=None, fmt="csv", compress=False):
+        """Export data to path.
+
+        Parameters
+        ----------
+        path : str
+            Output directory; defaults to scenario exports path
+        fmt : str
+            Filer format type (csv, h5)
+        compress : bool
+            Compress data
+
+        """
+        if path is None:
+            path = os.path.join(self._project_path, "Exports", self._name)
+        os.makedirs(path, exist_ok=True)
+        self._export_element_timeseries(path, fmt, compress)
+        self._export_element_values(path, fmt, compress)
+        self._export_summed_element_timeseries(path, fmt, compress)
+        self._export_summed_element_values(path, fmt, compress)
+
+    def _export_element_timeseries(self, path, fmt, compress):
+        for elem_class in self.list_element_classes():
+            for prop in self.list_element_properties(elem_class):
+                dataset = self._group[f"{elem_class}/ElementProperties/{prop}"]
+                prop_type = get_dataset_property_type(dataset)
+                if prop_type == DatasetPropertyType.FILTERED:
+                    self._export_filtered_dataframes(elem_class, prop, path, fmt, compress)
+                else:
+                    df = self.get_full_dataframe(elem_class, prop)
+                    base = "__".join([elem_class, prop])
+                    filename = os.path.join(path, base + "." + fmt.replace(".", ""))
+                    write_dataframe(df, filename, compress=compress)
+
+    def _export_element_values(self, path, fmt, compress):
+        elem_prop_nums = defaultdict(dict)
+        for elem_class in self._elem_values_by_prop:
+            for prop in self._elem_values_by_prop[elem_class]:
+                dataset = self._group[f"{elem_class}/ElementProperties/{prop}"]
+                for name in self._elem_values_by_prop[elem_class][prop]:
+                    col_range = self._get_element_column_range(elem_class, prop, name)
+                    start = col_range[0]
+                    length = col_range[1]
+                    if length == 1:
+                        val = dataset[:][0][start]
+                    else:
+                        val = dataset[:][0][start: start + length]
+                    if prop not in elem_prop_nums[elem_class]:
+                        elem_prop_nums[elem_class][prop] = {}
+                    elem_prop_nums[elem_class][prop][name] = val
+        if elem_prop_nums:
+            filename = os.path.join(path, "element_property_values.json")
+            dump_data(elem_prop_nums, filename, indent=2, default=make_json_serializable)
+
+        logger.info("Exported data to %s", path)
+
+    def _export_filtered_dataframes(self, elem_class, prop, path, fmt, compress):
+        for name, df in self.get_filtered_dataframes(elem_class, prop).items():
+            if df.empty:
+                logger.debug("Skip empty dataframe %s %s %s", elem_class, prop, name)
+                continue
+            base = "__".join([elem_class, prop, name])
+            filename = os.path.join(path, base + "." + fmt.replace(".", ""))
+            write_dataframe(df, filename, compress=compress)
+
+    def _export_summed_element_timeseries(self, path, fmt, compress):
+        for elem_class in self._summed_elem_timeseries_props:
+            for prop in self._summed_elem_timeseries_props[elem_class]:
+                fields = prop.split(ValueStorageBase.DELIMITER)
+                if len(fields) == 1:
+                    base = ValueStorageBase.DELIMITER.join([elem_class, prop])
+                else:
+                    assert len(fields) == 2, fields
+                    # This will be <elem_class>__<prop>__<group>
+                    base = ValueStorageBase.DELIMITER.join([elem_class, prop])
+                filename = os.path.join(path, base + "." + fmt.replace(".", ""))
+                dataset = self._group[elem_class]["SummedElementProperties"][prop]
+                prop_type = get_dataset_property_type(dataset)
+                if prop_type == DatasetPropertyType.PER_TIME_POINT:
+                    df = DatasetBuffer.to_dataframe(dataset)
+                    self._finalize_dataframe(df, dataset)
+                    write_dataframe(df, filename, compress=compress)
+
+    def _export_summed_element_values(self, path, fmt, compress):
+        filename = os.path.join(path, "summed_element_property_values.json")
+        dump_data(self._summed_elem_props, filename, default=make_json_serializable)
+
+    def get_dataframe(self, element_class, prop, element_name, real_only=False, abs_val=False, **kwargs):
+        """Return the dataframe for an element.
+
+        Parameters
+        ----------
+        element_class : str
+        prop : str
+        element_name : str
+        real_only : bool
+            If dtype of any column is complex, drop the imaginary component.
+        abs_val : bool
+            If dtype of any column is complex, compute its absolute value.
+        kwargs
+            Filter on options; values can be strings or regular expressions.
+
+        Returns
+        -------
+        pd.DataFrame
+
+        Raises
+        ------
+        InvalidParameter
+            Raised if the element is not stored.
+
+        """
+        if element_name not in self._elem_props:
+            raise InvalidParameter(f"element {element_name} is not stored")
+
+        dataset = self._group[f"{element_class}/ElementProperties/{prop}"]
+        prop_type = get_dataset_property_type(dataset)
+        if prop_type == DatasetPropertyType.PER_TIME_POINT:
+            return self._get_elem_prop_dataframe(
+                element_class, prop, element_name, dataset, real_only=real_only,
+                abs_val=abs_val, **kwargs
+            )
+        elif prop_type == DatasetPropertyType.FILTERED:
+            return self._get_filtered_dataframe(
+                element_class, prop, element_name, dataset, real_only=real_only,
+                abs_val=abs_val, **kwargs
+            )
+        assert False, str(prop_type)
+
+    def get_filtered_dataframes(self, element_class, prop, real_only=False, abs_val=False):
+        """Return the dataframes for all elements.
+
+        Calling this is much more efficient than calling get_dataframe for each
+        element.
+
+        Parameters
+        ----------
+        element_class : str
+        prop : str
+        element_name : str
+        real_only : bool
+            If dtype of any column is complex, drop the imaginary component.
+        abs_val : bool
+            If dtype of any column is complex, compute its absolute value.
+
+        Returns
+        -------
+        dict
+            key = str (name), val = pd.DataFrame
+            The dict will be empty if no data was stored.
+
+        """
+        if prop not in self.list_element_properties(element_class):
+            logger.debug("%s/%s is not stored", element_class, prop)
+            return {}
+
+        dataset = self._group[f"{element_class}/ElementProperties/{prop}"]
+        columns = DatasetBuffer.get_columns(dataset)
+        names = DatasetBuffer.get_names(dataset)
+        length = dataset.attrs["length"]
+        indices_df = self._get_indices_df()
+        data_vals = dataset[:length]
+        elem_data = defaultdict(list)
+        elem_timestamps = defaultdict(list)
+
+        # The time_step_dataset has these columns:
+        # 1. time step index
+        # 2. element index
+        # Each row describes the source data in the dataset row.
+        path = dataset.attrs["time_step_path"]
+        assert length == self._hdf_store[path].attrs["length"]
+        time_step_data = self._hdf_store[path][:length]
+
+        for i in range(length):
+            ts_index = time_step_data[:, 0][i]
+            elem_index = time_step_data[:, 1][i]
+            # TODO DT: more than one column?
+            val = data_vals[i, 0]
+            if real_only:
+                val = val.real
+            elif abs_val:
+                val = abs(val)
+            elem_data[elem_index].append(val)
+            elem_timestamps[elem_index].append(indices_df.iloc[ts_index, 0])
+
+        dfs = {}
+        for elem_index, vals in elem_data.items():
+            elem_name = names[elem_index]
+            cols = self._fix_columns(elem_name, columns)
+            dfs[elem_name] = pd.DataFrame(
+                vals,
+                columns=cols,
+                index=elem_timestamps[elem_index],
+            )
+        return dfs
+
+    def get_full_dataframe(self, element_class, prop, real_only=False, abs_val=False, **kwargs):
+        """Return a dataframe containing all data.  The dataframe is copied.
+
+        Parameters
+        ----------
+        element_class : str
+        prop : str
+        real_only : bool
+            If dtype of any column is complex, drop the imaginary component.
+        abs_val : bool
+            If dtype of any column is complex, compute its absolute value.
+        kwargs
+            Filter on options; values can be strings or regular expressions.
+
+        Returns
+        -------
+        pd.DataFrame
+
+        """
+        if prop not in self.list_element_properties(element_class):
+            raise InvalidParameter(f"property {prop} is not stored")
+
+        dataset = self._group[f"{element_class}/ElementProperties/{prop}"]
+        df = DatasetBuffer.to_dataframe(dataset)
+        if kwargs:
+            options = self._check_options(element_class, prop, **kwargs)
+            names = self._elems_by_class.get(element_class, set())
+            columns = ValueStorageBase.get_columns(df, names, options, **kwargs)
+            columns = list(columns)
+            columns.sort()
+            df = df[columns]
+        self._finalize_dataframe(df, dataset, real_only=real_only, abs_val=abs_val)
+        return df
+
+    def get_summed_element_total(self, element_class, prop, group=None):
+        """Return the total value for a summed element property.
+
+        Parameters
+        ----------
+        element_class : str
+        prop : str
+        group : str | None
+            Specify a group name if sum_groups was assigned.
+
+        Returns
+        -------
+        dict
+
+        Raises
+        ------
+        InvalidParameter
+            Raised if the element class is not stored.
+
+        """
+        if group is not None:
+            prop = ValueStorageBase.DELIMITER.join((prop, group))
+        if element_class not in self._summed_elem_props:
+            raise InvalidParameter(f"{element_class} is not stored")
+        if prop not in self._summed_elem_props[element_class]:
+            raise InvalidParameter(f"{prop} is not stored")
+
+        return self._summed_elem_props[element_class][prop]
+
+    def get_element_property_value(self, element_class, prop, element_name):
+        """Return the number stored for the element property."""
+        if element_class not in self._elem_values_by_prop:
+            raise InvalidParameter(f"{element_class} is not stored")
+        if prop not in self._elem_values_by_prop[element_class]:
+            raise InvalidParameter(f"{prop} is not stored")
+        if element_name not in self._elem_values_by_prop[element_class][prop]:
+            raise InvalidParameter(f"{element_name} is not stored")
+        dataset = self._group[f"{element_class}/ElementProperties/{prop}"]
+        col_range = self._get_element_column_range(element_class, prop, element_name)
+        start = col_range[0]
+        length = col_range[1]
+        if length == 1:
+            return dataset[:][0][start]
+        return dataset[:][0][start: start + length]
+
+    def get_option_values(self, element_class, prop, element_name):
+        """Return the option values for the element property.
+
+        element_class : str
+        prop : str
+        element_name : str
+
+        Returns
+        -------
+        list
+
+        """
+        df = self.get_dataframe(element_class, prop, element_name)
+        return ValueStorageBase.get_option_values(df, element_name)
+
+    def get_summed_element_dataframe(self, element_class, prop, real_only=False, abs_val=False, group=None):
+        """Return the dataframe for a summed element property.
+
+        Parameters
+        ----------
+        element_class : str
+        prop : str
+        group : str | None
+            Specify a group name if sum_groups was assigned.
+        real_only : bool
+            If dtype of any column is complex, drop the imaginary component.
+        abs_val : bool
+            If dtype of any column is complex, compute its absolute value.
+
+        Returns
+        -------
+        pd.DataFrame
+
+        Raises
+        ------
+        InvalidParameter
+            Raised if the element class is not stored.
+
+        """
+        if group is not None:
+            prop = ValueStorageBase.DELIMITER.join((prop, group))
+        if element_class not in self._summed_elem_timeseries_props:
+            raise InvalidParameter(f"{element_class} is not stored")
+        if prop not in self._summed_elem_timeseries_props[element_class]:
+            raise InvalidParameter(f"{prop} is not stored")
+
+        elem_group = self._group[element_class]["SummedElementProperties"]
+        dataset = elem_group[prop]
+        df = DatasetBuffer.to_dataframe(dataset)
+        self._add_indices_to_dataframe(df)
+
+        if real_only:
+            for column in df.columns:
+                if df[column].dtype == complex:
+                    df[column] = np.real(df[column])
+        elif abs_val:
+            for column in df.columns:
+                if df[column].dtype == complex:
+                    df[column] = df[column].apply(np.absolute)
+
+        return df
+
+    def get_timestamps(self):
+        """Return the timestamps of the simulation in a pandas.Series."""
+        return self._get_indices_df()["Timestamp"]
+
+    def iterate_dataframes(self, element_class, prop, real_only=False, abs_val=False, **kwargs):
+        """Returns a generator over the dataframes by element name.
+
+        Parameters
+        ----------
+        element_class : str
+        prop : str
+        real_only : bool
+            If dtype of any column is complex, drop the imaginary component.
+        abs_val : bool
+            If dtype of any column is complex, compute its absolute value.
+        kwargs : dict
+            Filter on options; values can be strings or regular expressions.
+
+        Returns
+        -------
+        tuple
+            Tuple containing the name or property and a pd.DataFrame
+
+        """
+        for name in self.list_element_names(element_class):
+            if prop in self._elem_props[name]:
+                df = self.get_dataframe(
+                    element_class, prop, name, real_only=real_only, abs_val=abs_val, **kwargs
+                )
+                yield name, df
+
+    def iterate_element_property_values(self):
+        """Return a generator over all element properties stored as values.
+
+        Yields
+        ------
+        tuple
+            element_class, property, element_name, value
+
+        """
+        for elem_class in self._elem_values_by_prop:
+            for prop in self._elem_values_by_prop[elem_class]:
+                for name in self._elem_values_by_prop[elem_class][prop]:
+                    val = self.get_element_property_value(elem_class, prop, name)
+                    yield elem_class, prop, name, val
+
+    def list_element_classes(self):
+        """Return the element classes stored in the results.
+
+        Returns
+        -------
+        list
+
+        """
+        return self._elem_classes[:]
+
+    def list_element_names(self, element_class, prop=None):
+        """Return the element names for a property stored in the results.
+
+        Parameters
+        ----------
+        element_class : str
+        prop : str
+
+        Returns
+        -------
+        list
+
+        """
+        # TODO: prop is deprecated
+        return sorted(list(self._elems_by_class.get(element_class, [])))
+
+    def list_element_properties(self, element_class, element_name=None):
+        """Return the properties stored in the results for a class.
+
+        Parameters
+        ----------
+        element_class : str
+        element_name : str | None
+            If not None, list properties only for that name.
+
+        Returns
+        -------
+        list
+
+        """
+        if element_class not in self._props_by_class:
+            return []
+        if element_name is None:
+            return sorted(list(self._props_by_class[element_class]))
+        return self._elem_props.get(element_name, [])
+
+    def list_element_value_names(self, element_class, prop):
+        if element_class not in self._elem_values_by_prop:
+            raise InvalidParameter(f"{element_class} is not stored")
+        if prop not in self._elem_values_by_prop[element_class]:
+            raise InvalidParameter(f"{element_class} / {prop} is not stored")
+        return sorted(self._elem_values_by_prop[element_class][prop])
+
+    def list_element_property_values(self, element_name):
+        nums = []
+        for elem_class in self._elem_prop_nums:
+            for prop in self._elem_prop_nums[elem_class]:
+                for name in self._elem_prop_nums[elem_class][prop]:
+                    if name == element_name:
+                        nums.append(prop)
+        return nums
+
+    def list_element_property_options(self, element_class, prop):
+        """List the possible options for the element class and property.
+
+        Parameters
+        ----------
+        element_class : str
+        prop : str
+
+        Returns
+        -------
+        list
+
+        """
+        return self._options.list_options(element_class, prop)
+
+    def list_element_info_files(self):
+        """Return the files describing the OpenDSS element objects.
+
+        Returns
+        -------
+        list
+            list of filenames (str)
+
+        """
+        return self._metadata.get("element_info_files", [])
+
+    def list_summed_element_properties(self, element_class):
+        """Return the properties stored for a class where the values are a sum
+        of all elements.
+
+        Parameters
+        ----------
+        element_class : str
+
+        Returns
+        -------
+        list
+
+        Raises
+        ------
+        InvalidParameter
+            Raised if the element_class is not stored.
+
+        """
+        if element_class not in self._summed_elem_props:
+            raise InvalidParameter(f"class={element_class} is not stored")
+        return self._summed_elem_props[element_class]
+
+    def list_summed_element_time_series_properties(self, element_class):
+        """Return the properties stored for a class where the values are a sum
+        of all elements.
+
+        Parameters
+        ----------
+        element_class : str
+
+        Returns
+        -------
+        list
+
+        Raises
+        ------
+        InvalidParameter
+            Raised if the element_class is not stored.
+
+        """
+        if element_class not in self._summed_elem_timeseries_props:
+            raise InvalidParameter(f"class={element_class} is not stored")
+        return self._summed_elem_timeseries_props[element_class]
+
+    def read_element_info_file(self, filename):
+        """Return the contents of file describing an OpenDSS element object.
+
+        Parameters
+        ----------
+        filename : str
+            full path to a file (returned by list_element_info_files) or
+            an element class, like "Transformers"
+
+        Returns
+        -------
+        pd.DataFrame
+
+        """
+        if "." not in filename:
+            actual = None
+            for _file in self.list_element_info_files():
+                basename = os.path.splitext(os.path.basename(_file))[0]
+                if basename.replace("Info", "") == filename:
+                    actual = _file
+            if actual is None:
+                raise InvalidParameter(
+                    f"element info file for {filename} is not stored"
+                )
+            filename = actual
+
+        return self._fs_intf.read_csv(filename)
+
+    def read_capacitor_changes(self):
+        """Read the capacitor state changes from the OpenDSS event log.
+
+        Returns
+        -------
+        dict
+            Maps capacitor names to count of state changes.
+
+        """
+        text = self.read_file(self._metadata.get("event_log", ""))
+        return _read_capacitor_changes(text)
+
+    def read_event_log(self):
+        """Returns the event log for the scenario.
+
+        Returns
+        -------
+        list
+            list of dictionaries (one dict for each row in the file)
+
+        """
+        text = self.read_file(self._metadata.get("event_log", ""))
+        return _read_event_log(text)
+
+    def read_pv_profiles(self):
+        """Returns exported PV profiles for all PV systems.
+
+        Returns
+        -------
+        dict
+
+        """
+        return self._fs_intf.read_scenario_pv_profiles(self._name)
+
+    def _check_options(self, element_class, prop, **kwargs):
+        """Checks that kwargs are valid and returns available option names."""
+        for option in kwargs:
+            if not self._options.is_option_valid(element_class, prop, option):
+                raise InvalidParameter(
+                    f"class={element_class} property={prop} option={option} is invalid"
+                )
+
+        return self._options.list_options(element_class, prop)
+
+    def read_feeder_head_info(self):
+        """Read the feeder head information.
+
+        Returns
+        -------
+        dict
+
+        """
+        return json.loads(self.read_file(f"Exports/{self._name}/FeederHeadInfo.json"))
+
+    def read_file(self, path):
+        """Read a file from the pydss project.
+
+        Parameters
+        ----------
+        path : str
+            Path to the file relative from the project directory.
+
+        Returns
+        -------
+        str
+            Contents of the file
+
+        """
+        return self._fs_intf.read_file(path)
+
+    def _add_indices_to_dataframe(self, df):
+        indices_df = self._get_indices_df()
+        df["Timestamp"] = indices_df["Timestamp"]
+        if self._add_frequency:
+            df["Frequency"] = indices_df["Frequency"]
+        if self._add_mode:
+            df["Simulation Mode"] = indices_df["Simulation Mode"]
+        df.set_index("Timestamp", inplace=True)
+
+    def _finalize_dataframe(self, df, dataset, real_only=False, abs_val=False):
+        if df.empty:
+            return
+        dataset_property_type = get_dataset_property_type(dataset)
+        if dataset_property_type == DatasetPropertyType.FILTERED:
+            time_step_path = get_time_step_path(dataset)
+            time_step_dataset = self._hdf_store[time_step_path]
+            df["TimeStep"] = DatasetBuffer.to_datetime(time_step_dataset)
+            df.set_index("TimeStep", inplace=True)
+        else:
+            self._add_indices_to_dataframe(df)
+
+        if real_only:
+            for column in df.columns:
+                if df[column].dtype == complex:
+                    df[column] = np.real(df[column])
+        elif abs_val:
+            for column in df.columns:
+                if df[column].dtype == complex:
+                    df[column] = df[column].apply(np.absolute)
+
+    @staticmethod
+    def _fix_columns(name, columns):
+        cols = []
+        for column in columns:
+            fields = column.split(ValueStorageBase.DELIMITER)
+            fields[0] = name
+            cols.append(ValueStorageBase.DELIMITER.join(fields))
+        return cols
+
+    def _get_elem_prop_dataframe(self, elem_class, prop, name, dataset, real_only=False, abs_val=False, **kwargs):
+        col_range = self._get_element_column_range(elem_class, prop, name)
+        df = DatasetBuffer.to_dataframe(dataset, column_range=col_range)
+
+        if kwargs:
+            options = self._check_options(elem_class, prop, **kwargs)
+            columns = ValueStorageBase.get_columns(df, name, options, **kwargs)
+            df = df[columns]
+
+        self._finalize_dataframe(df, dataset, real_only=real_only, abs_val=abs_val)
+        return df
+
+    def _get_element_column_range(self, elem_class, prop, name):
+        elem_index = self._elem_indices_by_prop[elem_class][prop][name]
+        col_range = self._column_ranges_per_elem[elem_class][prop][elem_index]
+        return col_range
+
+    def _get_filtered_dataframe(self, elem_class, prop, name, dataset,
+                                real_only=False, abs_val=False, **kwargs):
+        indices_df = self._get_indices_df()
+        elem_index = self._elem_indices_by_prop[elem_class][prop][name]
+        length = dataset.attrs["length"]
+        data_vals = dataset[:length]
+
+        # The time_step_dataset has these columns:
+        # 1. time step index
+        # 2. element index
+        # Each row describes the source data in the dataset row.
+        path = dataset.attrs["time_step_path"]
+        time_step_data = self._hdf_store[path][:length]
+
+        assert length == self._hdf_store[path].attrs["length"]
+        data = []
+        timestamps = []
+        for i in range(length):
+            stored_elem_index = time_step_data[:, 1][i]
+            if stored_elem_index == elem_index:
+                ts_index = time_step_data[:, 0][i]
+                # TODO DT: more than one column?
+                val = data_vals[i, 0]
+                # TODO: profile this vs a df operation at end
+                if real_only:
+                    val = val.real
+                elif abs_val:
+                    val = abs(val)
+                data.append(val)
+                timestamps.append(indices_df.iloc[ts_index, 0])
+
+        columns = self._fix_columns(name, DatasetBuffer.get_columns(dataset))
+        return pd.DataFrame(data, columns=columns, index=timestamps)
+
+    def _get_indices_df(self):
+        if self._indices_df is None:
+            self._make_indices_df()
+        return self._indices_df
+
+    def _make_indices_df(self):
+        data = {
+            "Timestamp": make_timestamps(self._group["Timestamp"][:, 0])
+        }
+        if self._add_frequency:
+            data["Frequency"] = self._group["Frequency"][:, 0]
+        if self._add_mode:
+            data["Simulation Mode"] = self._group["Mode"][:, 0]
+        df = pd.DataFrame(data)
+        self._indices_df = df
+
+
+def _read_capacitor_changes(event_log_text):
+    """Read the capacitor state changes from an OpenDSS event log.
+
+    Parameters
+    ----------
+    event_log_text : str
+        Text of event log
+
+    Returns
+    -------
+    dict
+        Maps capacitor names to count of state changes.
+
+    """
+    capacitor_changes = {}
+    regex = re.compile(r"(Capacitor\.\w+)")
+
+    data = _read_event_log(event_log_text)
+    for row in data:
+        match = regex.search(row["Element"])
+        if match:
+            name = match.group(1)
+            if name not in capacitor_changes:
+                capacitor_changes[name] = 0
+            action = row["Action"].replace("*", "")
+            if action in ("OPENED", "CLOSED", "STEP UP"):
+                capacitor_changes[name] += 1
+
+    return capacitor_changes
+
+
+def _read_event_log(event_log_text):
+    """Return OpenDSS event log information.
+
+    Parameters
+    ----------
+    event_log_text : str
+        Text of event log
+
+
+    Returns
+    -------
+    list
+        list of dictionaries (one dict for each row in the file)
+
+    """
+    data = []
+    if not event_log_text:
+        return data
+
+    for line in event_log_text.split("\n"):
+        if line == "":
+            continue
+        tokens = [x.strip() for x in line.split(",")]
+        row = {}
+        for token in tokens:
+            name_and_value = [x.strip() for x in token.split("=")]
+            name = name_and_value[0]
+            value = name_and_value[1]
+            row[name] = value
+        data.append(row)
+
+    return data
```

### Comparing `nrel_pydss-3.1.3/src/pydss/storage_filters.py` & `nrel_pydss-3.1.4/src/pydss/storage_filters.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,259 +1,259 @@
-
-import copy
-import abc
-
-from loguru import logger
-import numpy as np
-
-from pydss.utils.simulation_utils import CircularBufferHelper
-from pydss.value_storage import ValueContainer
-from pydss.common import StoreValuesType
-
-
-class StorageFilterBase(abc.ABC):
-    """Base class for storage containers.
-    Subclasses can perform custom filtering based on StoreValuesType.
-
-    """
-    def __init__(self, hdf_store, path, prop, num_steps, max_chunk_bytes, values, elem_names, **kwargs):
-        self._prop = prop
-        self._container = self.make_container(
-            hdf_store,
-            path,
-            prop,
-            num_steps,
-            max_chunk_bytes,
-            values,
-            elem_names,
-        )
-        logger.debug("Created %s path=%s", self.__class__.__name__, path)
-
-    @abc.abstractmethod
-    def append_values(self, values, time_step):
-        """Store a new set of values for each element."""
-
-    def close(self):
-        """Perform any final writes to the container."""
-        self.flush_data()
-
-    def flush_data(self):
-        """Flush data to disk."""
-        self._container.flush_data()
-
-    def max_num_bytes(self):
-        """Return the maximum number of bytes the container could hold.
-
-        Returns
-        -------
-        int
-
-        """
-        return self._container.max_num_bytes()
-
-    @staticmethod
-    def make_container(hdf_store, path, prop, num_steps, max_chunk_bytes, values, elem_names):
-        """Return an instance of ValueContainer for storing values."""
-        container = ValueContainer(
-            values,
-            hdf_store,
-            path,
-            prop.get_max_size(num_steps),
-            elem_names,
-            prop.get_dataset_property_type(),
-            max_chunk_bytes=max_chunk_bytes,
-            store_time_step=prop.should_store_time_step(),
-        )
-        logger.debug("Created storage container path=%s", path)
-        return container
-
-
-class StorageAll(StorageFilterBase):
-    """Store values at every time point, optionally filtered."""
-
-    def append_values(self, values, time_step):
-        if self._prop.limits:
-            for i, value in enumerate(values):
-                if value.is_nan():
-                    break
-                if self._prop.should_store_value(value.value):
-                    self._container.append_by_time_step(value, time_step, i)
-        else:
-            self._container.append(values)
-
-
-"""
-class StorageChangeCount(StorageFilterBase):
-    def __init__(self, *args):
-        super().__init__(*args)
-        self._last_value = None
-        self._change_count = (None, 0)
-
-    def append_values(self, values, time_step):
-        assert False
-"""
-
-
-class StorageMin(StorageFilterBase):
-    """Stores the min value across time points."""
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._min = None
-
-    def append_values(self, values, time_step):
-        if values[0].is_nan():
-            return
-        self._handle_values(values)
-
-    def close(self):
-        if self._min is not None:
-            self._container.append(self._min)
-            self._container.flush_data()
-
-    def _handle_values(self, values):
-        if self._min is None:
-            self._min = [copy.deepcopy(x) for x in values]
-        else:
-            for i, new_val in enumerate(values):
-                cur_val = self._min[i]
-                if (np.isnan(cur_val.value) and not np.isnan(new_val.value)) or \
-                        new_val < cur_val:
-                    self._min[i].set_value(new_val.value)
-
-
-class StorageMax(StorageFilterBase):
-    """Stores the max value across time points."""
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._max = None
-
-    def append_values(self, values, time_step):
-        if values[0].is_nan():
-            return
-        self._handle_values(values)
-
-    def close(self):
-        if self._max is not None:
-            self._container.append(self._max)
-            self._container.flush_data()
-
-    def _handle_values(self, values):
-        if self._max is None:
-            self._max = [copy.deepcopy(x) for x in values]
-        else:
-            for i, new_val in enumerate(values):
-                cur_val = self._max[i]
-                if (np.isnan(cur_val.value) and not np.isnan(new_val.value)) or \
-                        new_val > cur_val:
-                    self._max[i].set_value(new_val.value)
-
-
-class StorageMovingAverage(StorageFilterBase):
-    """Stores a moving average across time points."""
-    def __init__(self, *args, **kwargs):
-        """Constructor for StorageMovingAverage.
-
-        window_size comes from either the passed `prop` variable or an optional
-        `window_sizes` keyword-argument variable. In the former case one size
-        is applied to all elements being tracked. In the latter case
-        `window_sizes` must be a list of integers that is a window_size for
-        each corresponding element index.
-
-        """
-        super().__init__(*args, **kwargs)
-        self._averages = None
-        self._bufs = None
-        self._window_sizes = kwargs.get("window_sizes")
-
-    def append_values(self, values, time_step):
-        if values[0].is_nan():
-            return
-        # Store every value in the circular buffer. Apply limits to the
-        # moving average.
-        if self._bufs is None:
-            self._averages = [copy.deepcopy(x) for x in values]
-            self._bufs = _make_circular_buffers(len(values), self._prop, self._window_sizes)
-
-        for i, val in enumerate(values):
-            buf = self._bufs[i]
-            buf.append(val.value)
-            self._averages[i].set_value(buf.average())
-
-        if self._prop.limits:
-            for i, avg in enumerate(self._averages):
-                if self._prop.should_store_value(avg.value):
-                    self._container.append_by_time_step(avg, time_step, i)
-        else:
-            self._container.append(self._averages)
-
-
-class StorageMovingAverageMax(StorageMax):
-    """Stores the max value of a moving average across time points."""
-    def __init__(self, *args, **kwargs):
-        """Constructor for StorageMovingAverageMax.
-
-        window_size comes from either the passed `prop` variable or an optional
-        `window_sizes` keyword-argument variable. In the former case one size
-        is applied to all elements being tracked. In the latter case
-        `window_sizes` must be a list of integers that is a window_size for
-        each corresponding element index.
-
-        """
-        super().__init__(*args, **kwargs)
-        self._bufs = None
-        self._averages = None
-        self._window_sizes = kwargs.get("window_sizes")
-
-    def append_values(self, values, time_step):
-        if values[0].is_nan():
-            return
-        if self._bufs is None:
-            self._averages = [copy.deepcopy(x) for x in values]
-            self._bufs = _make_circular_buffers(len(values), self._prop, self._window_sizes)
-
-        for i, val in enumerate(values):
-            buf = self._bufs[i]
-            buf.append(val.value)
-            self._averages[i].set_value(buf.average())
-
-        self._handle_values(self._averages)
-
-
-class StorageSum(StorageFilterBase):
-    """Keeps a running sum of all values and records the total."""
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._sum = None
-
-    def append_values(self, values, _time_step):
-        if values[0].is_nan():
-            return
-        if self._sum is None:
-            self._sum = [copy.deepcopy(x) for x in values]
-        else:
-            for i, val in enumerate(values):
-                self._sum[i] += val
-
-    def close(self):
-        if self._sum is not None:
-            self._container.append(self._sum)
-            self._container.flush_data()
-
-
-def _make_circular_buffers(num_elements, prop, window_sizes):
-    if window_sizes is None:
-        window_size = prop.window_size
-        bufs = [CircularBufferHelper(window_size) for _ in range(num_elements)]
-    else:
-        bufs = [CircularBufferHelper(window_sizes[i]) for i in range(num_elements)]
-    return bufs
-
-
-STORAGE_TYPE_MAP = {
-    StoreValuesType.ALL: StorageAll,
-    #StoreValuesType.CHANGE_COUNT: StorageChangeCount,
-    StoreValuesType.MAX: StorageMax,
-    StoreValuesType.MIN: StorageMin,
-    StoreValuesType.MOVING_AVERAGE: StorageMovingAverage,
-    StoreValuesType.MOVING_AVERAGE_MAX: StorageMovingAverageMax,
-    StoreValuesType.SUM: StorageSum,
-}
+
+import copy
+import abc
+
+from loguru import logger
+import numpy as np
+
+from pydss.utils.simulation_utils import CircularBufferHelper
+from pydss.value_storage import ValueContainer
+from pydss.common import StoreValuesType
+
+
+class StorageFilterBase(abc.ABC):
+    """Base class for storage containers.
+    Subclasses can perform custom filtering based on StoreValuesType.
+
+    """
+    def __init__(self, hdf_store, path, prop, num_steps, max_chunk_bytes, values, elem_names, **kwargs):
+        self._prop = prop
+        self._container = self.make_container(
+            hdf_store,
+            path,
+            prop,
+            num_steps,
+            max_chunk_bytes,
+            values,
+            elem_names,
+        )
+        logger.debug("Created %s path=%s", self.__class__.__name__, path)
+
+    @abc.abstractmethod
+    def append_values(self, values, time_step):
+        """Store a new set of values for each element."""
+
+    def close(self):
+        """Perform any final writes to the container."""
+        self.flush_data()
+
+    def flush_data(self):
+        """Flush data to disk."""
+        self._container.flush_data()
+
+    def max_num_bytes(self):
+        """Return the maximum number of bytes the container could hold.
+
+        Returns
+        -------
+        int
+
+        """
+        return self._container.max_num_bytes()
+
+    @staticmethod
+    def make_container(hdf_store, path, prop, num_steps, max_chunk_bytes, values, elem_names):
+        """Return an instance of ValueContainer for storing values."""
+        container = ValueContainer(
+            values,
+            hdf_store,
+            path,
+            prop.get_max_size(num_steps),
+            elem_names,
+            prop.get_dataset_property_type(),
+            max_chunk_bytes=max_chunk_bytes,
+            store_time_step=prop.should_store_time_step(),
+        )
+        logger.debug("Created storage container path=%s", path)
+        return container
+
+
+class StorageAll(StorageFilterBase):
+    """Store values at every time point, optionally filtered."""
+
+    def append_values(self, values, time_step):
+        if self._prop.limits:
+            for i, value in enumerate(values):
+                if value.is_nan():
+                    break
+                if self._prop.should_store_value(value.value):
+                    self._container.append_by_time_step(value, time_step, i)
+        else:
+            self._container.append(values)
+
+
+"""
+class StorageChangeCount(StorageFilterBase):
+    def __init__(self, *args):
+        super().__init__(*args)
+        self._last_value = None
+        self._change_count = (None, 0)
+
+    def append_values(self, values, time_step):
+        assert False
+"""
+
+
+class StorageMin(StorageFilterBase):
+    """Stores the min value across time points."""
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self._min = None
+
+    def append_values(self, values, time_step):
+        if values[0].is_nan():
+            return
+        self._handle_values(values)
+
+    def close(self):
+        if self._min is not None:
+            self._container.append(self._min)
+            self._container.flush_data()
+
+    def _handle_values(self, values):
+        if self._min is None:
+            self._min = [copy.deepcopy(x) for x in values]
+        else:
+            for i, new_val in enumerate(values):
+                cur_val = self._min[i]
+                if (np.isnan(cur_val.value) and not np.isnan(new_val.value)) or \
+                        new_val < cur_val:
+                    self._min[i].set_value(new_val.value)
+
+
+class StorageMax(StorageFilterBase):
+    """Stores the max value across time points."""
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self._max = None
+
+    def append_values(self, values, time_step):
+        if values[0].is_nan():
+            return
+        self._handle_values(values)
+
+    def close(self):
+        if self._max is not None:
+            self._container.append(self._max)
+            self._container.flush_data()
+
+    def _handle_values(self, values):
+        if self._max is None:
+            self._max = [copy.deepcopy(x) for x in values]
+        else:
+            for i, new_val in enumerate(values):
+                cur_val = self._max[i]
+                if (np.isnan(cur_val.value) and not np.isnan(new_val.value)) or \
+                        new_val > cur_val:
+                    self._max[i].set_value(new_val.value)
+
+
+class StorageMovingAverage(StorageFilterBase):
+    """Stores a moving average across time points."""
+    def __init__(self, *args, **kwargs):
+        """Constructor for StorageMovingAverage.
+
+        window_size comes from either the passed `prop` variable or an optional
+        `window_sizes` keyword-argument variable. In the former case one size
+        is applied to all elements being tracked. In the latter case
+        `window_sizes` must be a list of integers that is a window_size for
+        each corresponding element index.
+
+        """
+        super().__init__(*args, **kwargs)
+        self._averages = None
+        self._bufs = None
+        self._window_sizes = kwargs.get("window_sizes")
+
+    def append_values(self, values, time_step):
+        if values[0].is_nan():
+            return
+        # Store every value in the circular buffer. Apply limits to the
+        # moving average.
+        if self._bufs is None:
+            self._averages = [copy.deepcopy(x) for x in values]
+            self._bufs = _make_circular_buffers(len(values), self._prop, self._window_sizes)
+
+        for i, val in enumerate(values):
+            buf = self._bufs[i]
+            buf.append(val.value)
+            self._averages[i].set_value(buf.average())
+
+        if self._prop.limits:
+            for i, avg in enumerate(self._averages):
+                if self._prop.should_store_value(avg.value):
+                    self._container.append_by_time_step(avg, time_step, i)
+        else:
+            self._container.append(self._averages)
+
+
+class StorageMovingAverageMax(StorageMax):
+    """Stores the max value of a moving average across time points."""
+    def __init__(self, *args, **kwargs):
+        """Constructor for StorageMovingAverageMax.
+
+        window_size comes from either the passed `prop` variable or an optional
+        `window_sizes` keyword-argument variable. In the former case one size
+        is applied to all elements being tracked. In the latter case
+        `window_sizes` must be a list of integers that is a window_size for
+        each corresponding element index.
+
+        """
+        super().__init__(*args, **kwargs)
+        self._bufs = None
+        self._averages = None
+        self._window_sizes = kwargs.get("window_sizes")
+
+    def append_values(self, values, time_step):
+        if values[0].is_nan():
+            return
+        if self._bufs is None:
+            self._averages = [copy.deepcopy(x) for x in values]
+            self._bufs = _make_circular_buffers(len(values), self._prop, self._window_sizes)
+
+        for i, val in enumerate(values):
+            buf = self._bufs[i]
+            buf.append(val.value)
+            self._averages[i].set_value(buf.average())
+
+        self._handle_values(self._averages)
+
+
+class StorageSum(StorageFilterBase):
+    """Keeps a running sum of all values and records the total."""
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self._sum = None
+
+    def append_values(self, values, _time_step):
+        if values[0].is_nan():
+            return
+        if self._sum is None:
+            self._sum = [copy.deepcopy(x) for x in values]
+        else:
+            for i, val in enumerate(values):
+                self._sum[i] += val
+
+    def close(self):
+        if self._sum is not None:
+            self._container.append(self._sum)
+            self._container.flush_data()
+
+
+def _make_circular_buffers(num_elements, prop, window_sizes):
+    if window_sizes is None:
+        window_size = prop.window_size
+        bufs = [CircularBufferHelper(window_size) for _ in range(num_elements)]
+    else:
+        bufs = [CircularBufferHelper(window_sizes[i]) for i in range(num_elements)]
+    return bufs
+
+
+STORAGE_TYPE_MAP = {
+    StoreValuesType.ALL: StorageAll,
+    #StoreValuesType.CHANGE_COUNT: StorageChangeCount,
+    StoreValuesType.MAX: StorageMax,
+    StoreValuesType.MIN: StorageMin,
+    StoreValuesType.MOVING_AVERAGE: StorageMovingAverage,
+    StoreValuesType.MOVING_AVERAGE_MAX: StorageMovingAverageMax,
+    StoreValuesType.SUM: StorageSum,
+}
```

### Comparing `nrel_pydss-3.1.3/src/pydss/thermal_metrics.py` & `nrel_pydss-3.1.4/src/pydss/thermal_metrics.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,394 +1,394 @@
-from typing import Dict, Union, Annotated, Optional
-from collections import defaultdict
-import math
-import os
-
-from loguru import logger
-from pydantic import BaseModel, Field
-
-from pydss.utils.simulation_utils import CircularBufferHelper
-from pydss.utils.utils import dump_data, load_data
-from pydantic import ConfigDict
-
-class ThermalMetricsBaseModel(BaseModel):
-    model_config = ConfigDict(title="ThermalMetricsBaseModel", str_strip_whitespace=True, validate_assignment=True, validate_default=True, extra="forbid", use_enum_values=False)
-
-
-class ThermalMetricsModel(ThermalMetricsBaseModel):
-    max_instantaneous_loadings_pct: Annotated[
-        Dict[str, float],
-        Field(
-            {},
-            title="max_instantaneous_loadings_pct",
-            description="maximum instantaneous loading percent for each element",
-        )]
-    max_instantaneous_loading_pct: Annotated[
-        float,
-        Field(
-            default = 120,
-            title="max_instantaneous_loading_pct",
-            description="maximum instantaneous loading percent overall",
-        )]
-    max_moving_average_loadings_pct: Annotated[
-        Dict[str, float],
-        Field(
-            {},
-            title="max_moving_average_loadings_pct",
-            description="maximum moving average loading percent for each element",
-        )]
-    max_moving_average_loading_pct: Annotated[
-        float,
-        Field(
-            100,
-            title="max_moving_average_loading_pct",
-            description="maximum moving average loading percent overall",
-        )]
-    window_size_hours: Annotated[
-        Optional[int],
-        Field(
-            None,
-            title="window_size_hours",
-            description="window size used to calculate the moving average",
-        )]
-    num_time_points_with_instantaneous_violations: Annotated[
-        Optional[int],
-        Field(
-            None,
-            title="num_time_points_with_instantaneous_violations",
-            description="number of time points where the instantaneous threshold was violated",
-        )]
-    num_time_points_with_moving_average_violations: Annotated[
-        Optional[int],
-        Field(
-            None,
-            title="num_time_points_with_moving_average_violations",
-            description="number of time points where the moving average threshold was violated",
-        )]
-    instantaneous_threshold: Annotated[
-        Optional[int],
-        Field(
-            None,
-            title="instantaneous_threshold",
-            description="instantaneous threshold",
-        )]
-    moving_average_threshold: Annotated[
-        Optional[int],
-        Field(
-            None,
-            title="moving_average_threshold",
-            description="moving average threshold",
-        )]
-
-
-def compare_thermal_metrics(metrics1: ThermalMetricsModel, metrics2: ThermalMetricsModel, rel_tol=0.001):
-    """Compares the values of two instances of ThermalMetricsModel.
-    Uses a tolerance of 0.001 for moving averages.
-
-    Returns
-    -------
-    bool
-        Return True if they match.
-
-    """
-    match = True
-    fields = (
-        "max_instantaneous_loading_pct", "window_size_hours",
-        "num_time_points_with_instantaneous_violations",
-        "num_time_points_with_moving_average_violations",
-        "instantaneous_threshold", "moving_average_threshold",
-    )
-    for field in fields:
-        val1 = getattr(metrics1, field)
-        val2 = getattr(metrics2, field)
-        if val1 != val2:
-            logger.error("field=%s mismatch %s != %s", field, val1, val2)
-            match = False
-
-    if not math.isclose(metrics1.max_moving_average_loading_pct, metrics2.max_instantaneous_loading_pct, rel_tol=rel_tol):
-        logger.error("max_moving_average_loading_pct mismatch %s != %s",
-                     metrics1.max_moving_average_loading_pct, metrics2.max_instantaneous_loading_pct)
-        match = False
-
-    for name, val1 in metrics1.max_instantaneous_loadings_pct.items():
-        val2 = metrics2.max_instantaneous_loadings_pct[name]
-        if val1 != val2:
-            logger.error("max_instantaneous_loadings_pct mismatch %s != %s", name, val1, val2)
-            match = False
-
-    for name, val1 in metrics1.max_moving_average_loadings_pct.items():
-        val2 = metrics2.max_moving_average_loadings_pct[name]
-        if not math.isclose(val1, val2, rel_tol=rel_tol):
-            logger.error("max_moving_average_loadings_pct mismatch %s != %s", name, val1, val2)
-            match = False
-
-    return match
-
-
-class ThermalMetricsSummaryModel(ThermalMetricsBaseModel):
-    line_loadings: Annotated[
-        ThermalMetricsModel,
-        Field(
-            title="line_loadings",
-            description="line loading metrics",
-        )]
-    transformer_loadings: Annotated[
-        Union[ThermalMetricsModel, None],
-        Field(
-            title="transformer_loadings",
-            description="transformer loading metrics",
-        )]
-
-
-class SimulationThermalMetricsModel(ThermalMetricsBaseModel):
-    scenarios: Annotated[
-        Dict[str, ThermalMetricsSummaryModel],
-        Field(
-            title="scenarios",
-            description="thermal metrics by pydss scenario name",
-        )]
-
-
-def create_summary(filename):
-    data = load_data(filename)
-    return create_summary_from_dict(data)
-
-
-def create_summary_from_dict(data):
-    """Create a summary of the metrics values for use in a table.
-
-    Parameters
-    ----------
-    filename: str
-        File containing a serialized SimulationThermalMetricsModel instance
-
-    Returns
-    -------
-    dict
-        Two-level dict. First level keys are scenario names. Second level has line/transform
-        metric names and values.
-
-    """
-    summary = SimulationThermalMetricsModel(**data)
-    report = defaultdict(dict)
-    for scenario in summary.scenarios:
-        for model, elem_type in zip(("line_loadings", "transformer_loadings"), ("line", "transformer")):
-            model = getattr(summary.scenarios[scenario], model)
-            if model is None:
-                continue
-            for column in model.model_fields:
-                val = getattr(model, column)
-                if not isinstance(val, dict):
-                    new_name = elem_type + "_" + column
-                    report[scenario][new_name] = val
-
-    return report
-
-
-class ThermalMetrics:
-    """Stores thermal metrics in memory.
-
-    The metrics are defined in this paper:
-    https://www.sciencedirect.com/science/article/pii/S0306261920311351
-
-    """
-
-    FILENAME = "thermal_metrics.json"
-
-    def __init__(
-        self,
-        prop,
-        start_time,
-        sim_resolution,
-        line_window_size_hours,
-        line_window_size,
-        transformer_window_size_hours,
-        transformer_window_size,
-        line_loading_percent_threshold,
-        line_loading_percent_moving_average_threshold,
-        transformer_loading_percent_threshold,
-        transformer_loading_percent_moving_average_threshold,
-        store_per_element_data,
-    ):
-        self._prop = prop
-        self._start_time = start_time
-        self._resolution = sim_resolution
-        self._line_window_size_hours = line_window_size_hours
-        self._line_window_size = line_window_size
-        self._line_loading_percent_threshold = line_loading_percent_threshold
-        self._line_loading_percent_mavg_threshold = line_loading_percent_moving_average_threshold
-        self._transformer_loading_percent_threshold = transformer_loading_percent_threshold
-        self._transformer_window_size_hours = transformer_window_size_hours
-        self._transformer_window_size = transformer_window_size
-        self._transformer_loading_percent_mavg_threshold = transformer_loading_percent_moving_average_threshold
-        self._num_time_points_inst_line_violations = 0
-        self._num_time_points_mavg_line_violations = 0
-        self._num_time_points_inst_transformer_violations = 0
-        self._num_time_points_mavg_transformer_violations = 0
-        self._max_inst_line_violations = None
-        self._max_mavg_line_violations = None
-        self._max_inst_transformer_violations = None
-        self._max_mavg_transformer_violations = None
-        self._line_bufs = None
-        self._transformer_bufs = None
-        self._num_time_points = 0
-        self._line_names = None
-        self._transformer_names = None
-        self._store_per_element_data = store_per_element_data
-
-    def generate_report(self, path):
-        """Create a summary file containing all metrics.
-
-        Parameters
-        ----------
-        path : str
-
-        Returns
-        -------
-        str
-            report filename
-
-        """
-        if self._num_time_points == 0 or self._max_inst_line_violations is None:
-            logger.error("Cannot generate report with no data")
-            return
-
-        inst_violations_by_line = {}
-        mavg_violations_by_line = {}
-        for i in range(len(self._max_inst_line_violations)):
-            inst_violations_by_line[self._line_names[i]] = self._max_inst_line_violations[i]
-        for i in range(len(self._max_mavg_line_violations)):
-            mavg_violations_by_line[self._line_names[i]] = self._max_mavg_line_violations[i]
-        line_metric = ThermalMetricsModel(
-            max_instantaneous_loadings_pct=inst_violations_by_line,
-            max_instantaneous_loading_pct=max(inst_violations_by_line.values()),
-            max_moving_average_loadings_pct=mavg_violations_by_line,
-            max_moving_average_loading_pct=max(mavg_violations_by_line.values()),
-            window_size_hours=self._line_window_size_hours,
-            num_time_points_with_instantaneous_violations=self._num_time_points_inst_line_violations,
-            num_time_points_with_moving_average_violations=self._num_time_points_mavg_line_violations,
-            instantaneous_threshold=self._line_loading_percent_threshold,
-            moving_average_threshold=self._line_loading_percent_mavg_threshold,
-        )
-
-        inst_violations_by_transformer = {}
-        mavg_violations_by_transformer = {}
-        for i in range(len(self._max_inst_transformer_violations)):
-            inst_violations_by_transformer[self._transformer_names[i]] = self._max_inst_transformer_violations[i]
-        for i in range(len(self._max_mavg_transformer_violations)):
-            mavg_violations_by_transformer[self._transformer_names[i]] = self._max_mavg_transformer_violations[i]
-
-        if self.has_transformers():
-            transformer_metric = ThermalMetricsModel(
-                max_instantaneous_loadings_pct=inst_violations_by_transformer,
-                max_instantaneous_loading_pct=max(inst_violations_by_transformer.values()),
-                max_moving_average_loadings_pct=mavg_violations_by_transformer,
-                max_moving_average_loading_pct=max(mavg_violations_by_transformer.values()),
-                window_size_hours=self._transformer_window_size_hours,
-                num_time_points_with_instantaneous_violations=self._num_time_points_inst_transformer_violations,
-                num_time_points_with_moving_average_violations=self._num_time_points_mavg_transformer_violations,
-                instantaneous_threshold=self._transformer_loading_percent_threshold,
-                moving_average_threshold=self._transformer_loading_percent_mavg_threshold,
-            )
-        else:
-            transformer_metric = None
-
-        if not self._store_per_element_data:
-            line_metric.max_instantaneous_loadings_pct.clear()
-            line_metric.max_moving_average_loadings_pct.clear()
-            if self.has_transformers():
-                transformer_metric.max_instantaneous_loadings_pct.clear()
-                transformer_metric.max_moving_average_loadings_pct.clear()
-
-        summary = ThermalMetricsSummaryModel(
-            line_loadings=line_metric,
-            transformer_loadings=transformer_metric,
-        )
-
-        filename = os.path.join(path, "thermal_metrics.json")
-        with open(filename, "w") as f_out:
-            f_out.write(summary.model_dump_json(indent=2))
-            f_out.write("\n")
-        logger.info("Generated thermal metric report in %s", filename)
-
-    def has_transformers(self):
-        return bool(self._transformer_names)
-
-    @property
-    def line_names(self):
-        return self._line_names
-
-    @property
-    def transformer_names(self):
-        return self._transformer_names
-
-    @line_names.setter
-    def line_names(self, names):
-        self._line_names = names
-
-    @transformer_names.setter
-    def transformer_names(self, names):
-        self._transformer_names = names
-
-    def increment_steps(self):
-        """Increment the time step counter."""
-        self._num_time_points += 1
-
-    def update(self, time_step, line_loadings, transformer_loadings):
-        """Update the metrics for the time step.
-
-        Parameters
-        ----------
-        time_step : int
-        voltages : list
-            list of ValueStorageBase
-
-        """
-        if self._line_bufs is None:
-            self._line_bufs = [CircularBufferHelper(self._line_window_size) for _ in range(len(self._line_names))]
-            self._transformer_bufs = [CircularBufferHelper(self._transformer_window_size) for _ in range(len(self._transformer_names))]
-            self._max_inst_line_violations = [0.0] * len(self._line_names)
-            self._max_mavg_line_violations = [0.0] * len(self._line_names)
-            self._max_inst_transformer_violations = [0.0] * len(self._transformer_names)
-            self._max_mavg_transformer_violations = [0.0] * len(self._transformer_names)
-
-        has_inst_line_violation = False
-        has_mavg_line_violation = False
-        for i, loading in enumerate(line_loadings):
-            if loading.value > self._max_inst_line_violations[i]:
-                self._max_inst_line_violations[i] = loading.value
-            if not has_inst_line_violation and loading.value > self._line_loading_percent_threshold:
-                has_inst_line_violation = True
-
-            buf = self._line_bufs[i]
-            buf.append(loading.value)
-            moving_avg = buf.average()
-
-            if moving_avg > self._max_mavg_line_violations[i]:
-                self._max_mavg_line_violations[i] = moving_avg
-            if not has_mavg_line_violation and moving_avg > self._line_loading_percent_mavg_threshold:
-                has_mavg_line_violation = True
-
-        has_inst_transformer_violation = False
-        has_mavg_transformer_violation = False
-        for i, loading in enumerate(transformer_loadings):
-            if loading.value > self._max_inst_transformer_violations[i]:
-                self._max_inst_transformer_violations[i] = loading.value
-            if not has_inst_transformer_violation and loading.value > self._transformer_loading_percent_threshold:
-                has_inst_transformer_violation = True
-
-            buf = self._transformer_bufs[i]
-            buf.append(loading.value)
-            moving_avg = buf.average()
-            if moving_avg > self._max_mavg_transformer_violations[i]:
-                self._max_mavg_transformer_violations[i] = moving_avg
-            if not has_mavg_transformer_violation and moving_avg > self._transformer_loading_percent_mavg_threshold:
-                has_mavg_transformer_violation = True
-
-        if has_inst_line_violation:
-            self._num_time_points_inst_line_violations += 1
-        if has_mavg_line_violation:
-            self._num_time_points_mavg_line_violations += 1
-        if has_inst_transformer_violation:
-            self._num_time_points_inst_transformer_violations += 1
-        if has_mavg_transformer_violation:
-            self._num_time_points_mavg_transformer_violations += 1
+from typing import Dict, Union, Annotated, Optional
+from collections import defaultdict
+import math
+import os
+
+from loguru import logger
+from pydantic import BaseModel, Field
+
+from pydss.utils.simulation_utils import CircularBufferHelper
+from pydss.utils.utils import dump_data, load_data
+from pydantic import ConfigDict
+
+class ThermalMetricsBaseModel(BaseModel):
+    model_config = ConfigDict(title="ThermalMetricsBaseModel", str_strip_whitespace=True, validate_assignment=True, validate_default=True, extra="forbid", use_enum_values=False)
+
+
+class ThermalMetricsModel(ThermalMetricsBaseModel):
+    max_instantaneous_loadings_pct: Annotated[
+        Dict[str, float],
+        Field(
+            {},
+            title="max_instantaneous_loadings_pct",
+            description="maximum instantaneous loading percent for each element",
+        )]
+    max_instantaneous_loading_pct: Annotated[
+        float,
+        Field(
+            default = 120,
+            title="max_instantaneous_loading_pct",
+            description="maximum instantaneous loading percent overall",
+        )]
+    max_moving_average_loadings_pct: Annotated[
+        Dict[str, float],
+        Field(
+            {},
+            title="max_moving_average_loadings_pct",
+            description="maximum moving average loading percent for each element",
+        )]
+    max_moving_average_loading_pct: Annotated[
+        float,
+        Field(
+            100,
+            title="max_moving_average_loading_pct",
+            description="maximum moving average loading percent overall",
+        )]
+    window_size_hours: Annotated[
+        Optional[int],
+        Field(
+            None,
+            title="window_size_hours",
+            description="window size used to calculate the moving average",
+        )]
+    num_time_points_with_instantaneous_violations: Annotated[
+        Optional[int],
+        Field(
+            None,
+            title="num_time_points_with_instantaneous_violations",
+            description="number of time points where the instantaneous threshold was violated",
+        )]
+    num_time_points_with_moving_average_violations: Annotated[
+        Optional[int],
+        Field(
+            None,
+            title="num_time_points_with_moving_average_violations",
+            description="number of time points where the moving average threshold was violated",
+        )]
+    instantaneous_threshold: Annotated[
+        Optional[int],
+        Field(
+            None,
+            title="instantaneous_threshold",
+            description="instantaneous threshold",
+        )]
+    moving_average_threshold: Annotated[
+        Optional[int],
+        Field(
+            None,
+            title="moving_average_threshold",
+            description="moving average threshold",
+        )]
+
+
+def compare_thermal_metrics(metrics1: ThermalMetricsModel, metrics2: ThermalMetricsModel, rel_tol=0.001):
+    """Compares the values of two instances of ThermalMetricsModel.
+    Uses a tolerance of 0.001 for moving averages.
+
+    Returns
+    -------
+    bool
+        Return True if they match.
+
+    """
+    match = True
+    fields = (
+        "max_instantaneous_loading_pct", "window_size_hours",
+        "num_time_points_with_instantaneous_violations",
+        "num_time_points_with_moving_average_violations",
+        "instantaneous_threshold", "moving_average_threshold",
+    )
+    for field in fields:
+        val1 = getattr(metrics1, field)
+        val2 = getattr(metrics2, field)
+        if val1 != val2:
+            logger.error("field=%s mismatch %s != %s", field, val1, val2)
+            match = False
+
+    if not math.isclose(metrics1.max_moving_average_loading_pct, metrics2.max_instantaneous_loading_pct, rel_tol=rel_tol):
+        logger.error("max_moving_average_loading_pct mismatch %s != %s",
+                     metrics1.max_moving_average_loading_pct, metrics2.max_instantaneous_loading_pct)
+        match = False
+
+    for name, val1 in metrics1.max_instantaneous_loadings_pct.items():
+        val2 = metrics2.max_instantaneous_loadings_pct[name]
+        if val1 != val2:
+            logger.error("max_instantaneous_loadings_pct mismatch %s != %s", name, val1, val2)
+            match = False
+
+    for name, val1 in metrics1.max_moving_average_loadings_pct.items():
+        val2 = metrics2.max_moving_average_loadings_pct[name]
+        if not math.isclose(val1, val2, rel_tol=rel_tol):
+            logger.error("max_moving_average_loadings_pct mismatch %s != %s", name, val1, val2)
+            match = False
+
+    return match
+
+
+class ThermalMetricsSummaryModel(ThermalMetricsBaseModel):
+    line_loadings: Annotated[
+        ThermalMetricsModel,
+        Field(
+            title="line_loadings",
+            description="line loading metrics",
+        )]
+    transformer_loadings: Annotated[
+        Union[ThermalMetricsModel, None],
+        Field(
+            title="transformer_loadings",
+            description="transformer loading metrics",
+        )]
+
+
+class SimulationThermalMetricsModel(ThermalMetricsBaseModel):
+    scenarios: Annotated[
+        Dict[str, ThermalMetricsSummaryModel],
+        Field(
+            title="scenarios",
+            description="thermal metrics by pydss scenario name",
+        )]
+
+
+def create_summary(filename):
+    data = load_data(filename)
+    return create_summary_from_dict(data)
+
+
+def create_summary_from_dict(data):
+    """Create a summary of the metrics values for use in a table.
+
+    Parameters
+    ----------
+    filename: str
+        File containing a serialized SimulationThermalMetricsModel instance
+
+    Returns
+    -------
+    dict
+        Two-level dict. First level keys are scenario names. Second level has line/transform
+        metric names and values.
+
+    """
+    summary = SimulationThermalMetricsModel(**data)
+    report = defaultdict(dict)
+    for scenario in summary.scenarios:
+        for model, elem_type in zip(("line_loadings", "transformer_loadings"), ("line", "transformer")):
+            model = getattr(summary.scenarios[scenario], model)
+            if model is None:
+                continue
+            for column in model.model_fields:
+                val = getattr(model, column)
+                if not isinstance(val, dict):
+                    new_name = elem_type + "_" + column
+                    report[scenario][new_name] = val
+
+    return report
+
+
+class ThermalMetrics:
+    """Stores thermal metrics in memory.
+
+    The metrics are defined in this paper:
+    https://www.sciencedirect.com/science/article/pii/S0306261920311351
+
+    """
+
+    FILENAME = "thermal_metrics.json"
+
+    def __init__(
+        self,
+        prop,
+        start_time,
+        sim_resolution,
+        line_window_size_hours,
+        line_window_size,
+        transformer_window_size_hours,
+        transformer_window_size,
+        line_loading_percent_threshold,
+        line_loading_percent_moving_average_threshold,
+        transformer_loading_percent_threshold,
+        transformer_loading_percent_moving_average_threshold,
+        store_per_element_data,
+    ):
+        self._prop = prop
+        self._start_time = start_time
+        self._resolution = sim_resolution
+        self._line_window_size_hours = line_window_size_hours
+        self._line_window_size = line_window_size
+        self._line_loading_percent_threshold = line_loading_percent_threshold
+        self._line_loading_percent_mavg_threshold = line_loading_percent_moving_average_threshold
+        self._transformer_loading_percent_threshold = transformer_loading_percent_threshold
+        self._transformer_window_size_hours = transformer_window_size_hours
+        self._transformer_window_size = transformer_window_size
+        self._transformer_loading_percent_mavg_threshold = transformer_loading_percent_moving_average_threshold
+        self._num_time_points_inst_line_violations = 0
+        self._num_time_points_mavg_line_violations = 0
+        self._num_time_points_inst_transformer_violations = 0
+        self._num_time_points_mavg_transformer_violations = 0
+        self._max_inst_line_violations = None
+        self._max_mavg_line_violations = None
+        self._max_inst_transformer_violations = None
+        self._max_mavg_transformer_violations = None
+        self._line_bufs = None
+        self._transformer_bufs = None
+        self._num_time_points = 0
+        self._line_names = None
+        self._transformer_names = None
+        self._store_per_element_data = store_per_element_data
+
+    def generate_report(self, path):
+        """Create a summary file containing all metrics.
+
+        Parameters
+        ----------
+        path : str
+
+        Returns
+        -------
+        str
+            report filename
+
+        """
+        if self._num_time_points == 0 or self._max_inst_line_violations is None:
+            logger.error("Cannot generate report with no data")
+            return
+
+        inst_violations_by_line = {}
+        mavg_violations_by_line = {}
+        for i in range(len(self._max_inst_line_violations)):
+            inst_violations_by_line[self._line_names[i]] = self._max_inst_line_violations[i]
+        for i in range(len(self._max_mavg_line_violations)):
+            mavg_violations_by_line[self._line_names[i]] = self._max_mavg_line_violations[i]
+        line_metric = ThermalMetricsModel(
+            max_instantaneous_loadings_pct=inst_violations_by_line,
+            max_instantaneous_loading_pct=max(inst_violations_by_line.values()),
+            max_moving_average_loadings_pct=mavg_violations_by_line,
+            max_moving_average_loading_pct=max(mavg_violations_by_line.values()),
+            window_size_hours=self._line_window_size_hours,
+            num_time_points_with_instantaneous_violations=self._num_time_points_inst_line_violations,
+            num_time_points_with_moving_average_violations=self._num_time_points_mavg_line_violations,
+            instantaneous_threshold=self._line_loading_percent_threshold,
+            moving_average_threshold=self._line_loading_percent_mavg_threshold,
+        )
+
+        inst_violations_by_transformer = {}
+        mavg_violations_by_transformer = {}
+        for i in range(len(self._max_inst_transformer_violations)):
+            inst_violations_by_transformer[self._transformer_names[i]] = self._max_inst_transformer_violations[i]
+        for i in range(len(self._max_mavg_transformer_violations)):
+            mavg_violations_by_transformer[self._transformer_names[i]] = self._max_mavg_transformer_violations[i]
+
+        if self.has_transformers():
+            transformer_metric = ThermalMetricsModel(
+                max_instantaneous_loadings_pct=inst_violations_by_transformer,
+                max_instantaneous_loading_pct=max(inst_violations_by_transformer.values()),
+                max_moving_average_loadings_pct=mavg_violations_by_transformer,
+                max_moving_average_loading_pct=max(mavg_violations_by_transformer.values()),
+                window_size_hours=self._transformer_window_size_hours,
+                num_time_points_with_instantaneous_violations=self._num_time_points_inst_transformer_violations,
+                num_time_points_with_moving_average_violations=self._num_time_points_mavg_transformer_violations,
+                instantaneous_threshold=self._transformer_loading_percent_threshold,
+                moving_average_threshold=self._transformer_loading_percent_mavg_threshold,
+            )
+        else:
+            transformer_metric = None
+
+        if not self._store_per_element_data:
+            line_metric.max_instantaneous_loadings_pct.clear()
+            line_metric.max_moving_average_loadings_pct.clear()
+            if self.has_transformers():
+                transformer_metric.max_instantaneous_loadings_pct.clear()
+                transformer_metric.max_moving_average_loadings_pct.clear()
+
+        summary = ThermalMetricsSummaryModel(
+            line_loadings=line_metric,
+            transformer_loadings=transformer_metric,
+        )
+
+        filename = os.path.join(path, "thermal_metrics.json")
+        with open(filename, "w") as f_out:
+            f_out.write(summary.model_dump_json(indent=2))
+            f_out.write("\n")
+        logger.info("Generated thermal metric report in %s", filename)
+
+    def has_transformers(self):
+        return bool(self._transformer_names)
+
+    @property
+    def line_names(self):
+        return self._line_names
+
+    @property
+    def transformer_names(self):
+        return self._transformer_names
+
+    @line_names.setter
+    def line_names(self, names):
+        self._line_names = names
+
+    @transformer_names.setter
+    def transformer_names(self, names):
+        self._transformer_names = names
+
+    def increment_steps(self):
+        """Increment the time step counter."""
+        self._num_time_points += 1
+
+    def update(self, time_step, line_loadings, transformer_loadings):
+        """Update the metrics for the time step.
+
+        Parameters
+        ----------
+        time_step : int
+        voltages : list
+            list of ValueStorageBase
+
+        """
+        if self._line_bufs is None:
+            self._line_bufs = [CircularBufferHelper(self._line_window_size) for _ in range(len(self._line_names))]
+            self._transformer_bufs = [CircularBufferHelper(self._transformer_window_size) for _ in range(len(self._transformer_names))]
+            self._max_inst_line_violations = [0.0] * len(self._line_names)
+            self._max_mavg_line_violations = [0.0] * len(self._line_names)
+            self._max_inst_transformer_violations = [0.0] * len(self._transformer_names)
+            self._max_mavg_transformer_violations = [0.0] * len(self._transformer_names)
+
+        has_inst_line_violation = False
+        has_mavg_line_violation = False
+        for i, loading in enumerate(line_loadings):
+            if loading.value > self._max_inst_line_violations[i]:
+                self._max_inst_line_violations[i] = loading.value
+            if not has_inst_line_violation and loading.value > self._line_loading_percent_threshold:
+                has_inst_line_violation = True
+
+            buf = self._line_bufs[i]
+            buf.append(loading.value)
+            moving_avg = buf.average()
+
+            if moving_avg > self._max_mavg_line_violations[i]:
+                self._max_mavg_line_violations[i] = moving_avg
+            if not has_mavg_line_violation and moving_avg > self._line_loading_percent_mavg_threshold:
+                has_mavg_line_violation = True
+
+        has_inst_transformer_violation = False
+        has_mavg_transformer_violation = False
+        for i, loading in enumerate(transformer_loadings):
+            if loading.value > self._max_inst_transformer_violations[i]:
+                self._max_inst_transformer_violations[i] = loading.value
+            if not has_inst_transformer_violation and loading.value > self._transformer_loading_percent_threshold:
+                has_inst_transformer_violation = True
+
+            buf = self._transformer_bufs[i]
+            buf.append(loading.value)
+            moving_avg = buf.average()
+            if moving_avg > self._max_mavg_transformer_violations[i]:
+                self._max_mavg_transformer_violations[i] = moving_avg
+            if not has_mavg_transformer_violation and moving_avg > self._transformer_loading_percent_mavg_threshold:
+                has_mavg_transformer_violation = True
+
+        if has_inst_line_violation:
+            self._num_time_points_inst_line_violations += 1
+        if has_mavg_line_violation:
+            self._num_time_points_mavg_line_violations += 1
+        if has_inst_transformer_violation:
+            self._num_time_points_inst_transformer_violations += 1
+        if has_mavg_transformer_violation:
+            self._num_time_points_mavg_transformer_violations += 1
```

### Comparing `nrel_pydss-3.1.3/src/pydss/unitDefinations.py` & `nrel_pydss-3.1.4/src/pydss/unitDefinations.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,125 +1,125 @@
-
-standard_mapper = {
-        'current_mag' : ["CurrentsMagAng"],
-        'current_angle' : ["CurrentsMagAng"],
-        'current_real': ["Currents"],
-        'current_imag': ["Currents"],
-        'current_mag_rated': ["RatedCurrent", "NormalAmps", "normamps"],
-        'current_mag_emgcy': ["EmergAmps"],
-        'losses_real': ["Losses", 'PhaseLosses'],
-        'losses_imag': ["Losses", 'PhaseLosses'],
-        'sub_losses_real': ["SubstationLosses"],
-        'sub_losses_imag': ["SubstationLosses"],
-        'line_losses_real': ["LineLosses"],
-        'line_losses_imag': ["LineLosses"],
-        'power_real': ["Powers", 'TotalPower'],
-        'power_imag': ["Powers", 'TotalPower'],
-        'voltage_mag': ["puVmagAngle", "VoltagesMagAng"],
-        'voltage_angle': ["puVmagAngle", "VoltagesMagAng"],
-        'voltage_real': ["Voltages"],
-        'voltage_imag': ["Voltages"],
-        'voltage_mag_max_setpoint' : ['Vmaxpu'],
-        'voltage_mag_min_setpoint' : ['Vminpu'],
-        'frequency': ['Frequency'],
-        'tap': ['Taps'],
-        'soc': ['%stored'],
-        'power_real_setpoint': ['kW'],
-        'power_imagl_setpoint': ['kvar'],
-        'voltage_setpoint': ['kV'],
-        'power_mag_rated': ['kVARated'],
-        'power_mag_base': ['kVABase'],
-        'energy': ['kWh'],
-}
-
-unit_info = {
-        'CurrentsMagAng' : {'E' : 'amp', 'O' : 'deg'},
-        'Currents': {'E': 'amp', 'O': 'amp:'},
-        'RatedCurrent': 'amp',
-        'EmergAmps': 'amp',
-        'NormalAmps': 'amp',
-        'normamps': 'amp',
-        'Losses': {'E' : 'kw', 'O' : 'kvar'},
-        'PhaseLosses': {'E' : 'kw', 'O' : 'kvar'},
-        'Powers':  {'E' : 'kw', 'O' : 'kvar'},
-        'TotalPower':  {'E' : 'kw', 'O' : 'kvar'},
-        'LineLosses':  {'E' : 'kw', 'O' : 'kvar'},
-        'SubstationLosses':  {'E' : 'kw', 'O' : 'kvar'},
-        'kV': 'kV',
-        'kVARated': 'kvar',
-        'kvar': 'kvar',
-        'kW': 'kw',
-        'kVABase': 'kva',
-        'kWh': 'kwh',
-        'puVmagAngle' : {'E' : 'pu', 'O' : 'deg'},
-        'VoltagesMagAng': {'E' : 'v', 'O' : 'deg'},
-        'VMagAngle': {'E' : 'v', 'O' : 'deg'},
-        'Voltages': {'E': 'v', 'O': 'v'},
-        'Vmaxpu': 'pu',
-        'Vminpu': 'pu',
-        'Frequency' : 'Hz',
-        'Taps' : 'pu',
-        '%stored' : 'SOC',
-        'Distance' : 'mi'
-    }
-
-info_map = {
-        'CurrentsMagAng' : {'E' : 'current_mag', 'O' : 'current_angle'},
-        'Currents': {'E': 'current_real', 'O': 'current_imag'},
-        'Losses': {'E' : 'losses_real', 'O' : 'losses_imag'},
-        'PhaseLosses': {'E' : 'losses_real', 'O' : 'losses_imag'},
-        'Powers':  {'E' : 'power_real', 'O' : 'power_imag'},
-        'TotalPower':  {'E' : 'power_real', 'O' : 'power_imag'},
-        'LineLosses':  {'E' : 'line_losses_real', 'O' : 'line_losses_imag'},
-        'SubstationLosses':  {'E' : 'sub_losses_real', 'O' : 'sub_losses_imag'},
-        'puVmagAngle' : {'E' : 'voltage_mag', 'O' : 'voltage_angle'},
-        'VoltagesMagAng': {'E' : 'voltage_mag', 'O' : 'voltage_angle'},
-        'VMagAngle': {'E' : 'voltage_mag', 'O' : 'voltage_angle'},
-        'Voltages': {'E': 'voltage_real', 'O': 'voltage_imag'},
-        'Vmaxpu': 'voltage_mag_max_setpoint',
-        'Vminpu': 'voltage_mag_min_setpoint',
-        'EmergAmps': 'current_mag_emgcy',
-        'RatedCurrent': 'current_mag_rated',
-        'NormalAmps': 'current_mag_rated',
-        'normamps': 'current_mag_rated',
-        'Frequency' : 'frequency',
-        'Taps' : 'tap',
-        '%stored' : 'soc',
-        'Distance' : 'distance',
-        'kV': 'voltage_setpoint',
-        'kVARated': 'power_mag_rated',
-        'kvar': 'power_imag_setpoint',
-        'kW': 'power_real_setpoint',
-        'kVABase': 'power_mag_base',
-        'kWh': 'energy',
-    }
-
-type_info = {
-        'CurrentsMagAng': 'vector',
-        'Currents': 'vector',
-        'RatedCurrent': 'double',
-        'EmergAmps': 'double',
-        'NormalAmps': 'double',
-        'normamps': 'Amp',
-        'Losses': 'vector',
-        'PhaseLosses': 'vector',
-        'Powers':  'vector',
-        'TotalPower':  'vector',
-        'LineLosses':  'vector',
-        'SubstationLosses':  'vector',
-        'kV': 'double',
-        'kVARated': 'double',
-        'kvar': 'double',
-        'kW': 'double',
-        'kVABase': 'double',
-        'kWh': 'double',
-        'puVmagAngle': 'vector',
-        'VoltagesMagAng': 'vector',
-        'VMagAngle': 'vector',
-        'Voltages': 'vector',
-        'Vmaxpu': 'double',
-        'Vminpu': 'double',
-        'Frequency': 'double',
-        'Taps': 'vector',
-        '%stored': 'double',
-        'Distance': 'double'
-    }
+
+standard_mapper = {
+        'current_mag' : ["CurrentsMagAng"],
+        'current_angle' : ["CurrentsMagAng"],
+        'current_real': ["Currents"],
+        'current_imag': ["Currents"],
+        'current_mag_rated': ["RatedCurrent", "NormalAmps", "normamps"],
+        'current_mag_emgcy': ["EmergAmps"],
+        'losses_real': ["Losses", 'PhaseLosses'],
+        'losses_imag': ["Losses", 'PhaseLosses'],
+        'sub_losses_real': ["SubstationLosses"],
+        'sub_losses_imag': ["SubstationLosses"],
+        'line_losses_real': ["LineLosses"],
+        'line_losses_imag': ["LineLosses"],
+        'power_real': ["Powers", 'TotalPower'],
+        'power_imag': ["Powers", 'TotalPower'],
+        'voltage_mag': ["puVmagAngle", "VoltagesMagAng"],
+        'voltage_angle': ["puVmagAngle", "VoltagesMagAng"],
+        'voltage_real': ["Voltages"],
+        'voltage_imag': ["Voltages"],
+        'voltage_mag_max_setpoint' : ['Vmaxpu'],
+        'voltage_mag_min_setpoint' : ['Vminpu'],
+        'frequency': ['Frequency'],
+        'tap': ['Taps'],
+        'soc': ['%stored'],
+        'power_real_setpoint': ['kW'],
+        'power_imagl_setpoint': ['kvar'],
+        'voltage_setpoint': ['kV'],
+        'power_mag_rated': ['kVARated'],
+        'power_mag_base': ['kVABase'],
+        'energy': ['kWh'],
+}
+
+unit_info = {
+        'CurrentsMagAng' : {'E' : 'amp', 'O' : 'deg'},
+        'Currents': {'E': 'amp', 'O': 'amp:'},
+        'RatedCurrent': 'amp',
+        'EmergAmps': 'amp',
+        'NormalAmps': 'amp',
+        'normamps': 'amp',
+        'Losses': {'E' : 'kw', 'O' : 'kvar'},
+        'PhaseLosses': {'E' : 'kw', 'O' : 'kvar'},
+        'Powers':  {'E' : 'kw', 'O' : 'kvar'},
+        'TotalPower':  {'E' : 'kw', 'O' : 'kvar'},
+        'LineLosses':  {'E' : 'kw', 'O' : 'kvar'},
+        'SubstationLosses':  {'E' : 'kw', 'O' : 'kvar'},
+        'kV': 'kV',
+        'kVARated': 'kvar',
+        'kvar': 'kvar',
+        'kW': 'kw',
+        'kVABase': 'kva',
+        'kWh': 'kwh',
+        'puVmagAngle' : {'E' : 'pu', 'O' : 'deg'},
+        'VoltagesMagAng': {'E' : 'v', 'O' : 'deg'},
+        'VMagAngle': {'E' : 'v', 'O' : 'deg'},
+        'Voltages': {'E': 'v', 'O': 'v'},
+        'Vmaxpu': 'pu',
+        'Vminpu': 'pu',
+        'Frequency' : 'Hz',
+        'Taps' : 'pu',
+        '%stored' : 'SOC',
+        'Distance' : 'mi'
+    }
+
+info_map = {
+        'CurrentsMagAng' : {'E' : 'current_mag', 'O' : 'current_angle'},
+        'Currents': {'E': 'current_real', 'O': 'current_imag'},
+        'Losses': {'E' : 'losses_real', 'O' : 'losses_imag'},
+        'PhaseLosses': {'E' : 'losses_real', 'O' : 'losses_imag'},
+        'Powers':  {'E' : 'power_real', 'O' : 'power_imag'},
+        'TotalPower':  {'E' : 'power_real', 'O' : 'power_imag'},
+        'LineLosses':  {'E' : 'line_losses_real', 'O' : 'line_losses_imag'},
+        'SubstationLosses':  {'E' : 'sub_losses_real', 'O' : 'sub_losses_imag'},
+        'puVmagAngle' : {'E' : 'voltage_mag', 'O' : 'voltage_angle'},
+        'VoltagesMagAng': {'E' : 'voltage_mag', 'O' : 'voltage_angle'},
+        'VMagAngle': {'E' : 'voltage_mag', 'O' : 'voltage_angle'},
+        'Voltages': {'E': 'voltage_real', 'O': 'voltage_imag'},
+        'Vmaxpu': 'voltage_mag_max_setpoint',
+        'Vminpu': 'voltage_mag_min_setpoint',
+        'EmergAmps': 'current_mag_emgcy',
+        'RatedCurrent': 'current_mag_rated',
+        'NormalAmps': 'current_mag_rated',
+        'normamps': 'current_mag_rated',
+        'Frequency' : 'frequency',
+        'Taps' : 'tap',
+        '%stored' : 'soc',
+        'Distance' : 'distance',
+        'kV': 'voltage_setpoint',
+        'kVARated': 'power_mag_rated',
+        'kvar': 'power_imag_setpoint',
+        'kW': 'power_real_setpoint',
+        'kVABase': 'power_mag_base',
+        'kWh': 'energy',
+    }
+
+type_info = {
+        'CurrentsMagAng': 'vector',
+        'Currents': 'vector',
+        'RatedCurrent': 'double',
+        'EmergAmps': 'double',
+        'NormalAmps': 'double',
+        'normamps': 'Amp',
+        'Losses': 'vector',
+        'PhaseLosses': 'vector',
+        'Powers':  'vector',
+        'TotalPower':  'vector',
+        'LineLosses':  'vector',
+        'SubstationLosses':  'vector',
+        'kV': 'double',
+        'kVARated': 'double',
+        'kvar': 'double',
+        'kW': 'double',
+        'kVABase': 'double',
+        'kWh': 'double',
+        'puVmagAngle': 'vector',
+        'VoltagesMagAng': 'vector',
+        'VMagAngle': 'vector',
+        'Voltages': 'vector',
+        'Vmaxpu': 'double',
+        'Vminpu': 'double',
+        'Frequency': 'double',
+        'Taps': 'vector',
+        '%stored': 'double',
+        'Distance': 'double'
+    }
```

### Comparing `nrel_pydss-3.1.3/src/pydss/value_storage.py` & `nrel_pydss-3.1.4/src/pydss/value_storage.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,1259 +1,1216 @@
-00000000: 0d0a 696d 706f 7274 2061 6263 0d0a 696d  ..import abc..im
-00000010: 706f 7274 206f 730d 0a69 6d70 6f72 7420  port os..import 
-00000020: 7265 0d0a 0d0a 6672 6f6d 206c 6f67 7572  re....from logur
-00000030: 7520 696d 706f 7274 206c 6f67 6765 720d  u import logger.
-00000040: 0a69 6d70 6f72 7420 6e75 6d70 7920 6173  .import numpy as
-00000050: 206e 700d 0a0d 0a66 726f 6d20 7079 6473   np....from pyds
-00000060: 732e 636f 6d6d 6f6e 2069 6d70 6f72 7420  s.common import 
-00000070: 4461 7461 7365 7450 726f 7065 7274 7954  DatasetPropertyT
-00000080: 7970 652c 2049 4e54 4547 4552 5f4e 414e  ype, INTEGER_NAN
-00000090: 0d0a 6672 6f6d 2070 7964 7373 2e64 6174  ..from pydss.dat
-000000a0: 6173 6574 5f62 7566 6665 7220 696d 706f  aset_buffer impo
-000000b0: 7274 2044 6174 6173 6574 4275 6666 6572  rt DatasetBuffer
-000000c0: 0d0a 6672 6f6d 2070 7964 7373 2e65 7863  ..from pydss.exc
-000000d0: 6570 7469 6f6e 7320 696d 706f 7274 2049  eptions import I
-000000e0: 6e76 616c 6964 5061 7261 6d65 7465 722c  nvalidParameter,
-000000f0: 2049 6e76 616c 6964 436f 6e66 6967 7572   InvalidConfigur
-00000100: 6174 696f 6e0d 0a0d 0a0d 0a63 6c61 7373  ation......class
-00000110: 2056 616c 7565 5374 6f72 6167 6542 6173   ValueStorageBas
-00000120: 6528 6162 632e 4142 4329 3a0d 0a0d 0a20  e(abc.ABC):.... 
-00000130: 2020 2044 454c 494d 4954 4552 203d 2022     DELIMITER = "
-00000140: 5f5f 220d 0a0d 0a20 2020 2064 6566 205f  __"....    def _
-00000150: 5f69 6e69 745f 5f28 7365 6c66 293a 0d0a  _init__(self):..
-00000160: 2020 2020 2020 2020 7365 6c66 2e5f 6461          self._da
-00000170: 7461 7365 7420 3d20 4e6f 6e65 0d0a 0d0a  taset = None....
-00000180: 2020 2020 4073 7461 7469 636d 6574 686f      @staticmetho
-00000190: 640d 0a20 2020 2064 6566 2067 6574 5f63  d..    def get_c
-000001a0: 6f6c 756d 6e73 2864 662c 206e 616d 6573  olumns(df, names
-000001b0: 2c20 6f70 7469 6f6e 732c 202a 2a6b 7761  , options, **kwa
-000001c0: 7267 7329 3a0d 0a20 2020 2020 2020 2022  rgs):..        "
-000001d0: 2222 5265 7475 726e 2074 6865 2063 6f6c  ""Return the col
-000001e0: 756d 6e20 6e61 6d65 7320 696e 2074 6865  umn names in the
-000001f0: 2064 6174 6166 7261 6d65 2074 6861 7420   dataframe that 
-00000200: 6d61 7463 6820 6e61 6d65 7320 616e 6420  match names and 
-00000210: 6b77 6172 6773 2e0d 0a0d 0a20 2020 2020  kwargs.....     
-00000220: 2020 2050 6172 616d 6574 6572 730d 0a20     Parameters.. 
-00000230: 2020 2020 2020 202d 2d2d 2d2d 2d2d 2d2d         ---------
-00000240: 2d0d 0a20 2020 2020 2020 2064 6620 3a20  -..        df : 
-00000250: 7064 2e44 6174 6146 7261 6d65 0d0a 2020  pd.DataFrame..  
-00000260: 2020 2020 2020 6e61 6d65 7320 3a20 7374        names : st
-00000270: 7220 7c20 6c69 7374 0d0a 2020 2020 2020  r | list..      
-00000280: 2020 2020 2020 7369 6e67 6c65 206e 616d        single nam
-00000290: 6520 6f72 206c 6973 7420 6f66 206e 616d  e or list of nam
-000002a0: 6573 0d0a 2020 2020 2020 2020 6b77 6172  es..        kwar
-000002b0: 6773 203a 2064 6963 740d 0a20 2020 2020  gs : dict..     
-000002c0: 2020 2020 2020 2046 696c 7465 7220 6f6e         Filter on
-000002d0: 206f 7074 696f 6e73 3b20 7661 6c75 6573   options; values
-000002e0: 2063 616e 2062 6520 7374 7269 6e67 7320   can be strings 
-000002f0: 6f72 2072 6567 756c 6172 2065 7870 7265  or regular expre
-00000300: 7373 696f 6e73 2e0d 0a0d 0a20 2020 2020  ssions.....     
-00000310: 2020 2052 6574 7572 6e73 0d0a 2020 2020     Returns..    
-00000320: 2020 2020 2d2d 2d2d 2d2d 2d0d 0a20 2020      -------..   
-00000330: 2020 2020 206c 6973 740d 0a0d 0a20 2020       list....   
-00000340: 2020 2020 2022 2222 0d0a 2020 2020 2020       """..      
-00000350: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(
-00000360: 6e61 6d65 732c 2073 7472 293a 0d0a 2020  names, str):..  
-00000370: 2020 2020 2020 2020 2020 6e61 6d65 7320            names 
-00000380: 3d20 7365 7428 5b6e 616d 6573 5d29 0d0a  = set([names])..
-00000390: 2020 2020 2020 2020 656c 6966 2069 7369          elif isi
-000003a0: 6e73 7461 6e63 6528 6e61 6d65 732c 2073  nstance(names, s
-000003b0: 6574 293a 0d0a 2020 2020 2020 2020 2020  et):..          
-000003c0: 2020 7061 7373 0d0a 2020 2020 2020 2020    pass..        
-000003d0: 656c 7365 3a0d 0a20 2020 2020 2020 2020  else:..         
-000003e0: 2020 206e 616d 6573 203d 2073 6574 286e     names = set(n
-000003f0: 616d 6573 290d 0a20 2020 2020 2020 2066  ames)..        f
-00000400: 6965 6c64 5f69 6e64 6963 6573 203d 207b  ield_indices = {
-00000410: 6f70 7469 6f6e 3a20 6920 2b20 3120 666f  option: i + 1 fo
-00000420: 7220 692c 206f 7074 696f 6e20 696e 2065  r i, option in e
-00000430: 6e75 6d65 7261 7465 286f 7074 696f 6e73  numerate(options
-00000440: 297d 0d0a 2020 2020 2020 2020 636f 6c75  )}..        colu
-00000450: 6d6e 7320 3d20 5b5d 0d0a 2020 2020 2020  mns = []..      
-00000460: 2020 666f 7220 636f 6c75 6d6e 2069 6e20    for column in 
-00000470: 6466 2e63 6f6c 756d 6e73 3a0d 0a20 2020  df.columns:..   
-00000480: 2020 2020 2020 2020 2063 6f6c 203d 2063           col = c
-00000490: 6f6c 756d 6e0d 0a20 2020 2020 2020 2020  olumn..         
-000004a0: 2020 2069 6e64 6578 203d 2063 6f6c 756d     index = colum
-000004b0: 6e2e 6669 6e64 2822 205b 2229 0d0a 2020  n.find(" [")..  
-000004c0: 2020 2020 2020 2020 2020 6966 2069 6e64            if ind
-000004d0: 6578 2021 3d20 2d31 3a0d 0a20 2020 2020  ex != -1:..     
-000004e0: 2020 2020 2020 2020 2020 2063 6f6c 203d             col =
-000004f0: 2063 6f6c 756d 6e5b 3a69 6e64 6578 5d0d   column[:index].
-00000500: 0a20 2020 2020 2020 2020 2020 2023 205b  .            # [
-00000510: 6e61 6d65 2c20 6f70 7469 6f6e 312c 206f  name, option1, o
-00000520: 7074 696f 6e32 2c20 2e2e 2e5d 0d0a 2020  ption2, ...]..  
-00000530: 2020 2020 2020 2020 2020 6669 656c 6473            fields
-00000540: 203d 2056 616c 7565 5374 6f72 6167 6542   = ValueStorageB
-00000550: 6173 652e 6765 745f 6669 656c 6473 2863  ase.get_fields(c
-00000560: 6f6c 2c20 6e65 7874 2869 7465 7228 6e61  ol, next(iter(na
-00000570: 6d65 7329 2929 0d0a 2020 2020 2020 2020  mes)))..        
-00000580: 2020 2020 6966 206f 7074 696f 6e73 2061      if options a
-00000590: 6e64 206b 7761 7267 733a 0d0a 2020 2020  nd kwargs:..    
-000005a0: 2020 2020 2020 2020 2020 2020 6173 7365              asse
-000005b0: 7274 206c 656e 2866 6965 6c64 7329 203d  rt len(fields) =
-000005c0: 3d20 3120 2b20 6c65 6e28 6f70 7469 6f6e  = 1 + len(option
-000005d0: 7329 2c20 6622 6669 656c 6473 3d7b 6669  s), f"fields={fi
-000005e0: 656c 6473 7d20 6f70 7469 6f6e 733d 7b6f  elds} options={o
-000005f0: 7074 696f 6e73 7d22 0d0a 2020 2020 2020  ptions}"..      
-00000600: 2020 2020 2020 5f6e 616d 6520 3d20 6669        _name = fi
-00000610: 656c 6473 5b30 5d0d 0a20 2020 2020 2020  elds[0]..       
-00000620: 2020 2020 2069 6620 5f6e 616d 6520 6e6f       if _name no
-00000630: 7420 696e 206e 616d 6573 3a0d 0a20 2020  t in names:..   
-00000640: 2020 2020 2020 2020 2020 2020 2063 6f6e               con
-00000650: 7469 6e75 650d 0a20 2020 2020 2020 2020  tinue..         
-00000660: 2020 206d 6174 6368 203d 2054 7275 650d     match = True.
-00000670: 0a20 2020 2020 2020 2020 2020 2066 6f72  .            for
-00000680: 206b 6579 2c20 7661 6c20 696e 206b 7761   key, val in kwa
-00000690: 7267 732e 6974 656d 7328 293a 0d0a 2020  rgs.items():..  
-000006a0: 2020 2020 2020 2020 2020 2020 2020 6966                if
-000006b0: 2069 7369 6e73 7461 6e63 6528 7661 6c2c   isinstance(val,
-000006c0: 2073 7472 293a 0d0a 2020 2020 2020 2020   str):..        
-000006d0: 2020 2020 2020 2020 2020 2020 6966 2066              if f
-000006e0: 6965 6c64 735b 6669 656c 645f 696e 6469  ields[field_indi
-000006f0: 6365 735b 6b65 795d 5d20 213d 2076 616c  ces[key]] != val
-00000700: 3a0d 0a20 2020 2020 2020 2020 2020 2020  :..             
-00000710: 2020 2020 2020 2020 2020 206d 6174 6368             match
-00000720: 203d 2046 616c 7365 0d0a 2020 2020 2020   = False..      
-00000730: 2020 2020 2020 2020 2020 656c 6966 2069            elif i
-00000740: 7369 6e73 7461 6e63 6528 7661 6c2c 2072  sinstance(val, r
-00000750: 652e 5061 7474 6572 6e29 3a0d 0a20 2020  e.Pattern):..   
-00000760: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000770: 2069 6620 7661 6c2e 7365 6172 6368 2866   if val.search(f
-00000780: 6965 6c64 735b 6669 656c 645f 696e 6469  ields[field_indi
-00000790: 6365 735b 6b65 795d 5d29 2069 7320 4e6f  ces[key]]) is No
-000007a0: 6e65 3a0d 0a20 2020 2020 2020 2020 2020  ne:..           
-000007b0: 2020 2020 2020 2020 2020 2020 206d 6174               mat
-000007c0: 6368 203d 2046 616c 7365 0d0a 2020 2020  ch = False..    
-000007d0: 2020 2020 2020 2020 2020 2020 656c 6966              elif
-000007e0: 2076 616c 2069 7320 4e6f 6e65 3a0d 0a20   val is None:.. 
-000007f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000800: 2020 2063 6f6e 7469 6e75 650d 0a20 2020     continue..   
-00000810: 2020 2020 2020 2020 2020 2020 2065 6c73               els
-00000820: 653a 0d0a 2020 2020 2020 2020 2020 2020  e:..            
-00000830: 2020 2020 2020 2020 7261 6973 6520 496e          raise In
-00000840: 7661 6c69 6450 6172 616d 6574 6572 2866  validParameter(f
-00000850: 2275 6e68 616e 646c 6564 206f 7074 696f  "unhandled optio
-00000860: 6e20 7661 6c75 6520 277b 7661 6c7d 2722  n value '{val}'"
-00000870: 290d 0a20 2020 2020 2020 2020 2020 2020  )..             
-00000880: 2020 2069 6620 6e6f 7420 6d61 7463 683a     if not match:
-00000890: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-000008a0: 2020 2020 2020 6272 6561 6b0d 0a20 2020        break..   
-000008b0: 2020 2020 2020 2020 2069 6620 6d61 7463           if matc
-000008c0: 683a 0d0a 2020 2020 2020 2020 2020 2020  h:..            
-000008d0: 2020 2020 636f 6c75 6d6e 732e 6170 7065      columns.appe
-000008e0: 6e64 2863 6f6c 756d 6e29 0d0a 0d0a 2020  nd(column)....  
-000008f0: 2020 2020 2020 6966 206e 6f74 2063 6f6c        if not col
-00000900: 756d 6e73 3a0d 0a20 2020 2020 2020 2020  umns:..         
-00000910: 2020 2072 6169 7365 2049 6e76 616c 6964     raise Invalid
-00000920: 5061 7261 6d65 7465 7228 6622 7b6e 616d  Parameter(f"{nam
-00000930: 6573 7d20 646f 6573 206e 6f74 2065 7869  es} does not exi
-00000940: 7374 2069 6e20 4461 7461 4672 616d 6522  st in DataFrame"
-00000950: 290d 0a0d 0a20 2020 2020 2020 2072 6574  )....        ret
-00000960: 7572 6e20 636f 6c75 6d6e 730d 0a0d 0a20  urn columns.... 
-00000970: 2020 2040 7374 6174 6963 6d65 7468 6f64     @staticmethod
-00000980: 0d0a 2020 2020 6465 6620 6765 745f 6f70  ..    def get_op
-00000990: 7469 6f6e 5f76 616c 7565 7328 6466 2c20  tion_values(df, 
-000009a0: 6e61 6d65 293a 0d0a 2020 2020 2020 2020  name):..        
-000009b0: 2222 2252 6574 7572 6e20 7468 6520 6f70  """Return the op
-000009c0: 7469 6f6e 2076 616c 7565 7320 7061 7273  tion values pars
-000009d0: 6564 2066 726f 6d20 7468 6520 636f 6c75  ed from the colu
-000009e0: 6d6e 206e 616d 6573 2e0d 0a0d 0a20 2020  mn names.....   
-000009f0: 2020 2020 2050 6172 616d 6574 6572 730d       Parameters.
-00000a00: 0a20 2020 2020 2020 202d 2d2d 2d2d 2d2d  .        -------
-00000a10: 2d2d 2d0d 0a20 2020 2020 2020 2064 6620  ---..        df 
-00000a20: 3a20 7064 2e44 6174 6146 7261 6d65 0d0a  : pd.DataFrame..
-00000a30: 2020 2020 2020 2020 6e61 6d65 203a 2073          name : s
-00000a40: 7472 0d0a 0d0a 2020 2020 2020 2020 5265  tr....        Re
-00000a50: 7475 726e 730d 0a20 2020 2020 2020 202d  turns..        -
-00000a60: 2d2d 2d2d 2d2d 0d0a 2020 2020 2020 2020  ------..        
-00000a70: 6c69 7374 0d0a 0d0a 2020 2020 2020 2020  list....        
-00000a80: 2222 220d 0a20 2020 2020 2020 2076 616c  """..        val
-00000a90: 7565 7320 3d20 5b5d 0d0a 2020 2020 2020  ues = []..      
-00000aa0: 2020 666f 7220 636f 6c75 6d6e 2069 6e20    for column in 
-00000ab0: 6466 2e63 6f6c 756d 6e73 3a0d 0a20 2020  df.columns:..   
+00000000: 0a69 6d70 6f72 7420 6162 630a 696d 706f  .import abc.impo
+00000010: 7274 206f 730a 696d 706f 7274 2072 650a  rt os.import re.
+00000020: 0a66 726f 6d20 6c6f 6775 7275 2069 6d70  .from loguru imp
+00000030: 6f72 7420 6c6f 6767 6572 0a69 6d70 6f72  ort logger.impor
+00000040: 7420 6e75 6d70 7920 6173 206e 700a 0a66  t numpy as np..f
+00000050: 726f 6d20 7079 6473 732e 636f 6d6d 6f6e  rom pydss.common
+00000060: 2069 6d70 6f72 7420 4461 7461 7365 7450   import DatasetP
+00000070: 726f 7065 7274 7954 7970 652c 2049 4e54  ropertyType, INT
+00000080: 4547 4552 5f4e 414e 0a66 726f 6d20 7079  EGER_NAN.from py
+00000090: 6473 732e 6461 7461 7365 745f 6275 6666  dss.dataset_buff
+000000a0: 6572 2069 6d70 6f72 7420 4461 7461 7365  er import Datase
+000000b0: 7442 7566 6665 720a 6672 6f6d 2070 7964  tBuffer.from pyd
+000000c0: 7373 2e65 7863 6570 7469 6f6e 7320 696d  ss.exceptions im
+000000d0: 706f 7274 2049 6e76 616c 6964 5061 7261  port InvalidPara
+000000e0: 6d65 7465 722c 2049 6e76 616c 6964 436f  meter, InvalidCo
+000000f0: 6e66 6967 7572 6174 696f 6e0a 0a0a 636c  nfiguration...cl
+00000100: 6173 7320 5661 6c75 6553 746f 7261 6765  ass ValueStorage
+00000110: 4261 7365 2861 6263 2e41 4243 293a 0a0a  Base(abc.ABC):..
+00000120: 2020 2020 4445 4c49 4d49 5445 5220 3d20      DELIMITER = 
+00000130: 225f 5f22 0a0a 2020 2020 6465 6620 5f5f  "__"..    def __
+00000140: 696e 6974 5f5f 2873 656c 6629 3a0a 2020  init__(self):.  
+00000150: 2020 2020 2020 7365 6c66 2e5f 6461 7461        self._data
+00000160: 7365 7420 3d20 4e6f 6e65 0a0a 2020 2020  set = None..    
+00000170: 4073 7461 7469 636d 6574 686f 640a 2020  @staticmethod.  
+00000180: 2020 6465 6620 6765 745f 636f 6c75 6d6e    def get_column
+00000190: 7328 6466 2c20 6e61 6d65 732c 206f 7074  s(df, names, opt
+000001a0: 696f 6e73 2c20 2a2a 6b77 6172 6773 293a  ions, **kwargs):
+000001b0: 0a20 2020 2020 2020 2022 2222 5265 7475  .        """Retu
+000001c0: 726e 2074 6865 2063 6f6c 756d 6e20 6e61  rn the column na
+000001d0: 6d65 7320 696e 2074 6865 2064 6174 6166  mes in the dataf
+000001e0: 7261 6d65 2074 6861 7420 6d61 7463 6820  rame that match 
+000001f0: 6e61 6d65 7320 616e 6420 6b77 6172 6773  names and kwargs
+00000200: 2e0a 0a20 2020 2020 2020 2050 6172 616d  ...        Param
+00000210: 6574 6572 730a 2020 2020 2020 2020 2d2d  eters.        --
+00000220: 2d2d 2d2d 2d2d 2d2d 0a20 2020 2020 2020  --------.       
+00000230: 2064 6620 3a20 7064 2e44 6174 6146 7261   df : pd.DataFra
+00000240: 6d65 0a20 2020 2020 2020 206e 616d 6573  me.        names
+00000250: 203a 2073 7472 207c 206c 6973 740a 2020   : str | list.  
+00000260: 2020 2020 2020 2020 2020 7369 6e67 6c65            single
+00000270: 206e 616d 6520 6f72 206c 6973 7420 6f66   name or list of
+00000280: 206e 616d 6573 0a20 2020 2020 2020 206b   names.        k
+00000290: 7761 7267 7320 3a20 6469 6374 0a20 2020  wargs : dict.   
+000002a0: 2020 2020 2020 2020 2046 696c 7465 7220           Filter 
+000002b0: 6f6e 206f 7074 696f 6e73 3b20 7661 6c75  on options; valu
+000002c0: 6573 2063 616e 2062 6520 7374 7269 6e67  es can be string
+000002d0: 7320 6f72 2072 6567 756c 6172 2065 7870  s or regular exp
+000002e0: 7265 7373 696f 6e73 2e0a 0a20 2020 2020  ressions...     
+000002f0: 2020 2052 6574 7572 6e73 0a20 2020 2020     Returns.     
+00000300: 2020 202d 2d2d 2d2d 2d2d 0a20 2020 2020     -------.     
+00000310: 2020 206c 6973 740a 0a20 2020 2020 2020     list..       
+00000320: 2022 2222 0a20 2020 2020 2020 2069 6620   """.        if 
+00000330: 6973 696e 7374 616e 6365 286e 616d 6573  isinstance(names
+00000340: 2c20 7374 7229 3a0a 2020 2020 2020 2020  , str):.        
+00000350: 2020 2020 6e61 6d65 7320 3d20 7365 7428      names = set(
+00000360: 5b6e 616d 6573 5d29 0a20 2020 2020 2020  [names]).       
+00000370: 2065 6c69 6620 6973 696e 7374 616e 6365   elif isinstance
+00000380: 286e 616d 6573 2c20 7365 7429 3a0a 2020  (names, set):.  
+00000390: 2020 2020 2020 2020 2020 7061 7373 0a20            pass. 
+000003a0: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
+000003b0: 2020 2020 2020 2020 206e 616d 6573 203d           names =
+000003c0: 2073 6574 286e 616d 6573 290a 2020 2020   set(names).    
+000003d0: 2020 2020 6669 656c 645f 696e 6469 6365      field_indice
+000003e0: 7320 3d20 7b6f 7074 696f 6e3a 2069 202b  s = {option: i +
+000003f0: 2031 2066 6f72 2069 2c20 6f70 7469 6f6e   1 for i, option
+00000400: 2069 6e20 656e 756d 6572 6174 6528 6f70   in enumerate(op
+00000410: 7469 6f6e 7329 7d0a 2020 2020 2020 2020  tions)}.        
+00000420: 636f 6c75 6d6e 7320 3d20 5b5d 0a20 2020  columns = [].   
+00000430: 2020 2020 2066 6f72 2063 6f6c 756d 6e20       for column 
+00000440: 696e 2064 662e 636f 6c75 6d6e 733a 0a20  in df.columns:. 
+00000450: 2020 2020 2020 2020 2020 2063 6f6c 203d             col =
+00000460: 2063 6f6c 756d 6e0a 2020 2020 2020 2020   column.        
+00000470: 2020 2020 696e 6465 7820 3d20 636f 6c75      index = colu
+00000480: 6d6e 2e66 696e 6428 2220 5b22 290a 2020  mn.find(" [").  
+00000490: 2020 2020 2020 2020 2020 6966 2069 6e64            if ind
+000004a0: 6578 2021 3d20 2d31 3a0a 2020 2020 2020  ex != -1:.      
+000004b0: 2020 2020 2020 2020 2020 636f 6c20 3d20            col = 
+000004c0: 636f 6c75 6d6e 5b3a 696e 6465 785d 0a20  column[:index]. 
+000004d0: 2020 2020 2020 2020 2020 2023 205b 6e61             # [na
+000004e0: 6d65 2c20 6f70 7469 6f6e 312c 206f 7074  me, option1, opt
+000004f0: 696f 6e32 2c20 2e2e 2e5d 0a20 2020 2020  ion2, ...].     
+00000500: 2020 2020 2020 2066 6965 6c64 7320 3d20         fields = 
+00000510: 5661 6c75 6553 746f 7261 6765 4261 7365  ValueStorageBase
+00000520: 2e67 6574 5f66 6965 6c64 7328 636f 6c2c  .get_fields(col,
+00000530: 206e 6578 7428 6974 6572 286e 616d 6573   next(iter(names
+00000540: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
+00000550: 6966 206f 7074 696f 6e73 2061 6e64 206b  if options and k
+00000560: 7761 7267 733a 0a20 2020 2020 2020 2020  wargs:.         
+00000570: 2020 2020 2020 2061 7373 6572 7420 6c65         assert le
+00000580: 6e28 6669 656c 6473 2920 3d3d 2031 202b  n(fields) == 1 +
+00000590: 206c 656e 286f 7074 696f 6e73 292c 2066   len(options), f
+000005a0: 2266 6965 6c64 733d 7b66 6965 6c64 737d  "fields={fields}
+000005b0: 206f 7074 696f 6e73 3d7b 6f70 7469 6f6e   options={option
+000005c0: 737d 220a 2020 2020 2020 2020 2020 2020  s}".            
+000005d0: 5f6e 616d 6520 3d20 6669 656c 6473 5b30  _name = fields[0
+000005e0: 5d0a 2020 2020 2020 2020 2020 2020 6966  ].            if
+000005f0: 205f 6e61 6d65 206e 6f74 2069 6e20 6e61   _name not in na
+00000600: 6d65 733a 0a20 2020 2020 2020 2020 2020  mes:.           
+00000610: 2020 2020 2063 6f6e 7469 6e75 650a 2020       continue.  
+00000620: 2020 2020 2020 2020 2020 6d61 7463 6820            match 
+00000630: 3d20 5472 7565 0a20 2020 2020 2020 2020  = True.         
+00000640: 2020 2066 6f72 206b 6579 2c20 7661 6c20     for key, val 
+00000650: 696e 206b 7761 7267 732e 6974 656d 7328  in kwargs.items(
+00000660: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             
+00000670: 2020 2069 6620 6973 696e 7374 616e 6365     if isinstance
+00000680: 2876 616c 2c20 7374 7229 3a0a 2020 2020  (val, str):.    
+00000690: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000006a0: 6966 2066 6965 6c64 735b 6669 656c 645f  if fields[field_
+000006b0: 696e 6469 6365 735b 6b65 795d 5d20 213d  indices[key]] !=
+000006c0: 2076 616c 3a0a 2020 2020 2020 2020 2020   val:.          
+000006d0: 2020 2020 2020 2020 2020 2020 2020 6d61                ma
+000006e0: 7463 6820 3d20 4661 6c73 650a 2020 2020  tch = False.    
+000006f0: 2020 2020 2020 2020 2020 2020 656c 6966              elif
+00000700: 2069 7369 6e73 7461 6e63 6528 7661 6c2c   isinstance(val,
+00000710: 2072 652e 5061 7474 6572 6e29 3a0a 2020   re.Pattern):.  
+00000720: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000730: 2020 6966 2076 616c 2e73 6561 7263 6828    if val.search(
+00000740: 6669 656c 6473 5b66 6965 6c64 5f69 6e64  fields[field_ind
+00000750: 6963 6573 5b6b 6579 5d5d 2920 6973 204e  ices[key]]) is N
+00000760: 6f6e 653a 0a20 2020 2020 2020 2020 2020  one:.           
+00000770: 2020 2020 2020 2020 2020 2020 206d 6174               mat
+00000780: 6368 203d 2046 616c 7365 0a20 2020 2020  ch = False.     
+00000790: 2020 2020 2020 2020 2020 2065 6c69 6620             elif 
+000007a0: 7661 6c20 6973 204e 6f6e 653a 0a20 2020  val is None:.   
+000007b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000007c0: 2063 6f6e 7469 6e75 650a 2020 2020 2020   continue.      
+000007d0: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.
+000007e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000007f0: 2020 2020 7261 6973 6520 496e 7661 6c69      raise Invali
+00000800: 6450 6172 616d 6574 6572 2866 2275 6e68  dParameter(f"unh
+00000810: 616e 646c 6564 206f 7074 696f 6e20 7661  andled option va
+00000820: 6c75 6520 277b 7661 6c7d 2722 290a 2020  lue '{val}'").  
+00000830: 2020 2020 2020 2020 2020 2020 2020 6966                if
+00000840: 206e 6f74 206d 6174 6368 3a0a 2020 2020   not match:.    
+00000850: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000860: 6272 6561 6b0a 2020 2020 2020 2020 2020  break.          
+00000870: 2020 6966 206d 6174 6368 3a0a 2020 2020    if match:.    
+00000880: 2020 2020 2020 2020 2020 2020 636f 6c75              colu
+00000890: 6d6e 732e 6170 7065 6e64 2863 6f6c 756d  mns.append(colum
+000008a0: 6e29 0a0a 2020 2020 2020 2020 6966 206e  n)..        if n
+000008b0: 6f74 2063 6f6c 756d 6e73 3a0a 2020 2020  ot columns:.    
+000008c0: 2020 2020 2020 2020 7261 6973 6520 496e          raise In
+000008d0: 7661 6c69 6450 6172 616d 6574 6572 2866  validParameter(f
+000008e0: 227b 6e61 6d65 737d 2064 6f65 7320 6e6f  "{names} does no
+000008f0: 7420 6578 6973 7420 696e 2044 6174 6146  t exist in DataF
+00000900: 7261 6d65 2229 0a0a 2020 2020 2020 2020  rame")..        
+00000910: 7265 7475 726e 2063 6f6c 756d 6e73 0a0a  return columns..
+00000920: 2020 2020 4073 7461 7469 636d 6574 686f      @staticmetho
+00000930: 640a 2020 2020 6465 6620 6765 745f 6f70  d.    def get_op
+00000940: 7469 6f6e 5f76 616c 7565 7328 6466 2c20  tion_values(df, 
+00000950: 6e61 6d65 293a 0a20 2020 2020 2020 2022  name):.        "
+00000960: 2222 5265 7475 726e 2074 6865 206f 7074  ""Return the opt
+00000970: 696f 6e20 7661 6c75 6573 2070 6172 7365  ion values parse
+00000980: 6420 6672 6f6d 2074 6865 2063 6f6c 756d  d from the colum
+00000990: 6e20 6e61 6d65 732e 0a0a 2020 2020 2020  n names...      
+000009a0: 2020 5061 7261 6d65 7465 7273 0a20 2020    Parameters.   
+000009b0: 2020 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a       ----------.
+000009c0: 2020 2020 2020 2020 6466 203a 2070 642e          df : pd.
+000009d0: 4461 7461 4672 616d 650a 2020 2020 2020  DataFrame.      
+000009e0: 2020 6e61 6d65 203a 2073 7472 0a0a 2020    name : str..  
+000009f0: 2020 2020 2020 5265 7475 726e 730a 2020        Returns.  
+00000a00: 2020 2020 2020 2d2d 2d2d 2d2d 2d0a 2020        -------.  
+00000a10: 2020 2020 2020 6c69 7374 0a0a 2020 2020        list..    
+00000a20: 2020 2020 2222 220a 2020 2020 2020 2020      """.        
+00000a30: 7661 6c75 6573 203d 205b 5d0a 2020 2020  values = [].    
+00000a40: 2020 2020 666f 7220 636f 6c75 6d6e 2069      for column i
+00000a50: 6e20 6466 2e63 6f6c 756d 6e73 3a0a 2020  n df.columns:.  
+00000a60: 2020 2020 2020 2020 2020 636f 6c20 3d20            col = 
+00000a70: 636f 6c75 6d6e 0a20 2020 2020 2020 2020  column.         
+00000a80: 2020 2069 6e64 6578 203d 2063 6f6c 756d     index = colum
+00000a90: 6e2e 6669 6e64 2822 205b 2229 0a20 2020  n.find(" [").   
+00000aa0: 2020 2020 2020 2020 2069 6620 696e 6465           if inde
+00000ab0: 7820 213d 202d 313a 0a20 2020 2020 2020  x != -1:.       
 00000ac0: 2020 2020 2020 2020 2063 6f6c 203d 2063           col = c
-00000ad0: 6f6c 756d 6e0d 0a20 2020 2020 2020 2020  olumn..         
-00000ae0: 2020 2069 6e64 6578 203d 2063 6f6c 756d     index = colum
-00000af0: 6e2e 6669 6e64 2822 205b 2229 0d0a 2020  n.find(" [")..  
-00000b00: 2020 2020 2020 2020 2020 6966 2069 6e64            if ind
-00000b10: 6578 2021 3d20 2d31 3a0d 0a20 2020 2020  ex != -1:..     
-00000b20: 2020 2020 2020 2020 2020 2063 6f6c 203d             col =
-00000b30: 2063 6f6c 756d 6e5b 3a69 6e64 6578 5d0d   column[:index].
-00000b40: 0a20 2020 2020 2020 2020 2020 2023 205b  .            # [
-00000b50: 6e61 6d65 2c20 6f70 7469 6f6e 312c 206f  name, option1, o
-00000b60: 7074 696f 6e32 2c20 2e2e 2e5d 0d0a 2020  ption2, ...]..  
-00000b70: 2020 2020 2020 2020 2020 6669 656c 6473            fields
-00000b80: 203d 2056 616c 7565 5374 6f72 6167 6542   = ValueStorageB
-00000b90: 6173 652e 6765 745f 6669 656c 6473 2863  ase.get_fields(c
-00000ba0: 6f6c 2c20 6e61 6d65 290d 0a20 2020 2020  ol, name)..     
-00000bb0: 2020 2020 2020 205f 6e61 6d65 203d 2066         _name = f
-00000bc0: 6965 6c64 735b 305d 0d0a 2020 2020 2020  ields[0]..      
-00000bd0: 2020 2020 2020 6966 205f 6e61 6d65 2021        if _name !
-00000be0: 3d20 6e61 6d65 3a0d 0a20 2020 2020 2020  = name:..       
-00000bf0: 2020 2020 2020 2020 2063 6f6e 7469 6e75           continu
-00000c00: 650d 0a20 2020 2020 2020 2020 2020 2076  e..            v
-00000c10: 616c 7565 7320 2b3d 2066 6965 6c64 735b  alues += fields[
-00000c20: 313a 5d0d 0a0d 0a20 2020 2020 2020 2069  1:]....        i
-00000c30: 6620 6e6f 7420 7661 6c75 6573 3a0d 0a20  f not values:.. 
-00000c40: 2020 2020 2020 2020 2020 2072 6169 7365             raise
-00000c50: 2049 6e76 616c 6964 5061 7261 6d65 7465   InvalidParamete
-00000c60: 7228 6622 7b6e 616d 657d 2064 6f65 7320  r(f"{name} does 
-00000c70: 6e6f 7420 6578 6973 7420 696e 2044 6174  not exist in Dat
-00000c80: 6146 7261 6d65 2229 0d0a 0d0a 2020 2020  aFrame")....    
-00000c90: 2020 2020 7265 7475 726e 2076 616c 7565      return value
-00000ca0: 730d 0a0d 0a20 2020 2040 7374 6174 6963  s....    @static
-00000cb0: 6d65 7468 6f64 0d0a 2020 2020 6465 6620  method..    def 
-00000cc0: 6765 745f 6669 656c 6473 2863 6f6c 2c20  get_fields(col, 
-00000cd0: 6e61 6d65 293a 0d0a 2020 2020 2020 2020  name):..        
-00000ce0: 2320 4861 6e64 6c65 2063 6173 6520 7768  # Handle case wh
-00000cf0: 6572 6520 7468 6520 6e61 6d65 2065 6e64  ere the name end
-00000d00: 7320 7769 7468 2070 6172 7420 6f66 2074  s with part of t
-00000d10: 6865 2044 454c 494d 4954 4552 2e0d 0a20  he DELIMITER... 
-00000d20: 2020 2020 2020 2063 6f6c 5f74 6d70 203d         col_tmp =
-00000d30: 2063 6f6c 2e72 6570 6c61 6365 286e 616d   col.replace(nam
-00000d40: 652c 2022 222c 2031 290d 0a20 2020 2020  e, "", 1)..     
-00000d50: 2020 2066 6965 6c64 7320 3d20 636f 6c5f     fields = col_
-00000d60: 746d 702e 7370 6c69 7428 5661 6c75 6553  tmp.split(ValueS
-00000d70: 746f 7261 6765 4261 7365 2e44 454c 494d  torageBase.DELIM
-00000d80: 4954 4552 295b 313a 5d0d 0a20 2020 2020  ITER)[1:]..     
-00000d90: 2020 2066 6965 6c64 732e 696e 7365 7274     fields.insert
-00000da0: 2830 2c20 6e61 6d65 290d 0a20 2020 2020  (0, name)..     
-00000db0: 2020 2072 6574 7572 6e20 6669 656c 6473     return fields
-00000dc0: 0d0a 0d0a 2020 2020 4061 6263 2e61 6273  ....    @abc.abs
-00000dd0: 7472 6163 746d 6574 686f 640d 0a20 2020  tractmethod..   
-00000de0: 2064 6566 2069 735f 6e61 6e28 7365 6c66   def is_nan(self
-00000df0: 293a 0d0a 2020 2020 2020 2020 2222 2252  ):..        """R
-00000e00: 6574 7572 6e20 5472 7565 2069 6620 7468  eturn True if th
-00000e10: 6520 7661 6c75 6520 6973 204e 614e 2e0d  e value is NaN..
-00000e20: 0a0d 0a20 2020 2020 2020 2052 6574 7572  ...        Retur
-00000e30: 6e73 0d0a 2020 2020 2020 2020 2d2d 2d2d  ns..        ----
-00000e40: 2d2d 2d0d 0a20 2020 2020 2020 2062 6f6f  ---..        boo
-00000e50: 6c0d 0a0d 0a20 2020 2020 2020 2022 2222  l....        """
-00000e60: 0d0a 0d0a 2020 2020 4061 6263 2e61 6273  ....    @abc.abs
-00000e70: 7472 6163 746d 6574 686f 640d 0a20 2020  tractmethod..   
-00000e80: 2064 6566 206d 616b 655f 636f 6c75 6d6e   def make_column
-00000e90: 7328 7365 6c66 293a 0d0a 2020 2020 2020  s(self):..      
-00000ea0: 2020 2222 2252 6574 7572 6e20 6120 6c69    """Return a li
-00000eb0: 7374 206f 6620 636f 6c75 6d6e 206e 616d  st of column nam
-00000ec0: 6573 0d0a 0d0a 2020 2020 2020 2020 5265  es....        Re
-00000ed0: 7475 726e 730d 0a20 2020 2020 2020 202d  turns..        -
-00000ee0: 2d2d 2d2d 2d2d 0d0a 2020 2020 2020 2020  ------..        
-00000ef0: 6c69 7374 0d0a 0d0a 2020 2020 2020 2020  list....        
-00000f00: 2222 220d 0a0d 0a20 2020 2040 7072 6f70  """....    @prop
-00000f10: 6572 7479 0d0a 2020 2020 6465 6620 6e61  erty..    def na
-00000f20: 6d65 2873 656c 6629 3a0d 0a20 2020 2020  me(self):..     
-00000f30: 2020 2072 6574 7572 6e20 7365 6c66 2e5f     return self._
-00000f40: 6e61 6d65 0d0a 0d0a 2020 2020 4070 726f  name....    @pro
-00000f50: 7065 7274 790d 0a20 2020 2040 6162 632e  perty..    @abc.
-00000f60: 6162 7374 7261 6374 6d65 7468 6f64 0d0a  abstractmethod..
-00000f70: 2020 2020 6465 6620 6e75 6d5f 636f 6c75      def num_colu
-00000f80: 6d6e 7328 7365 6c66 293a 0d0a 2020 2020  mns(self):..    
-00000f90: 2020 2020 2222 2252 6574 7572 6e20 7468      """Return th
-00000fa0: 6520 6e75 6d62 6572 206f 6620 636f 6c75  e number of colu
-00000fb0: 6d6e 7320 696e 2074 6865 2064 6174 612e  mns in the data.
-00000fc0: 0d0a 0d0a 2020 2020 2020 2020 5265 7475  ....        Retu
-00000fd0: 726e 730d 0a20 2020 2020 2020 202d 2d2d  rns..        ---
-00000fe0: 2d2d 2d2d 0d0a 2020 2020 2020 2020 696e  ----..        in
-00000ff0: 740d 0a0d 0a20 2020 2020 2020 2022 2222  t....        """
-00001000: 0d0a 0d0a 2020 2020 4061 6263 2e61 6273  ....    @abc.abs
-00001010: 7472 6163 746d 6574 686f 640d 0a20 2020  tractmethod..   
-00001020: 2064 6566 2073 6574 5f65 6c65 6d65 6e74   def set_element
-00001030: 5f70 726f 7065 7274 7928 7365 6c66 2c20  _property(self, 
-00001040: 7072 6f70 293a 0d0a 2020 2020 2020 2020  prop):..        
-00001050: 2222 2253 6574 2074 6865 2065 6c65 6d65  """Set the eleme
-00001060: 6e74 2070 726f 7065 7274 7920 6e61 6d65  nt property name
-00001070: 2e0d 0a0d 0a20 2020 2020 2020 2050 6172  .....        Par
-00001080: 616d 6574 6572 730d 0a20 2020 2020 2020  ameters..       
-00001090: 202d 2d2d 2d2d 2d2d 2d2d 2d0d 0a20 2020   ----------..   
-000010a0: 2020 2020 2070 726f 7020 3a20 7374 720d       prop : str.
-000010b0: 0a0d 0a20 2020 2020 2020 2022 2222 0d0a  ...        """..
-000010c0: 0d0a 2020 2020 4061 6263 2e61 6273 7472  ..    @abc.abstr
-000010d0: 6163 746d 6574 686f 640d 0a20 2020 2064  actmethod..    d
-000010e0: 6566 2073 6574 5f6e 616d 6528 7365 6c66  ef set_name(self
-000010f0: 2c20 6e61 6d65 293a 0d0a 2020 2020 2020  , name):..      
-00001100: 2020 2222 2253 6574 2074 6865 206e 616d    """Set the nam
-00001110: 652e 0d0a 0d0a 2020 2020 2020 2020 5061  e.....        Pa
-00001120: 7261 6d65 7465 7273 0d0a 2020 2020 2020  rameters..      
-00001130: 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0d0a 2020    ----------..  
-00001140: 2020 2020 2020 6e61 6d65 203a 2073 7472        name : str
-00001150: 0d0a 0d0a 2020 2020 2020 2020 2222 220d  ....        """.
-00001160: 0a0d 0a20 2020 2040 6162 632e 6162 7374  ...    @abc.abst
-00001170: 7261 6374 6d65 7468 6f64 0d0a 2020 2020  ractmethod..    
-00001180: 6465 6620 7365 745f 6e61 6e28 7365 6c66  def set_nan(self
-00001190: 293a 0d0a 2020 2020 2020 2020 2222 2253  ):..        """S
-000011a0: 6574 2074 6865 2076 616c 7565 2074 6f20  et the value to 
-000011b0: 4e61 4e20 6f72 2065 7175 6976 616c 656e  NaN or equivalen
-000011c0: 742e 2222 220d 0a0d 0a20 2020 2040 6162  t."""....    @ab
-000011d0: 632e 6162 7374 7261 6374 6d65 7468 6f64  c.abstractmethod
-000011e0: 0d0a 2020 2020 6465 6620 7365 745f 7661  ..    def set_va
-000011f0: 6c75 6528 7365 6c66 2c20 7661 6c75 6529  lue(self, value)
-00001200: 3a0d 0a20 2020 2020 2020 2022 2222 5365  :..        """Se
-00001210: 7420 7468 6520 7661 6c75 6520 6672 6f6d  t the value from
-00001220: 2061 6e6f 7468 6572 2069 6e73 7461 6e63   another instanc
-00001230: 652e 0d0a 0d0a 2020 2020 2020 2020 5061  e.....        Pa
-00001240: 7261 6d65 7465 7273 0d0a 2020 2020 2020  rameters..      
-00001250: 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0d0a 2020    ----------..  
-00001260: 2020 2020 2020 7661 6c75 6520 3a20 5661        value : Va
-00001270: 6c75 6553 746f 7261 6765 4261 7365 0d0a  lueStorageBase..
-00001280: 0d0a 2020 2020 2020 2020 2222 220d 0a0d  ..        """...
-00001290: 0a20 2020 2040 6162 632e 6162 7374 7261  .    @abc.abstra
-000012a0: 6374 6d65 7468 6f64 0d0a 2020 2020 6465  ctmethod..    de
-000012b0: 6620 7365 745f 7661 6c75 655f 6672 6f6d  f set_value_from
-000012c0: 5f72 6177 2873 656c 662c 2076 616c 7565  _raw(self, value
-000012d0: 293a 0d0a 2020 2020 2020 2020 2222 2253  ):..        """S
-000012e0: 6574 2074 6865 2076 616c 7565 2066 726f  et the value fro
-000012f0: 6d20 6120 6e65 7720 7261 7720 7661 6c75  m a new raw valu
-00001300: 6520 6672 6f6d 206f 7065 6e64 7373 6469  e from opendssdi
-00001310: 7265 6374 2e0d 0a0d 0a20 2020 2020 2020  rect.....       
-00001320: 2050 6172 616d 6574 6572 730d 0a20 2020   Parameters..   
-00001330: 2020 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0d       ----------.
-00001340: 0a20 2020 2020 2020 2076 616c 7565 203a  .        value :
-00001350: 206c 6973 7420 7c20 666c 6f61 7420 7c20   list | float | 
-00001360: 636f 6d70 6c65 780d 0a0d 0a20 2020 2020  complex....     
-00001370: 2020 2022 2222 0d0a 0d0a 2020 2020 4070     """....    @p
-00001380: 726f 7065 7274 790d 0a20 2020 2040 6162  roperty..    @ab
-00001390: 632e 6162 7374 7261 6374 6d65 7468 6f64  c.abstractmethod
-000013a0: 0d0a 2020 2020 6465 6620 7661 6c75 6528  ..    def value(
-000013b0: 7365 6c66 293a 0d0a 2020 2020 2020 2020  self):..        
-000013c0: 2222 2252 6574 7572 6e20 7468 6520 7661  """Return the va
-000013d0: 6c75 652e 0d0a 0d0a 2020 2020 2020 2020  lue.....        
-000013e0: 5265 7475 726e 730d 0a20 2020 2020 2020  Returns..       
-000013f0: 202d 2d2d 2d2d 2d2d 0d0a 2020 2020 2020   -------..      
-00001400: 2020 6c69 7374 207c 2066 6c6f 6174 207c    list | float |
-00001410: 2063 6f6d 706c 6578 0d0a 0d0a 2020 2020   complex....    
-00001420: 2020 2020 2222 220d 0a0d 0a20 2020 2040      """....    @
-00001430: 7072 6f70 6572 7479 0d0a 2020 2020 4061  property..    @a
-00001440: 6263 2e61 6273 7472 6163 746d 6574 686f  bc.abstractmetho
-00001450: 640d 0a20 2020 2064 6566 2076 616c 7565  d..    def value
-00001460: 5f74 7970 6528 7365 6c66 293a 0d0a 2020  _type(self):..  
-00001470: 2020 2020 2020 2222 2252 6574 7572 6e20        """Return 
-00001480: 7468 6520 7479 7065 206f 6620 7661 6c75  the type of valu
-00001490: 6520 6265 696e 6720 7374 6f72 6564 2e0d  e being stored..
-000014a0: 0a0d 0a20 2020 2020 2020 2052 6574 7572  ...        Retur
-000014b0: 6e73 0d0a 2020 2020 2020 2020 2d2d 2d2d  ns..        ----
-000014c0: 2d2d 2d0d 0a20 2020 2020 2020 2054 7970  ---..        Typ
-000014d0: 650d 0a0d 0a20 2020 2020 2020 2022 2222  e....        """
-000014e0: 0d0a 0d0a 0d0a 636c 6173 7320 5661 6c75  ......class Valu
-000014f0: 6542 794c 6973 7428 5661 6c75 6553 746f  eByList(ValueSto
-00001500: 7261 6765 4261 7365 293a 0d0a 2020 2020  rageBase):..    
-00001510: 2222 2222 5374 6f72 6573 2061 206c 6973  """"Stores a lis
-00001520: 7420 6f66 206c 6973 7473 206f 6620 6e75  t of lists of nu
-00001530: 6d62 6572 7320 6279 2061 6e20 6172 6269  mbers by an arbi
-00001540: 7472 6172 7920 7375 6666 6978 2e20 5468  trary suffix. Th
-00001550: 6973 2069 7320 6120 6765 6e65 7269 6320  is is a generic 
-00001560: 6d65 7468 6f64 2074 6f20 6861 6e64 6c65  method to handle
-00001570: 206c 6973 7473 2072 6574 7572 6e65 6420   lists returned 
-00001580: 6672 6f6d 0d0a 2020 2020 6120 6675 6e63  from..    a func
-00001590: 7469 6f6e 2063 616c 6c2e 2041 6e20 6578  tion call. An ex
-000015a0: 616d 706c 6520 776f 756c 6420 6265 2072  ample would be r
-000015b0: 6574 7572 6e65 6420 7661 6c75 6573 2022  eturned values "
-000015c0: 7461 7073 2220 6675 6e63 7469 6f6e 2066  taps" function f
-000015d0: 6f72 2074 7261 6e73 666f 726d 6572 2065  or transformer e
-000015e0: 6c65 6d65 6e74 732e 2054 6865 2063 6c61  lements. The cla
-000015f0: 7373 2063 616e 2062 650d 0a20 2020 2075  ss can be..    u
-00001600: 7365 6420 666f 7220 616e 7920 6d65 7468  sed for any meth
-00001610: 6f64 7320 7468 6174 2072 6574 7572 6e20  ods that return 
-00001620: 6120 6c69 7374 2e0d 0a20 2020 2022 2222  a list...    """
-00001630: 0d0a 2020 2020 6465 6620 5f5f 696e 6974  ..    def __init
-00001640: 5f5f 2873 656c 662c 206e 616d 652c 2070  __(self, name, p
-00001650: 726f 702c 2076 616c 7565 732c 206c 6162  rop, values, lab
-00001660: 656c 5f73 7566 6669 7865 7329 3a0d 0a20  el_suffixes):.. 
-00001670: 2020 2020 2020 2022 2222 436f 6e73 7472         """Constr
-00001680: 7563 746f 7220 666f 7220 5661 6c75 6542  uctor for ValueB
-00001690: 794c 6162 656c 0d0a 0d0a 2020 2020 2020  yLabel....      
-000016a0: 2020 5061 7261 6d65 7465 7273 0d0a 2020    Parameters..  
-000016b0: 2020 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d        ----------
-000016c0: 0d0a 2020 2020 2020 2020 6e61 6d65 203a  ..        name :
-000016d0: 2073 7472 0d0a 2020 2020 2020 2020 7072   str..        pr
-000016e0: 6f70 203a 2073 7472 0d0a 2020 2020 2020  op : str..      
-000016f0: 2020 6c61 6265 6c5f 7072 6566 6978 203a    label_prefix :
-00001700: 2073 7472 0d0a 2020 2020 2020 2020 2020   str..          
-00001710: 2020 5465 7874 2074 6f20 7573 6520 6173    Text to use as
-00001720: 2061 2070 7265 6669 7820 666f 7220 636f   a prefix for co
-00001730: 6c75 6d6e 206c 6162 656c 732e 2045 783a  lumn labels. Ex:
-00001740: 2050 6861 7365 0d0a 2020 2020 2020 2020   Phase..        
-00001750: 6c61 6265 6c73 203a 206c 6973 740d 0a20  labels : list.. 
-00001760: 2020 2020 2020 2020 2020 206c 6973 7420             list 
-00001770: 6f66 2073 7472 0d0a 2020 2020 2020 2020  of str..        
-00001780: 7661 6c75 6573 203a 206c 6973 740d 0a20  values : list.. 
-00001790: 2020 2020 2020 2020 2020 2050 6169 7273             Pairs
-000017a0: 206f 6620 7661 6c75 6573 2074 6861 7420   of values that 
-000017b0: 6361 6e20 6265 2069 6e74 6572 7072 6574  can be interpret
-000017c0: 6564 2061 7320 636f 6d70 6c65 7820 6e75  ed as complex nu
-000017d0: 6d62 6572 732e 0d0a 0d0a 2020 2020 2020  mbers.....      
-000017e0: 2020 2222 220d 0a20 2020 2020 2020 2073    """..        s
-000017f0: 7570 6572 2829 2e5f 5f69 6e69 745f 5f28  uper().__init__(
-00001800: 290d 0a20 2020 2020 2020 2073 656c 662e  )..        self.
-00001810: 5f6e 616d 6520 3d20 6e61 6d65 0d0a 2020  _name = name..  
-00001820: 2020 2020 2020 7365 6c66 2e5f 7072 6f70        self._prop
-00001830: 203d 2070 726f 700d 0a20 2020 2020 2020   = prop..       
-00001840: 2073 656c 662e 5f6c 6162 656c 7320 3d20   self._labels = 
-00001850: 5b5d 0d0a 2020 2020 2020 2020 7365 6c66  []..        self
-00001860: 2e5f 7661 6c75 655f 7479 7065 203d 204e  ._value_type = N
-00001870: 6f6e 650d 0a20 2020 2020 2020 2073 656c  one..        sel
-00001880: 662e 5f76 616c 7565 203d 205b 5d0d 0a20  f._value = [].. 
-00001890: 2020 2020 2020 2061 7373 6572 7420 2869         assert (i
-000018a0: 7369 6e73 7461 6e63 6528 7661 6c75 6573  sinstance(values
-000018b0: 2c20 6c69 7374 2920 616e 6420 6c65 6e28  , list) and len(
-000018c0: 7661 6c75 6573 2920 3d3d 206c 656e 286c  values) == len(l
-000018d0: 6162 656c 5f73 7566 6669 7865 7329 292c  abel_suffixes)),
-000018e0: 205c 0d0a 2020 2020 2020 2020 2020 2020   \..            
-000018f0: 2722 7661 6c75 6573 2220 616e 6420 226c  '"values" and "l
-00001900: 6162 656c 5f73 7566 6669 7865 7322 2073  abel_suffixes" s
-00001910: 686f 756c 6420 6265 206c 6973 7473 206f  hould be lists o
-00001920: 6620 6571 7561 6c20 6c65 6e67 7468 7327  f equal lengths'
-00001930: 0d0a 2020 2020 2020 2020 666f 7220 7661  ..        for va
-00001940: 6c2c 206c 6162 5f73 7566 2069 6e20 7a69  l, lab_suf in zi
-00001950: 7028 7661 6c75 6573 2c20 6c61 6265 6c5f  p(values, label_
-00001960: 7375 6666 6978 6573 293a 0d0a 2020 2020  suffixes):..    
-00001970: 2020 2020 2020 2020 6c61 6265 6c20 3d20          label = 
-00001980: 7072 6f70 202b 2073 656c 662e 4445 4c49  prop + self.DELI
-00001990: 4d49 5445 5220 2b20 6c61 625f 7375 660d  MITER + lab_suf.
-000019a0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-000019b0: 662e 5f6c 6162 656c 732e 6170 7065 6e64  f._labels.append
-000019c0: 286c 6162 656c 290d 0a20 2020 2020 2020  (label)..       
-000019d0: 2020 2020 2073 656c 662e 5f76 616c 7565       self._value
-000019e0: 2e61 7070 656e 6428 7661 6c29 0d0a 2020  .append(val)..  
-000019f0: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
-00001a00: 662e 5f76 616c 7565 5f74 7970 6520 6973  f._value_type is
-00001a10: 204e 6f6e 653a 0d0a 2020 2020 2020 2020   None:..        
-00001a20: 2020 2020 2020 2020 7365 6c66 2e5f 7661          self._va
-00001a30: 6c75 655f 7479 7065 203d 2074 7970 6528  lue_type = type(
-00001a40: 7661 6c29 0d0a 0d0a 2020 2020 6465 6620  val)....    def 
-00001a50: 5f5f 6961 6464 5f5f 2873 656c 662c 206f  __iadd__(self, o
-00001a60: 7468 6572 293a 0d0a 2020 2020 2020 2020  ther):..        
-00001a70: 666f 7220 6920 696e 2072 616e 6765 286c  for i in range(l
-00001a80: 656e 2873 656c 662e 5f76 616c 7565 2929  en(self._value))
-00001a90: 3a0d 0a20 2020 2020 2020 2020 2020 2073  :..            s
-00001aa0: 656c 662e 5f76 616c 7565 5b69 5d20 2b3d  elf._value[i] +=
-00001ab0: 206f 7468 6572 2e76 616c 7565 5b69 5d0d   other.value[i].
-00001ac0: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
-00001ad0: 7365 6c66 0d0a 0d0a 2020 2020 6465 6620  self....    def 
-00001ae0: 5f5f 6774 5f5f 2873 656c 662c 206f 7468  __gt__(self, oth
-00001af0: 6572 293a 0d0a 2020 2020 2020 2020 2320  er):..        # 
-00001b00: 544f 444f 0d0a 2020 2020 2020 2020 7265  TODO..        re
-00001b10: 7475 726e 2073 756d 2873 656c 662e 5f76  turn sum(self._v
-00001b20: 616c 7565 2920 3e20 7375 6d28 6f74 6865  alue) > sum(othe
-00001b30: 722e 7661 6c75 6529 0d0a 0d0a 2020 2020  r.value)....    
-00001b40: 6465 6620 6973 5f6e 616e 2873 656c 6629  def is_nan(self)
-00001b50: 3a0d 0a20 2020 2020 2020 2069 6620 6e70  :..        if np
-00001b60: 2e69 7373 7562 6474 7970 6528 7365 6c66  .issubdtype(self
-00001b70: 2e5f 7661 6c75 655f 7479 7065 2c20 6e70  ._value_type, np
-00001b80: 2e69 6e74 3634 293a 0d0a 2020 2020 2020  .int64):..      
-00001b90: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
-00001ba0: 662e 5f76 616c 7565 5b30 5d20 3d3d 2049  f._value[0] == I
-00001bb0: 4e54 4547 4552 5f4e 414e 0d0a 2020 2020  NTEGER_NAN..    
-00001bc0: 2020 2020 7265 7475 726e 206e 702e 6973      return np.is
-00001bd0: 6e61 6e28 7365 6c66 2e5f 7661 6c75 655b  nan(self._value[
-00001be0: 305d 290d 0a0d 0a20 2020 2064 6566 206d  0])....    def m
-00001bf0: 616b 655f 636f 6c75 6d6e 7328 7365 6c66  ake_columns(self
-00001c00: 293a 0d0a 2020 2020 2020 2020 7265 7475  ):..        retu
-00001c10: 726e 205b 0d0a 2020 2020 2020 2020 2020  rn [..          
-00001c20: 2020 7365 6c66 2e44 454c 494d 4954 4552    self.DELIMITER
-00001c30: 2e6a 6f69 6e28 2873 656c 662e 5f6e 616d  .join((self._nam
-00001c40: 652c 2066 227b 787d 2229 2920 666f 7220  e, f"{x}")) for 
-00001c50: 7820 696e 2073 656c 662e 5f6c 6162 656c  x in self._label
-00001c60: 730d 0a20 2020 2020 2020 205d 0d0a 0d0a  s..        ]....
-00001c70: 2020 2020 4070 726f 7065 7274 790d 0a20      @property.. 
-00001c80: 2020 2064 6566 206e 756d 5f63 6f6c 756d     def num_colum
-00001c90: 6e73 2873 656c 6629 3a0d 0a20 2020 2020  ns(self):..     
-00001ca0: 2020 2072 6574 7572 6e20 6c65 6e28 7365     return len(se
-00001cb0: 6c66 2e5f 6c61 6265 6c73 290d 0a0d 0a20  lf._labels).... 
-00001cc0: 2020 2064 6566 2073 6574 5f65 6c65 6d65     def set_eleme
-00001cd0: 6e74 5f70 726f 7065 7274 7928 7365 6c66  nt_property(self
-00001ce0: 2c20 7072 6f70 293a 0d0a 2020 2020 2020  , prop):..      
-00001cf0: 2020 7365 6c66 2e5f 7072 6f70 203d 2070    self._prop = p
-00001d00: 726f 700d 0a0d 0a20 2020 2020 2020 2023  rop....        #
-00001d10: 2055 7064 6174 6520 7468 6520 7072 6f70   Update the prop
-00001d20: 6572 7479 2069 6e73 6964 6520 6561 6368  erty inside each
-00001d30: 206c 6162 656c 2e0d 0a20 2020 2020 2020   label...       
-00001d40: 2066 6f72 2069 2c20 6c61 6265 6c20 696e   for i, label in
-00001d50: 2065 6e75 6d65 7261 7465 2873 656c 662e   enumerate(self.
-00001d60: 5f6c 6162 656c 7329 3a0d 0a20 2020 2020  _labels):..     
-00001d70: 2020 2020 2020 2066 6965 6c64 7320 3d20         fields = 
-00001d80: 6c61 6265 6c2e 7370 6c69 7428 7365 6c66  label.split(self
-00001d90: 2e44 454c 494d 4954 4552 290d 0a20 2020  .DELIMITER)..   
-00001da0: 2020 2020 2020 2020 2061 7373 6572 7420           assert 
-00001db0: 6c65 6e28 6669 656c 6473 2920 3d3d 2032  len(fields) == 2
-00001dc0: 0d0a 2020 2020 2020 2020 2020 2020 6669  ..            fi
-00001dd0: 656c 6473 5b30 5d20 3d20 7072 6f70 0d0a  elds[0] = prop..
-00001de0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00001df0: 2e5f 6c61 6265 6c73 5b69 5d20 3d20 7365  ._labels[i] = se
-00001e00: 6c66 2e44 454c 494d 4954 4552 2e6a 6f69  lf.DELIMITER.joi
-00001e10: 6e28 6669 656c 6473 290d 0a0d 0a20 2020  n(fields)....   
-00001e20: 2064 6566 2073 6574 5f6e 616d 6528 7365   def set_name(se
-00001e30: 6c66 2c20 6e61 6d65 293a 0d0a 2020 2020  lf, name):..    
-00001e40: 2020 2020 7365 6c66 2e5f 6e61 6d65 203d      self._name =
-00001e50: 206e 616d 650d 0a0d 0a20 2020 2064 6566   name....    def
-00001e60: 2073 6574 5f6e 616e 2873 656c 6629 3a0d   set_nan(self):.
-00001e70: 0a20 2020 2020 2020 2066 6f72 2069 2069  .        for i i
-00001e80: 6e20 7261 6e67 6528 6c65 6e28 7365 6c66  n range(len(self
-00001e90: 2e5f 7661 6c75 6529 293a 0d0a 2020 2020  ._value)):..    
-00001ea0: 2020 2020 2020 2020 7365 6c66 2e5f 7661          self._va
-00001eb0: 6c75 655b 695d 203d 206e 702e 4e61 4e0d  lue[i] = np.NaN.
-00001ec0: 0a0d 0a20 2020 2064 6566 2073 6574 5f76  ...    def set_v
-00001ed0: 616c 7565 2873 656c 662c 2076 616c 7565  alue(self, value
-00001ee0: 293a 0d0a 2020 2020 2020 2020 7365 6c66  ):..        self
-00001ef0: 2e5f 7661 6c75 6520 3d20 7661 6c75 650d  ._value = value.
-00001f00: 0a20 2020 2020 2020 2069 6620 6e6f 7420  .        if not 
-00001f10: 6973 696e 7374 616e 6365 2876 616c 7565  isinstance(value
-00001f20: 5b30 5d2c 2073 656c 662e 5f76 616c 7565  [0], self._value
-00001f30: 5f74 7970 6529 3a0d 0a20 2020 2020 2020  _type):..       
-00001f40: 2020 2020 2073 656c 662e 5f76 616c 7565       self._value
-00001f50: 5f74 7970 6520 3d20 7479 7065 2876 616c  _type = type(val
-00001f60: 7565 5b30 5d29 0d0a 0d0a 2020 2020 6465  ue[0])....    de
-00001f70: 6620 7365 745f 7661 6c75 655f 6672 6f6d  f set_value_from
-00001f80: 5f72 6177 2873 656c 662c 2076 616c 7565  _raw(self, value
-00001f90: 293a 0d0a 2020 2020 2020 2020 7365 6c66  ):..        self
-00001fa0: 2e5f 7661 6c75 6520 3d20 7661 6c75 650d  ._value = value.
-00001fb0: 0a0d 0a20 2020 2040 7072 6f70 6572 7479  ...    @property
-00001fc0: 0d0a 2020 2020 6465 6620 7661 6c75 6528  ..    def value(
-00001fd0: 7365 6c66 293a 0d0a 2020 2020 2020 2020  self):..        
-00001fe0: 7265 7475 726e 2073 656c 662e 5f76 616c  return self._val
-00001ff0: 7565 0d0a 0d0a 2020 2020 4070 726f 7065  ue....    @prope
-00002000: 7274 790d 0a20 2020 2064 6566 2076 616c  rty..    def val
-00002010: 7565 5f74 7970 6528 7365 6c66 293a 0d0a  ue_type(self):..
-00002020: 2020 2020 2020 2020 7265 7475 726e 2073          return s
-00002030: 656c 662e 5f76 616c 7565 5f74 7970 650d  elf._value_type.
-00002040: 0a0d 0a0d 0a63 6c61 7373 2056 616c 7565  .....class Value
-00002050: 4279 4e75 6d62 6572 2856 616c 7565 5374  ByNumber(ValueSt
-00002060: 6f72 6167 6542 6173 6529 3a0d 0a20 2020  orageBase):..   
-00002070: 2022 2222 5374 6f72 6573 2061 206c 6973   """Stores a lis
-00002080: 7420 6f66 206e 756d 6265 7273 2066 6f72  t of numbers for
-00002090: 2061 6e20 656c 656d 656e 742f 7072 6f70   an element/prop
-000020a0: 6572 7479 2e22 2222 0d0a 2020 2020 6465  erty."""..    de
-000020b0: 6620 5f5f 696e 6974 5f5f 2873 656c 662c  f __init__(self,
-000020c0: 206e 616d 652c 2070 726f 702c 2076 616c   name, prop, val
-000020d0: 7565 293a 0d0a 2020 2020 2020 2020 7375  ue):..        su
-000020e0: 7065 7228 292e 5f5f 696e 6974 5f5f 2829  per().__init__()
-000020f0: 0d0a 2020 2020 2020 2020 6173 7365 7274  ..        assert
-00002100: 206e 6f74 2069 7369 6e73 7461 6e63 6528   not isinstance(
-00002110: 7661 6c75 652c 206c 6973 7429 2c20 7374  value, list), st
-00002120: 7228 7661 6c75 6529 0d0a 2020 2020 2020  r(value)..      
-00002130: 2020 7365 6c66 2e5f 6e61 6d65 203d 206e    self._name = n
-00002140: 616d 650d 0a20 2020 2020 2020 2073 656c  ame..        sel
-00002150: 662e 5f70 726f 7020 3d20 7072 6f70 0d0a  f._prop = prop..
-00002160: 2020 2020 2020 2020 7365 6c66 2e5f 7661          self._va
-00002170: 6c75 655f 7479 7065 203d 2074 7970 6528  lue_type = type(
-00002180: 7661 6c75 6529 0d0a 2020 2020 2020 2020  value)..        
-00002190: 6966 2073 656c 662e 5f76 616c 7565 5f74  if self._value_t
-000021a0: 7970 6520 3d3d 2073 7472 3a0d 0a20 2020  ype == str:..   
-000021b0: 2020 2020 2020 2020 2072 6169 7365 2049           raise I
-000021c0: 6e76 616c 6964 436f 6e66 6967 7572 6174  nvalidConfigurat
-000021d0: 696f 6e28 0d0a 2020 2020 2020 2020 2020  ion(..          
-000021e0: 2020 2020 2020 6622 4461 7461 2065 7870        f"Data exp
-000021f0: 6f72 7420 6665 6174 7572 6520 646f 6573  ort feature does
-00002200: 206e 6f74 2073 7570 706f 7274 2073 7472   not support str
-00002210: 696e 6773 3a20 6e61 6d65 3d7b 6e61 6d65  ings: name={name
-00002220: 7d20 7072 6f70 3d7b 7072 6f70 7d20 7661  } prop={prop} va
-00002230: 6c75 653d 7b76 616c 7565 7d22 0d0a 2020  lue={value}"..  
-00002240: 2020 2020 2020 2020 2020 290d 0a20 2020            )..   
-00002250: 2020 2020 2073 656c 662e 5f76 616c 7565       self._value
-00002260: 203d 2076 616c 7565 0d0a 2020 2020 2020   = value..      
-00002270: 2020 7365 6c66 2e5f 6973 5f63 6f6d 706c    self._is_compl
-00002280: 6578 203d 2069 7369 6e73 7461 6e63 6528  ex = isinstance(
-00002290: 7365 6c66 2e5f 7661 6c75 655f 7479 7065  self._value_type
-000022a0: 2c20 636f 6d70 6c65 7829 0d0a 0d0a 2020  , complex)....  
-000022b0: 2020 6465 6620 5f5f 6961 6464 5f5f 2873    def __iadd__(s
-000022c0: 656c 662c 206f 7468 6572 293a 0d0a 2020  elf, other):..  
-000022d0: 2020 2020 2020 7365 6c66 2e5f 7661 6c75        self._valu
-000022e0: 6520 2b3d 206f 7468 6572 2e76 616c 7565  e += other.value
-000022f0: 0d0a 2020 2020 2020 2020 7265 7475 726e  ..        return
-00002300: 2073 656c 660d 0a0d 0a20 2020 2064 6566   self....    def
-00002310: 205f 5f67 745f 5f28 7365 6c66 2c20 6f74   __gt__(self, ot
-00002320: 6865 7229 3a0d 0a20 2020 2020 2020 2072  her):..        r
-00002330: 6574 7572 6e20 7365 6c66 2e5f 7661 6c75  eturn self._valu
-00002340: 6520 3e20 6f74 6865 722e 7661 6c75 650d  e > other.value.
-00002350: 0a0d 0a20 2020 2064 6566 2069 735f 6e61  ...    def is_na
-00002360: 6e28 7365 6c66 293a 0d0a 2020 2020 2020  n(self):..      
-00002370: 2020 6966 206e 702e 6973 7375 6264 7479    if np.issubdty
-00002380: 7065 2873 656c 662e 5f76 616c 7565 5f74  pe(self._value_t
-00002390: 7970 652c 206e 702e 696e 7436 3429 3a0d  ype, np.int64):.
-000023a0: 0a20 2020 2020 2020 2020 2020 2072 6574  .            ret
-000023b0: 7572 6e20 7365 6c66 2e5f 7661 6c75 6520  urn self._value 
-000023c0: 3d3d 2049 4e54 4547 4552 5f4e 414e 0d0a  == INTEGER_NAN..
-000023d0: 2020 2020 2020 2020 7265 7475 726e 206e          return n
-000023e0: 702e 6973 6e61 6e28 7365 6c66 2e5f 7661  p.isnan(self._va
-000023f0: 6c75 6529 0d0a 0d0a 2020 2020 6465 6620  lue)....    def 
-00002400: 6d61 6b65 5f63 6f6c 756d 6e73 2873 656c  make_columns(sel
-00002410: 6629 3a0d 0a20 2020 2020 2020 2072 6574  f):..        ret
-00002420: 7572 6e20 5b56 616c 7565 5374 6f72 6167  urn [ValueStorag
-00002430: 6542 6173 652e 4445 4c49 4d49 5445 522e  eBase.DELIMITER.
-00002440: 6a6f 696e 2828 7365 6c66 2e5f 6e61 6d65  join((self._name
-00002450: 2c20 7365 6c66 2e5f 7072 6f70 2929 5d0d  , self._prop))].
-00002460: 0a0d 0a20 2020 2040 7072 6f70 6572 7479  ...    @property
-00002470: 0d0a 2020 2020 6465 6620 6e75 6d5f 636f  ..    def num_co
-00002480: 6c75 6d6e 7328 7365 6c66 293a 0d0a 2020  lumns(self):..  
-00002490: 2020 2020 2020 7265 7475 726e 2031 0d0a        return 1..
-000024a0: 0d0a 2020 2020 6465 6620 7365 745f 656c  ..    def set_el
-000024b0: 656d 656e 745f 7072 6f70 6572 7479 2873  ement_property(s
-000024c0: 656c 662c 2070 726f 7029 3a0d 0a20 2020  elf, prop):..   
-000024d0: 2020 2020 2073 656c 662e 5f70 726f 7020       self._prop 
-000024e0: 3d20 7072 6f70 0d0a 0d0a 2020 2020 6465  = prop....    de
-000024f0: 6620 7365 745f 6e61 6d65 2873 656c 662c  f set_name(self,
-00002500: 206e 616d 6529 3a0d 0a20 2020 2020 2020   name):..       
-00002510: 2073 656c 662e 5f6e 616d 6520 3d20 6e61   self._name = na
-00002520: 6d65 0d0a 0d0a 2020 2020 6465 6620 7365  me....    def se
-00002530: 745f 6e61 6e28 7365 6c66 293a 0d0a 2020  t_nan(self):..  
-00002540: 2020 2020 2020 6966 206e 702e 6973 7375        if np.issu
-00002550: 6264 7479 7065 2873 656c 662e 5f76 616c  bdtype(self._val
-00002560: 7565 5f74 7970 652c 206e 702e 696e 7436  ue_type, np.int6
-00002570: 3429 3a0d 0a20 2020 2020 2020 2020 2020  4):..           
-00002580: 2073 656c 662e 5f76 616c 7565 203d 2049   self._value = I
-00002590: 4e54 4547 4552 5f4e 414e 0d0a 2020 2020  NTEGER_NAN..    
-000025a0: 2020 2020 656c 7365 3a0d 0a20 2020 2020      else:..     
-000025b0: 2020 2020 2020 2073 656c 662e 5f76 616c         self._val
-000025c0: 7565 203d 206e 702e 4e61 4e0d 0a0d 0a20  ue = np.NaN.... 
-000025d0: 2020 2064 6566 2073 6574 5f76 616c 7565     def set_value
-000025e0: 2873 656c 662c 2076 616c 7565 293a 0d0a  (self, value):..
-000025f0: 2020 2020 2020 2020 7365 6c66 2e5f 7661          self._va
-00002600: 6c75 6520 3d20 7661 6c75 650d 0a20 2020  lue = value..   
-00002610: 2020 2020 2069 6620 6e6f 7420 6973 696e       if not isin
-00002620: 7374 616e 6365 2876 616c 7565 2c20 7365  stance(value, se
-00002630: 6c66 2e5f 7661 6c75 655f 7479 7065 293a  lf._value_type):
-00002640: 0d0a 2020 2020 2020 2020 2020 2020 7365  ..            se
-00002650: 6c66 2e5f 7661 6c75 655f 7479 7065 203d  lf._value_type =
-00002660: 2074 7970 6528 7661 6c75 6529 0d0a 0d0a   type(value)....
-00002670: 2020 2020 6465 6620 7365 745f 7661 6c75      def set_valu
-00002680: 655f 6672 6f6d 5f72 6177 2873 656c 662c  e_from_raw(self,
-00002690: 2076 616c 7565 293a 0d0a 2020 2020 2020   value):..      
-000026a0: 2020 7365 6c66 2e5f 7661 6c75 6520 3d20    self._value = 
-000026b0: 7661 6c75 650d 0a0d 0a20 2020 2040 7072  value....    @pr
-000026c0: 6f70 6572 7479 0d0a 2020 2020 6465 6620  operty..    def 
-000026d0: 7661 6c75 6528 7365 6c66 293a 0d0a 2020  value(self):..  
-000026e0: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
-000026f0: 662e 5f76 616c 7565 0d0a 0d0a 2020 2020  f._value....    
-00002700: 4070 726f 7065 7274 790d 0a20 2020 2064  @property..    d
-00002710: 6566 2076 616c 7565 5f74 7970 6528 7365  ef value_type(se
-00002720: 6c66 293a 0d0a 2020 2020 2020 2020 7265  lf):..        re
-00002730: 7475 726e 2073 656c 662e 5f76 616c 7565  turn self._value
-00002740: 5f74 7970 650d 0a0d 0a0d 0a63 6c61 7373  _type......class
-00002750: 2056 616c 7565 4279 4c61 6265 6c28 5661   ValueByLabel(Va
-00002760: 6c75 6553 746f 7261 6765 4261 7365 293a  lueStorageBase):
-00002770: 0d0a 2020 2020 2222 2253 746f 7265 7320  ..    """Stores 
-00002780: 6120 6c69 7374 206f 6620 6c69 7374 7320  a list of lists 
-00002790: 6f66 206e 756d 6265 7273 2062 7920 616e  of numbers by an
-000027a0: 2061 7262 6974 7261 7279 206c 6162 656c   arbitrary label
-000027b0: 2e20 5573 6520 7468 6973 2063 6c61 7373  . Use this class
-000027c0: 2077 6865 6e20 776f 726b 696e 6720 7769   when working wi
-000027d0: 7468 2063 6b74 456c 656d 656e 7420 6675  th cktElement fu
-000027e0: 6e63 7469 6f6e 0d0a 2020 2020 6361 6c6c  nction..    call
-000027f0: 7320 6c69 6b65 2043 7572 7265 6e74 732c  s like Currents,
-00002800: 2063 7572 7265 6e74 4d61 6741 6e67 2077   currentMagAng w
-00002810: 6865 7265 2065 7665 7279 2074 776f 2063  here every two c
-00002820: 6f6e 7365 6375 7469 7665 2076 616c 7565  onsecutive value
-00002830: 7320 696e 2074 6865 2072 6574 7572 6e65  s in the returne
-00002840: 6420 6c69 7374 2061 7265 2072 6570 7265  d list are repre
-00002850: 7365 6e74 696e 6720 6f6e 650d 0a20 2020  senting one..   
-00002860: 2071 7561 6e74 6974 792e 2054 6865 2063   quantity. The c
-00002870: 6c61 7373 2064 6966 6665 7265 6e74 6961  lass differentia
-00002880: 7465 7320 6265 7477 6565 6e20 636f 6d70  tes between comp
-00002890: 6c65 7820 616e 6420 6d61 6720 2f20 616e  lex and mag / an
-000028a0: 676c 6520 7265 7072 6573 656e 7461 7469  gle representati
-000028b0: 6f6e 2061 6e64 2073 746f 7265 7320 7468  on and stores th
-000028c0: 6520 7661 6c75 6573 2061 7070 726f 7072  e values appropr
-000028d0: 6961 7465 6c79 0d0a 2020 2020 2222 220d  iately..    """.
-000028e0: 0a20 2020 2064 6566 205f 5f69 6e69 745f  .    def __init_
-000028f0: 5f28 7365 6c66 2c20 6e61 6d65 2c20 7072  _(self, name, pr
-00002900: 6f70 2c20 7661 6c75 652c 204e 6f64 6573  op, value, Nodes
-00002910: 2c20 6973 5f63 6f6d 706c 6578 2c20 756e  , is_complex, un
-00002920: 6974 7329 3a0d 0a20 2020 2020 2020 2022  its):..        "
-00002930: 2222 436f 6e73 7472 7563 746f 7220 666f  ""Constructor fo
-00002940: 7220 5661 6c75 6542 794c 6162 656c 0d0a  r ValueByLabel..
-00002950: 0d0a 2020 2020 2020 2020 5061 7261 6d65  ..        Parame
-00002960: 7465 7273 0d0a 2020 2020 2020 2020 2d2d  ters..        --
-00002970: 2d2d 2d2d 2d2d 2d2d 0d0a 2020 2020 2020  --------..      
-00002980: 2020 6e61 6d65 203a 2073 7472 0d0a 2020    name : str..  
-00002990: 2020 2020 2020 7072 6f70 203a 2073 7472        prop : str
-000029a0: 0d0a 2020 2020 2020 2020 6c61 6265 6c5f  ..        label_
-000029b0: 7072 6566 6978 203a 2073 7472 0d0a 2020  prefix : str..  
-000029c0: 2020 2020 2020 2020 2020 5465 7874 2074            Text t
-000029d0: 6f20 7573 6520 6173 2061 2070 7265 6669  o use as a prefi
-000029e0: 7820 666f 7220 636f 6c75 6d6e 206c 6162  x for column lab
-000029f0: 656c 732e 2045 783a 2050 6861 7365 0d0a  els. Ex: Phase..
-00002a00: 2020 2020 2020 2020 6c61 6265 6c73 203a          labels :
-00002a10: 206c 6973 740d 0a20 2020 2020 2020 2020   list..         
-00002a20: 2020 206c 6973 7420 6f66 2073 7472 0d0a     list of str..
-00002a30: 2020 2020 2020 2020 7661 6c75 6573 203a          values :
-00002a40: 206c 6973 740d 0a20 2020 2020 2020 2020   list..         
-00002a50: 2020 2050 6169 7273 206f 6620 7661 6c75     Pairs of valu
-00002a60: 6573 2074 6861 7420 6361 6e20 6265 2069  es that can be i
-00002a70: 6e74 6572 7072 6574 6564 2061 7320 636f  nterpreted as co
-00002a80: 6d70 6c65 7820 6e75 6d62 6572 732e 0d0a  mplex numbers...
-00002a90: 0d0a 2020 2020 2020 2020 2222 220d 0a20  ..        """.. 
-00002aa0: 2020 2020 2020 2073 7570 6572 2829 2e5f         super()._
-00002ab0: 5f69 6e69 745f 5f28 290d 0a20 2020 2020  _init__()..     
-00002ac0: 2020 2070 6873 203d 207b 0d0a 2020 2020     phs = {..    
-00002ad0: 2020 2020 2020 2020 313a 2027 4127 2c0d          1: 'A',.
-00002ae0: 0a20 2020 2020 2020 2020 2020 2032 3a20  .            2: 
-00002af0: 2742 272c 0d0a 2020 2020 2020 2020 2020  'B',..          
-00002b00: 2020 333a 2027 4327 2c0d 0a20 2020 2020    3: 'C',..     
-00002b10: 2020 2020 2020 2030 3a20 274e 272c 0d0a         0: 'N',..
-00002b20: 2020 2020 2020 2020 7d0d 0a0d 0a20 2020          }....   
-00002b30: 2020 2020 2073 656c 662e 5f6e 616d 6520       self._name 
-00002b40: 3d20 6e61 6d65 0d0a 2020 2020 2020 2020  = name..        
-00002b50: 7365 6c66 2e5f 7072 6f70 203d 2070 726f  self._prop = pro
-00002b60: 700d 0a20 2020 2020 2020 2073 656c 662e  p..        self.
-00002b70: 5f6e 6f64 6573 203d 204e 6f64 6573 0d0a  _nodes = Nodes..
-00002b80: 2020 2020 2020 2020 7365 6c66 2e5f 6c61          self._la
-00002b90: 6265 6c73 203d 205b 5d0d 0a20 2020 2020  bels = []..     
-00002ba0: 2020 2073 656c 662e 5f76 616c 7565 203d     self._value =
-00002bb0: 205b 5d0d 0a20 2020 2020 2020 2073 656c   []..        sel
-00002bc0: 662e 5f76 616c 7565 5f74 7970 6520 3d20  f._value_type = 
-00002bd0: 636f 6d70 6c65 7820 6966 2069 735f 636f  complex if is_co
-00002be0: 6d70 6c65 7820 656c 7365 2066 6c6f 6174  mplex else float
-00002bf0: 0d0a 2020 2020 2020 2020 7365 6c66 2e5f  ..        self._
-00002c00: 6973 5f63 6f6d 706c 6578 203d 2069 735f  is_complex = is_
-00002c10: 636f 6d70 6c65 780d 0a0d 0a20 2020 2020  complex....     
-00002c20: 2020 206e 203d 2032 0d0a 2020 2020 2020     n = 2..      
-00002c30: 2020 6d20 3d20 696e 7428 6c65 6e28 7661    m = int(len(va
-00002c40: 6c75 6529 202f 2028 6c65 6e28 4e6f 6465  lue) / (len(Node
-00002c50: 7329 2a6e 2929 0d0a 0d0a 2020 2020 2020  s)*n))....      
-00002c60: 2020 7365 6c66 2e5f 6d20 3d20 6d0d 0a20    self._m = m.. 
-00002c70: 2020 2020 2020 2073 656c 662e 5f6e 203d         self._n =
-00002c80: 206e 0d0a 2020 2020 2020 2020 7365 6c66   n..        self
-00002c90: 2e5f 7661 6c75 655f 6c65 6e67 7468 203d  ._value_length =
-00002ca0: 206c 656e 2876 616c 7565 290d 0a20 2020   len(value)..   
-00002cb0: 2020 2020 2076 616c 7565 203d 2073 656c       value = sel
-00002cc0: 662e 5f66 6978 5f76 616c 7565 2876 616c  f._fix_value(val
-00002cd0: 7565 290d 0a20 2020 2020 2020 200d 0a20  ue)..        .. 
-00002ce0: 2020 2020 2020 2023 2043 6875 6e6b 5f6c         # Chunk_l
-00002cf0: 6973 7420 6578 616d 706c 650d 0a20 2020  ist example..   
-00002d00: 2020 2020 2023 2058 203d 206c 6973 7428       # X = list(
-00002d10: 7261 6e67 6528 3132 2929 202c 206e 4c69  range(12)) , nLi
-00002d20: 7374 3d20 320d 0a20 2020 2020 2020 2023  st= 2..        #
-00002d30: 2059 203d 2063 6875 6e6b 5f6c 6973 7428   Y = chunk_list(
-00002d40: 582c 206e 4c69 7374 2920 2d3e 205b 5b30  X, nList) -> [[0
-00002d50: 2c20 315d 2c20 5b32 2c20 335d 2c20 5b34  , 1], [2, 3], [4
-00002d60: 2c20 355d 2c20 5b36 2c20 375d 2c20 5b38  , 5], [6, 7], [8
-00002d70: 2c20 395d 2c20 5b31 302c 2031 315d 5d0d  , 9], [10, 11]].
-00002d80: 0a20 2020 2020 2020 2023 2047 6976 656e  .        # Given
-00002d90: 2065 6c65 6d65 6e74 2068 6173 2032 2074   element has 2 t
-00002da0: 6572 6d69 6e61 6c73 206d 203d 2031 3220  erminals m = 12 
-00002db0: 2f20 2832 2a32 2920 3d20 330d 0a20 2020  / (2*2) = 3..   
-00002dc0: 2020 2020 2023 205a 203d 2020 6368 756e       # Z =  chun
-00002dd0: 6b5f 6c69 7374 2859 2c20 6d29 202d 203e  k_list(Y, m) - >
-00002de0: 205b 0d0a 2020 2020 2020 2020 2320 2020   [..        #   
-00002df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002e00: 2020 2020 2020 2020 205b 5b30 2c20 315d           [[0, 1]
-00002e10: 2c20 5b32 2c20 335d 2c20 5b34 2c20 355d  , [2, 3], [4, 5]
-00002e20: 5d2c 2020 5465 726d 696e 616c 206f 6e65  ],  Terminal one
-00002e30: 2063 6f6d 706c 6578 2070 6169 7273 0d0a   complex pairs..
-00002e40: 2020 2020 2020 2020 2320 2020 2020 2020          #       
+00000ad0: 6f6c 756d 6e5b 3a69 6e64 6578 5d0a 2020  olumn[:index].  
+00000ae0: 2020 2020 2020 2020 2020 2320 5b6e 616d            # [nam
+00000af0: 652c 206f 7074 696f 6e31 2c20 6f70 7469  e, option1, opti
+00000b00: 6f6e 322c 202e 2e2e 5d0a 2020 2020 2020  on2, ...].      
+00000b10: 2020 2020 2020 6669 656c 6473 203d 2056        fields = V
+00000b20: 616c 7565 5374 6f72 6167 6542 6173 652e  alueStorageBase.
+00000b30: 6765 745f 6669 656c 6473 2863 6f6c 2c20  get_fields(col, 
+00000b40: 6e61 6d65 290a 2020 2020 2020 2020 2020  name).          
+00000b50: 2020 5f6e 616d 6520 3d20 6669 656c 6473    _name = fields
+00000b60: 5b30 5d0a 2020 2020 2020 2020 2020 2020  [0].            
+00000b70: 6966 205f 6e61 6d65 2021 3d20 6e61 6d65  if _name != name
+00000b80: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00000b90: 2020 636f 6e74 696e 7565 0a20 2020 2020    continue.     
+00000ba0: 2020 2020 2020 2076 616c 7565 7320 2b3d         values +=
+00000bb0: 2066 6965 6c64 735b 313a 5d0a 0a20 2020   fields[1:]..   
+00000bc0: 2020 2020 2069 6620 6e6f 7420 7661 6c75       if not valu
+00000bd0: 6573 3a0a 2020 2020 2020 2020 2020 2020  es:.            
+00000be0: 7261 6973 6520 496e 7661 6c69 6450 6172  raise InvalidPar
+00000bf0: 616d 6574 6572 2866 227b 6e61 6d65 7d20  ameter(f"{name} 
+00000c00: 646f 6573 206e 6f74 2065 7869 7374 2069  does not exist i
+00000c10: 6e20 4461 7461 4672 616d 6522 290a 0a20  n DataFrame").. 
+00000c20: 2020 2020 2020 2072 6574 7572 6e20 7661         return va
+00000c30: 6c75 6573 0a0a 2020 2020 4073 7461 7469  lues..    @stati
+00000c40: 636d 6574 686f 640a 2020 2020 6465 6620  cmethod.    def 
+00000c50: 6765 745f 6669 656c 6473 2863 6f6c 2c20  get_fields(col, 
+00000c60: 6e61 6d65 293a 0a20 2020 2020 2020 2023  name):.        #
+00000c70: 2048 616e 646c 6520 6361 7365 2077 6865   Handle case whe
+00000c80: 7265 2074 6865 206e 616d 6520 656e 6473  re the name ends
+00000c90: 2077 6974 6820 7061 7274 206f 6620 7468   with part of th
+00000ca0: 6520 4445 4c49 4d49 5445 522e 0a20 2020  e DELIMITER..   
+00000cb0: 2020 2020 2063 6f6c 5f74 6d70 203d 2063       col_tmp = c
+00000cc0: 6f6c 2e72 6570 6c61 6365 286e 616d 652c  ol.replace(name,
+00000cd0: 2022 222c 2031 290a 2020 2020 2020 2020   "", 1).        
+00000ce0: 6669 656c 6473 203d 2063 6f6c 5f74 6d70  fields = col_tmp
+00000cf0: 2e73 706c 6974 2856 616c 7565 5374 6f72  .split(ValueStor
+00000d00: 6167 6542 6173 652e 4445 4c49 4d49 5445  ageBase.DELIMITE
+00000d10: 5229 5b31 3a5d 0a20 2020 2020 2020 2066  R)[1:].        f
+00000d20: 6965 6c64 732e 696e 7365 7274 2830 2c20  ields.insert(0, 
+00000d30: 6e61 6d65 290a 2020 2020 2020 2020 7265  name).        re
+00000d40: 7475 726e 2066 6965 6c64 730a 0a20 2020  turn fields..   
+00000d50: 2040 6162 632e 6162 7374 7261 6374 6d65   @abc.abstractme
+00000d60: 7468 6f64 0a20 2020 2064 6566 2069 735f  thod.    def is_
+00000d70: 6e61 6e28 7365 6c66 293a 0a20 2020 2020  nan(self):.     
+00000d80: 2020 2022 2222 5265 7475 726e 2054 7275     """Return Tru
+00000d90: 6520 6966 2074 6865 2076 616c 7565 2069  e if the value i
+00000da0: 7320 4e61 4e2e 0a0a 2020 2020 2020 2020  s NaN...        
+00000db0: 5265 7475 726e 730a 2020 2020 2020 2020  Returns.        
+00000dc0: 2d2d 2d2d 2d2d 2d0a 2020 2020 2020 2020  -------.        
+00000dd0: 626f 6f6c 0a0a 2020 2020 2020 2020 2222  bool..        ""
+00000de0: 220a 0a20 2020 2040 6162 632e 6162 7374  "..    @abc.abst
+00000df0: 7261 6374 6d65 7468 6f64 0a20 2020 2064  ractmethod.    d
+00000e00: 6566 206d 616b 655f 636f 6c75 6d6e 7328  ef make_columns(
+00000e10: 7365 6c66 293a 0a20 2020 2020 2020 2022  self):.        "
+00000e20: 2222 5265 7475 726e 2061 206c 6973 7420  ""Return a list 
+00000e30: 6f66 2063 6f6c 756d 6e20 6e61 6d65 730a  of column names.
+00000e40: 0a20 2020 2020 2020 2052 6574 7572 6e73  .        Returns
+00000e50: 0a20 2020 2020 2020 202d 2d2d 2d2d 2d2d  .        -------
+00000e60: 0a20 2020 2020 2020 206c 6973 740a 0a20  .        list.. 
+00000e70: 2020 2020 2020 2022 2222 0a0a 2020 2020         """..    
+00000e80: 4070 726f 7065 7274 790a 2020 2020 6465  @property.    de
+00000e90: 6620 6e61 6d65 2873 656c 6629 3a0a 2020  f name(self):.  
+00000ea0: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
+00000eb0: 662e 5f6e 616d 650a 0a20 2020 2040 7072  f._name..    @pr
+00000ec0: 6f70 6572 7479 0a20 2020 2040 6162 632e  operty.    @abc.
+00000ed0: 6162 7374 7261 6374 6d65 7468 6f64 0a20  abstractmethod. 
+00000ee0: 2020 2064 6566 206e 756d 5f63 6f6c 756d     def num_colum
+00000ef0: 6e73 2873 656c 6629 3a0a 2020 2020 2020  ns(self):.      
+00000f00: 2020 2222 2252 6574 7572 6e20 7468 6520    """Return the 
+00000f10: 6e75 6d62 6572 206f 6620 636f 6c75 6d6e  number of column
+00000f20: 7320 696e 2074 6865 2064 6174 612e 0a0a  s in the data...
+00000f30: 2020 2020 2020 2020 5265 7475 726e 730a          Returns.
+00000f40: 2020 2020 2020 2020 2d2d 2d2d 2d2d 2d0a          -------.
+00000f50: 2020 2020 2020 2020 696e 740a 0a20 2020          int..   
+00000f60: 2020 2020 2022 2222 0a0a 2020 2020 4061       """..    @a
+00000f70: 6263 2e61 6273 7472 6163 746d 6574 686f  bc.abstractmetho
+00000f80: 640a 2020 2020 6465 6620 7365 745f 656c  d.    def set_el
+00000f90: 656d 656e 745f 7072 6f70 6572 7479 2873  ement_property(s
+00000fa0: 656c 662c 2070 726f 7029 3a0a 2020 2020  elf, prop):.    
+00000fb0: 2020 2020 2222 2253 6574 2074 6865 2065      """Set the e
+00000fc0: 6c65 6d65 6e74 2070 726f 7065 7274 7920  lement property 
+00000fd0: 6e61 6d65 2e0a 0a20 2020 2020 2020 2050  name...        P
+00000fe0: 6172 616d 6574 6572 730a 2020 2020 2020  arameters.      
+00000ff0: 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020    ----------.   
+00001000: 2020 2020 2070 726f 7020 3a20 7374 720a       prop : str.
+00001010: 0a20 2020 2020 2020 2022 2222 0a0a 2020  .        """..  
+00001020: 2020 4061 6263 2e61 6273 7472 6163 746d    @abc.abstractm
+00001030: 6574 686f 640a 2020 2020 6465 6620 7365  ethod.    def se
+00001040: 745f 6e61 6d65 2873 656c 662c 206e 616d  t_name(self, nam
+00001050: 6529 3a0a 2020 2020 2020 2020 2222 2253  e):.        """S
+00001060: 6574 2074 6865 206e 616d 652e 0a0a 2020  et the name...  
+00001070: 2020 2020 2020 5061 7261 6d65 7465 7273        Parameters
+00001080: 0a20 2020 2020 2020 202d 2d2d 2d2d 2d2d  .        -------
+00001090: 2d2d 2d0a 2020 2020 2020 2020 6e61 6d65  ---.        name
+000010a0: 203a 2073 7472 0a0a 2020 2020 2020 2020   : str..        
+000010b0: 2222 220a 0a20 2020 2040 6162 632e 6162  """..    @abc.ab
+000010c0: 7374 7261 6374 6d65 7468 6f64 0a20 2020  stractmethod.   
+000010d0: 2064 6566 2073 6574 5f6e 616e 2873 656c   def set_nan(sel
+000010e0: 6629 3a0a 2020 2020 2020 2020 2222 2253  f):.        """S
+000010f0: 6574 2074 6865 2076 616c 7565 2074 6f20  et the value to 
+00001100: 4e61 4e20 6f72 2065 7175 6976 616c 656e  NaN or equivalen
+00001110: 742e 2222 220a 0a20 2020 2040 6162 632e  t."""..    @abc.
+00001120: 6162 7374 7261 6374 6d65 7468 6f64 0a20  abstractmethod. 
+00001130: 2020 2064 6566 2073 6574 5f76 616c 7565     def set_value
+00001140: 2873 656c 662c 2076 616c 7565 293a 0a20  (self, value):. 
+00001150: 2020 2020 2020 2022 2222 5365 7420 7468         """Set th
+00001160: 6520 7661 6c75 6520 6672 6f6d 2061 6e6f  e value from ano
+00001170: 7468 6572 2069 6e73 7461 6e63 652e 0a0a  ther instance...
+00001180: 2020 2020 2020 2020 5061 7261 6d65 7465          Paramete
+00001190: 7273 0a20 2020 2020 2020 202d 2d2d 2d2d  rs.        -----
+000011a0: 2d2d 2d2d 2d0a 2020 2020 2020 2020 7661  -----.        va
+000011b0: 6c75 6520 3a20 5661 6c75 6553 746f 7261  lue : ValueStora
+000011c0: 6765 4261 7365 0a0a 2020 2020 2020 2020  geBase..        
+000011d0: 2222 220a 0a20 2020 2040 6162 632e 6162  """..    @abc.ab
+000011e0: 7374 7261 6374 6d65 7468 6f64 0a20 2020  stractmethod.   
+000011f0: 2064 6566 2073 6574 5f76 616c 7565 5f66   def set_value_f
+00001200: 726f 6d5f 7261 7728 7365 6c66 2c20 7661  rom_raw(self, va
+00001210: 6c75 6529 3a0a 2020 2020 2020 2020 2222  lue):.        ""
+00001220: 2253 6574 2074 6865 2076 616c 7565 2066  "Set the value f
+00001230: 726f 6d20 6120 6e65 7720 7261 7720 7661  rom a new raw va
+00001240: 6c75 6520 6672 6f6d 206f 7065 6e64 7373  lue from opendss
+00001250: 6469 7265 6374 2e0a 0a20 2020 2020 2020  direct...       
+00001260: 2050 6172 616d 6574 6572 730a 2020 2020   Parameters.    
+00001270: 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20      ----------. 
+00001280: 2020 2020 2020 2076 616c 7565 203a 206c         value : l
+00001290: 6973 7420 7c20 666c 6f61 7420 7c20 636f  ist | float | co
+000012a0: 6d70 6c65 780a 0a20 2020 2020 2020 2022  mplex..        "
+000012b0: 2222 0a0a 2020 2020 4070 726f 7065 7274  ""..    @propert
+000012c0: 790a 2020 2020 4061 6263 2e61 6273 7472  y.    @abc.abstr
+000012d0: 6163 746d 6574 686f 640a 2020 2020 6465  actmethod.    de
+000012e0: 6620 7661 6c75 6528 7365 6c66 293a 0a20  f value(self):. 
+000012f0: 2020 2020 2020 2022 2222 5265 7475 726e         """Return
+00001300: 2074 6865 2076 616c 7565 2e0a 0a20 2020   the value...   
+00001310: 2020 2020 2052 6574 7572 6e73 0a20 2020       Returns.   
+00001320: 2020 2020 202d 2d2d 2d2d 2d2d 0a20 2020       -------.   
+00001330: 2020 2020 206c 6973 7420 7c20 666c 6f61       list | floa
+00001340: 7420 7c20 636f 6d70 6c65 780a 0a20 2020  t | complex..   
+00001350: 2020 2020 2022 2222 0a0a 2020 2020 4070       """..    @p
+00001360: 726f 7065 7274 790a 2020 2020 4061 6263  roperty.    @abc
+00001370: 2e61 6273 7472 6163 746d 6574 686f 640a  .abstractmethod.
+00001380: 2020 2020 6465 6620 7661 6c75 655f 7479      def value_ty
+00001390: 7065 2873 656c 6629 3a0a 2020 2020 2020  pe(self):.      
+000013a0: 2020 2222 2252 6574 7572 6e20 7468 6520    """Return the 
+000013b0: 7479 7065 206f 6620 7661 6c75 6520 6265  type of value be
+000013c0: 696e 6720 7374 6f72 6564 2e0a 0a20 2020  ing stored...   
+000013d0: 2020 2020 2052 6574 7572 6e73 0a20 2020       Returns.   
+000013e0: 2020 2020 202d 2d2d 2d2d 2d2d 0a20 2020       -------.   
+000013f0: 2020 2020 2054 7970 650a 0a20 2020 2020       Type..     
+00001400: 2020 2022 2222 0a0a 0a63 6c61 7373 2056     """...class V
+00001410: 616c 7565 4279 4c69 7374 2856 616c 7565  alueByList(Value
+00001420: 5374 6f72 6167 6542 6173 6529 3a0a 2020  StorageBase):.  
+00001430: 2020 2222 2222 5374 6f72 6573 2061 206c    """"Stores a l
+00001440: 6973 7420 6f66 206c 6973 7473 206f 6620  ist of lists of 
+00001450: 6e75 6d62 6572 7320 6279 2061 6e20 6172  numbers by an ar
+00001460: 6269 7472 6172 7920 7375 6666 6978 2e20  bitrary suffix. 
+00001470: 5468 6973 2069 7320 6120 6765 6e65 7269  This is a generi
+00001480: 6320 6d65 7468 6f64 2074 6f20 6861 6e64  c method to hand
+00001490: 6c65 206c 6973 7473 2072 6574 7572 6e65  le lists returne
+000014a0: 6420 6672 6f6d 0a20 2020 2061 2066 756e  d from.    a fun
+000014b0: 6374 696f 6e20 6361 6c6c 2e20 416e 2065  ction call. An e
+000014c0: 7861 6d70 6c65 2077 6f75 6c64 2062 6520  xample would be 
+000014d0: 7265 7475 726e 6564 2076 616c 7565 7320  returned values 
+000014e0: 2274 6170 7322 2066 756e 6374 696f 6e20  "taps" function 
+000014f0: 666f 7220 7472 616e 7366 6f72 6d65 7220  for transformer 
+00001500: 656c 656d 656e 7473 2e20 5468 6520 636c  elements. The cl
+00001510: 6173 7320 6361 6e20 6265 0a20 2020 2075  ass can be.    u
+00001520: 7365 6420 666f 7220 616e 7920 6d65 7468  sed for any meth
+00001530: 6f64 7320 7468 6174 2072 6574 7572 6e20  ods that return 
+00001540: 6120 6c69 7374 2e0a 2020 2020 2222 220a  a list..    """.
+00001550: 2020 2020 6465 6620 5f5f 696e 6974 5f5f      def __init__
+00001560: 2873 656c 662c 206e 616d 652c 2070 726f  (self, name, pro
+00001570: 702c 2076 616c 7565 732c 206c 6162 656c  p, values, label
+00001580: 5f73 7566 6669 7865 7329 3a0a 2020 2020  _suffixes):.    
+00001590: 2020 2020 2222 2243 6f6e 7374 7275 6374      """Construct
+000015a0: 6f72 2066 6f72 2056 616c 7565 4279 4c61  or for ValueByLa
+000015b0: 6265 6c0a 0a20 2020 2020 2020 2050 6172  bel..        Par
+000015c0: 616d 6574 6572 730a 2020 2020 2020 2020  ameters.        
+000015d0: 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020 2020  ----------.     
+000015e0: 2020 206e 616d 6520 3a20 7374 720a 2020     name : str.  
+000015f0: 2020 2020 2020 7072 6f70 203a 2073 7472        prop : str
+00001600: 0a20 2020 2020 2020 206c 6162 656c 5f70  .        label_p
+00001610: 7265 6669 7820 3a20 7374 720a 2020 2020  refix : str.    
+00001620: 2020 2020 2020 2020 5465 7874 2074 6f20          Text to 
+00001630: 7573 6520 6173 2061 2070 7265 6669 7820  use as a prefix 
+00001640: 666f 7220 636f 6c75 6d6e 206c 6162 656c  for column label
+00001650: 732e 2045 783a 2050 6861 7365 0a20 2020  s. Ex: Phase.   
+00001660: 2020 2020 206c 6162 656c 7320 3a20 6c69       labels : li
+00001670: 7374 0a20 2020 2020 2020 2020 2020 206c  st.            l
+00001680: 6973 7420 6f66 2073 7472 0a20 2020 2020  ist of str.     
+00001690: 2020 2076 616c 7565 7320 3a20 6c69 7374     values : list
+000016a0: 0a20 2020 2020 2020 2020 2020 2050 6169  .            Pai
+000016b0: 7273 206f 6620 7661 6c75 6573 2074 6861  rs of values tha
+000016c0: 7420 6361 6e20 6265 2069 6e74 6572 7072  t can be interpr
+000016d0: 6574 6564 2061 7320 636f 6d70 6c65 7820  eted as complex 
+000016e0: 6e75 6d62 6572 732e 0a0a 2020 2020 2020  numbers...      
+000016f0: 2020 2222 220a 2020 2020 2020 2020 7375    """.        su
+00001700: 7065 7228 292e 5f5f 696e 6974 5f5f 2829  per().__init__()
+00001710: 0a20 2020 2020 2020 2073 656c 662e 5f6e  .        self._n
+00001720: 616d 6520 3d20 6e61 6d65 0a20 2020 2020  ame = name.     
+00001730: 2020 2073 656c 662e 5f70 726f 7020 3d20     self._prop = 
+00001740: 7072 6f70 0a20 2020 2020 2020 2073 656c  prop.        sel
+00001750: 662e 5f6c 6162 656c 7320 3d20 5b5d 0a20  f._labels = []. 
+00001760: 2020 2020 2020 2073 656c 662e 5f76 616c         self._val
+00001770: 7565 5f74 7970 6520 3d20 4e6f 6e65 0a20  ue_type = None. 
+00001780: 2020 2020 2020 2073 656c 662e 5f76 616c         self._val
+00001790: 7565 203d 205b 5d0a 2020 2020 2020 2020  ue = [].        
+000017a0: 6173 7365 7274 2028 6973 696e 7374 616e  assert (isinstan
+000017b0: 6365 2876 616c 7565 732c 206c 6973 7429  ce(values, list)
+000017c0: 2061 6e64 206c 656e 2876 616c 7565 7329   and len(values)
+000017d0: 203d 3d20 6c65 6e28 6c61 6265 6c5f 7375   == len(label_su
+000017e0: 6666 6978 6573 2929 2c20 5c0a 2020 2020  ffixes)), \.    
+000017f0: 2020 2020 2020 2020 2722 7661 6c75 6573          '"values
+00001800: 2220 616e 6420 226c 6162 656c 5f73 7566  " and "label_suf
+00001810: 6669 7865 7322 2073 686f 756c 6420 6265  fixes" should be
+00001820: 206c 6973 7473 206f 6620 6571 7561 6c20   lists of equal 
+00001830: 6c65 6e67 7468 7327 0a20 2020 2020 2020  lengths'.       
+00001840: 2066 6f72 2076 616c 2c20 6c61 625f 7375   for val, lab_su
+00001850: 6620 696e 207a 6970 2876 616c 7565 732c  f in zip(values,
+00001860: 206c 6162 656c 5f73 7566 6669 7865 7329   label_suffixes)
+00001870: 3a0a 2020 2020 2020 2020 2020 2020 6c61  :.            la
+00001880: 6265 6c20 3d20 7072 6f70 202b 2073 656c  bel = prop + sel
+00001890: 662e 4445 4c49 4d49 5445 5220 2b20 6c61  f.DELIMITER + la
+000018a0: 625f 7375 660a 2020 2020 2020 2020 2020  b_suf.          
+000018b0: 2020 7365 6c66 2e5f 6c61 6265 6c73 2e61    self._labels.a
+000018c0: 7070 656e 6428 6c61 6265 6c29 0a20 2020  ppend(label).   
+000018d0: 2020 2020 2020 2020 2073 656c 662e 5f76           self._v
+000018e0: 616c 7565 2e61 7070 656e 6428 7661 6c29  alue.append(val)
+000018f0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+00001900: 7365 6c66 2e5f 7661 6c75 655f 7479 7065  self._value_type
+00001910: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      
+00001920: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
+00001930: 7661 6c75 655f 7479 7065 203d 2074 7970  value_type = typ
+00001940: 6528 7661 6c29 0a0a 2020 2020 6465 6620  e(val)..    def 
+00001950: 5f5f 6961 6464 5f5f 2873 656c 662c 206f  __iadd__(self, o
+00001960: 7468 6572 293a 0a20 2020 2020 2020 2066  ther):.        f
+00001970: 6f72 2069 2069 6e20 7261 6e67 6528 6c65  or i in range(le
+00001980: 6e28 7365 6c66 2e5f 7661 6c75 6529 293a  n(self._value)):
+00001990: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+000019a0: 662e 5f76 616c 7565 5b69 5d20 2b3d 206f  f._value[i] += o
+000019b0: 7468 6572 2e76 616c 7565 5b69 5d0a 2020  ther.value[i].  
+000019c0: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
+000019d0: 660a 0a20 2020 2064 6566 205f 5f67 745f  f..    def __gt_
+000019e0: 5f28 7365 6c66 2c20 6f74 6865 7229 3a0a  _(self, other):.
+000019f0: 2020 2020 2020 2020 2320 544f 444f 0a20          # TODO. 
+00001a00: 2020 2020 2020 2072 6574 7572 6e20 7375         return su
+00001a10: 6d28 7365 6c66 2e5f 7661 6c75 6529 203e  m(self._value) >
+00001a20: 2073 756d 286f 7468 6572 2e76 616c 7565   sum(other.value
+00001a30: 290a 0a20 2020 2064 6566 2069 735f 6e61  )..    def is_na
+00001a40: 6e28 7365 6c66 293a 0a20 2020 2020 2020  n(self):.       
+00001a50: 2069 6620 6e70 2e69 7373 7562 6474 7970   if np.issubdtyp
+00001a60: 6528 7365 6c66 2e5f 7661 6c75 655f 7479  e(self._value_ty
+00001a70: 7065 2c20 6e70 2e69 6e74 3634 293a 0a20  pe, np.int64):. 
+00001a80: 2020 2020 2020 2020 2020 2072 6574 7572             retur
+00001a90: 6e20 7365 6c66 2e5f 7661 6c75 655b 305d  n self._value[0]
+00001aa0: 203d 3d20 494e 5445 4745 525f 4e41 4e0a   == INTEGER_NAN.
+00001ab0: 2020 2020 2020 2020 7265 7475 726e 206e          return n
+00001ac0: 702e 6973 6e61 6e28 7365 6c66 2e5f 7661  p.isnan(self._va
+00001ad0: 6c75 655b 305d 290a 0a20 2020 2064 6566  lue[0])..    def
+00001ae0: 206d 616b 655f 636f 6c75 6d6e 7328 7365   make_columns(se
+00001af0: 6c66 293a 0a20 2020 2020 2020 2072 6574  lf):.        ret
+00001b00: 7572 6e20 5b0a 2020 2020 2020 2020 2020  urn [.          
+00001b10: 2020 7365 6c66 2e44 454c 494d 4954 4552    self.DELIMITER
+00001b20: 2e6a 6f69 6e28 2873 656c 662e 5f6e 616d  .join((self._nam
+00001b30: 652c 2066 227b 787d 2229 2920 666f 7220  e, f"{x}")) for 
+00001b40: 7820 696e 2073 656c 662e 5f6c 6162 656c  x in self._label
+00001b50: 730a 2020 2020 2020 2020 5d0a 0a20 2020  s.        ]..   
+00001b60: 2040 7072 6f70 6572 7479 0a20 2020 2064   @property.    d
+00001b70: 6566 206e 756d 5f63 6f6c 756d 6e73 2873  ef num_columns(s
+00001b80: 656c 6629 3a0a 2020 2020 2020 2020 7265  elf):.        re
+00001b90: 7475 726e 206c 656e 2873 656c 662e 5f6c  turn len(self._l
+00001ba0: 6162 656c 7329 0a0a 2020 2020 6465 6620  abels)..    def 
+00001bb0: 7365 745f 656c 656d 656e 745f 7072 6f70  set_element_prop
+00001bc0: 6572 7479 2873 656c 662c 2070 726f 7029  erty(self, prop)
+00001bd0: 3a0a 2020 2020 2020 2020 7365 6c66 2e5f  :.        self._
+00001be0: 7072 6f70 203d 2070 726f 700a 0a20 2020  prop = prop..   
+00001bf0: 2020 2020 2023 2055 7064 6174 6520 7468       # Update th
+00001c00: 6520 7072 6f70 6572 7479 2069 6e73 6964  e property insid
+00001c10: 6520 6561 6368 206c 6162 656c 2e0a 2020  e each label..  
+00001c20: 2020 2020 2020 666f 7220 692c 206c 6162        for i, lab
+00001c30: 656c 2069 6e20 656e 756d 6572 6174 6528  el in enumerate(
+00001c40: 7365 6c66 2e5f 6c61 6265 6c73 293a 0a20  self._labels):. 
+00001c50: 2020 2020 2020 2020 2020 2066 6965 6c64             field
+00001c60: 7320 3d20 6c61 6265 6c2e 7370 6c69 7428  s = label.split(
+00001c70: 7365 6c66 2e44 454c 494d 4954 4552 290a  self.DELIMITER).
+00001c80: 2020 2020 2020 2020 2020 2020 6173 7365              asse
+00001c90: 7274 206c 656e 2866 6965 6c64 7329 203d  rt len(fields) =
+00001ca0: 3d20 320a 2020 2020 2020 2020 2020 2020  = 2.            
+00001cb0: 6669 656c 6473 5b30 5d20 3d20 7072 6f70  fields[0] = prop
+00001cc0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00001cd0: 662e 5f6c 6162 656c 735b 695d 203d 2073  f._labels[i] = s
+00001ce0: 656c 662e 4445 4c49 4d49 5445 522e 6a6f  elf.DELIMITER.jo
+00001cf0: 696e 2866 6965 6c64 7329 0a0a 2020 2020  in(fields)..    
+00001d00: 6465 6620 7365 745f 6e61 6d65 2873 656c  def set_name(sel
+00001d10: 662c 206e 616d 6529 3a0a 2020 2020 2020  f, name):.      
+00001d20: 2020 7365 6c66 2e5f 6e61 6d65 203d 206e    self._name = n
+00001d30: 616d 650a 0a20 2020 2064 6566 2073 6574  ame..    def set
+00001d40: 5f6e 616e 2873 656c 6629 3a0a 2020 2020  _nan(self):.    
+00001d50: 2020 2020 666f 7220 6920 696e 2072 616e      for i in ran
+00001d60: 6765 286c 656e 2873 656c 662e 5f76 616c  ge(len(self._val
+00001d70: 7565 2929 3a0a 2020 2020 2020 2020 2020  ue)):.          
+00001d80: 2020 7365 6c66 2e5f 7661 6c75 655b 695d    self._value[i]
+00001d90: 203d 206e 702e 4e61 4e0a 0a20 2020 2064   = np.NaN..    d
+00001da0: 6566 2073 6574 5f76 616c 7565 2873 656c  ef set_value(sel
+00001db0: 662c 2076 616c 7565 293a 0a20 2020 2020  f, value):.     
+00001dc0: 2020 2073 656c 662e 5f76 616c 7565 203d     self._value =
+00001dd0: 2076 616c 7565 0a20 2020 2020 2020 2069   value.        i
+00001de0: 6620 6e6f 7420 6973 696e 7374 616e 6365  f not isinstance
+00001df0: 2876 616c 7565 5b30 5d2c 2073 656c 662e  (value[0], self.
+00001e00: 5f76 616c 7565 5f74 7970 6529 3a0a 2020  _value_type):.  
+00001e10: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
+00001e20: 7661 6c75 655f 7479 7065 203d 2074 7970  value_type = typ
+00001e30: 6528 7661 6c75 655b 305d 290a 0a20 2020  e(value[0])..   
+00001e40: 2064 6566 2073 6574 5f76 616c 7565 5f66   def set_value_f
+00001e50: 726f 6d5f 7261 7728 7365 6c66 2c20 7661  rom_raw(self, va
+00001e60: 6c75 6529 3a0a 2020 2020 2020 2020 7365  lue):.        se
+00001e70: 6c66 2e5f 7661 6c75 6520 3d20 7661 6c75  lf._value = valu
+00001e80: 650a 0a20 2020 2040 7072 6f70 6572 7479  e..    @property
+00001e90: 0a20 2020 2064 6566 2076 616c 7565 2873  .    def value(s
+00001ea0: 656c 6629 3a0a 2020 2020 2020 2020 7265  elf):.        re
+00001eb0: 7475 726e 2073 656c 662e 5f76 616c 7565  turn self._value
+00001ec0: 0a0a 2020 2020 4070 726f 7065 7274 790a  ..    @property.
+00001ed0: 2020 2020 6465 6620 7661 6c75 655f 7479      def value_ty
+00001ee0: 7065 2873 656c 6629 3a0a 2020 2020 2020  pe(self):.      
+00001ef0: 2020 7265 7475 726e 2073 656c 662e 5f76    return self._v
+00001f00: 616c 7565 5f74 7970 650a 0a0a 636c 6173  alue_type...clas
+00001f10: 7320 5661 6c75 6542 794e 756d 6265 7228  s ValueByNumber(
+00001f20: 5661 6c75 6553 746f 7261 6765 4261 7365  ValueStorageBase
+00001f30: 293a 0a20 2020 2022 2222 5374 6f72 6573  ):.    """Stores
+00001f40: 2061 206c 6973 7420 6f66 206e 756d 6265   a list of numbe
+00001f50: 7273 2066 6f72 2061 6e20 656c 656d 656e  rs for an elemen
+00001f60: 742f 7072 6f70 6572 7479 2e22 2222 0a20  t/property.""". 
+00001f70: 2020 2064 6566 205f 5f69 6e69 745f 5f28     def __init__(
+00001f80: 7365 6c66 2c20 6e61 6d65 2c20 7072 6f70  self, name, prop
+00001f90: 2c20 7661 6c75 6529 3a0a 2020 2020 2020  , value):.      
+00001fa0: 2020 7375 7065 7228 292e 5f5f 696e 6974    super().__init
+00001fb0: 5f5f 2829 0a20 2020 2020 2020 2061 7373  __().        ass
+00001fc0: 6572 7420 6e6f 7420 6973 696e 7374 616e  ert not isinstan
+00001fd0: 6365 2876 616c 7565 2c20 6c69 7374 292c  ce(value, list),
+00001fe0: 2073 7472 2876 616c 7565 290a 2020 2020   str(value).    
+00001ff0: 2020 2020 7365 6c66 2e5f 6e61 6d65 203d      self._name =
+00002000: 206e 616d 650a 2020 2020 2020 2020 7365   name.        se
+00002010: 6c66 2e5f 7072 6f70 203d 2070 726f 700a  lf._prop = prop.
+00002020: 2020 2020 2020 2020 7365 6c66 2e5f 7661          self._va
+00002030: 6c75 655f 7479 7065 203d 2074 7970 6528  lue_type = type(
+00002040: 7661 6c75 6529 0a20 2020 2020 2020 2069  value).        i
+00002050: 6620 7365 6c66 2e5f 7661 6c75 655f 7479  f self._value_ty
+00002060: 7065 203d 3d20 7374 723a 0a20 2020 2020  pe == str:.     
+00002070: 2020 2020 2020 2072 6169 7365 2049 6e76         raise Inv
+00002080: 616c 6964 436f 6e66 6967 7572 6174 696f  alidConfiguratio
+00002090: 6e28 0a20 2020 2020 2020 2020 2020 2020  n(.             
+000020a0: 2020 2066 2244 6174 6120 6578 706f 7274     f"Data export
+000020b0: 2066 6561 7475 7265 2064 6f65 7320 6e6f   feature does no
+000020c0: 7420 7375 7070 6f72 7420 7374 7269 6e67  t support string
+000020d0: 733a 206e 616d 653d 7b6e 616d 657d 2070  s: name={name} p
+000020e0: 726f 703d 7b70 726f 707d 2076 616c 7565  rop={prop} value
+000020f0: 3d7b 7661 6c75 657d 220a 2020 2020 2020  ={value}".      
+00002100: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
+00002110: 7365 6c66 2e5f 7661 6c75 6520 3d20 7661  self._value = va
+00002120: 6c75 650a 2020 2020 2020 2020 7365 6c66  lue.        self
+00002130: 2e5f 6973 5f63 6f6d 706c 6578 203d 2069  ._is_complex = i
+00002140: 7369 6e73 7461 6e63 6528 7365 6c66 2e5f  sinstance(self._
+00002150: 7661 6c75 655f 7479 7065 2c20 636f 6d70  value_type, comp
+00002160: 6c65 7829 0a0a 2020 2020 6465 6620 5f5f  lex)..    def __
+00002170: 6961 6464 5f5f 2873 656c 662c 206f 7468  iadd__(self, oth
+00002180: 6572 293a 0a20 2020 2020 2020 2073 656c  er):.        sel
+00002190: 662e 5f76 616c 7565 202b 3d20 6f74 6865  f._value += othe
+000021a0: 722e 7661 6c75 650a 2020 2020 2020 2020  r.value.        
+000021b0: 7265 7475 726e 2073 656c 660a 0a20 2020  return self..   
+000021c0: 2064 6566 205f 5f67 745f 5f28 7365 6c66   def __gt__(self
+000021d0: 2c20 6f74 6865 7229 3a0a 2020 2020 2020  , other):.      
+000021e0: 2020 7265 7475 726e 2073 656c 662e 5f76    return self._v
+000021f0: 616c 7565 203e 206f 7468 6572 2e76 616c  alue > other.val
+00002200: 7565 0a0a 2020 2020 6465 6620 6973 5f6e  ue..    def is_n
+00002210: 616e 2873 656c 6629 3a0a 2020 2020 2020  an(self):.      
+00002220: 2020 6966 206e 702e 6973 7375 6264 7479    if np.issubdty
+00002230: 7065 2873 656c 662e 5f76 616c 7565 5f74  pe(self._value_t
+00002240: 7970 652c 206e 702e 696e 7436 3429 3a0a  ype, np.int64):.
+00002250: 2020 2020 2020 2020 2020 2020 7265 7475              retu
+00002260: 726e 2073 656c 662e 5f76 616c 7565 203d  rn self._value =
+00002270: 3d20 494e 5445 4745 525f 4e41 4e0a 2020  = INTEGER_NAN.  
+00002280: 2020 2020 2020 7265 7475 726e 206e 702e        return np.
+00002290: 6973 6e61 6e28 7365 6c66 2e5f 7661 6c75  isnan(self._valu
+000022a0: 6529 0a0a 2020 2020 6465 6620 6d61 6b65  e)..    def make
+000022b0: 5f63 6f6c 756d 6e73 2873 656c 6629 3a0a  _columns(self):.
+000022c0: 2020 2020 2020 2020 7265 7475 726e 205b          return [
+000022d0: 5661 6c75 6553 746f 7261 6765 4261 7365  ValueStorageBase
+000022e0: 2e44 454c 494d 4954 4552 2e6a 6f69 6e28  .DELIMITER.join(
+000022f0: 2873 656c 662e 5f6e 616d 652c 2073 656c  (self._name, sel
+00002300: 662e 5f70 726f 7029 295d 0a0a 2020 2020  f._prop))]..    
+00002310: 4070 726f 7065 7274 790a 2020 2020 6465  @property.    de
+00002320: 6620 6e75 6d5f 636f 6c75 6d6e 7328 7365  f num_columns(se
+00002330: 6c66 293a 0a20 2020 2020 2020 2072 6574  lf):.        ret
+00002340: 7572 6e20 310a 0a20 2020 2064 6566 2073  urn 1..    def s
+00002350: 6574 5f65 6c65 6d65 6e74 5f70 726f 7065  et_element_prope
+00002360: 7274 7928 7365 6c66 2c20 7072 6f70 293a  rty(self, prop):
+00002370: 0a20 2020 2020 2020 2073 656c 662e 5f70  .        self._p
+00002380: 726f 7020 3d20 7072 6f70 0a0a 2020 2020  rop = prop..    
+00002390: 6465 6620 7365 745f 6e61 6d65 2873 656c  def set_name(sel
+000023a0: 662c 206e 616d 6529 3a0a 2020 2020 2020  f, name):.      
+000023b0: 2020 7365 6c66 2e5f 6e61 6d65 203d 206e    self._name = n
+000023c0: 616d 650a 0a20 2020 2064 6566 2073 6574  ame..    def set
+000023d0: 5f6e 616e 2873 656c 6629 3a0a 2020 2020  _nan(self):.    
+000023e0: 2020 2020 6966 206e 702e 6973 7375 6264      if np.issubd
+000023f0: 7479 7065 2873 656c 662e 5f76 616c 7565  type(self._value
+00002400: 5f74 7970 652c 206e 702e 696e 7436 3429  _type, np.int64)
+00002410: 3a0a 2020 2020 2020 2020 2020 2020 7365  :.            se
+00002420: 6c66 2e5f 7661 6c75 6520 3d20 494e 5445  lf._value = INTE
+00002430: 4745 525f 4e41 4e0a 2020 2020 2020 2020  GER_NAN.        
+00002440: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+00002450: 2020 7365 6c66 2e5f 7661 6c75 6520 3d20    self._value = 
+00002460: 6e70 2e4e 614e 0a0a 2020 2020 6465 6620  np.NaN..    def 
+00002470: 7365 745f 7661 6c75 6528 7365 6c66 2c20  set_value(self, 
+00002480: 7661 6c75 6529 3a0a 2020 2020 2020 2020  value):.        
+00002490: 7365 6c66 2e5f 7661 6c75 6520 3d20 7661  self._value = va
+000024a0: 6c75 650a 2020 2020 2020 2020 6966 206e  lue.        if n
+000024b0: 6f74 2069 7369 6e73 7461 6e63 6528 7661  ot isinstance(va
+000024c0: 6c75 652c 2073 656c 662e 5f76 616c 7565  lue, self._value
+000024d0: 5f74 7970 6529 3a0a 2020 2020 2020 2020  _type):.        
+000024e0: 2020 2020 7365 6c66 2e5f 7661 6c75 655f      self._value_
+000024f0: 7479 7065 203d 2074 7970 6528 7661 6c75  type = type(valu
+00002500: 6529 0a0a 2020 2020 6465 6620 7365 745f  e)..    def set_
+00002510: 7661 6c75 655f 6672 6f6d 5f72 6177 2873  value_from_raw(s
+00002520: 656c 662c 2076 616c 7565 293a 0a20 2020  elf, value):.   
+00002530: 2020 2020 2073 656c 662e 5f76 616c 7565       self._value
+00002540: 203d 2076 616c 7565 0a0a 2020 2020 4070   = value..    @p
+00002550: 726f 7065 7274 790a 2020 2020 6465 6620  roperty.    def 
+00002560: 7661 6c75 6528 7365 6c66 293a 0a20 2020  value(self):.   
+00002570: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
+00002580: 2e5f 7661 6c75 650a 0a20 2020 2040 7072  ._value..    @pr
+00002590: 6f70 6572 7479 0a20 2020 2064 6566 2076  operty.    def v
+000025a0: 616c 7565 5f74 7970 6528 7365 6c66 293a  alue_type(self):
+000025b0: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
+000025c0: 7365 6c66 2e5f 7661 6c75 655f 7479 7065  self._value_type
+000025d0: 0a0a 0a63 6c61 7373 2056 616c 7565 4279  ...class ValueBy
+000025e0: 4c61 6265 6c28 5661 6c75 6553 746f 7261  Label(ValueStora
+000025f0: 6765 4261 7365 293a 0a20 2020 2022 2222  geBase):.    """
+00002600: 5374 6f72 6573 2061 206c 6973 7420 6f66  Stores a list of
+00002610: 206c 6973 7473 206f 6620 6e75 6d62 6572   lists of number
+00002620: 7320 6279 2061 6e20 6172 6269 7472 6172  s by an arbitrar
+00002630: 7920 6c61 6265 6c2e 2055 7365 2074 6869  y label. Use thi
+00002640: 7320 636c 6173 7320 7768 656e 2077 6f72  s class when wor
+00002650: 6b69 6e67 2077 6974 6820 636b 7445 6c65  king with cktEle
+00002660: 6d65 6e74 2066 756e 6374 696f 6e0a 2020  ment function.  
+00002670: 2020 6361 6c6c 7320 6c69 6b65 2043 7572    calls like Cur
+00002680: 7265 6e74 732c 2063 7572 7265 6e74 4d61  rents, currentMa
+00002690: 6741 6e67 2077 6865 7265 2065 7665 7279  gAng where every
+000026a0: 2074 776f 2063 6f6e 7365 6375 7469 7665   two consecutive
+000026b0: 2076 616c 7565 7320 696e 2074 6865 2072   values in the r
+000026c0: 6574 7572 6e65 6420 6c69 7374 2061 7265  eturned list are
+000026d0: 2072 6570 7265 7365 6e74 696e 6720 6f6e   representing on
+000026e0: 650a 2020 2020 7175 616e 7469 7479 2e20  e.    quantity. 
+000026f0: 5468 6520 636c 6173 7320 6469 6666 6572  The class differ
+00002700: 656e 7469 6174 6573 2062 6574 7765 656e  entiates between
+00002710: 2063 6f6d 706c 6578 2061 6e64 206d 6167   complex and mag
+00002720: 202f 2061 6e67 6c65 2072 6570 7265 7365   / angle represe
+00002730: 6e74 6174 696f 6e20 616e 6420 7374 6f72  ntation and stor
+00002740: 6573 2074 6865 2076 616c 7565 7320 6170  es the values ap
+00002750: 7072 6f70 7269 6174 656c 790a 2020 2020  propriately.    
+00002760: 2222 220a 2020 2020 6465 6620 5f5f 696e  """.    def __in
+00002770: 6974 5f5f 2873 656c 662c 206e 616d 652c  it__(self, name,
+00002780: 2070 726f 702c 2076 616c 7565 2c20 4e6f   prop, value, No
+00002790: 6465 732c 2069 735f 636f 6d70 6c65 782c  des, is_complex,
+000027a0: 2075 6e69 7473 293a 0a20 2020 2020 2020   units):.       
+000027b0: 2022 2222 436f 6e73 7472 7563 746f 7220   """Constructor 
+000027c0: 666f 7220 5661 6c75 6542 794c 6162 656c  for ValueByLabel
+000027d0: 0a0a 2020 2020 2020 2020 5061 7261 6d65  ..        Parame
+000027e0: 7465 7273 0a20 2020 2020 2020 202d 2d2d  ters.        ---
+000027f0: 2d2d 2d2d 2d2d 2d0a 2020 2020 2020 2020  -------.        
+00002800: 6e61 6d65 203a 2073 7472 0a20 2020 2020  name : str.     
+00002810: 2020 2070 726f 7020 3a20 7374 720a 2020     prop : str.  
+00002820: 2020 2020 2020 6c61 6265 6c5f 7072 6566        label_pref
+00002830: 6978 203a 2073 7472 0a20 2020 2020 2020  ix : str.       
+00002840: 2020 2020 2054 6578 7420 746f 2075 7365       Text to use
+00002850: 2061 7320 6120 7072 6566 6978 2066 6f72   as a prefix for
+00002860: 2063 6f6c 756d 6e20 6c61 6265 6c73 2e20   column labels. 
+00002870: 4578 3a20 5068 6173 650a 2020 2020 2020  Ex: Phase.      
+00002880: 2020 6c61 6265 6c73 203a 206c 6973 740a    labels : list.
+00002890: 2020 2020 2020 2020 2020 2020 6c69 7374              list
+000028a0: 206f 6620 7374 720a 2020 2020 2020 2020   of str.        
+000028b0: 7661 6c75 6573 203a 206c 6973 740a 2020  values : list.  
+000028c0: 2020 2020 2020 2020 2020 5061 6972 7320            Pairs 
+000028d0: 6f66 2076 616c 7565 7320 7468 6174 2063  of values that c
+000028e0: 616e 2062 6520 696e 7465 7270 7265 7465  an be interprete
+000028f0: 6420 6173 2063 6f6d 706c 6578 206e 756d  d as complex num
+00002900: 6265 7273 2e0a 0a20 2020 2020 2020 2022  bers...        "
+00002910: 2222 0a20 2020 2020 2020 2073 7570 6572  "".        super
+00002920: 2829 2e5f 5f69 6e69 745f 5f28 290a 2020  ().__init__().  
+00002930: 2020 2020 2020 7068 7320 3d20 7b0a 2020        phs = {.  
+00002940: 2020 2020 2020 2020 2020 313a 2027 4127            1: 'A'
+00002950: 2c0a 2020 2020 2020 2020 2020 2020 323a  ,.            2:
+00002960: 2027 4227 2c0a 2020 2020 2020 2020 2020   'B',.          
+00002970: 2020 333a 2027 4327 2c0a 2020 2020 2020    3: 'C',.      
+00002980: 2020 2020 2020 303a 2027 4e27 2c0a 2020        0: 'N',.  
+00002990: 2020 2020 2020 7d0a 0a20 2020 2020 2020        }..       
+000029a0: 2073 656c 662e 5f6e 616d 6520 3d20 6e61   self._name = na
+000029b0: 6d65 0a20 2020 2020 2020 2073 656c 662e  me.        self.
+000029c0: 5f70 726f 7020 3d20 7072 6f70 0a20 2020  _prop = prop.   
+000029d0: 2020 2020 2073 656c 662e 5f6e 6f64 6573       self._nodes
+000029e0: 203d 204e 6f64 6573 0a20 2020 2020 2020   = Nodes.       
+000029f0: 2073 656c 662e 5f6c 6162 656c 7320 3d20   self._labels = 
+00002a00: 5b5d 0a20 2020 2020 2020 2073 656c 662e  [].        self.
+00002a10: 5f76 616c 7565 203d 205b 5d0a 2020 2020  _value = [].    
+00002a20: 2020 2020 7365 6c66 2e5f 7661 6c75 655f      self._value_
+00002a30: 7479 7065 203d 2063 6f6d 706c 6578 2069  type = complex i
+00002a40: 6620 6973 5f63 6f6d 706c 6578 2065 6c73  f is_complex els
+00002a50: 6520 666c 6f61 740a 2020 2020 2020 2020  e float.        
+00002a60: 7365 6c66 2e5f 6973 5f63 6f6d 706c 6578  self._is_complex
+00002a70: 203d 2069 735f 636f 6d70 6c65 780a 0a20   = is_complex.. 
+00002a80: 2020 2020 2020 206e 203d 2032 0a20 2020         n = 2.   
+00002a90: 2020 2020 206d 203d 2069 6e74 286c 656e       m = int(len
+00002aa0: 2876 616c 7565 2920 2f20 286c 656e 284e  (value) / (len(N
+00002ab0: 6f64 6573 292a 6e29 290a 0a20 2020 2020  odes)*n))..     
+00002ac0: 2020 2073 656c 662e 5f6d 203d 206d 0a20     self._m = m. 
+00002ad0: 2020 2020 2020 2073 656c 662e 5f6e 203d         self._n =
+00002ae0: 206e 0a20 2020 2020 2020 2073 656c 662e   n.        self.
+00002af0: 5f76 616c 7565 5f6c 656e 6774 6820 3d20  _value_length = 
+00002b00: 6c65 6e28 7661 6c75 6529 0a20 2020 2020  len(value).     
+00002b10: 2020 2076 616c 7565 203d 2073 656c 662e     value = self.
+00002b20: 5f66 6978 5f76 616c 7565 2876 616c 7565  _fix_value(value
+00002b30: 290a 2020 2020 2020 2020 0a20 2020 2020  ).        .     
+00002b40: 2020 2023 2043 6875 6e6b 5f6c 6973 7420     # Chunk_list 
+00002b50: 6578 616d 706c 650a 2020 2020 2020 2020  example.        
+00002b60: 2320 5820 3d20 6c69 7374 2872 616e 6765  # X = list(range
+00002b70: 2831 3229 2920 2c20 6e4c 6973 743d 2032  (12)) , nList= 2
+00002b80: 0a20 2020 2020 2020 2023 2059 203d 2063  .        # Y = c
+00002b90: 6875 6e6b 5f6c 6973 7428 582c 206e 4c69  hunk_list(X, nLi
+00002ba0: 7374 2920 2d3e 205b 5b30 2c20 315d 2c20  st) -> [[0, 1], 
+00002bb0: 5b32 2c20 335d 2c20 5b34 2c20 355d 2c20  [2, 3], [4, 5], 
+00002bc0: 5b36 2c20 375d 2c20 5b38 2c20 395d 2c20  [6, 7], [8, 9], 
+00002bd0: 5b31 302c 2031 315d 5d0a 2020 2020 2020  [10, 11]].      
+00002be0: 2020 2320 4769 7665 6e20 656c 656d 656e    # Given elemen
+00002bf0: 7420 6861 7320 3220 7465 726d 696e 616c  t has 2 terminal
+00002c00: 7320 6d20 3d20 3132 202f 2028 322a 3229  s m = 12 / (2*2)
+00002c10: 203d 2033 0a20 2020 2020 2020 2023 205a   = 3.        # Z
+00002c20: 203d 2020 6368 756e 6b5f 6c69 7374 2859   =  chunk_list(Y
+00002c30: 2c20 6d29 202d 203e 205b 0a20 2020 2020  , m) - > [.     
+00002c40: 2020 2023 2020 2020 2020 2020 2020 2020     #            
+00002c50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002c60: 5b5b 302c 2031 5d2c 205b 322c 2033 5d2c  [[0, 1], [2, 3],
+00002c70: 205b 342c 2035 5d5d 2c20 2054 6572 6d69   [4, 5]],  Termi
+00002c80: 6e61 6c20 6f6e 6520 636f 6d70 6c65 7820  nal one complex 
+00002c90: 7061 6972 730a 2020 2020 2020 2020 2320  pairs.        # 
+00002ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002cb0: 2020 2020 2020 2020 2020 205b 5b36 2c20             [[6, 
+00002cc0: 375d 2c20 5b38 2c20 395d 2c20 5b31 302c  7], [8, 9], [10,
+00002cd0: 2031 315d 5d20 5465 726d 696e 616c 2074   11]] Terminal t
+00002ce0: 776f 2063 6f6d 706c 6578 2070 6169 7273  wo complex pairs
+00002cf0: 0a20 2020 2020 2020 2023 2020 2020 2020  .        #      
+00002d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002d10: 2020 2020 2020 5d0a 0a20 2020 2020 2020        ]..       
+00002d20: 2066 6f72 2069 2c20 6e6f 6465 5f76 616c   for i, node_val
+00002d30: 2069 6e20 656e 756d 6572 6174 6528 7a69   in enumerate(zi
+00002d40: 7028 7365 6c66 2e5f 6e6f 6465 732c 2076  p(self._nodes, v
+00002d50: 616c 7565 2929 3a0a 2020 2020 2020 2020  alue)):.        
+00002d60: 2020 2020 6e6f 6465 2c20 7661 6c20 3d20      node, val = 
+00002d70: 6e6f 6465 5f76 616c 0a20 2020 2020 2020  node_val.       
+00002d80: 2020 2020 2066 6f72 2076 2c20 7820 696e       for v, x in
+00002d90: 207a 6970 286e 6f64 652c 2076 616c 293a   zip(node, val):
+00002da0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00002db0: 206c 6162 656c 203d 2027 7b7d 7b7d 272e   label = '{}{}'.
+00002dc0: 666f 726d 6174 2870 6873 5b76 5d2c 2073  format(phs[v], s
+00002dd0: 7472 2869 2b31 2929 0a20 2020 2020 2020  tr(i+1)).       
+00002de0: 2020 2020 2020 2020 2023 204e 6f74 6520           # Note 
+00002df0: 7468 6174 2074 6865 2076 616c 7565 206c  that the value l
+00002e00: 6f67 6963 2069 7320 6475 706c 6963 6174  ogic is duplicat
+00002e10: 6564 2069 6e20 7365 745f 7661 6c75 655f  ed in set_value_
+00002e20: 6672 6f6d 5f72 6177 0a20 2020 2020 2020  from_raw.       
+00002e30: 2020 2020 2020 2020 2069 6620 7365 6c66           if self
+00002e40: 2e5f 6973 5f63 6f6d 706c 6578 3a0a 2020  ._is_complex:.  
 00002e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002e60: 2020 2020 205b 5b36 2c20 375d 2c20 5b38       [[6, 7], [8
-00002e70: 2c20 395d 2c20 5b31 302c 2031 315d 5d20  , 9], [10, 11]] 
-00002e80: 5465 726d 696e 616c 2074 776f 2063 6f6d  Terminal two com
-00002e90: 706c 6578 2070 6169 7273 0d0a 2020 2020  plex pairs..    
-00002ea0: 2020 2020 2320 2020 2020 2020 2020 2020      #           
-00002eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002ec0: 205d 0d0a 0d0a 2020 2020 2020 2020 666f   ]....        fo
-00002ed0: 7220 692c 206e 6f64 655f 7661 6c20 696e  r i, node_val in
-00002ee0: 2065 6e75 6d65 7261 7465 287a 6970 2873   enumerate(zip(s
-00002ef0: 656c 662e 5f6e 6f64 6573 2c20 7661 6c75  elf._nodes, valu
-00002f00: 6529 293a 0d0a 2020 2020 2020 2020 2020  e)):..          
-00002f10: 2020 6e6f 6465 2c20 7661 6c20 3d20 6e6f    node, val = no
-00002f20: 6465 5f76 616c 0d0a 2020 2020 2020 2020  de_val..        
-00002f30: 2020 2020 666f 7220 762c 2078 2069 6e20      for v, x in 
-00002f40: 7a69 7028 6e6f 6465 2c20 7661 6c29 3a0d  zip(node, val):.
-00002f50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00002f60: 206c 6162 656c 203d 2027 7b7d 7b7d 272e   label = '{}{}'.
-00002f70: 666f 726d 6174 2870 6873 5b76 5d2c 2073  format(phs[v], s
-00002f80: 7472 2869 2b31 2929 0d0a 2020 2020 2020  tr(i+1))..      
-00002f90: 2020 2020 2020 2020 2020 2320 4e6f 7465            # Note
-00002fa0: 2074 6861 7420 7468 6520 7661 6c75 6520   that the value 
-00002fb0: 6c6f 6769 6320 6973 2064 7570 6c69 6361  logic is duplica
-00002fc0: 7465 6420 696e 2073 6574 5f76 616c 7565  ted in set_value
-00002fd0: 5f66 726f 6d5f 7261 770d 0a20 2020 2020  _from_raw..     
-00002fe0: 2020 2020 2020 2020 2020 2069 6620 7365             if se
-00002ff0: 6c66 2e5f 6973 5f63 6f6d 706c 6578 3a0d  lf._is_complex:.
-00003000: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00003010: 2020 2020 206c 6162 656c 202b 3d20 2220       label += " 
-00003020: 2220 2b20 756e 6974 735b 305d 0d0a 2020  " + units[0]..  
-00003030: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003040: 2020 7365 6c66 2e5f 6c61 6265 6c73 2e61    self._labels.a
-00003050: 7070 656e 6428 6c61 6265 6c29 0d0a 2020  ppend(label)..  
-00003060: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003070: 2020 7365 6c66 2e5f 7661 6c75 6520 2b3d    self._value +=
-00003080: 205b 636f 6d70 6c65 7828 785b 305d 2c20   [complex(x[0], 
-00003090: 785b 315d 295d 0d0a 2020 2020 2020 2020  x[1])]..        
-000030a0: 2020 2020 2020 2020 656c 7365 3a0d 0a20          else:.. 
-000030b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000030c0: 2020 206c 6162 656c 5f6d 6167 203d 206c     label_mag = l
-000030d0: 6162 656c 202b 2073 656c 662e 4445 4c49  abel + self.DELI
-000030e0: 4d49 5445 5220 2b20 226d 6167 2220 2b20  MITER + "mag" + 
-000030f0: 2720 2720 2b20 756e 6974 735b 305d 0d0a  ' ' + units[0]..
-00003100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003110: 2020 2020 6c61 6265 6c5f 616e 6720 3d20      label_ang = 
-00003120: 6c61 6265 6c20 2b20 7365 6c66 2e44 454c  label + self.DEL
-00003130: 494d 4954 4552 202b 2022 616e 6722 202b  IMITER + "ang" +
-00003140: 2027 2027 202b 2075 6e69 7473 5b31 5d0d   ' ' + units[1].
-00003150: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00003160: 2020 2020 2073 656c 662e 5f6c 6162 656c       self._label
-00003170: 732e 6578 7465 6e64 285b 6c61 6265 6c5f  s.extend([label_
-00003180: 6d61 672c 206c 6162 656c 5f61 6e67 5d29  mag, label_ang])
-00003190: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-000031a0: 2020 2020 2020 7365 6c66 2e5f 7661 6c75        self._valu
-000031b0: 6520 2b3d 205b 785b 305d 2c20 785b 315d  e += [x[0], x[1]
-000031c0: 5d0d 0a0d 0a20 2020 2064 6566 205f 5f69  ]....    def __i
-000031d0: 6164 645f 5f28 7365 6c66 2c20 6f74 6865  add__(self, othe
-000031e0: 7229 3a0d 0a20 2020 2020 2020 2066 6f72  r):..        for
-000031f0: 2069 2069 6e20 7261 6e67 6528 6c65 6e28   i in range(len(
-00003200: 7365 6c66 2e5f 7661 6c75 6529 293a 0d0a  self._value)):..
-00003210: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00003220: 2e5f 7661 6c75 655b 695d 202b 3d20 6f74  ._value[i] += ot
-00003230: 6865 722e 7661 6c75 655b 695d 0d0a 2020  her.value[i]..  
-00003240: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
-00003250: 660d 0a0d 0a20 2020 2064 6566 205f 5f67  f....    def __g
-00003260: 745f 5f28 7365 6c66 2c20 6f74 6865 7229  t__(self, other)
-00003270: 3a0d 0a20 2020 2020 2020 2023 2054 4f44  :..        # TOD
-00003280: 4f0d 0a20 2020 2020 2020 2072 6574 7572  O..        retur
-00003290: 6e20 7375 6d28 7365 6c66 2e5f 7661 6c75  n sum(self._valu
-000032a0: 6529 203e 2073 756d 286f 7468 6572 2e76  e) > sum(other.v
-000032b0: 616c 7565 290d 0a0d 0a20 2020 2040 7072  alue)....    @pr
-000032c0: 6f70 6572 7479 0d0a 2020 2020 6465 6620  operty..    def 
-000032d0: 7661 6c75 6528 7365 6c66 293a 0d0a 2020  value(self):..  
-000032e0: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
-000032f0: 662e 5f76 616c 7565 0d0a 0d0a 2020 2020  f._value....    
-00003300: 4073 7461 7469 636d 6574 686f 640d 0a20  @staticmethod.. 
-00003310: 2020 2064 6566 2063 6875 6e6b 5f6c 6973     def chunk_lis
-00003320: 7428 7661 6c75 6573 2c20 6e4c 6973 7473  t(values, nLists
-00003330: 293a 0d0a 2020 2020 2020 2020 2320 544f  ):..        # TO
-00003340: 444f 3a20 7468 6973 2062 7265 616b 7320  DO: this breaks 
-00003350: 666f 7220 4275 732e 7075 566d 6167 416e  for Bus.puVmagAn
-00003360: 676c 6520 696e 206d 6f6e 7465 2063 6172  gle in monte car
-00003370: 6c6f 2065 7861 6d70 6c65 2074 6573 740d  lo example test.
-00003380: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
-00003390: 205b 7661 6c75 6573 5b69 202a 206e 4c69   [values[i * nLi
-000033a0: 7374 733a 2869 202b 2031 2920 2a20 6e4c  sts:(i + 1) * nL
-000033b0: 6973 7473 5d20 666f 7220 6920 696e 2072  ists] for i in r
-000033c0: 616e 6765 2828 6c65 6e28 7661 6c75 6573  ange((len(values
-000033d0: 2920 2b20 6e4c 6973 7473 202d 2031 2920  ) + nLists - 1) 
-000033e0: 2f2f 206e 4c69 7374 7329 5d0d 0a0d 0a20  // nLists)].... 
-000033f0: 2020 2064 6566 205f 6669 785f 7661 6c75     def _fix_valu
-00003400: 6528 7365 6c66 2c20 7661 6c75 6529 3a0d  e(self, value):.
-00003410: 0a20 2020 2020 2020 2076 616c 7565 203d  .        value =
-00003420: 2073 656c 662e 6368 756e 6b5f 6c69 7374   self.chunk_list
-00003430: 2876 616c 7565 2c20 7365 6c66 2e5f 6e29  (value, self._n)
-00003440: 0d0a 2020 2020 2020 2020 7661 6c75 6520  ..        value 
-00003450: 3d20 7365 6c66 2e63 6875 6e6b 5f6c 6973  = self.chunk_lis
-00003460: 7428 7661 6c75 652c 2073 656c 662e 5f6d  t(value, self._m
-00003470: 290d 0a20 2020 2020 2020 2072 6574 7572  )..        retur
-00003480: 6e20 7661 6c75 650d 0a0d 0a20 2020 2064  n value....    d
-00003490: 6566 2069 735f 6e61 6e28 7365 6c66 293a  ef is_nan(self):
-000034a0: 0d0a 2020 2020 2020 2020 6966 206e 702e  ..        if np.
-000034b0: 6973 7375 6264 7479 7065 2873 656c 662e  issubdtype(self.
-000034c0: 5f76 616c 7565 5f74 7970 652c 206e 702e  _value_type, np.
-000034d0: 696e 7436 3429 3a0d 0a20 2020 2020 2020  int64):..       
-000034e0: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
-000034f0: 2e5f 7661 6c75 655b 305d 203d 3d20 494e  ._value[0] == IN
-00003500: 5445 4745 525f 4e41 4e0d 0a20 2020 2020  TEGER_NAN..     
-00003510: 2020 2072 6574 7572 6e20 6e70 2e69 736e     return np.isn
-00003520: 616e 2873 656c 662e 5f76 616c 7565 5b30  an(self._value[0
-00003530: 5d29 0d0a 0d0a 2020 2020 6465 6620 6d61  ])....    def ma
-00003540: 6b65 5f63 6f6c 756d 6e73 2873 656c 6629  ke_columns(self)
-00003550: 3a0d 0a20 2020 2020 2020 2072 6574 7572  :..        retur
-00003560: 6e20 5b0d 0a20 2020 2020 2020 2020 2020  n [..           
-00003570: 2073 656c 662e 4445 4c49 4d49 5445 522e   self.DELIMITER.
-00003580: 6a6f 696e 2828 7365 6c66 2e5f 6e61 6d65  join((self._name
-00003590: 2c20 6622 7b78 7d22 2929 2066 6f72 2078  , f"{x}")) for x
-000035a0: 2069 6e20 7365 6c66 2e5f 6c61 6265 6c73   in self._labels
-000035b0: 0d0a 2020 2020 2020 2020 5d0d 0a0d 0a20  ..        ].... 
-000035c0: 2020 2040 7072 6f70 6572 7479 0d0a 2020     @property..  
-000035d0: 2020 6465 6620 6e75 6d5f 636f 6c75 6d6e    def num_column
-000035e0: 7328 7365 6c66 293a 0d0a 2020 2020 2020  s(self):..      
-000035f0: 2020 7265 7475 726e 206c 656e 2873 656c    return len(sel
-00003600: 662e 5f6c 6162 656c 7329 0d0a 0d0a 2020  f._labels)....  
-00003610: 2020 6465 6620 7365 745f 656c 656d 656e    def set_elemen
-00003620: 745f 7072 6f70 6572 7479 2873 656c 662c  t_property(self,
-00003630: 2070 726f 7029 3a0d 0a20 2020 2020 2020   prop):..       
-00003640: 2073 656c 662e 5f70 726f 7020 3d20 7072   self._prop = pr
-00003650: 6f70 0d0a 0d0a 2020 2020 6465 6620 7365  op....    def se
-00003660: 745f 6e61 6d65 2873 656c 662c 206e 616d  t_name(self, nam
-00003670: 6529 3a0d 0a20 2020 2020 2020 2073 656c  e):..        sel
-00003680: 662e 5f6e 616d 6520 3d20 6e61 6d65 0d0a  f._name = name..
-00003690: 0d0a 2020 2020 6465 6620 7365 745f 6e61  ..    def set_na
-000036a0: 6e28 7365 6c66 293a 0d0a 2020 2020 2020  n(self):..      
-000036b0: 2020 666f 7220 6920 696e 2072 616e 6765    for i in range
-000036c0: 286c 656e 2873 656c 662e 5f76 616c 7565  (len(self._value
-000036d0: 2929 3a0d 0a20 2020 2020 2020 2020 2020  )):..           
-000036e0: 2073 656c 662e 5f76 616c 7565 5b69 5d20   self._value[i] 
-000036f0: 3d20 6e70 2e4e 614e 0d0a 0d0a 2020 2020  = np.NaN....    
-00003700: 6465 6620 7365 745f 7661 6c75 6528 7365  def set_value(se
-00003710: 6c66 2c20 7661 6c75 6529 3a0d 0a20 2020  lf, value):..   
-00003720: 2020 2020 2073 656c 662e 5f76 616c 7565       self._value
-00003730: 203d 2076 616c 7565 0d0a 2020 2020 2020   = value..      
-00003740: 2020 6966 206e 6f74 2069 7369 6e73 7461    if not isinsta
-00003750: 6e63 6528 7661 6c75 655b 305d 2c20 7365  nce(value[0], se
-00003760: 6c66 2e5f 7661 6c75 655f 7479 7065 293a  lf._value_type):
-00003770: 0d0a 2020 2020 2020 2020 2020 2020 7365  ..            se
-00003780: 6c66 2e5f 7661 6c75 655f 7479 7065 203d  lf._value_type =
-00003790: 2074 7970 6528 7661 6c75 655b 305d 290d   type(value[0]).
-000037a0: 0a0d 0a20 2020 2064 6566 2073 6574 5f76  ...    def set_v
-000037b0: 616c 7565 5f66 726f 6d5f 7261 7728 7365  alue_from_raw(se
-000037c0: 6c66 2c20 7661 6c75 6529 3a0d 0a20 2020  lf, value):..   
-000037d0: 2020 2020 2069 6620 6c65 6e28 7661 6c75       if len(valu
-000037e0: 6529 2021 3d20 7365 6c66 2e5f 7661 6c75  e) != self._valu
-000037f0: 655f 6c65 6e67 7468 3a0d 0a20 2020 2020  e_length:..     
-00003800: 2020 2020 2020 2076 616c 7565 203d 205b         value = [
-00003810: 6e70 2e4e 614e 2066 6f72 2069 2069 6e20  np.NaN for i in 
-00003820: 7261 6e67 6528 7365 6c66 2e5f 7661 6c75  range(self._valu
-00003830: 655f 6c65 6e67 7468 295d 0d0a 2020 2020  e_length)]..    
-00003840: 2020 2020 0d0a 2020 2020 2020 2020 7661      ..        va
-00003850: 6c75 6520 3d20 7365 6c66 2e5f 6669 785f  lue = self._fix_
-00003860: 7661 6c75 6528 7661 6c75 6529 0d0a 2020  value(value)..  
-00003870: 2020 2020 2020 7365 6c66 2e5f 7661 6c75        self._valu
-00003880: 652e 636c 6561 7228 290d 0a20 2020 2020  e.clear()..     
-00003890: 2020 200d 0a20 2020 2020 2020 2066 6f72     ..        for
-000038a0: 2069 2c20 6e6f 6465 5f76 616c 2069 6e20   i, node_val in 
-000038b0: 656e 756d 6572 6174 6528 7a69 7028 7365  enumerate(zip(se
-000038c0: 6c66 2e5f 6e6f 6465 732c 2076 616c 7565  lf._nodes, value
-000038d0: 2929 3a0d 0a20 2020 2020 2020 2020 2020  )):..           
-000038e0: 206e 6f64 652c 2076 616c 203d 206e 6f64   node, val = nod
-000038f0: 655f 7661 6c0d 0a20 2020 2020 2020 2020  e_val..         
-00003900: 2020 2066 6f72 2076 2c20 7820 696e 207a     for v, x in z
-00003910: 6970 286e 6f64 652c 2076 616c 293a 0d0a  ip(node, val):..
-00003920: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003930: 6966 2073 656c 662e 5f69 735f 636f 6d70  if self._is_comp
-00003940: 6c65 783a 0d0a 2020 2020 2020 2020 2020  lex:..          
-00003950: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
-00003960: 7661 6c75 6520 2b3d 205b 636f 6d70 6c65  value += [comple
-00003970: 7828 785b 305d 2c20 785b 315d 295d 0d0a  x(x[0], x[1])]..
-00003980: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003990: 656c 7365 3a0d 0a20 2020 2020 2020 2020  else:..         
-000039a0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-000039b0: 5f76 616c 7565 202b 3d20 5b78 5b30 5d2c  _value += [x[0],
-000039c0: 2078 5b31 5d5d 0d0a 2020 2020 4070 726f   x[1]]..    @pro
-000039d0: 7065 7274 790d 0a20 2020 2064 6566 2076  perty..    def v
-000039e0: 616c 7565 5f74 7970 6528 7365 6c66 293a  alue_type(self):
-000039f0: 0d0a 2020 2020 2020 2020 7265 7475 726e  ..        return
-00003a00: 2073 656c 662e 5f76 616c 7565 5f74 7970   self._value_typ
-00003a10: 650d 0a0d 0a0d 0a63 6c61 7373 2056 616c  e......class Val
-00003a20: 7565 436f 6e74 6169 6e65 723a 0d0a 2020  ueContainer:..  
-00003a30: 2020 2222 2243 6f6e 7461 696e 6572 2066    """Container f
-00003a40: 6f72 2061 2073 6571 7565 6e63 6520 6f66  or a sequence of
-00003a50: 2069 6e73 7461 6e63 6573 206f 6620 5661   instances of Va
-00003a60: 6c75 6553 746f 7261 6765 4261 7365 2e22  lueStorageBase."
-00003a70: 2222 0d0a 0d0a 2020 2020 6465 6620 5f5f  ""....    def __
-00003a80: 696e 6974 5f5f 2873 656c 662c 2076 616c  init__(self, val
-00003a90: 7565 732c 2068 6466 5f73 746f 7265 2c20  ues, hdf_store, 
-00003aa0: 7061 7468 2c20 6d61 785f 7369 7a65 2c20  path, max_size, 
-00003ab0: 656c 656d 5f6e 616d 6573 2c0d 0a20 2020  elem_names,..   
-00003ac0: 2020 2020 2020 2020 2020 2020 2020 6461                da
-00003ad0: 7461 7365 745f 7072 6f70 6572 7479 5f74  taset_property_t
-00003ae0: 7970 652c 206d 6178 5f63 6875 6e6b 5f62  ype, max_chunk_b
-00003af0: 7974 6573 3d4e 6f6e 652c 2073 746f 7265  ytes=None, store
-00003b00: 5f74 696d 655f 7374 6570 3d46 616c 7365  _time_step=False
-00003b10: 293a 0d0a 2020 2020 2020 2020 6772 6f75  ):..        grou
-00003b20: 705f 6e61 6d65 203d 206f 732e 7061 7468  p_name = os.path
-00003b30: 2e64 6972 6e61 6d65 2870 6174 6829 0d0a  .dirname(path)..
-00003b40: 2020 2020 2020 2020 6261 7365 6e61 6d65          basename
-00003b50: 203d 206f 732e 7061 7468 2e62 6173 656e   = os.path.basen
-00003b60: 616d 6528 7061 7468 290d 0a20 2020 2020  ame(path)..     
-00003b70: 2020 2073 656c 662e 6772 6f75 705f 6e61     self.group_na
-00003b80: 6d65 203d 2067 726f 7570 5f6e 616d 650d  me = group_name.
-00003b90: 0a20 2020 2020 2020 2073 656c 662e 6261  .        self.ba
-00003ba0: 7365 5f6e 616d 6520 3d20 6261 7365 6e61  se_name = basena
-00003bb0: 6d65 0d0a 2020 2020 2020 2020 7472 793a  me..        try:
-00003bc0: 0d0a 2020 2020 2020 2020 2020 2020 6966  ..            if
-00003bd0: 2062 6173 656e 616d 6520 696e 2068 6466   basename in hdf
-00003be0: 5f73 746f 7265 5b67 726f 7570 5f6e 616d  _store[group_nam
-00003bf0: 655d 3a0d 0a20 2020 2020 2020 2020 2020  e]:..           
-00003c00: 2020 2020 2072 6169 7365 2049 6e76 616c       raise Inval
-00003c10: 6964 5061 7261 6d65 7465 7228 6622 6475  idParameter(f"du
-00003c20: 706c 6963 6174 6520 6461 7461 7365 7420  plicate dataset 
-00003c30: 6e61 6d65 207b 6261 7365 6e61 6d65 7d22  name {basename}"
-00003c40: 290d 0a20 2020 2020 2020 2065 7863 6570  )..        excep
-00003c50: 7420 4b65 7945 7272 6f72 3a0d 0a20 2020  t KeyError:..   
-00003c60: 2020 2020 2020 2020 2023 2044 6f6e 2774           # Don't
-00003c70: 2062 6f74 6865 7220 6368 6563 6b69 6e67   bother checking
-00003c80: 2065 6163 6820 7375 6220 7061 7468 2e0d   each sub path..
-00003c90: 0a20 2020 2020 2020 2020 2020 2070 6173  .            pas
-00003ca0: 730d 0a20 2020 2020 2020 200d 0a20 2020  s..        ..   
-00003cb0: 2020 2020 2073 656c 662e 5f6c 656e 6774       self._lengt
-00003cc0: 683d 7b7d 0d0a 2020 2020 2020 2020 666f  h={}..        fo
-00003cd0: 7220 7661 6c75 6520 696e 2076 616c 7565  r value in value
-00003ce0: 733a 0d0a 2020 2020 2020 2020 2020 2020  s:..            
-00003cf0: 6966 2069 7369 6e73 7461 6e63 6528 7661  if isinstance(va
-00003d00: 6c75 652c 206c 6973 7429 3a0d 0a20 2020  lue, list):..   
-00003d10: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00003d20: 662e 5f6c 656e 6774 685b 7661 6c75 655d  f._length[value]
-00003d30: 203d 206c 656e 2876 616c 7565 2e76 616c   = len(value.val
-00003d40: 7565 290d 0a20 2020 2020 2020 2020 2020  ue)..           
-00003d50: 2065 6c73 653a 0d0a 2020 2020 2020 2020   else:..        
-00003d60: 2020 2020 2020 2020 7365 6c66 2e5f 6c65          self._le
-00003d70: 6e67 7468 5b76 616c 7565 5d20 3d20 310d  ngth[value] = 1.
-00003d80: 0a20 2020 2020 2020 200d 0a20 2020 2020  .        ..     
-00003d90: 2020 2064 7479 7065 203d 2076 616c 7565     dtype = value
-00003da0: 735b 305d 2e76 616c 7565 5f74 7970 650d  s[0].value_type.
-00003db0: 0a20 2020 2020 2020 2073 6361 6c65 6f66  .        scaleof
-00003dc0: 6673 6574 203d 204e 6f6e 650d 0a20 2020  fset = None..   
-00003dd0: 2020 2020 2023 2054 6865 7265 2069 7320       # There is 
-00003de0: 6e6f 206e 702e 666c 6f61 7431 3238 206f  no np.float128 o
-00003df0: 6e20 5769 6e64 6f77 732e 0d0a 2020 2020  n Windows...    
-00003e00: 2020 2020 6966 2064 7479 7065 2069 6e20      if dtype in 
-00003e10: 2866 6c6f 6174 2c20 6e70 2e66 6c6f 6174  (float, np.float
-00003e20: 3332 2c20 6e70 2e66 6c6f 6174 3634 2c20  32, np.float64, 
-00003e30: 6e70 2e6c 6f6e 6764 6f75 626c 6529 3a0d  np.longdouble):.
-00003e40: 0a20 2020 2020 2020 2020 2020 2073 6361  .            sca
-00003e50: 6c65 6f66 6673 6574 203d 2034 0d0a 2020  leoffset = 4..  
-00003e60: 2020 2020 2020 7469 6d65 5f73 7465 705f        time_step_
-00003e70: 7061 7468 203d 204e 6f6e 650d 0a20 2020  path = None..   
-00003e80: 2020 2020 206d 6178 5f73 697a 6520 3d20       max_size = 
-00003e90: 6d61 785f 7369 7a65 202a 206c 656e 2876  max_size * len(v
-00003ea0: 616c 7565 7329 2069 6620 7374 6f72 655f  alues) if store_
-00003eb0: 7469 6d65 5f73 7465 7020 656c 7365 206d  time_step else m
-00003ec0: 6178 5f73 697a 650d 0a0d 0a20 2020 2020  ax_size....     
-00003ed0: 2020 2069 6620 7374 6f72 655f 7469 6d65     if store_time
-00003ee0: 5f73 7465 703a 0d0a 2020 2020 2020 2020  _step:..        
-00003ef0: 2020 2020 2320 5374 6f72 6520 696e 6469      # Store indi
-00003f00: 6365 7320 666f 7220 7469 6d65 2073 7465  ces for time ste
-00003f10: 7020 616e 6420 656c 656d 656e 742e 0d0a  p and element...
-00003f20: 2020 2020 2020 2020 2020 2020 2320 4561              # Ea
-00003f30: 6368 2072 6f77 206f 6620 7468 6973 2064  ch row of this d
-00003f40: 6174 6173 6574 2063 6f72 7265 7370 6f6e  ataset correspon
-00003f50: 6473 2074 6f20 6120 726f 7720 696e 2074  ds to a row in t
-00003f60: 6865 2064 6174 612e 0d0a 2020 2020 2020  he data...      
-00003f70: 2020 2020 2020 2320 5468 6973 2077 696c        # This wil
-00003f80: 6c20 6265 2072 6571 7569 7265 6420 746f  l be required to
-00003f90: 2069 6e74 6572 7072 6574 2074 6865 2072   interpret the r
-00003fa0: 6177 2064 6174 612e 0d0a 2020 2020 2020  aw data...      
-00003fb0: 2020 2020 2020 6174 7472 6962 7574 6573        attributes
-00003fc0: 203d 207b 2274 7970 6522 3a20 4461 7461   = {"type": Data
-00003fd0: 7365 7450 726f 7065 7274 7954 7970 652e  setPropertyType.
-00003fe0: 5449 4d45 5f53 5445 502e 7661 6c75 657d  TIME_STEP.value}
-00003ff0: 0d0a 2020 2020 2020 2020 2020 2020 7469  ..            ti
-00004000: 6d65 5f73 7465 705f 7061 7468 203d 2073  me_step_path = s
-00004010: 656c 662e 7469 6d65 5f73 7465 705f 7061  elf.time_step_pa
-00004020: 7468 2870 6174 6829 0d0a 2020 2020 2020  th(path)..      
-00004030: 2020 2020 2020 7365 6c66 2e5f 7469 6d65        self._time
-00004040: 5f73 7465 7073 203d 2044 6174 6173 6574  _steps = Dataset
-00004050: 4275 6666 6572 280d 0a20 2020 2020 2020  Buffer(..       
-00004060: 2020 2020 2020 2020 2068 6466 5f73 746f           hdf_sto
-00004070: 7265 2c0d 0a20 2020 2020 2020 2020 2020  re,..           
-00004080: 2020 2020 2074 696d 655f 7374 6570 5f70       time_step_p
-00004090: 6174 682c 0d0a 2020 2020 2020 2020 2020  ath,..          
-000040a0: 2020 2020 2020 6d61 785f 7369 7a65 2c0d        max_size,.
-000040b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000040c0: 2069 6e74 2c0d 0a20 2020 2020 2020 2020   int,..         
-000040d0: 2020 2020 2020 205b 2254 696d 6522 2c20         ["Time", 
-000040e0: 224e 616d 6522 5d2c 0d0a 2020 2020 2020  "Name"],..      
-000040f0: 2020 2020 2020 2020 2020 7363 616c 656f            scaleo
-00004100: 6666 7365 743d 302c 0d0a 2020 2020 2020  ffset=0,..      
-00004110: 2020 2020 2020 2020 2020 6d61 785f 6368            max_ch
-00004120: 756e 6b5f 6279 7465 733d 6d61 785f 6368  unk_bytes=max_ch
-00004130: 756e 6b5f 6279 7465 732c 0d0a 2020 2020  unk_bytes,..    
-00004140: 2020 2020 2020 2020 2020 2020 6174 7472              attr
-00004150: 6962 7574 6573 3d61 7474 7269 6275 7465  ibutes=attribute
-00004160: 732c 0d0a 2020 2020 2020 2020 2020 2020  s,..            
-00004170: 290d 0a20 2020 2020 2020 2020 2020 2063  )..            c
-00004180: 6f6c 756d 6e73 203d 205b 5d0d 0a20 2020  olumns = []..   
-00004190: 2020 2020 2020 2020 2074 6d70 5f63 6f6c           tmp_col
-000041a0: 756d 6e73 203d 2076 616c 7565 735b 305d  umns = values[0]
-000041b0: 2e6d 616b 655f 636f 6c75 6d6e 7328 290d  .make_columns().
-000041c0: 0a20 2020 2020 2020 2020 2020 2066 6f72  .            for
-000041d0: 2063 6f6c 756d 6e20 696e 2074 6d70 5f63   column in tmp_c
-000041e0: 6f6c 756d 6e73 3a0d 0a20 2020 2020 2020  olumns:..       
-000041f0: 2020 2020 2020 2020 2066 6965 6c64 7320           fields 
-00004200: 3d20 636f 6c75 6d6e 2e73 706c 6974 2856  = column.split(V
-00004210: 616c 7565 5374 6f72 6167 6542 6173 652e  alueStorageBase.
-00004220: 4445 4c49 4d49 5445 5229 0d0a 2020 2020  DELIMITER)..    
-00004230: 2020 2020 2020 2020 2020 2020 6669 656c              fiel
-00004240: 6473 5b30 5d20 3d20 2241 6c6c 4e61 6d65  ds[0] = "AllName
-00004250: 7322 0d0a 2020 2020 2020 2020 2020 2020  s"..            
-00004260: 2020 2020 636f 6c75 6d6e 732e 6170 7065      columns.appe
-00004270: 6e64 2856 616c 7565 5374 6f72 6167 6542  nd(ValueStorageB
-00004280: 6173 652e 4445 4c49 4d49 5445 522e 6a6f  ase.DELIMITER.jo
-00004290: 696e 2866 6965 6c64 7329 290d 0a20 2020  in(fields))..   
-000042a0: 2020 2020 2020 2020 2063 6f6c 756d 6e5f           column_
-000042b0: 7261 6e67 6573 203d 205b 302c 206c 656e  ranges = [0, len
-000042c0: 2874 6d70 5f63 6f6c 756d 6e73 295d 0d0a  (tmp_columns)]..
-000042d0: 2020 2020 2020 2020 656c 7365 3a0d 0a20          else:.. 
-000042e0: 2020 2020 2020 2020 2020 2063 6f6c 756d             colum
-000042f0: 6e73 203d 205b 5d0d 0a20 2020 2020 2020  ns = []..       
-00004300: 2020 2020 2063 6f6c 756d 6e5f 7261 6e67       column_rang
-00004310: 6573 203d 205b 5d0d 0a20 2020 2020 2020  es = []..       
-00004320: 2020 2020 2063 6f6c 5f69 6e64 6578 203d       col_index =
-00004330: 2030 0d0a 2020 2020 2020 2020 2020 2020   0..            
-00004340: 666f 7220 7661 6c75 6520 696e 2076 616c  for value in val
-00004350: 7565 733a 0d0a 2020 2020 2020 2020 2020  ues:..          
-00004360: 2020 2020 2020 746d 705f 636f 6c75 6d6e        tmp_column
-00004370: 7320 3d20 7661 6c75 652e 6d61 6b65 5f63  s = value.make_c
-00004380: 6f6c 756d 6e73 2829 0d0a 2020 2020 2020  olumns()..      
-00004390: 2020 2020 2020 2020 2020 636f 6c5f 7261            col_ra
-000043a0: 6e67 6520 3d20 2863 6f6c 5f69 6e64 6578  nge = (col_index
-000043b0: 2c20 6c65 6e28 746d 705f 636f 6c75 6d6e  , len(tmp_column
-000043c0: 7329 290d 0a20 2020 2020 2020 2020 2020  s))..           
-000043d0: 2020 2020 2063 6f6c 756d 6e5f 7261 6e67       column_rang
-000043e0: 6573 2e61 7070 656e 6428 636f 6c5f 7261  es.append(col_ra
-000043f0: 6e67 6529 0d0a 2020 2020 2020 2020 2020  nge)..          
-00004400: 2020 2020 2020 666f 7220 636f 6c75 6d6e        for column
-00004410: 2069 6e20 746d 705f 636f 6c75 6d6e 733a   in tmp_columns:
-00004420: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00004430: 2020 2020 2020 636f 6c75 6d6e 732e 6170        columns.ap
-00004440: 7065 6e64 2863 6f6c 756d 6e29 0d0a 2020  pend(column)..  
-00004450: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004460: 2020 636f 6c5f 696e 6465 7820 2b3d 2031    col_index += 1
-00004470: 0d0a 2020 2020 2020 2020 2020 2020 7365  ..            se
-00004480: 6c66 2e5f 7469 6d65 5f73 7465 7073 203d  lf._time_steps =
-00004490: 204e 6f6e 650d 0a0d 0a20 2020 2020 2020   None....       
-000044a0: 2061 7474 7269 6275 7465 7320 3d20 7b22   attributes = {"
-000044b0: 7479 7065 223a 2064 6174 6173 6574 5f70  type": dataset_p
-000044c0: 726f 7065 7274 795f 7479 7065 2e76 616c  roperty_type.val
-000044d0: 7565 7d0d 0a20 2020 2020 2020 2069 6620  ue}..        if 
-000044e0: 7374 6f72 655f 7469 6d65 5f73 7465 703a  store_time_step:
-000044f0: 0d0a 2020 2020 2020 2020 2020 2020 6174  ..            at
-00004500: 7472 6962 7574 6573 5b22 7469 6d65 5f73  tributes["time_s
-00004510: 7465 705f 7061 7468 225d 203d 2074 696d  tep_path"] = tim
-00004520: 655f 7374 6570 5f70 6174 680d 0a0d 0a20  e_step_path.... 
-00004530: 2020 2020 2020 2073 656c 662e 5f64 6174         self._dat
-00004540: 6173 6574 203d 2044 6174 6173 6574 4275  aset = DatasetBu
-00004550: 6666 6572 280d 0a20 2020 2020 2020 2020  ffer(..         
-00004560: 2020 2068 6466 5f73 746f 7265 2c0d 0a20     hdf_store,.. 
-00004570: 2020 2020 2020 2020 2020 2070 6174 682c             path,
-00004580: 0d0a 2020 2020 2020 2020 2020 2020 6d61  ..            ma
-00004590: 785f 7369 7a65 2c0d 0a20 2020 2020 2020  x_size,..       
-000045a0: 2020 2020 2064 7479 7065 2c0d 0a20 2020       dtype,..   
-000045b0: 2020 2020 2020 2020 2063 6f6c 756d 6e73           columns
-000045c0: 2c0d 0a20 2020 2020 2020 2020 2020 2073  ,..            s
-000045d0: 6361 6c65 6f66 6673 6574 3d73 6361 6c65  caleoffset=scale
-000045e0: 6f66 6673 6574 2c0d 0a20 2020 2020 2020  offset,..       
-000045f0: 2020 2020 206d 6178 5f63 6875 6e6b 5f62       max_chunk_b
-00004600: 7974 6573 3d6d 6178 5f63 6875 6e6b 5f62  ytes=max_chunk_b
-00004610: 7974 6573 2c0d 0a20 2020 2020 2020 2020  ytes,..         
-00004620: 2020 2061 7474 7269 6275 7465 733d 6174     attributes=at
-00004630: 7472 6962 7574 6573 2c0d 0a20 2020 2020  tributes,..     
-00004640: 2020 2020 2020 206e 616d 6573 3d65 6c65         names=ele
-00004650: 6d5f 6e61 6d65 732c 0d0a 2020 2020 2020  m_names,..      
-00004660: 2020 2020 2020 636f 6c75 6d6e 5f72 616e        column_ran
-00004670: 6765 735f 7065 725f 6e61 6d65 3d63 6f6c  ges_per_name=col
-00004680: 756d 6e5f 7261 6e67 6573 2c0d 0a20 2020  umn_ranges,..   
-00004690: 2020 2020 2029 0d0a 0d0a 2020 2020 4073       )....    @s
-000046a0: 7461 7469 636d 6574 686f 640d 0a20 2020  taticmethod..   
-000046b0: 2064 6566 2074 696d 655f 7374 6570 5f70   def time_step_p
-000046c0: 6174 6828 7061 7468 293a 0d0a 2020 2020  ath(path):..    
-000046d0: 2020 2020 7265 7475 726e 2070 6174 6820      return path 
-000046e0: 2b20 2254 696d 6553 7465 7022 0d0a 0d0a  + "TimeStep"....
-000046f0: 2020 2020 6465 6620 6170 7065 6e64 2873      def append(s
-00004700: 656c 662c 2076 616c 7565 7329 3a0d 0a20  elf, values):.. 
-00004710: 2020 2020 2020 2022 2222 4170 7065 6e64         """Append
-00004720: 2061 2076 616c 7565 2074 6f20 7468 6520   a value to the 
-00004730: 636f 6e74 6169 6e65 722e 0d0a 0d0a 2020  container.....  
-00004740: 2020 2020 2020 5061 7261 6d65 7465 7273        Parameters
-00004750: 0d0a 2020 2020 2020 2020 2d2d 2d2d 2d2d  ..        ------
-00004760: 2d2d 2d2d 0d0a 2020 2020 2020 2020 7661  ----..        va
-00004770: 6c75 6520 3a20 6c69 7374 0d0a 2020 2020  lue : list..    
-00004780: 2020 2020 2020 2020 6c69 7374 206f 6620          list of 
-00004790: 5661 6c75 6553 746f 7261 6765 4261 7365  ValueStorageBase
-000047a0: 0d0a 0d0a 2020 2020 2020 2020 2222 220d  ....        """.
-000047b0: 0a20 2020 2020 2020 200d 0a20 2020 2020  .        ..     
-000047c0: 2020 2069 6620 7661 6c75 6573 3a0d 0a20     if values:.. 
-000047d0: 2020 2020 2020 2020 2020 2069 6620 6973             if is
-000047e0: 696e 7374 616e 6365 2876 616c 7565 735b  instance(values[
-000047f0: 305d 2e76 616c 7565 2c20 6c69 7374 293a  0].value, list):
-00004800: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00004810: 2020 7661 6c73 203d 205b 7820 666f 7220    vals = [x for 
-00004820: 7920 696e 2076 616c 7565 7320 666f 7220  y in values for 
-00004830: 7820 696e 2079 2e76 616c 7565 5d0d 0a20  x in y.value].. 
-00004840: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
-00004850: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00004860: 2020 7661 6c73 203d 205b 494e 5445 4745    vals = [INTEGE
-00004870: 525f 4e41 4e20 6966 2028 782e 6973 5f6e  R_NAN if (x.is_n
-00004880: 616e 2829 2061 6e64 2078 2e5f 7661 6c75  an() and x._valu
-00004890: 655f 7479 7065 203d 3d20 696e 7429 2065  e_type == int) e
-000048a0: 6c73 6520 782e 7661 6c75 6520 666f 7220  lse x.value for 
-000048b0: 7820 696e 2076 616c 7565 7320 5d0d 0a20  x in values ].. 
-000048c0: 2020 2020 2020 2065 6c73 653a 0d0a 2020         else:..  
-000048d0: 2020 2020 2020 2020 2020 7661 6c73 203d            vals =
-000048e0: 205b 7365 6c66 2e73 6574 5f6e 616e 2829   [self.set_nan()
-000048f0: 2066 6f72 206b 2c20 7620 696e 2073 656c   for k, v in sel
-00004900: 662e 5f6c 656e 6774 682e 6974 656d 7328  f._length.items(
-00004910: 2920 666f 7220 7820 696e 2072 616e 6765  ) for x in range
-00004920: 2876 295d 0d0a 2020 2020 2020 2020 7365  (v)]..        se
-00004930: 6c66 2e5f 6461 7461 7365 742e 7772 6974  lf._dataset.writ
-00004940: 655f 7661 6c75 6528 7661 6c73 290d 0a20  e_value(vals).. 
-00004950: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004960: 2020 2020 0d0a 0d0a 2020 2020 6465 6620      ....    def 
-00004970: 6170 7065 6e64 5f62 795f 7469 6d65 5f73  append_by_time_s
-00004980: 7465 7028 7365 6c66 2c20 7661 6c75 652c  tep(self, value,
-00004990: 2074 696d 655f 7374 6570 2c20 656c 656d   time_step, elem
-000049a0: 5f69 6e64 6578 293a 0d0a 2020 2020 2020  _index):..      
-000049b0: 2020 2222 2241 7070 656e 6420 6120 7661    """Append a va
-000049c0: 6c75 6520 746f 2074 6865 2063 6f6e 7461  lue to the conta
-000049d0: 696e 6572 2e0d 0a0d 0a20 2020 2020 2020  iner.....       
-000049e0: 2050 6172 616d 6574 6572 730d 0a20 2020   Parameters..   
-000049f0: 2020 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0d       ----------.
-00004a00: 0a20 2020 2020 2020 2076 616c 7565 203a  .        value :
-00004a10: 2056 616c 7565 5374 6f72 6167 6542 6173   ValueStorageBas
-00004a20: 650d 0a20 2020 2020 2020 2074 696d 655f  e..        time_
-00004a30: 7374 6570 203a 2069 6e74 0d0a 2020 2020  step : int..    
-00004a40: 2020 2020 656c 656d 5f69 6e64 6578 203a      elem_index :
-00004a50: 2069 6e74 0d0a 0d0a 2020 2020 2020 2020   int....        
-00004a60: 2222 220d 0a20 2020 2020 2020 200d 0a20  """..        .. 
-00004a70: 2020 2020 2020 2069 6620 6973 696e 7374         if isinst
-00004a80: 616e 6365 2876 616c 7565 2e76 616c 7565  ance(value.value
-00004a90: 2c20 6c69 7374 293a 0d0a 2020 2020 2020  , list):..      
-00004aa0: 2020 2020 2020 7661 6c73 203d 205b 7820        vals = [x 
-00004ab0: 666f 7220 7820 696e 2076 616c 7565 2e76  for x in value.v
-00004ac0: 616c 7565 5d0d 0a20 2020 2020 2020 2065  alue]..        e
-00004ad0: 6c73 653a 0d0a 2020 2020 2020 2020 2020  lse:..          
-00004ae0: 2020 7661 6c73 203d 2076 616c 7565 2e76    vals = value.v
-00004af0: 616c 7565 0d0a 0d0a 2020 2020 2020 2020  alue....        
-00004b00: 7365 6c66 2e5f 6461 7461 7365 742e 7772  self._dataset.wr
-00004b10: 6974 655f 7661 6c75 6528 7661 6c73 290d  ite_value(vals).
-00004b20: 0a20 2020 2020 2020 2073 656c 662e 5f74  .        self._t
-00004b30: 696d 655f 7374 6570 732e 7772 6974 655f  ime_steps.write_
-00004b40: 7661 6c75 6528 5b74 696d 655f 7374 6570  value([time_step
-00004b50: 2c20 656c 656d 5f69 6e64 6578 5d29 0d0a  , elem_index])..
-00004b60: 2020 2020 2020 2020 0d0a 2020 2020 2020          ..      
-00004b70: 2020 0d0a 2020 2020 6465 6620 666c 7573    ..    def flus
-00004b80: 685f 6461 7461 2873 656c 6629 3a0d 0a20  h_data(self):.. 
-00004b90: 2020 2020 2020 2022 2222 466c 7573 6820         """Flush 
-00004ba0: 616e 7920 6f75 7473 7461 6e64 696e 6720  any outstanding 
-00004bb0: 6461 7461 2074 6f20 6469 736b 2e22 2222  data to disk."""
-00004bc0: 0d0a 2020 2020 2020 2020 7365 6c66 2e5f  ..        self._
-00004bd0: 6461 7461 7365 742e 666c 7573 685f 6461  dataset.flush_da
-00004be0: 7461 2829 0d0a 2020 2020 2020 2020 6966  ta()..        if
-00004bf0: 2073 656c 662e 5f74 696d 655f 7374 6570   self._time_step
-00004c00: 7320 6973 206e 6f74 204e 6f6e 653a 0d0a  s is not None:..
-00004c10: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00004c20: 2e5f 7469 6d65 5f73 7465 7073 2e66 6c75  ._time_steps.flu
-00004c30: 7368 5f64 6174 6128 290d 0a0d 0a20 2020  sh_data()....   
-00004c40: 2064 6566 206d 6178 5f6e 756d 5f62 7974   def max_num_byt
-00004c50: 6573 2873 656c 6629 3a0d 0a20 2020 2020  es(self):..     
-00004c60: 2020 2022 2222 5265 7475 726e 2074 6865     """Return the
-00004c70: 206d 6178 696d 756d 206e 756d 6265 7220   maximum number 
-00004c80: 6f66 2062 7974 6573 2074 6865 2063 6f6e  of bytes the con
-00004c90: 7461 696e 6572 2063 6f75 6c64 2068 6f6c  tainer could hol
-00004ca0: 642e 0d0a 0d0a 2020 2020 2020 2020 5265  d.....        Re
-00004cb0: 7475 726e 730d 0a20 2020 2020 2020 202d  turns..        -
-00004cc0: 2d2d 2d2d 2d2d 0d0a 2020 2020 2020 2020  ------..        
-00004cd0: 696e 740d 0a0d 0a20 2020 2020 2020 2022  int....        "
-00004ce0: 2222 0d0a 2020 2020 2020 2020 7265 7475  ""..        retu
-00004cf0: 726e 2073 656c 662e 5f64 6174 6173 6574  rn self._dataset
-00004d00: 2e6d 6178 5f6e 756d 5f62 7974 6573 2829  .max_num_bytes()
-00004d10: 0d0a 0d0a 0d0a 6465 6620 6765 745f 6461  ......def get_da
-00004d20: 7461 7365 745f 7072 6f70 6572 7479 5f74  taset_property_t
-00004d30: 7970 6528 6461 7461 7365 7429 3a0d 0a20  ype(dataset):.. 
-00004d40: 2020 2022 2222 5265 7475 726e 2074 6865     """Return the
-00004d50: 2070 726f 7065 7274 7920 7479 7065 206f   property type o
-00004d60: 6620 7468 6973 2064 6174 6173 6574 2e0d  f this dataset..
-00004d70: 0a0d 0a20 2020 2052 6574 7572 6e73 0d0a  ...    Returns..
-00004d80: 2020 2020 2d2d 2d2d 2d2d 2d0d 0a20 2020      -------..   
-00004d90: 2044 6174 6173 6574 5072 6f70 6572 7479   DatasetProperty
-00004da0: 5479 7065 0d0a 0d0a 2020 2020 2222 220d  Type....    """.
-00004db0: 0a20 2020 2072 6574 7572 6e20 4461 7461  .    return Data
-00004dc0: 7365 7450 726f 7065 7274 7954 7970 6528  setPropertyType(
-00004dd0: 6461 7461 7365 742e 6174 7472 735b 2274  dataset.attrs["t
-00004de0: 7970 6522 5d29 0d0a 0d0a 0d0a 6465 6620  ype"])......def 
-00004df0: 6765 745f 7469 6d65 5f73 7465 705f 7061  get_time_step_pa
-00004e00: 7468 2864 6174 6173 6574 293a 0d0a 2020  th(dataset):..  
-00004e10: 2020 2222 2252 6574 7572 6e20 7468 6520    """Return the 
-00004e20: 7061 7468 2074 6f20 7468 6520 7469 6d65  path to the time
-00004e30: 5f73 7465 7073 2066 6f72 2074 6869 7320  _steps for this 
-00004e40: 6461 7461 7365 742e 0d0a 0d0a 2020 2020  dataset.....    
-00004e50: 5265 7475 726e 730d 0a20 2020 202d 2d2d  Returns..    ---
-00004e60: 2d2d 2d2d 0d0a 2020 2020 7374 720d 0a0d  ----..    str...
-00004e70: 0a20 2020 2022 2222 0d0a 2020 2020 7265  .    """..    re
-00004e80: 7475 726e 2064 6174 6173 6574 2e61 7474  turn dataset.att
-00004e90: 7273 5b22 7469 6d65 5f73 7465 705f 7061  rs["time_step_pa
-00004ea0: 7468 225d 0d0a                           th"]..
+00002e60: 2020 6c61 6265 6c20 2b3d 2022 2022 202b    label += " " +
+00002e70: 2075 6e69 7473 5b30 5d0a 2020 2020 2020   units[0].      
+00002e80: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00002e90: 6c66 2e5f 6c61 6265 6c73 2e61 7070 656e  lf._labels.appen
+00002ea0: 6428 6c61 6265 6c29 0a20 2020 2020 2020  d(label).       
+00002eb0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+00002ec0: 662e 5f76 616c 7565 202b 3d20 5b63 6f6d  f._value += [com
+00002ed0: 706c 6578 2878 5b30 5d2c 2078 5b31 5d29  plex(x[0], x[1])
+00002ee0: 5d0a 2020 2020 2020 2020 2020 2020 2020  ].              
+00002ef0: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
+00002f00: 2020 2020 2020 2020 2020 2020 6c61 6265              labe
+00002f10: 6c5f 6d61 6720 3d20 6c61 6265 6c20 2b20  l_mag = label + 
+00002f20: 7365 6c66 2e44 454c 494d 4954 4552 202b  self.DELIMITER +
+00002f30: 2022 6d61 6722 202b 2027 2027 202b 2075   "mag" + ' ' + u
+00002f40: 6e69 7473 5b30 5d0a 2020 2020 2020 2020  nits[0].        
+00002f50: 2020 2020 2020 2020 2020 2020 6c61 6265              labe
+00002f60: 6c5f 616e 6720 3d20 6c61 6265 6c20 2b20  l_ang = label + 
+00002f70: 7365 6c66 2e44 454c 494d 4954 4552 202b  self.DELIMITER +
+00002f80: 2022 616e 6722 202b 2027 2027 202b 2075   "ang" + ' ' + u
+00002f90: 6e69 7473 5b31 5d0a 2020 2020 2020 2020  nits[1].        
+00002fa0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00002fb0: 2e5f 6c61 6265 6c73 2e65 7874 656e 6428  ._labels.extend(
+00002fc0: 5b6c 6162 656c 5f6d 6167 2c20 6c61 6265  [label_mag, labe
+00002fd0: 6c5f 616e 675d 290a 2020 2020 2020 2020  l_ang]).        
+00002fe0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00002ff0: 2e5f 7661 6c75 6520 2b3d 205b 785b 305d  ._value += [x[0]
+00003000: 2c20 785b 315d 5d0a 0a20 2020 2064 6566  , x[1]]..    def
+00003010: 205f 5f69 6164 645f 5f28 7365 6c66 2c20   __iadd__(self, 
+00003020: 6f74 6865 7229 3a0a 2020 2020 2020 2020  other):.        
+00003030: 666f 7220 6920 696e 2072 616e 6765 286c  for i in range(l
+00003040: 656e 2873 656c 662e 5f76 616c 7565 2929  en(self._value))
+00003050: 3a0a 2020 2020 2020 2020 2020 2020 7365  :.            se
+00003060: 6c66 2e5f 7661 6c75 655b 695d 202b 3d20  lf._value[i] += 
+00003070: 6f74 6865 722e 7661 6c75 655b 695d 0a20  other.value[i]. 
+00003080: 2020 2020 2020 2072 6574 7572 6e20 7365         return se
+00003090: 6c66 0a0a 2020 2020 6465 6620 5f5f 6774  lf..    def __gt
+000030a0: 5f5f 2873 656c 662c 206f 7468 6572 293a  __(self, other):
+000030b0: 0a20 2020 2020 2020 2023 2054 4f44 4f0a  .        # TODO.
+000030c0: 2020 2020 2020 2020 7265 7475 726e 2073          return s
+000030d0: 756d 2873 656c 662e 5f76 616c 7565 2920  um(self._value) 
+000030e0: 3e20 7375 6d28 6f74 6865 722e 7661 6c75  > sum(other.valu
+000030f0: 6529 0a0a 2020 2020 4070 726f 7065 7274  e)..    @propert
+00003100: 790a 2020 2020 6465 6620 7661 6c75 6528  y.    def value(
+00003110: 7365 6c66 293a 0a20 2020 2020 2020 2072  self):.        r
+00003120: 6574 7572 6e20 7365 6c66 2e5f 7661 6c75  eturn self._valu
+00003130: 650a 0a20 2020 2040 7374 6174 6963 6d65  e..    @staticme
+00003140: 7468 6f64 0a20 2020 2064 6566 2063 6875  thod.    def chu
+00003150: 6e6b 5f6c 6973 7428 7661 6c75 6573 2c20  nk_list(values, 
+00003160: 6e4c 6973 7473 293a 0a20 2020 2020 2020  nLists):.       
+00003170: 2023 2054 4f44 4f3a 2074 6869 7320 6272   # TODO: this br
+00003180: 6561 6b73 2066 6f72 2042 7573 2e70 7556  eaks for Bus.puV
+00003190: 6d61 6741 6e67 6c65 2069 6e20 6d6f 6e74  magAngle in mont
+000031a0: 6520 6361 726c 6f20 6578 616d 706c 6520  e carlo example 
+000031b0: 7465 7374 0a20 2020 2020 2020 2072 6574  test.        ret
+000031c0: 7572 6e20 205b 7661 6c75 6573 5b69 202a  urn  [values[i *
+000031d0: 206e 4c69 7374 733a 2869 202b 2031 2920   nLists:(i + 1) 
+000031e0: 2a20 6e4c 6973 7473 5d20 666f 7220 6920  * nLists] for i 
+000031f0: 696e 2072 616e 6765 2828 6c65 6e28 7661  in range((len(va
+00003200: 6c75 6573 2920 2b20 6e4c 6973 7473 202d  lues) + nLists -
+00003210: 2031 2920 2f2f 206e 4c69 7374 7329 5d0a   1) // nLists)].
+00003220: 0a20 2020 2064 6566 205f 6669 785f 7661  .    def _fix_va
+00003230: 6c75 6528 7365 6c66 2c20 7661 6c75 6529  lue(self, value)
+00003240: 3a0a 2020 2020 2020 2020 7661 6c75 6520  :.        value 
+00003250: 3d20 7365 6c66 2e63 6875 6e6b 5f6c 6973  = self.chunk_lis
+00003260: 7428 7661 6c75 652c 2073 656c 662e 5f6e  t(value, self._n
+00003270: 290a 2020 2020 2020 2020 7661 6c75 6520  ).        value 
+00003280: 3d20 7365 6c66 2e63 6875 6e6b 5f6c 6973  = self.chunk_lis
+00003290: 7428 7661 6c75 652c 2073 656c 662e 5f6d  t(value, self._m
+000032a0: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
+000032b0: 2076 616c 7565 0a0a 2020 2020 6465 6620   value..    def 
+000032c0: 6973 5f6e 616e 2873 656c 6629 3a0a 2020  is_nan(self):.  
+000032d0: 2020 2020 2020 6966 206e 702e 6973 7375        if np.issu
+000032e0: 6264 7479 7065 2873 656c 662e 5f76 616c  bdtype(self._val
+000032f0: 7565 5f74 7970 652c 206e 702e 696e 7436  ue_type, np.int6
+00003300: 3429 3a0a 2020 2020 2020 2020 2020 2020  4):.            
+00003310: 7265 7475 726e 2073 656c 662e 5f76 616c  return self._val
+00003320: 7565 5b30 5d20 3d3d 2049 4e54 4547 4552  ue[0] == INTEGER
+00003330: 5f4e 414e 0a20 2020 2020 2020 2072 6574  _NAN.        ret
+00003340: 7572 6e20 6e70 2e69 736e 616e 2873 656c  urn np.isnan(sel
+00003350: 662e 5f76 616c 7565 5b30 5d29 0a0a 2020  f._value[0])..  
+00003360: 2020 6465 6620 6d61 6b65 5f63 6f6c 756d    def make_colum
+00003370: 6e73 2873 656c 6629 3a0a 2020 2020 2020  ns(self):.      
+00003380: 2020 7265 7475 726e 205b 0a20 2020 2020    return [.     
+00003390: 2020 2020 2020 2073 656c 662e 4445 4c49         self.DELI
+000033a0: 4d49 5445 522e 6a6f 696e 2828 7365 6c66  MITER.join((self
+000033b0: 2e5f 6e61 6d65 2c20 6622 7b78 7d22 2929  ._name, f"{x}"))
+000033c0: 2066 6f72 2078 2069 6e20 7365 6c66 2e5f   for x in self._
+000033d0: 6c61 6265 6c73 0a20 2020 2020 2020 205d  labels.        ]
+000033e0: 0a0a 2020 2020 4070 726f 7065 7274 790a  ..    @property.
+000033f0: 2020 2020 6465 6620 6e75 6d5f 636f 6c75      def num_colu
+00003400: 6d6e 7328 7365 6c66 293a 0a20 2020 2020  mns(self):.     
+00003410: 2020 2072 6574 7572 6e20 6c65 6e28 7365     return len(se
+00003420: 6c66 2e5f 6c61 6265 6c73 290a 0a20 2020  lf._labels)..   
+00003430: 2064 6566 2073 6574 5f65 6c65 6d65 6e74   def set_element
+00003440: 5f70 726f 7065 7274 7928 7365 6c66 2c20  _property(self, 
+00003450: 7072 6f70 293a 0a20 2020 2020 2020 2073  prop):.        s
+00003460: 656c 662e 5f70 726f 7020 3d20 7072 6f70  elf._prop = prop
+00003470: 0a0a 2020 2020 6465 6620 7365 745f 6e61  ..    def set_na
+00003480: 6d65 2873 656c 662c 206e 616d 6529 3a0a  me(self, name):.
+00003490: 2020 2020 2020 2020 7365 6c66 2e5f 6e61          self._na
+000034a0: 6d65 203d 206e 616d 650a 0a20 2020 2064  me = name..    d
+000034b0: 6566 2073 6574 5f6e 616e 2873 656c 6629  ef set_nan(self)
+000034c0: 3a0a 2020 2020 2020 2020 666f 7220 6920  :.        for i 
+000034d0: 696e 2072 616e 6765 286c 656e 2873 656c  in range(len(sel
+000034e0: 662e 5f76 616c 7565 2929 3a0a 2020 2020  f._value)):.    
+000034f0: 2020 2020 2020 2020 7365 6c66 2e5f 7661          self._va
+00003500: 6c75 655b 695d 203d 206e 702e 4e61 4e0a  lue[i] = np.NaN.
+00003510: 0a20 2020 2064 6566 2073 6574 5f76 616c  .    def set_val
+00003520: 7565 2873 656c 662c 2076 616c 7565 293a  ue(self, value):
+00003530: 0a20 2020 2020 2020 2073 656c 662e 5f76  .        self._v
+00003540: 616c 7565 203d 2076 616c 7565 0a20 2020  alue = value.   
+00003550: 2020 2020 2069 6620 6e6f 7420 6973 696e       if not isin
+00003560: 7374 616e 6365 2876 616c 7565 5b30 5d2c  stance(value[0],
+00003570: 2073 656c 662e 5f76 616c 7565 5f74 7970   self._value_typ
+00003580: 6529 3a0a 2020 2020 2020 2020 2020 2020  e):.            
+00003590: 7365 6c66 2e5f 7661 6c75 655f 7479 7065  self._value_type
+000035a0: 203d 2074 7970 6528 7661 6c75 655b 305d   = type(value[0]
+000035b0: 290a 0a20 2020 2064 6566 2073 6574 5f76  )..    def set_v
+000035c0: 616c 7565 5f66 726f 6d5f 7261 7728 7365  alue_from_raw(se
+000035d0: 6c66 2c20 7661 6c75 6529 3a0a 2020 2020  lf, value):.    
+000035e0: 2020 2020 6966 206c 656e 2876 616c 7565      if len(value
+000035f0: 2920 213d 2073 656c 662e 5f76 616c 7565  ) != self._value
+00003600: 5f6c 656e 6774 683a 0a20 2020 2020 2020  _length:.       
+00003610: 2020 2020 2076 616c 7565 203d 205b 6e70       value = [np
+00003620: 2e4e 614e 2066 6f72 2069 2069 6e20 7261  .NaN for i in ra
+00003630: 6e67 6528 7365 6c66 2e5f 7661 6c75 655f  nge(self._value_
+00003640: 6c65 6e67 7468 295d 0a20 2020 2020 2020  length)].       
+00003650: 200a 2020 2020 2020 2020 7661 6c75 6520   .        value 
+00003660: 3d20 7365 6c66 2e5f 6669 785f 7661 6c75  = self._fix_valu
+00003670: 6528 7661 6c75 6529 0a20 2020 2020 2020  e(value).       
+00003680: 2073 656c 662e 5f76 616c 7565 2e63 6c65   self._value.cle
+00003690: 6172 2829 0a20 2020 2020 2020 200a 2020  ar().        .  
+000036a0: 2020 2020 2020 666f 7220 692c 206e 6f64        for i, nod
+000036b0: 655f 7661 6c20 696e 2065 6e75 6d65 7261  e_val in enumera
+000036c0: 7465 287a 6970 2873 656c 662e 5f6e 6f64  te(zip(self._nod
+000036d0: 6573 2c20 7661 6c75 6529 293a 0a20 2020  es, value)):.   
+000036e0: 2020 2020 2020 2020 206e 6f64 652c 2076           node, v
+000036f0: 616c 203d 206e 6f64 655f 7661 6c0a 2020  al = node_val.  
+00003700: 2020 2020 2020 2020 2020 666f 7220 762c            for v,
+00003710: 2078 2069 6e20 7a69 7028 6e6f 6465 2c20   x in zip(node, 
+00003720: 7661 6c29 3a0a 2020 2020 2020 2020 2020  val):.          
+00003730: 2020 2020 2020 6966 2073 656c 662e 5f69        if self._i
+00003740: 735f 636f 6d70 6c65 783a 0a20 2020 2020  s_complex:.     
+00003750: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00003760: 656c 662e 5f76 616c 7565 202b 3d20 5b63  elf._value += [c
+00003770: 6f6d 706c 6578 2878 5b30 5d2c 2078 5b31  omplex(x[0], x[1
+00003780: 5d29 5d0a 2020 2020 2020 2020 2020 2020  ])].            
+00003790: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+000037a0: 2020 2020 2020 2020 2020 2020 2020 7365                se
+000037b0: 6c66 2e5f 7661 6c75 6520 2b3d 205b 785b  lf._value += [x[
+000037c0: 305d 2c20 785b 315d 5d0a 2020 2020 4070  0], x[1]].    @p
+000037d0: 726f 7065 7274 790a 2020 2020 6465 6620  roperty.    def 
+000037e0: 7661 6c75 655f 7479 7065 2873 656c 6629  value_type(self)
+000037f0: 3a0a 2020 2020 2020 2020 7265 7475 726e  :.        return
+00003800: 2073 656c 662e 5f76 616c 7565 5f74 7970   self._value_typ
+00003810: 650a 0a0a 636c 6173 7320 5661 6c75 6543  e...class ValueC
+00003820: 6f6e 7461 696e 6572 3a0a 2020 2020 2222  ontainer:.    ""
+00003830: 2243 6f6e 7461 696e 6572 2066 6f72 2061  "Container for a
+00003840: 2073 6571 7565 6e63 6520 6f66 2069 6e73   sequence of ins
+00003850: 7461 6e63 6573 206f 6620 5661 6c75 6553  tances of ValueS
+00003860: 746f 7261 6765 4261 7365 2e22 2222 0a0a  torageBase."""..
+00003870: 2020 2020 6465 6620 5f5f 696e 6974 5f5f      def __init__
+00003880: 2873 656c 662c 2076 616c 7565 732c 2068  (self, values, h
+00003890: 6466 5f73 746f 7265 2c20 7061 7468 2c20  df_store, path, 
+000038a0: 6d61 785f 7369 7a65 2c20 656c 656d 5f6e  max_size, elem_n
+000038b0: 616d 6573 2c0a 2020 2020 2020 2020 2020  ames,.          
+000038c0: 2020 2020 2020 2064 6174 6173 6574 5f70         dataset_p
+000038d0: 726f 7065 7274 795f 7479 7065 2c20 6d61  roperty_type, ma
+000038e0: 785f 6368 756e 6b5f 6279 7465 733d 4e6f  x_chunk_bytes=No
+000038f0: 6e65 2c20 7374 6f72 655f 7469 6d65 5f73  ne, store_time_s
+00003900: 7465 703d 4661 6c73 6529 3a0a 2020 2020  tep=False):.    
+00003910: 2020 2020 6772 6f75 705f 6e61 6d65 203d      group_name =
+00003920: 206f 732e 7061 7468 2e64 6972 6e61 6d65   os.path.dirname
+00003930: 2870 6174 6829 0a20 2020 2020 2020 2062  (path).        b
+00003940: 6173 656e 616d 6520 3d20 6f73 2e70 6174  asename = os.pat
+00003950: 682e 6261 7365 6e61 6d65 2870 6174 6829  h.basename(path)
+00003960: 0a20 2020 2020 2020 2073 656c 662e 6772  .        self.gr
+00003970: 6f75 705f 6e61 6d65 203d 2067 726f 7570  oup_name = group
+00003980: 5f6e 616d 650a 2020 2020 2020 2020 7365  _name.        se
+00003990: 6c66 2e62 6173 655f 6e61 6d65 203d 2062  lf.base_name = b
+000039a0: 6173 656e 616d 650a 2020 2020 2020 2020  asename.        
+000039b0: 7472 793a 0a20 2020 2020 2020 2020 2020  try:.           
+000039c0: 2069 6620 6261 7365 6e61 6d65 2069 6e20   if basename in 
+000039d0: 6864 665f 7374 6f72 655b 6772 6f75 705f  hdf_store[group_
+000039e0: 6e61 6d65 5d3a 0a20 2020 2020 2020 2020  name]:.         
+000039f0: 2020 2020 2020 2072 6169 7365 2049 6e76         raise Inv
+00003a00: 616c 6964 5061 7261 6d65 7465 7228 6622  alidParameter(f"
+00003a10: 6475 706c 6963 6174 6520 6461 7461 7365  duplicate datase
+00003a20: 7420 6e61 6d65 207b 6261 7365 6e61 6d65  t name {basename
+00003a30: 7d22 290a 2020 2020 2020 2020 6578 6365  }").        exce
+00003a40: 7074 204b 6579 4572 726f 723a 0a20 2020  pt KeyError:.   
+00003a50: 2020 2020 2020 2020 2023 2044 6f6e 2774           # Don't
+00003a60: 2062 6f74 6865 7220 6368 6563 6b69 6e67   bother checking
+00003a70: 2065 6163 6820 7375 6220 7061 7468 2e0a   each sub path..
+00003a80: 2020 2020 2020 2020 2020 2020 7061 7373              pass
+00003a90: 0a20 2020 2020 2020 200a 2020 2020 2020  .        .      
+00003aa0: 2020 7365 6c66 2e5f 6c65 6e67 7468 3d7b    self._length={
+00003ab0: 7d0a 2020 2020 2020 2020 666f 7220 7661  }.        for va
+00003ac0: 6c75 6520 696e 2076 616c 7565 733a 0a20  lue in values:. 
+00003ad0: 2020 2020 2020 2020 2020 2069 6620 6973             if is
+00003ae0: 696e 7374 616e 6365 2876 616c 7565 2c20  instance(value, 
+00003af0: 6c69 7374 293a 0a20 2020 2020 2020 2020  list):.         
+00003b00: 2020 2020 2020 2073 656c 662e 5f6c 656e         self._len
+00003b10: 6774 685b 7661 6c75 655d 203d 206c 656e  gth[value] = len
+00003b20: 2876 616c 7565 2e76 616c 7565 290a 2020  (value.value).  
+00003b30: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.
+00003b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003b50: 7365 6c66 2e5f 6c65 6e67 7468 5b76 616c  self._length[val
+00003b60: 7565 5d20 3d20 310a 2020 2020 2020 2020  ue] = 1.        
+00003b70: 0a20 2020 2020 2020 2064 7479 7065 203d  .        dtype =
+00003b80: 2076 616c 7565 735b 305d 2e76 616c 7565   values[0].value
+00003b90: 5f74 7970 650a 2020 2020 2020 2020 7363  _type.        sc
+00003ba0: 616c 656f 6666 7365 7420 3d20 4e6f 6e65  aleoffset = None
+00003bb0: 0a20 2020 2020 2020 2023 2054 6865 7265  .        # There
+00003bc0: 2069 7320 6e6f 206e 702e 666c 6f61 7431   is no np.float1
+00003bd0: 3238 206f 6e20 5769 6e64 6f77 732e 0a20  28 on Windows.. 
+00003be0: 2020 2020 2020 2069 6620 6474 7970 6520         if dtype 
+00003bf0: 696e 2028 666c 6f61 742c 206e 702e 666c  in (float, np.fl
+00003c00: 6f61 7433 322c 206e 702e 666c 6f61 7436  oat32, np.float6
+00003c10: 342c 206e 702e 6c6f 6e67 646f 7562 6c65  4, np.longdouble
+00003c20: 293a 0a20 2020 2020 2020 2020 2020 2073  ):.            s
+00003c30: 6361 6c65 6f66 6673 6574 203d 2034 0a20  caleoffset = 4. 
+00003c40: 2020 2020 2020 2074 696d 655f 7374 6570         time_step
+00003c50: 5f70 6174 6820 3d20 4e6f 6e65 0a20 2020  _path = None.   
+00003c60: 2020 2020 206d 6178 5f73 697a 6520 3d20       max_size = 
+00003c70: 6d61 785f 7369 7a65 202a 206c 656e 2876  max_size * len(v
+00003c80: 616c 7565 7329 2069 6620 7374 6f72 655f  alues) if store_
+00003c90: 7469 6d65 5f73 7465 7020 656c 7365 206d  time_step else m
+00003ca0: 6178 5f73 697a 650a 0a20 2020 2020 2020  ax_size..       
+00003cb0: 2069 6620 7374 6f72 655f 7469 6d65 5f73   if store_time_s
+00003cc0: 7465 703a 0a20 2020 2020 2020 2020 2020  tep:.           
+00003cd0: 2023 2053 746f 7265 2069 6e64 6963 6573   # Store indices
+00003ce0: 2066 6f72 2074 696d 6520 7374 6570 2061   for time step a
+00003cf0: 6e64 2065 6c65 6d65 6e74 2e0a 2020 2020  nd element..    
+00003d00: 2020 2020 2020 2020 2320 4561 6368 2072          # Each r
+00003d10: 6f77 206f 6620 7468 6973 2064 6174 6173  ow of this datas
+00003d20: 6574 2063 6f72 7265 7370 6f6e 6473 2074  et corresponds t
+00003d30: 6f20 6120 726f 7720 696e 2074 6865 2064  o a row in the d
+00003d40: 6174 612e 0a20 2020 2020 2020 2020 2020  ata..           
+00003d50: 2023 2054 6869 7320 7769 6c6c 2062 6520   # This will be 
+00003d60: 7265 7175 6972 6564 2074 6f20 696e 7465  required to inte
+00003d70: 7270 7265 7420 7468 6520 7261 7720 6461  rpret the raw da
+00003d80: 7461 2e0a 2020 2020 2020 2020 2020 2020  ta..            
+00003d90: 6174 7472 6962 7574 6573 203d 207b 2274  attributes = {"t
+00003da0: 7970 6522 3a20 4461 7461 7365 7450 726f  ype": DatasetPro
+00003db0: 7065 7274 7954 7970 652e 5449 4d45 5f53  pertyType.TIME_S
+00003dc0: 5445 502e 7661 6c75 657d 0a20 2020 2020  TEP.value}.     
+00003dd0: 2020 2020 2020 2074 696d 655f 7374 6570         time_step
+00003de0: 5f70 6174 6820 3d20 7365 6c66 2e74 696d  _path = self.tim
+00003df0: 655f 7374 6570 5f70 6174 6828 7061 7468  e_step_path(path
+00003e00: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+00003e10: 6c66 2e5f 7469 6d65 5f73 7465 7073 203d  lf._time_steps =
+00003e20: 2044 6174 6173 6574 4275 6666 6572 280a   DatasetBuffer(.
+00003e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003e40: 6864 665f 7374 6f72 652c 0a20 2020 2020  hdf_store,.     
+00003e50: 2020 2020 2020 2020 2020 2074 696d 655f             time_
+00003e60: 7374 6570 5f70 6174 682c 0a20 2020 2020  step_path,.     
+00003e70: 2020 2020 2020 2020 2020 206d 6178 5f73             max_s
+00003e80: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
+00003e90: 2020 2020 2069 6e74 2c0a 2020 2020 2020       int,.      
+00003ea0: 2020 2020 2020 2020 2020 5b22 5469 6d65            ["Time
+00003eb0: 222c 2022 4e61 6d65 225d 2c0a 2020 2020  ", "Name"],.    
+00003ec0: 2020 2020 2020 2020 2020 2020 7363 616c              scal
+00003ed0: 656f 6666 7365 743d 302c 0a20 2020 2020  eoffset=0,.     
+00003ee0: 2020 2020 2020 2020 2020 206d 6178 5f63             max_c
+00003ef0: 6875 6e6b 5f62 7974 6573 3d6d 6178 5f63  hunk_bytes=max_c
+00003f00: 6875 6e6b 5f62 7974 6573 2c0a 2020 2020  hunk_bytes,.    
+00003f10: 2020 2020 2020 2020 2020 2020 6174 7472              attr
+00003f20: 6962 7574 6573 3d61 7474 7269 6275 7465  ibutes=attribute
+00003f30: 732c 0a20 2020 2020 2020 2020 2020 2029  s,.            )
+00003f40: 0a20 2020 2020 2020 2020 2020 2063 6f6c  .            col
+00003f50: 756d 6e73 203d 205b 5d0a 2020 2020 2020  umns = [].      
+00003f60: 2020 2020 2020 746d 705f 636f 6c75 6d6e        tmp_column
+00003f70: 7320 3d20 7661 6c75 6573 5b30 5d2e 6d61  s = values[0].ma
+00003f80: 6b65 5f63 6f6c 756d 6e73 2829 0a20 2020  ke_columns().   
+00003f90: 2020 2020 2020 2020 2066 6f72 2063 6f6c           for col
+00003fa0: 756d 6e20 696e 2074 6d70 5f63 6f6c 756d  umn in tmp_colum
+00003fb0: 6e73 3a0a 2020 2020 2020 2020 2020 2020  ns:.            
+00003fc0: 2020 2020 6669 656c 6473 203d 2063 6f6c      fields = col
+00003fd0: 756d 6e2e 7370 6c69 7428 5661 6c75 6553  umn.split(ValueS
+00003fe0: 746f 7261 6765 4261 7365 2e44 454c 494d  torageBase.DELIM
+00003ff0: 4954 4552 290a 2020 2020 2020 2020 2020  ITER).          
+00004000: 2020 2020 2020 6669 656c 6473 5b30 5d20        fields[0] 
+00004010: 3d20 2241 6c6c 4e61 6d65 7322 0a20 2020  = "AllNames".   
+00004020: 2020 2020 2020 2020 2020 2020 2063 6f6c               col
+00004030: 756d 6e73 2e61 7070 656e 6428 5661 6c75  umns.append(Valu
+00004040: 6553 746f 7261 6765 4261 7365 2e44 454c  eStorageBase.DEL
+00004050: 494d 4954 4552 2e6a 6f69 6e28 6669 656c  IMITER.join(fiel
+00004060: 6473 2929 0a20 2020 2020 2020 2020 2020  ds)).           
+00004070: 2063 6f6c 756d 6e5f 7261 6e67 6573 203d   column_ranges =
+00004080: 205b 302c 206c 656e 2874 6d70 5f63 6f6c   [0, len(tmp_col
+00004090: 756d 6e73 295d 0a20 2020 2020 2020 2065  umns)].        e
+000040a0: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+000040b0: 2063 6f6c 756d 6e73 203d 205b 5d0a 2020   columns = [].  
+000040c0: 2020 2020 2020 2020 2020 636f 6c75 6d6e            column
+000040d0: 5f72 616e 6765 7320 3d20 5b5d 0a20 2020  _ranges = [].   
+000040e0: 2020 2020 2020 2020 2063 6f6c 5f69 6e64           col_ind
+000040f0: 6578 203d 2030 0a20 2020 2020 2020 2020  ex = 0.         
+00004100: 2020 2066 6f72 2076 616c 7565 2069 6e20     for value in 
+00004110: 7661 6c75 6573 3a0a 2020 2020 2020 2020  values:.        
+00004120: 2020 2020 2020 2020 746d 705f 636f 6c75          tmp_colu
+00004130: 6d6e 7320 3d20 7661 6c75 652e 6d61 6b65  mns = value.make
+00004140: 5f63 6f6c 756d 6e73 2829 0a20 2020 2020  _columns().     
+00004150: 2020 2020 2020 2020 2020 2063 6f6c 5f72             col_r
+00004160: 616e 6765 203d 2028 636f 6c5f 696e 6465  ange = (col_inde
+00004170: 782c 206c 656e 2874 6d70 5f63 6f6c 756d  x, len(tmp_colum
+00004180: 6e73 2929 0a20 2020 2020 2020 2020 2020  ns)).           
+00004190: 2020 2020 2063 6f6c 756d 6e5f 7261 6e67       column_rang
+000041a0: 6573 2e61 7070 656e 6428 636f 6c5f 7261  es.append(col_ra
+000041b0: 6e67 6529 0a20 2020 2020 2020 2020 2020  nge).           
+000041c0: 2020 2020 2066 6f72 2063 6f6c 756d 6e20       for column 
+000041d0: 696e 2074 6d70 5f63 6f6c 756d 6e73 3a0a  in tmp_columns:.
+000041e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000041f0: 2020 2020 636f 6c75 6d6e 732e 6170 7065      columns.appe
+00004200: 6e64 2863 6f6c 756d 6e29 0a20 2020 2020  nd(column).     
+00004210: 2020 2020 2020 2020 2020 2020 2020 2063                 c
+00004220: 6f6c 5f69 6e64 6578 202b 3d20 310a 2020  ol_index += 1.  
+00004230: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
+00004240: 7469 6d65 5f73 7465 7073 203d 204e 6f6e  time_steps = Non
+00004250: 650a 0a20 2020 2020 2020 2061 7474 7269  e..        attri
+00004260: 6275 7465 7320 3d20 7b22 7479 7065 223a  butes = {"type":
+00004270: 2064 6174 6173 6574 5f70 726f 7065 7274   dataset_propert
+00004280: 795f 7479 7065 2e76 616c 7565 7d0a 2020  y_type.value}.  
+00004290: 2020 2020 2020 6966 2073 746f 7265 5f74        if store_t
+000042a0: 696d 655f 7374 6570 3a0a 2020 2020 2020  ime_step:.      
+000042b0: 2020 2020 2020 6174 7472 6962 7574 6573        attributes
+000042c0: 5b22 7469 6d65 5f73 7465 705f 7061 7468  ["time_step_path
+000042d0: 225d 203d 2074 696d 655f 7374 6570 5f70  "] = time_step_p
+000042e0: 6174 680a 0a20 2020 2020 2020 2073 656c  ath..        sel
+000042f0: 662e 5f64 6174 6173 6574 203d 2044 6174  f._dataset = Dat
+00004300: 6173 6574 4275 6666 6572 280a 2020 2020  asetBuffer(.    
+00004310: 2020 2020 2020 2020 6864 665f 7374 6f72          hdf_stor
+00004320: 652c 0a20 2020 2020 2020 2020 2020 2070  e,.            p
+00004330: 6174 682c 0a20 2020 2020 2020 2020 2020  ath,.           
+00004340: 206d 6178 5f73 697a 652c 0a20 2020 2020   max_size,.     
+00004350: 2020 2020 2020 2064 7479 7065 2c0a 2020         dtype,.  
+00004360: 2020 2020 2020 2020 2020 636f 6c75 6d6e            column
+00004370: 732c 0a20 2020 2020 2020 2020 2020 2073  s,.            s
+00004380: 6361 6c65 6f66 6673 6574 3d73 6361 6c65  caleoffset=scale
+00004390: 6f66 6673 6574 2c0a 2020 2020 2020 2020  offset,.        
+000043a0: 2020 2020 6d61 785f 6368 756e 6b5f 6279      max_chunk_by
+000043b0: 7465 733d 6d61 785f 6368 756e 6b5f 6279  tes=max_chunk_by
+000043c0: 7465 732c 0a20 2020 2020 2020 2020 2020  tes,.           
+000043d0: 2061 7474 7269 6275 7465 733d 6174 7472   attributes=attr
+000043e0: 6962 7574 6573 2c0a 2020 2020 2020 2020  ibutes,.        
+000043f0: 2020 2020 6e61 6d65 733d 656c 656d 5f6e      names=elem_n
+00004400: 616d 6573 2c0a 2020 2020 2020 2020 2020  ames,.          
+00004410: 2020 636f 6c75 6d6e 5f72 616e 6765 735f    column_ranges_
+00004420: 7065 725f 6e61 6d65 3d63 6f6c 756d 6e5f  per_name=column_
+00004430: 7261 6e67 6573 2c0a 2020 2020 2020 2020  ranges,.        
+00004440: 290a 0a20 2020 2040 7374 6174 6963 6d65  )..    @staticme
+00004450: 7468 6f64 0a20 2020 2064 6566 2074 696d  thod.    def tim
+00004460: 655f 7374 6570 5f70 6174 6828 7061 7468  e_step_path(path
+00004470: 293a 0a20 2020 2020 2020 2072 6574 7572  ):.        retur
+00004480: 6e20 7061 7468 202b 2022 5469 6d65 5374  n path + "TimeSt
+00004490: 6570 220a 0a20 2020 2064 6566 2061 7070  ep"..    def app
+000044a0: 656e 6428 7365 6c66 2c20 7661 6c75 6573  end(self, values
+000044b0: 293a 0a20 2020 2020 2020 2022 2222 4170  ):.        """Ap
+000044c0: 7065 6e64 2061 2076 616c 7565 2074 6f20  pend a value to 
+000044d0: 7468 6520 636f 6e74 6169 6e65 722e 0a0a  the container...
+000044e0: 2020 2020 2020 2020 5061 7261 6d65 7465          Paramete
+000044f0: 7273 0a20 2020 2020 2020 202d 2d2d 2d2d  rs.        -----
+00004500: 2d2d 2d2d 2d0a 2020 2020 2020 2020 7661  -----.        va
+00004510: 6c75 6520 3a20 6c69 7374 0a20 2020 2020  lue : list.     
+00004520: 2020 2020 2020 206c 6973 7420 6f66 2056         list of V
+00004530: 616c 7565 5374 6f72 6167 6542 6173 650a  alueStorageBase.
+00004540: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
+00004550: 2020 2020 200a 2020 2020 2020 2020 6966       .        if
+00004560: 2076 616c 7565 733a 0a20 2020 2020 2020   values:.       
+00004570: 2020 2020 2069 6620 6973 696e 7374 616e       if isinstan
+00004580: 6365 2876 616c 7565 735b 305d 2e76 616c  ce(values[0].val
+00004590: 7565 2c20 6c69 7374 293a 0a20 2020 2020  ue, list):.     
+000045a0: 2020 2020 2020 2020 2020 2076 616c 7320             vals 
+000045b0: 3d20 5b78 2066 6f72 2079 2069 6e20 7661  = [x for y in va
+000045c0: 6c75 6573 2066 6f72 2078 2069 6e20 792e  lues for x in y.
+000045d0: 7661 6c75 655d 0a20 2020 2020 2020 2020  value].         
+000045e0: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
+000045f0: 2020 2020 2020 2020 2076 616c 7320 3d20           vals = 
+00004600: 5b49 4e54 4547 4552 5f4e 414e 2069 6620  [INTEGER_NAN if 
+00004610: 2878 2e69 735f 6e61 6e28 2920 616e 6420  (x.is_nan() and 
+00004620: 782e 5f76 616c 7565 5f74 7970 6520 3d3d  x._value_type ==
+00004630: 2069 6e74 2920 656c 7365 2078 2e76 616c   int) else x.val
+00004640: 7565 2066 6f72 2078 2069 6e20 7661 6c75  ue for x in valu
+00004650: 6573 205d 0a20 2020 2020 2020 2065 6c73  es ].        els
+00004660: 653a 0a20 2020 2020 2020 2020 2020 2076  e:.            v
+00004670: 616c 7320 3d20 5b73 656c 662e 7365 745f  als = [self.set_
+00004680: 6e61 6e28 2920 666f 7220 6b2c 2076 2069  nan() for k, v i
+00004690: 6e20 7365 6c66 2e5f 6c65 6e67 7468 2e69  n self._length.i
+000046a0: 7465 6d73 2829 2066 6f72 2078 2069 6e20  tems() for x in 
+000046b0: 7261 6e67 6528 7629 5d0a 2020 2020 2020  range(v)].      
+000046c0: 2020 7365 6c66 2e5f 6461 7461 7365 742e    self._dataset.
+000046d0: 7772 6974 655f 7661 6c75 6528 7661 6c73  write_value(vals
+000046e0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+000046f0: 2020 2020 2020 200a 0a20 2020 2064 6566         ..    def
+00004700: 2061 7070 656e 645f 6279 5f74 696d 655f   append_by_time_
+00004710: 7374 6570 2873 656c 662c 2076 616c 7565  step(self, value
+00004720: 2c20 7469 6d65 5f73 7465 702c 2065 6c65  , time_step, ele
+00004730: 6d5f 696e 6465 7829 3a0a 2020 2020 2020  m_index):.      
+00004740: 2020 2222 2241 7070 656e 6420 6120 7661    """Append a va
+00004750: 6c75 6520 746f 2074 6865 2063 6f6e 7461  lue to the conta
+00004760: 696e 6572 2e0a 0a20 2020 2020 2020 2050  iner...        P
+00004770: 6172 616d 6574 6572 730a 2020 2020 2020  arameters.      
+00004780: 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020    ----------.   
+00004790: 2020 2020 2076 616c 7565 203a 2056 616c       value : Val
+000047a0: 7565 5374 6f72 6167 6542 6173 650a 2020  ueStorageBase.  
+000047b0: 2020 2020 2020 7469 6d65 5f73 7465 7020        time_step 
+000047c0: 3a20 696e 740a 2020 2020 2020 2020 656c  : int.        el
+000047d0: 656d 5f69 6e64 6578 203a 2069 6e74 0a0a  em_index : int..
+000047e0: 2020 2020 2020 2020 2222 220a 2020 2020          """.    
+000047f0: 2020 2020 0a20 2020 2020 2020 2069 6620      .        if 
+00004800: 6973 696e 7374 616e 6365 2876 616c 7565  isinstance(value
+00004810: 2e76 616c 7565 2c20 6c69 7374 293a 0a20  .value, list):. 
+00004820: 2020 2020 2020 2020 2020 2076 616c 7320             vals 
+00004830: 3d20 5b78 2066 6f72 2078 2069 6e20 7661  = [x for x in va
+00004840: 6c75 652e 7661 6c75 655d 0a20 2020 2020  lue.value].     
+00004850: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
+00004860: 2020 2020 2076 616c 7320 3d20 7661 6c75       vals = valu
+00004870: 652e 7661 6c75 650a 0a20 2020 2020 2020  e.value..       
+00004880: 2073 656c 662e 5f64 6174 6173 6574 2e77   self._dataset.w
+00004890: 7269 7465 5f76 616c 7565 2876 616c 7329  rite_value(vals)
+000048a0: 0a20 2020 2020 2020 2073 656c 662e 5f74  .        self._t
+000048b0: 696d 655f 7374 6570 732e 7772 6974 655f  ime_steps.write_
+000048c0: 7661 6c75 6528 5b74 696d 655f 7374 6570  value([time_step
+000048d0: 2c20 656c 656d 5f69 6e64 6578 5d29 0a20  , elem_index]). 
+000048e0: 2020 2020 2020 200a 2020 2020 2020 2020         .        
+000048f0: 0a20 2020 2064 6566 2066 6c75 7368 5f64  .    def flush_d
+00004900: 6174 6128 7365 6c66 293a 0a20 2020 2020  ata(self):.     
+00004910: 2020 2022 2222 466c 7573 6820 616e 7920     """Flush any 
+00004920: 6f75 7473 7461 6e64 696e 6720 6461 7461  outstanding data
+00004930: 2074 6f20 6469 736b 2e22 2222 0a20 2020   to disk.""".   
+00004940: 2020 2020 2073 656c 662e 5f64 6174 6173       self._datas
+00004950: 6574 2e66 6c75 7368 5f64 6174 6128 290a  et.flush_data().
+00004960: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
+00004970: 5f74 696d 655f 7374 6570 7320 6973 206e  _time_steps is n
+00004980: 6f74 204e 6f6e 653a 0a20 2020 2020 2020  ot None:.       
+00004990: 2020 2020 2073 656c 662e 5f74 696d 655f       self._time_
+000049a0: 7374 6570 732e 666c 7573 685f 6461 7461  steps.flush_data
+000049b0: 2829 0a0a 2020 2020 6465 6620 6d61 785f  ()..    def max_
+000049c0: 6e75 6d5f 6279 7465 7328 7365 6c66 293a  num_bytes(self):
+000049d0: 0a20 2020 2020 2020 2022 2222 5265 7475  .        """Retu
+000049e0: 726e 2074 6865 206d 6178 696d 756d 206e  rn the maximum n
+000049f0: 756d 6265 7220 6f66 2062 7974 6573 2074  umber of bytes t
+00004a00: 6865 2063 6f6e 7461 696e 6572 2063 6f75  he container cou
+00004a10: 6c64 2068 6f6c 642e 0a0a 2020 2020 2020  ld hold...      
+00004a20: 2020 5265 7475 726e 730a 2020 2020 2020    Returns.      
+00004a30: 2020 2d2d 2d2d 2d2d 2d0a 2020 2020 2020    -------.      
+00004a40: 2020 696e 740a 0a20 2020 2020 2020 2022    int..        "
+00004a50: 2222 0a20 2020 2020 2020 2072 6574 7572  "".        retur
+00004a60: 6e20 7365 6c66 2e5f 6461 7461 7365 742e  n self._dataset.
+00004a70: 6d61 785f 6e75 6d5f 6279 7465 7328 290a  max_num_bytes().
+00004a80: 0a0a 6465 6620 6765 745f 6461 7461 7365  ..def get_datase
+00004a90: 745f 7072 6f70 6572 7479 5f74 7970 6528  t_property_type(
+00004aa0: 6461 7461 7365 7429 3a0a 2020 2020 2222  dataset):.    ""
+00004ab0: 2252 6574 7572 6e20 7468 6520 7072 6f70  "Return the prop
+00004ac0: 6572 7479 2074 7970 6520 6f66 2074 6869  erty type of thi
+00004ad0: 7320 6461 7461 7365 742e 0a0a 2020 2020  s dataset...    
+00004ae0: 5265 7475 726e 730a 2020 2020 2d2d 2d2d  Returns.    ----
+00004af0: 2d2d 2d0a 2020 2020 4461 7461 7365 7450  ---.    DatasetP
+00004b00: 726f 7065 7274 7954 7970 650a 0a20 2020  ropertyType..   
+00004b10: 2022 2222 0a20 2020 2072 6574 7572 6e20   """.    return 
+00004b20: 4461 7461 7365 7450 726f 7065 7274 7954  DatasetPropertyT
+00004b30: 7970 6528 6461 7461 7365 742e 6174 7472  ype(dataset.attr
+00004b40: 735b 2274 7970 6522 5d29 0a0a 0a64 6566  s["type"])...def
+00004b50: 2067 6574 5f74 696d 655f 7374 6570 5f70   get_time_step_p
+00004b60: 6174 6828 6461 7461 7365 7429 3a0a 2020  ath(dataset):.  
+00004b70: 2020 2222 2252 6574 7572 6e20 7468 6520    """Return the 
+00004b80: 7061 7468 2074 6f20 7468 6520 7469 6d65  path to the time
+00004b90: 5f73 7465 7073 2066 6f72 2074 6869 7320  _steps for this 
+00004ba0: 6461 7461 7365 742e 0a0a 2020 2020 5265  dataset...    Re
+00004bb0: 7475 726e 730a 2020 2020 2d2d 2d2d 2d2d  turns.    ------
+00004bc0: 2d0a 2020 2020 7374 720a 0a20 2020 2022  -.    str..    "
+00004bd0: 2222 0a20 2020 2072 6574 7572 6e20 6461  "".    return da
+00004be0: 7461 7365 742e 6174 7472 735b 2274 696d  taset.attrs["tim
+00004bf0: 655f 7374 6570 5f70 6174 6822 5d0a       e_step_path"].
```

### Comparing `nrel_pydss-3.1.3/src/pydss/Extensions/MonteCarlo.py` & `nrel_pydss-3.1.4/src/pydss/Extensions/MonteCarlo.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,59 +1,59 @@
-from ast import literal_eval
-import os
-
-from loguru import logger
-from scipy import stats
-import numpy as np
-
-from pydss.simulation_input_models import SimulationSettingsModel
-from pydss.utils import utils
-
-class MonteCarloSim:
-
-    def __init__(self, settings: SimulationSettingsModel, dssPaths, dssObjects, dssObjectsByClass):
-        self.__dssPaths = dssPaths
-        self.__dssObjects = dssObjects
-        self._settings = settings
-        self.__dssObjectsByClass = dssObjectsByClass
-
-        try:
-            MCfile = os.path.join(self._settings.project.active_scenario, 'Monte_Carlo', 'MonteCarloSettings.toml')
-            MCfilePath = os.path.join(self.__dssPaths['Import'], MCfile)
-
-            logger.info('Reading monte carlo scenario settings file from ' + MCfilePath)
-            self.__MCsettingsDict = utils.load_data(MCfilePath)
-        except:
-            logger.error('Failed to read Monte Carlo scenario generation file %s', MCfilePath)
-            raise
-        return
-
-    def Create_Scenario(self):
-        for key, Properties in self.__MCsettingsDict.items():
-            if Properties['Class'] in self.__dssObjectsByClass:
-                Elements = self.__dssObjectsByClass[Properties['Class']]
-                ElmNames = list(Elements.keys())
-                if Properties['useWildCard']:
-                    ElmNames = [x for x in ElmNames if Properties['Wildcard'] in x]
-                NumElms = len(ElmNames)
-                distParams = literal_eval(Properties['Parameters'])
-
-                dist = getattr(stats, Properties['Distribution'].replace(' ', ''))
-                if not Properties['isList']:
-                    MCsamples = dist.rvs(*distParams, size=NumElms)
-                    if Properties['isInteger']:
-                        MCsamples = [int(round(x)) for x in MCsamples]
-                    for ElmName, Value in zip(ElmNames,MCsamples):
-                        Elements[ElmName].SetParameter(Properties['Property'], Value)
-                else:
-                    MCsamples = dist.rvs(*distParams, size=NumElms * Properties['ListLength'])
-                    if Properties['isInteger']:
-                        MCsamples = [int(round(x)) for x in MCsamples]
-                    MCsamples = np.reshape(MCsamples, (NumElms, Properties['ListLength']))
-                    for ElmName, Value in zip(ElmNames, MCsamples):
-                        Value = str(Value).replace('\n', '').replace('\r', '').replace('[ ', '[').replace(' ]', ']')
-                        Elements[ElmName].SetParameter(Properties['Property'], Value)
-            else:
-                logger.warning(Properties['Class'] + ' class not present in object dictionary.')
-        return
-
-
+from ast import literal_eval
+import os
+
+from loguru import logger
+from scipy import stats
+import numpy as np
+
+from pydss.simulation_input_models import SimulationSettingsModel
+from pydss.utils import utils
+
+class MonteCarloSim:
+
+    def __init__(self, settings: SimulationSettingsModel, dssPaths, dssObjects, dssObjectsByClass):
+        self.__dssPaths = dssPaths
+        self.__dssObjects = dssObjects
+        self._settings = settings
+        self.__dssObjectsByClass = dssObjectsByClass
+
+        try:
+            MCfile = os.path.join(self._settings.project.active_scenario, 'Monte_Carlo', 'MonteCarloSettings.toml')
+            MCfilePath = os.path.join(self.__dssPaths['Import'], MCfile)
+
+            logger.info('Reading monte carlo scenario settings file from ' + MCfilePath)
+            self.__MCsettingsDict = utils.load_data(MCfilePath)
+        except:
+            logger.error('Failed to read Monte Carlo scenario generation file %s', MCfilePath)
+            raise
+        return
+
+    def Create_Scenario(self):
+        for key, Properties in self.__MCsettingsDict.items():
+            if Properties['Class'] in self.__dssObjectsByClass:
+                Elements = self.__dssObjectsByClass[Properties['Class']]
+                ElmNames = list(Elements.keys())
+                if Properties['useWildCard']:
+                    ElmNames = [x for x in ElmNames if Properties['Wildcard'] in x]
+                NumElms = len(ElmNames)
+                distParams = literal_eval(Properties['Parameters'])
+
+                dist = getattr(stats, Properties['Distribution'].replace(' ', ''))
+                if not Properties['isList']:
+                    MCsamples = dist.rvs(*distParams, size=NumElms)
+                    if Properties['isInteger']:
+                        MCsamples = [int(round(x)) for x in MCsamples]
+                    for ElmName, Value in zip(ElmNames,MCsamples):
+                        Elements[ElmName].SetParameter(Properties['Property'], Value)
+                else:
+                    MCsamples = dist.rvs(*distParams, size=NumElms * Properties['ListLength'])
+                    if Properties['isInteger']:
+                        MCsamples = [int(round(x)) for x in MCsamples]
+                    MCsamples = np.reshape(MCsamples, (NumElms, Properties['ListLength']))
+                    for ElmName, Value in zip(ElmNames, MCsamples):
+                        Value = str(Value).replace('\n', '').replace('\r', '').replace('[ ', '[').replace(' ]', ']')
+                        Elements[ElmName].SetParameter(Properties['Property'], Value)
+            else:
+                logger.warning(Properties['Class'] + ' class not present in object dictionary.')
+        return
+
+
```

### Comparing `nrel_pydss-3.1.3/src/pydss/ProfileManager/ProfileInterface.py` & `nrel_pydss-3.1.4/src/pydss/ProfileManager/ProfileInterface.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,26 +1,26 @@
-from os.path import dirname, basename, isfile
-from pydss.ProfileManager import hooks
-import importlib
-import glob
-
-
-modules = glob.glob(hooks.__path__[0]+"/*.py")
-pythonFiles = [(basename(f)[:-3], f) for f in modules if isfile(f) and not f.endswith('__init__.py')]
-
-ProfileInterfaces = {}
-modules_instances = {}
-for module, file in pythonFiles:
-    spec = importlib.util.spec_from_file_location(module, file)
-    modules_instances[module] = importlib.util.module_from_spec(spec)
-    spec.loader.exec_module(modules_instances[module])
-    ProfileInterfaces[module] = modules_instances[module].ProfileManager
-
-def Create(sim_instance, solver, settings, logger, **kwargs):
-    source_type = settings.profiles.source_type.value
-    if source_type in ProfileInterfaces:
-        PostProcessor = ProfileInterfaces[source_type](
-            sim_instance, solver, settings, logger, **kwargs
-        )
-    else:
-        raise ModuleNotFoundError(f"{source_type} is not a valid source type. Valid values are {','.join(list(ProfileInterfaces.keys()))}")
-    return PostProcessor
+from os.path import dirname, basename, isfile
+from pydss.ProfileManager import hooks
+import importlib
+import glob
+
+
+modules = glob.glob(hooks.__path__[0]+"/*.py")
+pythonFiles = [(basename(f)[:-3], f) for f in modules if isfile(f) and not f.endswith('__init__.py')]
+
+ProfileInterfaces = {}
+modules_instances = {}
+for module, file in pythonFiles:
+    spec = importlib.util.spec_from_file_location(module, file)
+    modules_instances[module] = importlib.util.module_from_spec(spec)
+    spec.loader.exec_module(modules_instances[module])
+    ProfileInterfaces[module] = modules_instances[module].ProfileManager
+
+def Create(sim_instance, solver, settings, logger, **kwargs):
+    source_type = settings.profiles.source_type.value
+    if source_type in ProfileInterfaces:
+        PostProcessor = ProfileInterfaces[source_type](
+            sim_instance, solver, settings, logger, **kwargs
+        )
+    else:
+        raise ModuleNotFoundError(f"{source_type} is not a valid source type. Valid values are {','.join(list(ProfileInterfaces.keys()))}")
+    return PostProcessor
```

### Comparing `nrel_pydss-3.1.3/src/pydss/ProfileManager/base_definitions.py` & `nrel_pydss-3.1.4/src/pydss/ProfileManager/base_definitions.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,60 +1,60 @@
-from pydss.common import PROFILE_MAPPING
-import datetime
-import toml
-import abc
-import os
-
-from pydss.simulation_input_models import SimulationSettingsModel
-
-
-class BaseProfileManager(abc.ABC):
-    def __init__(self, sim_instance, solver, settings: SimulationSettingsModel, logger, **kwargs):
-        self.sim_instance = sim_instance
-        self.settings = settings
-        self.solver = solver
-        self.logger = logger
-        self.kwargs = kwargs
-        if not settings.profiles.is_relative_path:
-            self.basepath = settings.profiles.source
-        else:
-            self.basepath = os.path.join(
-                settings.project.active_project_path,
-                "Profiles",
-                settings.profiles.source,
-            )
-        self.mapping_file = os.path.join(
-            settings.project.active_project_path,
-            "Profiles",
-            PROFILE_MAPPING
-        )
-        self.Profiles = {}
-        self.mapping = toml.load(open(self.mapping_file , "r"))
-        self.sTime = None
-        self.eTime = None
-        self.simRes = None
-
-    @abc.abstractmethod
-    def setup_profiles(self):
-        pass
-
-    @abc.abstractmethod
-    def update(self):
-        pass
-
-
-class BaseProfile(abc.ABC):
-    def __init__(self, sim_instance, dataset, devices, solver, mapping_dict, logger, **kwargs):
-        self.sim_instance = sim_instance
-        self.mapping_dict = mapping_dict
-        self.dataset = dataset
-        self.devices = devices
-        self.logger = logger
-        self.solver = solver
-
-    @abc.abstractmethod
-    def update_profile_settings(self):
-        pass
-
-    @abc.abstractmethod
-    def update(self):
-        pass
+from pydss.common import PROFILE_MAPPING
+import datetime
+import toml
+import abc
+import os
+
+from pydss.simulation_input_models import SimulationSettingsModel
+
+
+class BaseProfileManager(abc.ABC):
+    def __init__(self, sim_instance, solver, settings: SimulationSettingsModel, logger, **kwargs):
+        self.sim_instance = sim_instance
+        self.settings = settings
+        self.solver = solver
+        self.logger = logger
+        self.kwargs = kwargs
+        if not settings.profiles.is_relative_path:
+            self.basepath = settings.profiles.source
+        else:
+            self.basepath = os.path.join(
+                settings.project.active_project_path,
+                "Profiles",
+                settings.profiles.source,
+            )
+        self.mapping_file = os.path.join(
+            settings.project.active_project_path,
+            "Profiles",
+            PROFILE_MAPPING
+        )
+        self.Profiles = {}
+        self.mapping = toml.load(open(self.mapping_file , "r"))
+        self.sTime = None
+        self.eTime = None
+        self.simRes = None
+
+    @abc.abstractmethod
+    def setup_profiles(self):
+        pass
+
+    @abc.abstractmethod
+    def update(self):
+        pass
+
+
+class BaseProfile(abc.ABC):
+    def __init__(self, sim_instance, dataset, devices, solver, mapping_dict, logger, **kwargs):
+        self.sim_instance = sim_instance
+        self.mapping_dict = mapping_dict
+        self.dataset = dataset
+        self.devices = devices
+        self.logger = logger
+        self.solver = solver
+
+    @abc.abstractmethod
+    def update_profile_settings(self):
+        pass
+
+    @abc.abstractmethod
+    def update(self):
+        pass
```

### Comparing `nrel_pydss-3.1.3/src/pydss/ProfileManager/hooks/MongoDB.py` & `nrel_pydss-3.1.4/src/pydss/ProfileManager/hooks/MongoDB.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,145 +1,145 @@
-from pydss.ProfileManager.base_definitions import BaseProfileManager, BaseProfile
-from bson.objectid import ObjectId
-from pymongo import MongoClient
-
-import json
-from pydss.ProfileManager.common import PROFILE_TYPES
-from pydss.exceptions import InvalidParameter
-from pydss.common import DATE_FORMAT
-from datetime import datetime
-import pandas as pd
-import numpy as np
-import datetime
-import copy
-import os
-
-class ProfileManager(BaseProfileManager):
-
-    def __init__(self,  sim_instance, solver, options, logger, **kwargs):
-        super(ProfileManager, self).__init__(sim_instance, solver, options, logger, **kwargs)
-        self.Objects = kwargs["objects"]
-        usersettings = options["Profiles"]['settings']
-
-        #self.client = MongoClient(self.basepath, username=usersettings["username"], password=usersettings["password"])
-        self.client = MongoClient("mongodb://user:password@bmather-131003:27017/?authSource=admin&readPreference=primary&appname=MongoDB%20Compass&ssl=false&connect=false")
-        self.db_name = usersettings["database"]
-        self.db = self.client[usersettings["database"]]
-        self.collections = self.db.collection_names()
-        self.mapping = usersettings["mapping"]
-        #self.one_Time_fix(usersettings['collection'])
-        self.setup_profiles()
-        #quit()
-        pass
-
-    def one_Time_fix(self, collection_name):
-        self.logger.info("Creating aggregated load profiles")
-        C = self.db[collection_name]
-        cursor = C.find({"name": "customer_metadata"})
-        customer_metadata = cursor.next()
-        i = 0
-        total_xfmr_load = {}
-        for customerID, customerInfo in customer_metadata.items():
-            if isinstance(customerInfo, dict):
-                agg_load = customerInfo['section_id']
-                if agg_load not in total_xfmr_load:
-                    total_xfmr_load[agg_load] = []
-                total_xfmr_load[agg_load].append(customerID)
-
-        record = C.find({"date": {"$exists" : 1}})
-        count = record.count()
-        posts = []
-        self.logger.info(f"Count: {count}")
-        for i in range(count):
-            total_load = {}
-            profile_data = record.next()
-            time = profile_data["date"]
-            total_load["_id"] = ObjectId()
-            total_load["date"] = time
-            total_load["aggregated"] = True
-            vars = ["kw", "kvar"]
-            for var in vars:
-                if var not in total_load:
-                    total_load[var] = {}
-                for load_name, loadlist in total_xfmr_load.items():
-                    if load_name not in total_load[var]:
-                        total_load[var][load_name] = 0
-                    for elm, val in profile_data[var].items():
-                        if elm in loadlist:
-                            total_load[var][load_name] += val
-            posts.append(total_load)
-            C.insert(total_load)
-            self.logger.info(f"Percentage complete: {(i+1)/count*100.0}")
-        self.logger.debug("Insert successful")
-
-    def setup_profiles(self):
-        for element_name, collection_name in self.mapping.items():
-            if collection_name not in self.collections:
-                raise Exception(f"{collection_name} is not a valid collection for database {self.db_name}")
-            self.logger.info(f"Reading collection: {collection_name}")
-        #for collection in self.collections:
-            C = self.db[collection_name]
-            record = C.find({"date": {"$exists" : 1}})
-            profile_data = record.next()
-            profile_data.pop('_id', None)
-            profile_data.pop('date', None)
-
-            for property, obj_info in profile_data.items():
-                if isinstance(obj_info, dict):
-                    profile_dict = {}
-                    for profile_name in obj_info:
-                        if profile_name not in profile_dict:
-                            profile_dict[profile_name] = []
-                        model_found = False
-                        for model_name, model in self.Objects.items():
-                            if element_name in model_name and profile_name in model_name:
-                                profile_dict[profile_name].append(model_name)
-                                model_found = True
-                                break
-                        if not model_found:
-                            self.logger.warning(f"Profile {profile_name} could not be mapped to any element of type {collection_name} in the OpenDSS model")
-                    for profile_name, model_list in profile_dict.items():
-                        self.Profiles[f"{element_name}.{profile_name}.{property}"] = Profile(
-                                        self.sim_instance,
-                                        (collection_name, element_name, profile_name, property),
-                                        [self.Objects[x] for x in model_list],
-                                        self.solver,
-                                        None,
-                                        self.logger,
-                                        **self.kwargs
-                                    )
-
-    def update(self):
-        self.logger.debug(self.solver.GetDateTime())
-        for element_name, collection_name in self.mapping.items():
-            C = self.db[collection_name]
-            record = C.find({'date': self.solver.GetDateTime()})
-            if not record.count():
-                self.logger.warning(f"No record found for time period {self.solver.GetDateTime()} in the database")
-            else:
-                profile_data = record.next()
-                profile_data.pop('_id', None)
-                profile_data.pop('date', None)
-                for profile in self.Profiles:
-                    self.Profiles[profile].update(profile_data)
-        pass
-
-class Profile(BaseProfile):
-
-    DEFAULT_SETTINGS = {
-        "multiplier": 1,
-        "normalize": False,
-        "interpolate": False
-    }
-
-    def __init__(self, sim_instance, dataset, devices, solver, mapping_dict, logger, **kwargs):
-        super(Profile, self).__init__(sim_instance, dataset, devices, solver, mapping_dict, logger, **kwargs)
-        self.collection_name, self.class_name, self.profile_name, self.property = self.dataset
-
-    def update_profile_settings(self):
-        pass
-
-    def update(self, updateObjectProperties=True):
-        data = updateObjectProperties
-        value = data[self.property][self.profile_name]
-        for device in self.devices:
-            device.SetParameter(self.property, value)
+from pydss.ProfileManager.base_definitions import BaseProfileManager, BaseProfile
+from bson.objectid import ObjectId
+from pymongo import MongoClient
+
+import json
+from pydss.ProfileManager.common import PROFILE_TYPES
+from pydss.exceptions import InvalidParameter
+from pydss.common import DATE_FORMAT
+from datetime import datetime
+import pandas as pd
+import numpy as np
+import datetime
+import copy
+import os
+
+class ProfileManager(BaseProfileManager):
+
+    def __init__(self,  sim_instance, solver, options, logger, **kwargs):
+        super(ProfileManager, self).__init__(sim_instance, solver, options, logger, **kwargs)
+        self.Objects = kwargs["objects"]
+        usersettings = options["Profiles"]['settings']
+
+        #self.client = MongoClient(self.basepath, username=usersettings["username"], password=usersettings["password"])
+        self.client = MongoClient("mongodb://user:password@bmather-131003:27017/?authSource=admin&readPreference=primary&appname=MongoDB%20Compass&ssl=false&connect=false")
+        self.db_name = usersettings["database"]
+        self.db = self.client[usersettings["database"]]
+        self.collections = self.db.collection_names()
+        self.mapping = usersettings["mapping"]
+        #self.one_Time_fix(usersettings['collection'])
+        self.setup_profiles()
+        #quit()
+        pass
+
+    def one_Time_fix(self, collection_name):
+        self.logger.info("Creating aggregated load profiles")
+        C = self.db[collection_name]
+        cursor = C.find({"name": "customer_metadata"})
+        customer_metadata = cursor.next()
+        i = 0
+        total_xfmr_load = {}
+        for customerID, customerInfo in customer_metadata.items():
+            if isinstance(customerInfo, dict):
+                agg_load = customerInfo['section_id']
+                if agg_load not in total_xfmr_load:
+                    total_xfmr_load[agg_load] = []
+                total_xfmr_load[agg_load].append(customerID)
+
+        record = C.find({"date": {"$exists" : 1}})
+        count = record.count()
+        posts = []
+        self.logger.info(f"Count: {count}")
+        for i in range(count):
+            total_load = {}
+            profile_data = record.next()
+            time = profile_data["date"]
+            total_load["_id"] = ObjectId()
+            total_load["date"] = time
+            total_load["aggregated"] = True
+            vars = ["kw", "kvar"]
+            for var in vars:
+                if var not in total_load:
+                    total_load[var] = {}
+                for load_name, loadlist in total_xfmr_load.items():
+                    if load_name not in total_load[var]:
+                        total_load[var][load_name] = 0
+                    for elm, val in profile_data[var].items():
+                        if elm in loadlist:
+                            total_load[var][load_name] += val
+            posts.append(total_load)
+            C.insert(total_load)
+            self.logger.info(f"Percentage complete: {(i+1)/count*100.0}")
+        self.logger.debug("Insert successful")
+
+    def setup_profiles(self):
+        for element_name, collection_name in self.mapping.items():
+            if collection_name not in self.collections:
+                raise Exception(f"{collection_name} is not a valid collection for database {self.db_name}")
+            self.logger.info(f"Reading collection: {collection_name}")
+        #for collection in self.collections:
+            C = self.db[collection_name]
+            record = C.find({"date": {"$exists" : 1}})
+            profile_data = record.next()
+            profile_data.pop('_id', None)
+            profile_data.pop('date', None)
+
+            for property, obj_info in profile_data.items():
+                if isinstance(obj_info, dict):
+                    profile_dict = {}
+                    for profile_name in obj_info:
+                        if profile_name not in profile_dict:
+                            profile_dict[profile_name] = []
+                        model_found = False
+                        for model_name, model in self.Objects.items():
+                            if element_name in model_name and profile_name in model_name:
+                                profile_dict[profile_name].append(model_name)
+                                model_found = True
+                                break
+                        if not model_found:
+                            self.logger.warning(f"Profile {profile_name} could not be mapped to any element of type {collection_name} in the OpenDSS model")
+                    for profile_name, model_list in profile_dict.items():
+                        self.Profiles[f"{element_name}.{profile_name}.{property}"] = Profile(
+                                        self.sim_instance,
+                                        (collection_name, element_name, profile_name, property),
+                                        [self.Objects[x] for x in model_list],
+                                        self.solver,
+                                        None,
+                                        self.logger,
+                                        **self.kwargs
+                                    )
+
+    def update(self):
+        self.logger.debug(self.solver.GetDateTime())
+        for element_name, collection_name in self.mapping.items():
+            C = self.db[collection_name]
+            record = C.find({'date': self.solver.GetDateTime()})
+            if not record.count():
+                self.logger.warning(f"No record found for time period {self.solver.GetDateTime()} in the database")
+            else:
+                profile_data = record.next()
+                profile_data.pop('_id', None)
+                profile_data.pop('date', None)
+                for profile in self.Profiles:
+                    self.Profiles[profile].update(profile_data)
+        pass
+
+class Profile(BaseProfile):
+
+    DEFAULT_SETTINGS = {
+        "multiplier": 1,
+        "normalize": False,
+        "interpolate": False
+    }
+
+    def __init__(self, sim_instance, dataset, devices, solver, mapping_dict, logger, **kwargs):
+        super(Profile, self).__init__(sim_instance, dataset, devices, solver, mapping_dict, logger, **kwargs)
+        self.collection_name, self.class_name, self.profile_name, self.property = self.dataset
+
+    def update_profile_settings(self):
+        pass
+
+    def update(self, updateObjectProperties=True):
+        data = updateObjectProperties
+        value = data[self.property][self.profile_name]
+        for device in self.devices:
+            device.SetParameter(self.property, value)
```

### Comparing `nrel_pydss-3.1.3/src/pydss/ProfileManager/hooks/h5.py` & `nrel_pydss-3.1.4/src/pydss/ProfileManager/hooks/h5.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,185 +1,185 @@
-from pydss.ProfileManager.base_definitions import BaseProfileManager, BaseProfile
-from pydss.ProfileManager.common import PROFILE_TYPES
-from pydss.exceptions import InvalidParameter
-from pydss.common import DATE_FORMAT
-from datetime import datetime
-import pandas as pd
-import numpy as np
-import datetime
-import h5py
-import copy
-import os
-
-class ProfileManager(BaseProfileManager):
-
-    def __init__(self,  sim_instance, solver, options, logger, **kwargs):
-        super(ProfileManager, self).__init__(sim_instance, solver, options, logger, **kwargs)
-        self.Objects = kwargs["objects"]
-        if os.path.exists(self.basepath):
-            self.logger.info("Loading existing h5 store")
-            self.store = h5py.File(self.basepath, "r+")
-        else:
-            self.logger.info("Creating new h5 store")
-            self.store = h5py.File(self.basepath, "w")
-            for profileGroup in PROFILE_TYPES.names():
-                self.store.create_group(profileGroup)
-        self.setup_profiles()
-        return
-
-    def setup_profiles(self):
-        self.Profiles = {}
-        for group, profileMap in self.mapping.items():
-            if group in self.store:
-                grp = self.store[group]
-                for profileName, mappingDict in profileMap.items():
-                    if profileName in grp:
-                        objects = {x['object']: self.Objects[x['object']] for x in mappingDict}
-                        self.Profiles[f"{group}/{profileName}"] = Profile(
-                            self.sim_instance,
-                            grp[profileName],
-                            objects,
-                            self.solver,
-                            mappingDict,
-                            self.logger,
-                            **self.kwargs
-                        )
-                    else:
-                        self.logger.warning("Group {} / data set {} not found in the h5 store".format(
-                            group, profileName
-                        ))
-            else:
-                self.logger.warning("Group {} not found in the h5 store".format(group))
-        return
-
-    def create_dataset(self, dname, pType, data ,startTime, resolution, units, info):
-        grp = self.store[pType]
-        if dname not in grp:
-            dset = grp.create_dataset(
-                dname,
-                data=data,
-                shape=(len(data),),
-                maxshape=(None,),
-                chunks=True,
-                compression="gzip",
-                compression_opts=4,
-                shuffle=True
-            )
-            self.createMetadata(
-                dset, startTime, resolution, data, units, info
-            )
-        else:
-            self.logger.error('Dataset "{}" already exists in group "{}".'.format(dname, pType))
-            raise Exception('Dataset "{}" already exists in group "{}".'.format(dname, pType))
-
-    def add_from_arrays(self, data, name, pType, startTime, resolution, units="", info=""):
-        r, c = data.shape
-        if r > c:
-            for i in range(c):
-                d = data[:, i]
-                dname = name if i==0 else "{}_{}".format(name, i)
-                self.create_dataset(dname=dname, pType=pType, data=d, startTime=startTime, resolution=resolution,
-                                    units=units, info=info)
-        else:
-            for i in range(r):
-                d = data[i, :]
-                dname = name if i==0 else "{}_{}".format(name, i)
-                self.create_dataset(dname=dname, pType=pType, data=d, startTime=startTime, resolution=resolution,
-                                    units=units, info=info)
-        return
-
-    def add_profiles_from_csv(self, csv_file, name, pType, startTime, resolution_sec=900, units="",
-                              info=""):
-        data = pd.read_csv(csv_file).values
-        self.add_profiles(data, name, pType, startTime, resolution_sec=resolution_sec, units=units, info=info)
-
-
-    def add_profiles(self, data, name, pType, startTime, resolution_sec=900, units="", info=""):
-        if type(startTime) is not datetime.datetime:
-            raise InvalidParameter("startTime should be a python datetime object")
-        if pType not in PROFILE_TYPES.names():
-            raise InvalidParameter("Valid values for pType are {}".format(PROFILE_TYPES.names()))
-        if data:
-            self.add_from_arrays(data, name, pType, startTime, resolution_sec, units=units, info=info)
-        self.store.flush()
-        return
-
-    def createMetadata(self, dSet, startTime, resolution, data, units, info):
-        metadata = {
-            "sTime": str(startTime),
-            "eTime": str(startTime + datetime.timedelta(seconds=resolution*len(data))),
-            "resTime": resolution,
-            "npts": len(data),
-            "min": min(data),
-            "max": max(data),
-            "mean": np.mean(data),
-            "units": units,
-            "info": info,
-        }
-        for key, value in metadata.items():
-            if isinstance(value, str):
-                value = np.string_(value)
-            dSet.attrs[key] = value
-        return
-
-    def remove_profile(self, profile_type, profile_name):
-        return
-
-    def update(self):
-        results = {}
-        for profileaName, profileObj in self.Profiles.items():
-            result = profileObj.update()
-            results[profileaName] = result
-        return results
-
-class Profile(BaseProfile):
-
-    DEFAULT_SETTINGS = {
-        "multiplier": 1,
-        "normalize": False,
-        "interpolate": False
-    }
-
-    def __init__(self, sim_instance, dataset, devices, solver, mapping_dict, logger, **kwargs):
-        super(Profile, self).__init__(sim_instance, dataset, devices, solver, mapping_dict, logger, **kwargs)
-        self.valueSettings = {x['object']: {**self.DEFAULT_SETTINGS, **x} for x in mapping_dict}
-
-        self.bufferSize = kwargs["bufferSize"]
-        self.buffer = np.zeros(self.bufferSize)
-        self.profile = dataset
-        self.neglectYear = kwargs["neglectYear"]
-        self.Objects = devices
-
-        self.attrs = self.profile.attrs
-        self.sTime = datetime.datetime.strptime(self.attrs["sTime"].decode(), DATE_FORMAT)
-        self.eTime = datetime.datetime.strptime(self.attrs["eTime"].decode(), '%Y-%m-%d %H:%M:%S.%f')
-        self.simRes = solver.GetStepSizeSec()
-        self.Time = copy.deepcopy(solver.GetDateTime())
-        return
-
-    def update_profile_settings(self):
-        return
-
-    def update(self, updateObjectProperties=True):
-        self.Time = copy.deepcopy(self.solver.GetDateTime())
-        if self.Time < self.sTime or self.Time > self.eTime:
-            value = 0
-            value1 = 0
-        else:
-            dT = (self.Time - self.sTime).total_seconds()
-            n = int(dT / self.attrs["resTime"])
-            value = self.profile[n]
-            dT2 = (self.Time - (self.sTime + datetime.timedelta(seconds=int(n * self.attrs["resTime"])))).total_seconds()
-            value1 = self.profile[n] + (self.profile[n+1] - self.profile[n]) * dT2 / self.attrs["resTime"]
-        if updateObjectProperties:
-            for objName, obj in self.Objects.items():
-                if self.valueSettings[objName]['interpolate']:
-                    value = value1
-                mult = self.valueSettings[objName]['multiplier']
-                if self.valueSettings[objName]['normalize']:
-                    valueF = value / self.attrs["max"] * mult
-                else:
-                    valueF = value * mult
-                obj.SetParameter(self.attrs["units"].decode(), valueF)
-        return value
-
-
+from pydss.ProfileManager.base_definitions import BaseProfileManager, BaseProfile
+from pydss.ProfileManager.common import PROFILE_TYPES
+from pydss.exceptions import InvalidParameter
+from pydss.common import DATE_FORMAT
+from datetime import datetime
+import pandas as pd
+import numpy as np
+import datetime
+import h5py
+import copy
+import os
+
+class ProfileManager(BaseProfileManager):
+
+    def __init__(self,  sim_instance, solver, options, logger, **kwargs):
+        super(ProfileManager, self).__init__(sim_instance, solver, options, logger, **kwargs)
+        self.Objects = kwargs["objects"]
+        if os.path.exists(self.basepath):
+            self.logger.info("Loading existing h5 store")
+            self.store = h5py.File(self.basepath, "r+")
+        else:
+            self.logger.info("Creating new h5 store")
+            self.store = h5py.File(self.basepath, "w")
+            for profileGroup in PROFILE_TYPES.names():
+                self.store.create_group(profileGroup)
+        self.setup_profiles()
+        return
+
+    def setup_profiles(self):
+        self.Profiles = {}
+        for group, profileMap in self.mapping.items():
+            if group in self.store:
+                grp = self.store[group]
+                for profileName, mappingDict in profileMap.items():
+                    if profileName in grp:
+                        objects = {x['object']: self.Objects[x['object']] for x in mappingDict}
+                        self.Profiles[f"{group}/{profileName}"] = Profile(
+                            self.sim_instance,
+                            grp[profileName],
+                            objects,
+                            self.solver,
+                            mappingDict,
+                            self.logger,
+                            **self.kwargs
+                        )
+                    else:
+                        self.logger.warning("Group {} / data set {} not found in the h5 store".format(
+                            group, profileName
+                        ))
+            else:
+                self.logger.warning("Group {} not found in the h5 store".format(group))
+        return
+
+    def create_dataset(self, dname, pType, data ,startTime, resolution, units, info):
+        grp = self.store[pType]
+        if dname not in grp:
+            dset = grp.create_dataset(
+                dname,
+                data=data,
+                shape=(len(data),),
+                maxshape=(None,),
+                chunks=True,
+                compression="gzip",
+                compression_opts=4,
+                shuffle=True
+            )
+            self.createMetadata(
+                dset, startTime, resolution, data, units, info
+            )
+        else:
+            self.logger.error('Dataset "{}" already exists in group "{}".'.format(dname, pType))
+            raise Exception('Dataset "{}" already exists in group "{}".'.format(dname, pType))
+
+    def add_from_arrays(self, data, name, pType, startTime, resolution, units="", info=""):
+        r, c = data.shape
+        if r > c:
+            for i in range(c):
+                d = data[:, i]
+                dname = name if i==0 else "{}_{}".format(name, i)
+                self.create_dataset(dname=dname, pType=pType, data=d, startTime=startTime, resolution=resolution,
+                                    units=units, info=info)
+        else:
+            for i in range(r):
+                d = data[i, :]
+                dname = name if i==0 else "{}_{}".format(name, i)
+                self.create_dataset(dname=dname, pType=pType, data=d, startTime=startTime, resolution=resolution,
+                                    units=units, info=info)
+        return
+
+    def add_profiles_from_csv(self, csv_file, name, pType, startTime, resolution_sec=900, units="",
+                              info=""):
+        data = pd.read_csv(csv_file).values
+        self.add_profiles(data, name, pType, startTime, resolution_sec=resolution_sec, units=units, info=info)
+
+
+    def add_profiles(self, data, name, pType, startTime, resolution_sec=900, units="", info=""):
+        if type(startTime) is not datetime.datetime:
+            raise InvalidParameter("startTime should be a python datetime object")
+        if pType not in PROFILE_TYPES.names():
+            raise InvalidParameter("Valid values for pType are {}".format(PROFILE_TYPES.names()))
+        if data:
+            self.add_from_arrays(data, name, pType, startTime, resolution_sec, units=units, info=info)
+        self.store.flush()
+        return
+
+    def createMetadata(self, dSet, startTime, resolution, data, units, info):
+        metadata = {
+            "sTime": str(startTime),
+            "eTime": str(startTime + datetime.timedelta(seconds=resolution*len(data))),
+            "resTime": resolution,
+            "npts": len(data),
+            "min": min(data),
+            "max": max(data),
+            "mean": np.mean(data),
+            "units": units,
+            "info": info,
+        }
+        for key, value in metadata.items():
+            if isinstance(value, str):
+                value = np.string_(value)
+            dSet.attrs[key] = value
+        return
+
+    def remove_profile(self, profile_type, profile_name):
+        return
+
+    def update(self):
+        results = {}
+        for profileaName, profileObj in self.Profiles.items():
+            result = profileObj.update()
+            results[profileaName] = result
+        return results
+
+class Profile(BaseProfile):
+
+    DEFAULT_SETTINGS = {
+        "multiplier": 1,
+        "normalize": False,
+        "interpolate": False
+    }
+
+    def __init__(self, sim_instance, dataset, devices, solver, mapping_dict, logger, **kwargs):
+        super(Profile, self).__init__(sim_instance, dataset, devices, solver, mapping_dict, logger, **kwargs)
+        self.valueSettings = {x['object']: {**self.DEFAULT_SETTINGS, **x} for x in mapping_dict}
+
+        self.bufferSize = kwargs["bufferSize"]
+        self.buffer = np.zeros(self.bufferSize)
+        self.profile = dataset
+        self.neglectYear = kwargs["neglectYear"]
+        self.Objects = devices
+
+        self.attrs = self.profile.attrs
+        self.sTime = datetime.datetime.strptime(self.attrs["sTime"].decode(), DATE_FORMAT)
+        self.eTime = datetime.datetime.strptime(self.attrs["eTime"].decode(), '%Y-%m-%d %H:%M:%S.%f')
+        self.simRes = solver.GetStepSizeSec()
+        self.Time = copy.deepcopy(solver.GetDateTime())
+        return
+
+    def update_profile_settings(self):
+        return
+
+    def update(self, updateObjectProperties=True):
+        self.Time = copy.deepcopy(self.solver.GetDateTime())
+        if self.Time < self.sTime or self.Time > self.eTime:
+            value = 0
+            value1 = 0
+        else:
+            dT = (self.Time - self.sTime).total_seconds()
+            n = int(dT / self.attrs["resTime"])
+            value = self.profile[n]
+            dT2 = (self.Time - (self.sTime + datetime.timedelta(seconds=int(n * self.attrs["resTime"])))).total_seconds()
+            value1 = self.profile[n] + (self.profile[n+1] - self.profile[n]) * dT2 / self.attrs["resTime"]
+        if updateObjectProperties:
+            for objName, obj in self.Objects.items():
+                if self.valueSettings[objName]['interpolate']:
+                    value = value1
+                mult = self.valueSettings[objName]['multiplier']
+                if self.valueSettings[objName]['normalize']:
+                    valueF = value / self.attrs["max"] * mult
+                else:
+                    valueF = value * mult
+                obj.SetParameter(self.attrs["units"].decode(), valueF)
+        return value
+
+
```

### Comparing `nrel_pydss-3.1.3/src/pydss/api/logging.yaml` & `nrel_pydss-3.1.4/src/pydss/api/logging.yaml`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,66 +1,66 @@
-version: 1
-disable_existing_loggers: False
-formatters:
-    # Log record attributes:
-    # https://docs.python.org/3/library/logging.html#logrecord-attributes
-    simple:
-        format: '%(asctime)s - NAERM - app_name - %(name)s - %(levelname)s - %(message)s'
-    json:
-        format: '{ "system": "NAERM", "logger_name":"%(name)s", "timestamp":"%(asctime)s", "epoch_timestamp":"%(created)f", "pid":"%(process)d", "function_name":"%(funcName)s", "line_no":"%(lineno)d", "level_name":"%(levelname)s", "message":"%(message)s" }'
-    notification:
-        format: '{ "system": "NAERM", "logger_name":"%(name)s", "timestamp":"%(asctime)s", "epoch_timestamp":"%(created)f", "pid":"%(process)d", "function_name":"%(funcName)s", "level_name":"%(levelname)s", "message":"%(message)s", "uuid": "%(uuid)s", "service_name": "%(service_name)s", "timestep": "%(timestep)s" }'
-handlers:
-    # Log handlers:
-    # https://docs.python.org/3/library/logging.handlers.html#module-logging.handlers
-    console:
-        class: logging.StreamHandler
-        level: DEBUG
-        formatter: simple
-        stream: ext://sys.stdout
-    rotatingFile:
-        class: logging.handlers.RotatingFileHandler
-        level: DEBUG
-        formatter: simple
-        filename: ../logs/app-name.log
-        maxBytes: 10240000 # 10MB
-        backupCount: 3
-    timedRotatingFile:
-        class: logging.handlers.TimedRotatingFileHandler
-        level: DEBUG
-        formatter: simple
-        filename: ../logs/timed-app-name.log
-        backupCount: 3
-        when: h
-        interval: 1
-    # syslog:
-    #     class: logging.handlers.SysLogHandler
-    #     level: DEBUG
-    #     formatter: json
-    #     # The port number change based on the socktype.
-    #     address: ['host.docker.internal', 1514] # 514/TCP 1514/UDP
-    #     socktype: 2 # 1=TCP 2=UDP
-    # syslog_notification:
-    #     # This handlers is used to log notifications to be forwarded to the
-    #     # visualization tools. 
-    #     class: logging.handlers.SysLogHandler
-    #     level: NOTSET
-    #     formatter: notification
-    #     # The port number change based on the socktype.
-    #     address: ['host.docker.internal', 1514] # 514/TCP 1514/UDP
-    #     socktype: 2 # 1=TCP 2=UDP
-loggers:
-    __main__:
-        level: DEBUG
-        handlers: [rotatingFile]
-        propagate: yes
-    web.handler:
-        level: DEBUG
-        handlers: [timedRotatingFile]
-        propagate: yes
-    core.notifier.notifier:
-        level: NOTSET
-        #handlers: [console, rotatingFile, syslog_notification]
-        propagate: no
-root:
-    level: DEBUG
+version: 1
+disable_existing_loggers: False
+formatters:
+    # Log record attributes:
+    # https://docs.python.org/3/library/logging.html#logrecord-attributes
+    simple:
+        format: '%(asctime)s - NAERM - app_name - %(name)s - %(levelname)s - %(message)s'
+    json:
+        format: '{ "system": "NAERM", "logger_name":"%(name)s", "timestamp":"%(asctime)s", "epoch_timestamp":"%(created)f", "pid":"%(process)d", "function_name":"%(funcName)s", "line_no":"%(lineno)d", "level_name":"%(levelname)s", "message":"%(message)s" }'
+    notification:
+        format: '{ "system": "NAERM", "logger_name":"%(name)s", "timestamp":"%(asctime)s", "epoch_timestamp":"%(created)f", "pid":"%(process)d", "function_name":"%(funcName)s", "level_name":"%(levelname)s", "message":"%(message)s", "uuid": "%(uuid)s", "service_name": "%(service_name)s", "timestep": "%(timestep)s" }'
+handlers:
+    # Log handlers:
+    # https://docs.python.org/3/library/logging.handlers.html#module-logging.handlers
+    console:
+        class: logging.StreamHandler
+        level: DEBUG
+        formatter: simple
+        stream: ext://sys.stdout
+    rotatingFile:
+        class: logging.handlers.RotatingFileHandler
+        level: DEBUG
+        formatter: simple
+        filename: ../logs/app-name.log
+        maxBytes: 10240000 # 10MB
+        backupCount: 3
+    timedRotatingFile:
+        class: logging.handlers.TimedRotatingFileHandler
+        level: DEBUG
+        formatter: simple
+        filename: ../logs/timed-app-name.log
+        backupCount: 3
+        when: h
+        interval: 1
+    # syslog:
+    #     class: logging.handlers.SysLogHandler
+    #     level: DEBUG
+    #     formatter: json
+    #     # The port number change based on the socktype.
+    #     address: ['host.docker.internal', 1514] # 514/TCP 1514/UDP
+    #     socktype: 2 # 1=TCP 2=UDP
+    # syslog_notification:
+    #     # This handlers is used to log notifications to be forwarded to the
+    #     # visualization tools. 
+    #     class: logging.handlers.SysLogHandler
+    #     level: NOTSET
+    #     formatter: notification
+    #     # The port number change based on the socktype.
+    #     address: ['host.docker.internal', 1514] # 514/TCP 1514/UDP
+    #     socktype: 2 # 1=TCP 2=UDP
+loggers:
+    __main__:
+        level: DEBUG
+        handlers: [rotatingFile]
+        propagate: yes
+    web.handler:
+        level: DEBUG
+        handlers: [timedRotatingFile]
+        propagate: yes
+    core.notifier.notifier:
+        level: NOTSET
+        #handlers: [console, rotatingFile, syslog_notification]
+        propagate: no
+root:
+    level: DEBUG
     #handlers: [console, rotatingFile, syslog]
```

### Comparing `nrel_pydss-3.1.3/src/pydss/api/server.py` & `nrel_pydss-3.1.4/src/pydss/api/server.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,82 +1,82 @@
-import threading
-import requests
-import json
-import time
-import os
-import io
-import re
-
-from aiohttp_swagger3 import *
-from loguru import logger
-from aiohttp import web
-
-from pydss.api.src.web.handler import Handler
-import pydss.api.schema as schema
-import pydss
-
-def read(*names, **kwargs):
-    with io.open(
-        os.path.join(os.path.dirname(pydss.__file__), *names),
-        encoding=kwargs.get("encoding", "utf8")
-    ) as fp:
-        return fp.read()
-
-def find_version(*file_paths):
-    version_file = read(*file_paths)
-    version_match = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]",
-                              version_file, re.M)
-    if version_match:
-        return version_match.group(1)
-    raise RuntimeError("Unable to find version string.")
-
-def getJSONschema(host, port):
-    base_path = schema.__path__._path[0]
-    path = os.path.join(base_path, 'pydss.v1.json')
-    base_url = f"http://{host}:{port}"
-    url = base_url + "/docs/swagger.json"
-    isValid = False
-    while not isValid:
-        time.sleep(3)
-        response = requests.get(url)
-        isValid = response.status_code == 200
-    with open(path, 'w') as outfile:
-        json.dump(response.json(), outfile, indent=4, sort_keys=True)
-    logger.info(f"Export the schema file to {path}")
-
-class pydss_server():
-    def __init__(self, host, port):
-        self.handler = Handler()
-        self.app = web.Application()
-        self.swagger = SwaggerDocs(
-            self.app,
-            title="Pydss RESTful API documentation",
-            version=find_version("__init__.py"),
-            description = "The API enables creating pydss instances, running simulations and creation of new projects.",
-            swagger_ui_settings=SwaggerUiSettings(path="/docs/"),
-            # components="components.yaml"
-        )
-        self.register_media_handlers()
-        self.add_routes()
-        t = threading.Thread(name='child procs', target=getJSONschema, args=(host, port,))
-        t.start()
-
-    def register_media_handlers(self):
-        self.swagger.register_media_type_handler("multipart/form-data", self.handler.post_pydss_create)
-        return
-
-    def add_routes(self):
-        self.swagger.add_routes([
-            web.get('/simulators/pydss/instances', self.handler.get_instance_uuids),
-            web.get('/simulators/pydss/status/uuid/{uuid}', self.handler.get_instance_status),
-            web.get('/simulators/pydss/info', self.handler.get_pydss_project_info),
-
-            web.put('/simulators/pydss', self.handler.put_pydss),
-
-            web.post('/simulators/pydss', self.handler.post_pydss),
-            web.post('/simulators/pydss/create', self.handler.post_pydss_create, validate=False),
-
-            web.delete('/simulators/pydss', self.handler.delete_pydss)
-        ])
-    async def clean_background_tasks(self, app):
-        logger.info("cleanup_background_tasks")
+import threading
+import requests
+import json
+import time
+import os
+import io
+import re
+
+from aiohttp_swagger3 import *
+from loguru import logger
+from aiohttp import web
+
+from pydss.api.src.web.handler import Handler
+import pydss.api.schema as schema
+import pydss
+
+def read(*names, **kwargs):
+    with io.open(
+        os.path.join(os.path.dirname(pydss.__file__), *names),
+        encoding=kwargs.get("encoding", "utf8")
+    ) as fp:
+        return fp.read()
+
+def find_version(*file_paths):
+    version_file = read(*file_paths)
+    version_match = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]",
+                              version_file, re.M)
+    if version_match:
+        return version_match.group(1)
+    raise RuntimeError("Unable to find version string.")
+
+def getJSONschema(host, port):
+    base_path = schema.__path__._path[0]
+    path = os.path.join(base_path, 'pydss.v1.json')
+    base_url = f"http://{host}:{port}"
+    url = base_url + "/docs/swagger.json"
+    isValid = False
+    while not isValid:
+        time.sleep(3)
+        response = requests.get(url)
+        isValid = response.status_code == 200
+    with open(path, 'w') as outfile:
+        json.dump(response.json(), outfile, indent=4, sort_keys=True)
+    logger.info(f"Export the schema file to {path}")
+
+class pydss_server():
+    def __init__(self, host, port):
+        self.handler = Handler()
+        self.app = web.Application()
+        self.swagger = SwaggerDocs(
+            self.app,
+            title="Pydss RESTful API documentation",
+            version=find_version("__init__.py"),
+            description = "The API enables creating pydss instances, running simulations and creation of new projects.",
+            swagger_ui_settings=SwaggerUiSettings(path="/docs/"),
+            # components="components.yaml"
+        )
+        self.register_media_handlers()
+        self.add_routes()
+        t = threading.Thread(name='child procs', target=getJSONschema, args=(host, port,))
+        t.start()
+
+    def register_media_handlers(self):
+        self.swagger.register_media_type_handler("multipart/form-data", self.handler.post_pydss_create)
+        return
+
+    def add_routes(self):
+        self.swagger.add_routes([
+            web.get('/simulators/pydss/instances', self.handler.get_instance_uuids),
+            web.get('/simulators/pydss/status/uuid/{uuid}', self.handler.get_instance_status),
+            web.get('/simulators/pydss/info', self.handler.get_pydss_project_info),
+
+            web.put('/simulators/pydss', self.handler.put_pydss),
+
+            web.post('/simulators/pydss', self.handler.post_pydss),
+            web.post('/simulators/pydss/create', self.handler.post_pydss_create, validate=False),
+
+            web.delete('/simulators/pydss', self.handler.delete_pydss)
+        ])
+    async def clean_background_tasks(self, app):
+        logger.info("cleanup_background_tasks")
         self.handler.event.set()
```

### Comparing `nrel_pydss-3.1.3/src/pydss/api/src/app/DataWriter.py` & `nrel_pydss-3.1.4/src/pydss/api/src/app/DataWriter.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-from pydss.api.src.app.HDF5 import hdf5Writer
-from pydss.api.src.app.JSON_writer import JSONwriter
-
-class DataWriter:
-    modes = {
-        'h5': hdf5Writer,
-        'json': JSONwriter,
-        # 'parquet': parquetWriter,
-        # "pickle": pickleWriter,
-    }
-
-    def __init__(self, log_dir, format, columnLength):
-        self.writer = self.modes[format](log_dir, columnLength)
-
-    def write(self, fed_name, currenttime, powerflow_output, index):
+from pydss.api.src.app.HDF5 import hdf5Writer
+from pydss.api.src.app.JSON_writer import JSONwriter
+
+class DataWriter:
+    modes = {
+        'h5': hdf5Writer,
+        'json': JSONwriter,
+        # 'parquet': parquetWriter,
+        # "pickle": pickleWriter,
+    }
+
+    def __init__(self, log_dir, format, columnLength):
+        self.writer = self.modes[format](log_dir, columnLength)
+
+    def write(self, fed_name, currenttime, powerflow_output, index):
         self.writer.write(fed_name, currenttime, powerflow_output, index)
```

### Comparing `nrel_pydss-3.1.3/src/pydss/api/src/app/JSON_writer.py` & `nrel_pydss-3.1.4/src/pydss/api/src/app/JSON_writer.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,127 +1,127 @@
-import json
-import os
-
-class JSONwriter:
-    def __init__(self, log_dir, columnLength=None):
-        self.log_dir = log_dir
-        self.metadata = False
-        self.payload = False
-        return
-
-    def parse_metadata(self, cName, LFresults, results):
-        for k, v in LFresults[cName].items():
-            data = k.split("__")
-            name = data[0]
-            if len(data) == 3:
-                ppty = f"{data[1]}__{data[2]}"
-            else:
-                ppty = data[1]
-            asset = self.check_asset(cName, None, results)
-            if not asset:
-                asset = {
-                    "assetType": cName,
-                    "keyColumns": "Name",
-                    "keyValues": [name],
-                    "measurementColumns": [
-                        {
-                          "name": ppty,
-                          "type": "numeric" if not isinstance(v, str) else "string",
-                          "mappings": {}
-                        }
-                    ]
-                }
-                results["assets"].append(asset)
-            else:
-                if name not in asset["keyValues"]:
-                    asset["keyValues"].append(name)
-                colExists = False
-                for cols in asset["measurementColumns"]:
-                    if cols["name"] == ppty:
-                        colExists = True
-                if not colExists:
-                    res = {
-                        "name": ppty,
-                        "type": "numeric" if not isinstance(v, str) else "string",
-                        "mappings": {},
-                    }
-                    asset["measurementColumns"].append(res)
-        return results
-
-    def create_meta_data(self, LFresults, fed_name, fed_uuid, cosim_uuid, circuit, currenttime):
-        results = {}
-        results["cosimUUID"] = cosim_uuid
-        results["federateUUID"] = fed_uuid
-        results["interconnect"] = "distribution"
-        results["assets"] = []
-        for key in LFresults:
-            results = self.parse_metadata(key, LFresults, results)
-        return results
-
-    def update_payload(self, timestep, fed_uuid, cosim_uuid):
-        if not self.payload:
-            self.payload = {
-                "cosimUUID": cosim_uuid,
-                "federateUUID": fed_uuid,
-                "timeSteps": [timestep]
-            }
-        else:
-            self.payload["timeSteps"].append(timestep)
-        return
-
-    def write(self, fed_name, currenttime, LFresults, index=None, circuit=None, fed_uuid=None, cosim_uuid=None):
-        jFile = open(os.path.join(self.log_dir, f"Results_{int(currenttime)}.json"), "w")
-        pFile = open(os.path.join(self.log_dir, f"payload.json"), "w")
-        if not self.metadata:
-            mFile = open(os.path.join(self.log_dir, f"metadata.json"), "w")
-            self.metadata = self.create_meta_data(LFresults, fed_name, fed_uuid, cosim_uuid, circuit, currenttime)
-            json.dump(self.metadata, mFile, indent=4, sort_keys=True)
-            mFile.close()
-        results = self.remap(LFresults, fed_name, fed_uuid, cosim_uuid, circuit, currenttime)
-        self.update_payload(currenttime, fed_uuid, cosim_uuid)
-        json.dump(results, jFile, indent=4, sort_keys=True)
-        json.dump(self.payload, pFile, indent=4, sort_keys=True)
-        jFile.close()
-        pFile.close()
-        return
-
-    def __del__(self):
-        return
-
-    def check_asset(self, className, ppty, results):
-        for asset in results["assets"]:
-            if ppty is not None:
-                if "assetType" in asset and asset["assetType"] == className and ppty in asset:
-                    return asset
-            else:
-                if "assetType" in asset and asset["assetType"] == className:
-                    return asset
-        return False
-
-    def parse_class(self, cName, LFresults, results):
-        for k, v in LFresults[cName].items():
-            data = k.split("__")
-            name = data[0]
-            if len(data) == 3:
-                ppty = f"{data[1]}__{data[2]}"
-            else:
-                ppty = data[1]
-            asset = self.check_asset(cName, ppty, results)
-            if not asset:
-                asset = {
-                    "assetType": cName,
-                    ppty: [v]
-                }
-                results["assets"].append(asset)
-            else:
-                asset[ppty].append(v)
-        return results
-
-    def remap(self, LFresults, fed_name, fed_uuid, cosim_uuid, circuit, currenttime):
-        results = {}
-        results["cosimUUID"] = cosim_uuid
-        results["federateUUID"] = fed_uuid
-        results["timeStep"] = currenttime
-        results["assets"] = []
-        for key in LFresults:
-            results = self.parse_class(key, LFresults, results)
+import json
+import os
+
+class JSONwriter:
+    def __init__(self, log_dir, columnLength=None):
+        self.log_dir = log_dir
+        self.metadata = False
+        self.payload = False
+        return
+
+    def parse_metadata(self, cName, LFresults, results):
+        for k, v in LFresults[cName].items():
+            data = k.split("__")
+            name = data[0]
+            if len(data) == 3:
+                ppty = f"{data[1]}__{data[2]}"
+            else:
+                ppty = data[1]
+            asset = self.check_asset(cName, None, results)
+            if not asset:
+                asset = {
+                    "assetType": cName,
+                    "keyColumns": "Name",
+                    "keyValues": [name],
+                    "measurementColumns": [
+                        {
+                          "name": ppty,
+                          "type": "numeric" if not isinstance(v, str) else "string",
+                          "mappings": {}
+                        }
+                    ]
+                }
+                results["assets"].append(asset)
+            else:
+                if name not in asset["keyValues"]:
+                    asset["keyValues"].append(name)
+                colExists = False
+                for cols in asset["measurementColumns"]:
+                    if cols["name"] == ppty:
+                        colExists = True
+                if not colExists:
+                    res = {
+                        "name": ppty,
+                        "type": "numeric" if not isinstance(v, str) else "string",
+                        "mappings": {},
+                    }
+                    asset["measurementColumns"].append(res)
+        return results
+
+    def create_meta_data(self, LFresults, fed_name, fed_uuid, cosim_uuid, circuit, currenttime):
+        results = {}
+        results["cosimUUID"] = cosim_uuid
+        results["federateUUID"] = fed_uuid
+        results["interconnect"] = "distribution"
+        results["assets"] = []
+        for key in LFresults:
+            results = self.parse_metadata(key, LFresults, results)
+        return results
+
+    def update_payload(self, timestep, fed_uuid, cosim_uuid):
+        if not self.payload:
+            self.payload = {
+                "cosimUUID": cosim_uuid,
+                "federateUUID": fed_uuid,
+                "timeSteps": [timestep]
+            }
+        else:
+            self.payload["timeSteps"].append(timestep)
+        return
+
+    def write(self, fed_name, currenttime, LFresults, index=None, circuit=None, fed_uuid=None, cosim_uuid=None):
+        jFile = open(os.path.join(self.log_dir, f"Results_{int(currenttime)}.json"), "w")
+        pFile = open(os.path.join(self.log_dir, f"payload.json"), "w")
+        if not self.metadata:
+            mFile = open(os.path.join(self.log_dir, f"metadata.json"), "w")
+            self.metadata = self.create_meta_data(LFresults, fed_name, fed_uuid, cosim_uuid, circuit, currenttime)
+            json.dump(self.metadata, mFile, indent=4, sort_keys=True)
+            mFile.close()
+        results = self.remap(LFresults, fed_name, fed_uuid, cosim_uuid, circuit, currenttime)
+        self.update_payload(currenttime, fed_uuid, cosim_uuid)
+        json.dump(results, jFile, indent=4, sort_keys=True)
+        json.dump(self.payload, pFile, indent=4, sort_keys=True)
+        jFile.close()
+        pFile.close()
+        return
+
+    def __del__(self):
+        return
+
+    def check_asset(self, className, ppty, results):
+        for asset in results["assets"]:
+            if ppty is not None:
+                if "assetType" in asset and asset["assetType"] == className and ppty in asset:
+                    return asset
+            else:
+                if "assetType" in asset and asset["assetType"] == className:
+                    return asset
+        return False
+
+    def parse_class(self, cName, LFresults, results):
+        for k, v in LFresults[cName].items():
+            data = k.split("__")
+            name = data[0]
+            if len(data) == 3:
+                ppty = f"{data[1]}__{data[2]}"
+            else:
+                ppty = data[1]
+            asset = self.check_asset(cName, ppty, results)
+            if not asset:
+                asset = {
+                    "assetType": cName,
+                    ppty: [v]
+                }
+                results["assets"].append(asset)
+            else:
+                asset[ppty].append(v)
+        return results
+
+    def remap(self, LFresults, fed_name, fed_uuid, cosim_uuid, circuit, currenttime):
+        results = {}
+        results["cosimUUID"] = cosim_uuid
+        results["federateUUID"] = fed_uuid
+        results["timeStep"] = currenttime
+        results["assets"] = []
+        for key in LFresults:
+            results = self.parse_class(key, LFresults, results)
         return results
```

### Comparing `nrel_pydss-3.1.3/src/pydss/api/src/app/Tester.py` & `nrel_pydss-3.1.4/src/pydss/api/src/app/Tester.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,46 +1,46 @@
-
-from pathlib import Path
-import time
-import os
-
-from loguru import logger
-
-from pydss.simulation_input_models import load_simulation_settings
-from pydss.api.src.app.DataWriter import DataWriter
-from pydss.dssInstance import OpenDSS
-
-
-def run_test(tomlpath):
-    try:
-        settings = load_simulation_settings(Path(tomlpath))
-    except Exception as e:
-        logger.error(f"Invalid simulation settings passed, {e}")
-        return
-
-    pydss_obj = OpenDSS(settings)
-    export_path = os.path.join(pydss_obj._dssPath['Export'], settings.project.active_scenario)
-    Steps, sTime, eTime = pydss_obj._dssSolver.SimulationSteps()
-    writer = DataWriter(export_path, format="json", columnLength=Steps)
-
-    st = time.time()
-    for i in range(Steps):
-        results = pydss_obj.RunStep(i)
-        restructured_results = {}
-        for k, val in results.items():
-            if "." not in k:
-                class_name = "Bus"
-                elem_name = k
-            else:
-                class_name, elem_name = k.split(".")
-
-            if class_name not in restructured_results:
-                restructured_results[class_name] = {}
-            if not isinstance(val, complex):
-                restructured_results[class_name][elem_name] = val
-        writer.write(
-            pydss_obj._Options["Helics"]["Federate name"],
-            pydss_obj._dssSolver.GetTotalSeconds(),
-            restructured_results,
-            i
-        )
-    logger.debug("{} seconds".format(time.time() - st))
+
+from pathlib import Path
+import time
+import os
+
+from loguru import logger
+
+from pydss.simulation_input_models import load_simulation_settings
+from pydss.api.src.app.DataWriter import DataWriter
+from pydss.dssInstance import OpenDSS
+
+
+def run_test(tomlpath):
+    try:
+        settings = load_simulation_settings(Path(tomlpath))
+    except Exception as e:
+        logger.error(f"Invalid simulation settings passed, {e}")
+        return
+
+    pydss_obj = OpenDSS(settings)
+    export_path = os.path.join(pydss_obj._dssPath['Export'], settings.project.active_scenario)
+    Steps, sTime, eTime = pydss_obj._dssSolver.SimulationSteps()
+    writer = DataWriter(export_path, format="json", columnLength=Steps)
+
+    st = time.time()
+    for i in range(Steps):
+        results = pydss_obj.RunStep(i)
+        restructured_results = {}
+        for k, val in results.items():
+            if "." not in k:
+                class_name = "Bus"
+                elem_name = k
+            else:
+                class_name, elem_name = k.split(".")
+
+            if class_name not in restructured_results:
+                restructured_results[class_name] = {}
+            if not isinstance(val, complex):
+                restructured_results[class_name][elem_name] = val
+        writer.write(
+            pydss_obj._Options["Helics"]["Federate name"],
+            pydss_obj._dssSolver.GetTotalSeconds(),
+            restructured_results,
+            i
+        )
+    logger.debug("{} seconds".format(time.time() - st))
```

### Comparing `nrel_pydss-3.1.3/src/pydss/api/src/app/pydss.py` & `nrel_pydss-3.1.4/src/pydss/api/src/app/pydss.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,132 +1,132 @@
-from multiprocessing import current_process
-from queue import Empty
-import os
-
-from loguru import logger
-
-from pydss.simulation_input_models import SimulationSettingsModel
-from pydss.api.src.app.JSON_writer import JSONwriter
-from pydss.dssInstance import OpenDSS
-
-
-class PyDSS:
-    commands = {
-        "run": None
-    }
-
-    def __init__(self, event=None, queue=None, parameters=None):
-
-        self.initalized = False
-        self.uuid = current_process().name
-
-        ''' TODO: work on logging.yaml file'''
-
-        logger.info("{} - initialized ".format({self.uuid}))
-
-        self.shutdownevent = event
-        self.queue = queue
-
-        try:
-            settings = SimulationSettingsModel(**parameters['parameters'])
-            self.pydss_obj = OpenDSS(settings)
-            export_path = os.path.join(self.pydss_obj._dssPath['Export'], settings.project.active_scenario)
-            Steps, sTime, eTime = self.pydss_obj._dssSolver.SimulationSteps()
-            self.a_writer = JSONwriter(export_path, Steps)
-            self.initalized = True
-        except:
-            result = {"Status": 500, "Message": f"Failed to create a pydss instance"}
-            self.queue.put(result)
-            return
-
-        # self.RunSimulation()
-        logger.info("{} - pydss dispatched".format(self.uuid))
-
-        result = {
-            "Status": 200,
-            "Message": "Pydss {} successfully initialized.".format(self.uuid),
-            "UUID": self.uuid
-        }
-
-        if self.queue != None: self.queue.put(result)
-
-        self.run_process()
-
-    def run_process(self):
-        logger.info("Pydss simulation starting")
-        while not self.shutdownevent.is_set():
-            try:
-                task = self.queue.get()
-                if task == 'END':
-                    break
-                elif "parameters" not in task:
-                    result = {
-                        "Status": 500,
-                        "Message": "No parameters passed"
-                    }
-                else:
-                    command = task["command"]
-                    parameters = task["parameters"]
-                    if hasattr(self, command):
-                        func = getattr(self, command)
-                        status, msg = func(parameters)
-                        result = {"Status": status, "Message": msg, "UUID": self.uuid}
-                    else:
-                        logger.info(f"{command} is not a valid pydss command")
-                        result = {"Status": 500, "Message": f"{command} is not a valid pydss command"}
-                self.queue.put(result)
-
-            except Empty:
-                continue
-
-            except (KeyboardInterrupt, SystemExit):
-                break
-        logger.info(f"Pydss subprocess {self.uuid} has ended")
-
-    def close_instance(self):
-        del self.pydss_obj
-        logger.info(f'Pydss case {self.uuid} closed.')
-
-    def run(self, params):
-        if self.initalized:
-            try:
-                Steps, sTime, eTime = self.pydss_obj._dssSolver.SimulationSteps()
-                for i in range(Steps):
-                    results = self.pydss_obj.RunStep(i)
-                    restructured_results = {}
-                    for k, val in results.items():
-                        if "." not in k:
-                            class_name = "Bus"
-                            elem_name = k
-                        else:
-                            class_name, elem_name = k.split(".")
-                        if class_name not in restructured_results:
-                            restructured_results[class_name] = {}
-                        if not isinstance(val, complex):
-                            restructured_results[class_name][elem_name] = val
-                    self.a_writer.write(
-                        self.pydss_obj._Options["Helics"]["Federate name"],
-                        self.pydss_obj._dssSolver.GetTotalSeconds(),
-                        restructured_results,
-                        i
-                    )
-
-                self.initalized = False
-                return 200, f"Simulation complete..."
-            except Exception as e:
-                self.initalized = False
-                return 500, f"Simulation crashed at at simulation time step: {self.pydss_obj._dssSolver.GetDateTime()}, {e}"
-        else:
-            return 500, f"No project initialized. Load a project first using the 'init' command"
-
-    def registerPubSubs(self, params):
-        subs = params["Subscriptions"]
-        pubs = params["Publications"]
-        self.pydss_obj._HI.registerPubSubTags(pubs, subs)
-        return 200, f"Publications and subscriptions have been registered; Federate has entered execution mode"
-
-
-if __name__ == '__main__':
-    FORMAT = '%(asctime)s - %(levelname)s - %(message)s'
-    logger.basicConfig(level=logger.INFO, format=FORMAT)
-    a = PyDSS()
-    del a
+from multiprocessing import current_process
+from queue import Empty
+import os
+
+from loguru import logger
+
+from pydss.simulation_input_models import SimulationSettingsModel
+from pydss.api.src.app.JSON_writer import JSONwriter
+from pydss.dssInstance import OpenDSS
+
+
+class PyDSS:
+    commands = {
+        "run": None
+    }
+
+    def __init__(self, event=None, queue=None, parameters=None):
+
+        self.initalized = False
+        self.uuid = current_process().name
+
+        ''' TODO: work on logging.yaml file'''
+
+        logger.info("{} - initialized ".format({self.uuid}))
+
+        self.shutdownevent = event
+        self.queue = queue
+
+        try:
+            settings = SimulationSettingsModel(**parameters['parameters'])
+            self.pydss_obj = OpenDSS(settings)
+            export_path = os.path.join(self.pydss_obj._dssPath['Export'], settings.project.active_scenario)
+            Steps, sTime, eTime = self.pydss_obj._dssSolver.SimulationSteps()
+            self.a_writer = JSONwriter(export_path, Steps)
+            self.initalized = True
+        except:
+            result = {"Status": 500, "Message": f"Failed to create a pydss instance"}
+            self.queue.put(result)
+            return
+
+        # self.RunSimulation()
+        logger.info("{} - pydss dispatched".format(self.uuid))
+
+        result = {
+            "Status": 200,
+            "Message": "Pydss {} successfully initialized.".format(self.uuid),
+            "UUID": self.uuid
+        }
+
+        if self.queue != None: self.queue.put(result)
+
+        self.run_process()
+
+    def run_process(self):
+        logger.info("Pydss simulation starting")
+        while not self.shutdownevent.is_set():
+            try:
+                task = self.queue.get()
+                if task == 'END':
+                    break
+                elif "parameters" not in task:
+                    result = {
+                        "Status": 500,
+                        "Message": "No parameters passed"
+                    }
+                else:
+                    command = task["command"]
+                    parameters = task["parameters"]
+                    if hasattr(self, command):
+                        func = getattr(self, command)
+                        status, msg = func(parameters)
+                        result = {"Status": status, "Message": msg, "UUID": self.uuid}
+                    else:
+                        logger.info(f"{command} is not a valid pydss command")
+                        result = {"Status": 500, "Message": f"{command} is not a valid pydss command"}
+                self.queue.put(result)
+
+            except Empty:
+                continue
+
+            except (KeyboardInterrupt, SystemExit):
+                break
+        logger.info(f"Pydss subprocess {self.uuid} has ended")
+
+    def close_instance(self):
+        del self.pydss_obj
+        logger.info(f'Pydss case {self.uuid} closed.')
+
+    def run(self, params):
+        if self.initalized:
+            try:
+                Steps, sTime, eTime = self.pydss_obj._dssSolver.SimulationSteps()
+                for i in range(Steps):
+                    results = self.pydss_obj.RunStep(i)
+                    restructured_results = {}
+                    for k, val in results.items():
+                        if "." not in k:
+                            class_name = "Bus"
+                            elem_name = k
+                        else:
+                            class_name, elem_name = k.split(".")
+                        if class_name not in restructured_results:
+                            restructured_results[class_name] = {}
+                        if not isinstance(val, complex):
+                            restructured_results[class_name][elem_name] = val
+                    self.a_writer.write(
+                        self.pydss_obj._Options["Helics"]["Federate name"],
+                        self.pydss_obj._dssSolver.GetTotalSeconds(),
+                        restructured_results,
+                        i
+                    )
+
+                self.initalized = False
+                return 200, f"Simulation complete..."
+            except Exception as e:
+                self.initalized = False
+                return 500, f"Simulation crashed at at simulation time step: {self.pydss_obj._dssSolver.GetDateTime()}, {e}"
+        else:
+            return 500, f"No project initialized. Load a project first using the 'init' command"
+
+    def registerPubSubs(self, params):
+        subs = params["Subscriptions"]
+        pubs = params["Publications"]
+        self.pydss_obj._HI.registerPubSubTags(pubs, subs)
+        return 200, f"Publications and subscriptions have been registered; Federate has entered execution mode"
+
+
+if __name__ == '__main__':
+    FORMAT = '%(asctime)s - %(levelname)s - %(message)s'
+    logger.basicConfig(level=logger.INFO, format=FORMAT)
+    a = PyDSS()
+    del a
```

### Comparing `nrel_pydss-3.1.3/src/pydss/api/src/web/parser.py` & `nrel_pydss-3.1.4/src/pydss/api/src/web/parser.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,110 +1,110 @@
-"""
-Takes GET/POST variable dictionary, as might be returned by ``cgi``,
-and turns them into lists and dictionaries.
-Keys (variable names) can have subkeys, with a ``.`` and
-can be numbered with ``-``, like ``a.b-3=something`` means that
-the value ``a`` is a dictionary with a key ``b``, and ``b``
-is a list, the third(-ish) element with the value ``something``.
-Numbers are used to sort, missing numbers are ignored.
-This doesn't deal with multiple keys, like in a query string of
-``id=10&id=20``, which returns something like ``{'id': ['10',
-'20']}``.  That's left to someplace else to interpret.  If you want to
-represent lists in this model, you use indexes, and the lists are
-explicitly ordered.
-If you want to change the character that determines when to split for
-a dict or list, both variable_decode and variable_encode take dict_char
-and list_char keyword args. For example, to have the GET/POST variables,
-``a_1=something`` as a list, you would use a ``list_char='_'``.
-"""
-
-import pydss.defaults as defaultSettings
-from pydss.utils.utils import load_data
-import ast
-import os
-
-defaultSettings.__file__
-__all__ = ['variable_decode']
-
-master_dict = {
-    "Project" : ["Start Year", "Start Day", "Start Time (min)", "End Day", "End Time (min)", "Date offset",
-                 "Step resolution (sec)", "Max Control Iterations", "Error tolerance", "Control mode",
-                 "Disable pydss controllers", "Simulation Type", "Project Path", "Active Project", "Active Scenario",
-                 "DSS File", "DSS File Absolute Path", "Return Results"],
-    "Exports" : ["Export Mode", "Export Style", "Export Format", "Export Compression", "Export Iteration Order",
-                 "Export Elements", "Export Data Tables", "Export Data In Memory", "HDF Max Chunk Bytes",
-                 "Export Event Log", "Log Results"],
-    "Frequency" : ["Enable frequency sweep", "Fundamental frequency", "Start frequency", "End frequency",
-                   "frequency increment", "Neglect shunt admittance", "Percentage load in series"],
-    "Helics" : ["Co-simulation Mode", "Federate name", "Time delta", "Core type", "Uninterruptible",
-                "Helics logging level", "Iterative Mode", 'Max co-iterations', "Error tolerance", 'Broker',
-                "Broker port"],
-    "Logging" : ["Logging Level", "Log to external file", "Display on screen", "Clear old log file"],
-    "MonteCarlo" : ["Number of Monte Carlo scenarios"],
-    "Plots" : ["Create dynamic plots", "Open plots in browser"],
-}
-
-def bytestream_decode(s):
-    lines = s.splitlines()
-    filter = b'Content-Disposition: form-data; name='
-    result = {}
-    for i, l in enumerate(lines):
-        if l.startswith(filter):
-            l = l.replace(filter, b"").replace(b'"', b'').decode('ascii')
-            if '; filename=' in l:
-                data = l.replace("; filename=", ":").split(":")
-                result[data[0]] = data[1]
-            else:
-                val = lines[i + 2].decode('ascii')
-                result[l] = val
-    return result
-
-# TODO: It does not appear that this function is used. Can it be deleted?
-# If not, it needs to be updated.
-def restructure_dictionary(d):
-    global master_dict
-    pydss_settings = {}
-    path = defaultSettings.__path__._path[0]
-    complete_path = os.path.join(path, "simulation.toml")
-    defaults = load_data(complete_path)
-
-    for k, i in d.items():
-        for key, valid_entries in master_dict.items():
-            if k in valid_entries:
-                if key not in pydss_settings:
-                    pydss_settings[key] = {}
-                if i == "False" or i == "false" or i == False:
-                    i = False
-                elif i == "True" or i == "true" or i == True:
-                    i = True
-                else:
-                    try:
-                        i = int(i)
-                    except:
-                        try:
-                            i = float(i)
-                        except:
-                            pass
-                pydss_settings[key][k] = i
-
-    for key, valid_entries in master_dict.items():
-        if key in pydss_settings:
-            defaults[key].update(pydss_settings[key])
-
-    defaults["Project"]["Scenarios"] =[
-        {
-            "name": d["Active Scenario"],
-            "post_process_infos": []
-        }
-    ]
-
-    return defaults
-
-
-def variable_decode(d, dict_char='.', list_char='-'):
-    """
-    Decode the flat dictionary d into a nested structure.
-    """
-    result = {}
-    for key, value in d.items():
-        result[key] = value
-    return result
+"""
+Takes GET/POST variable dictionary, as might be returned by ``cgi``,
+and turns them into lists and dictionaries.
+Keys (variable names) can have subkeys, with a ``.`` and
+can be numbered with ``-``, like ``a.b-3=something`` means that
+the value ``a`` is a dictionary with a key ``b``, and ``b``
+is a list, the third(-ish) element with the value ``something``.
+Numbers are used to sort, missing numbers are ignored.
+This doesn't deal with multiple keys, like in a query string of
+``id=10&id=20``, which returns something like ``{'id': ['10',
+'20']}``.  That's left to someplace else to interpret.  If you want to
+represent lists in this model, you use indexes, and the lists are
+explicitly ordered.
+If you want to change the character that determines when to split for
+a dict or list, both variable_decode and variable_encode take dict_char
+and list_char keyword args. For example, to have the GET/POST variables,
+``a_1=something`` as a list, you would use a ``list_char='_'``.
+"""
+
+import pydss.defaults as defaultSettings
+from pydss.utils.utils import load_data
+import ast
+import os
+
+defaultSettings.__file__
+__all__ = ['variable_decode']
+
+master_dict = {
+    "Project" : ["Start Year", "Start Day", "Start Time (min)", "End Day", "End Time (min)", "Date offset",
+                 "Step resolution (sec)", "Max Control Iterations", "Error tolerance", "Control mode",
+                 "Disable pydss controllers", "Simulation Type", "Project Path", "Active Project", "Active Scenario",
+                 "DSS File", "DSS File Absolute Path", "Return Results"],
+    "Exports" : ["Export Mode", "Export Style", "Export Format", "Export Compression", "Export Iteration Order",
+                 "Export Elements", "Export Data Tables", "Export Data In Memory", "HDF Max Chunk Bytes",
+                 "Export Event Log", "Log Results"],
+    "Frequency" : ["Enable frequency sweep", "Fundamental frequency", "Start frequency", "End frequency",
+                   "frequency increment", "Neglect shunt admittance", "Percentage load in series"],
+    "Helics" : ["Co-simulation Mode", "Federate name", "Time delta", "Core type", "Uninterruptible",
+                "Helics logging level", "Iterative Mode", 'Max co-iterations', "Error tolerance", 'Broker',
+                "Broker port"],
+    "Logging" : ["Logging Level", "Log to external file", "Display on screen", "Clear old log file"],
+    "MonteCarlo" : ["Number of Monte Carlo scenarios"],
+    "Plots" : ["Create dynamic plots", "Open plots in browser"],
+}
+
+def bytestream_decode(s):
+    lines = s.splitlines()
+    filter = b'Content-Disposition: form-data; name='
+    result = {}
+    for i, l in enumerate(lines):
+        if l.startswith(filter):
+            l = l.replace(filter, b"").replace(b'"', b'').decode('ascii')
+            if '; filename=' in l:
+                data = l.replace("; filename=", ":").split(":")
+                result[data[0]] = data[1]
+            else:
+                val = lines[i + 2].decode('ascii')
+                result[l] = val
+    return result
+
+# TODO: It does not appear that this function is used. Can it be deleted?
+# If not, it needs to be updated.
+def restructure_dictionary(d):
+    global master_dict
+    pydss_settings = {}
+    path = defaultSettings.__path__._path[0]
+    complete_path = os.path.join(path, "simulation.toml")
+    defaults = load_data(complete_path)
+
+    for k, i in d.items():
+        for key, valid_entries in master_dict.items():
+            if k in valid_entries:
+                if key not in pydss_settings:
+                    pydss_settings[key] = {}
+                if i == "False" or i == "false" or i == False:
+                    i = False
+                elif i == "True" or i == "true" or i == True:
+                    i = True
+                else:
+                    try:
+                        i = int(i)
+                    except:
+                        try:
+                            i = float(i)
+                        except:
+                            pass
+                pydss_settings[key][k] = i
+
+    for key, valid_entries in master_dict.items():
+        if key in pydss_settings:
+            defaults[key].update(pydss_settings[key])
+
+    defaults["Project"]["Scenarios"] =[
+        {
+            "name": d["Active Scenario"],
+            "post_process_infos": []
+        }
+    ]
+
+    return defaults
+
+
+def variable_decode(d, dict_char='.', list_char='-'):
+    """
+    Decode the flat dictionary d into a nested structure.
+    """
+    result = {}
+    for key, value in d.items():
+        result[key] = value
+    return result
```

### Comparing `nrel_pydss-3.1.3/src/pydss/apps/data_viewer.py` & `nrel_pydss-3.1.4/src/pydss/apps/data_viewer.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,404 +1,404 @@
-import ast
-import copy
-import os
-import re
-import sys
-from collections import defaultdict
-
-import ipywidgets as widgets
-import pandas as pd
-from IPython.display import display
-
-try:
-    import plotly
-except ImportError:
-    print("plotly is required to run the DataViewer. Please run 'pip install plotly'")
-    sys.exit(1)
-
-
-from pydss.pydss_results import PyDssResults
-
-
-class DataViewer:
-    """Provides a UI for viewing pydss results."""
-
-    DEFAULTS = {
-        "project_path": os.environ.get("PYDSS_PROJECT_PATH"),
-    }
-
-    def __init__(self, **kwargs):
-        pd.options.plotting.backend = "plotly"
-        self._results = None
-        self._project_path = None
-        self._scenario = None
-        self._scenario_names = [""]
-        self._element_classes = [""]
-        self._element_props = [""]
-        self._timestamps = None
-        self._df = None
-        self._defaults = copy.deepcopy(self.DEFAULTS)
-        if kwargs:
-            self._defaults.update(kwargs)
-        self._make_widgets()
-        self._display_widgets()
-        if self._project_path_text.value != "":
-            self._on_load_project_click(None)
-        self._first_plot = True
-
-    @property
-    def results(self):
-        """Return the current PyDssResults object."""
-        return self._results
-
-    @property
-    def scenarios(self):
-        """Return the current PyDssScenarioResults objects."""
-        return self._results.scenarios
-
-    @property
-    def df(self):
-        """Return the current DataFrame."""
-        return self._df
-
-    def _make_widgets(self):
-        self._main_label = widgets.HTML("<b>Pydss Data Viewer</b>")
-        text_layout_long = widgets.Layout(width="400px")
-        text_layout_med = widgets.Layout(width="200px")
-        text_layout_short = widgets.Layout(width="100px")
-        button_layout = widgets.Layout(width="200px")
-        path = "" if self._defaults["project_path"] is None else self._defaults["project_path"]
-        self._project_path_text = widgets.Text(
-            path,
-            description="Project Path",
-            layout=text_layout_long,
-            placeholder="pydss_projects/project1/",
-        )
-        self._load_project_btn = widgets.Button(description="Load project", layout=button_layout)
-        self._load_project_btn.on_click(self._on_load_project_click)
-        self._scenario_label = widgets.HTML("Scenario", layout=text_layout_med)
-        self._elem_class_label = widgets.HTML("Element Class", layout=text_layout_med)
-        self._elem_prop_label = widgets.HTML("Element Property", layout=text_layout_med)
-        self._prop_available_options_label = widgets.HTML("Options")
-        self._prop_options_label = widgets.HTML("Element Property Options", layout=text_layout_med)
-        self._num_elems_label = widgets.HTML("Number of Elements", layout=text_layout_med)
-        self._elem_regex_label = widgets.HTML("Element Name Regex", layout=text_layout_med)
-        self._start_time_label = widgets.HTML("Start Time", layout=text_layout_med)
-        self._end_time_label = widgets.HTML("End Time", layout=text_layout_med)
-        self._resolution_label = widgets.HTML("Resolution", layout=text_layout_med)
-        self._moving_avg_cb = widgets.Checkbox(
-            value=False, description="Moving Average", disabled=True
-        )
-        self._window_label = widgets.HTML("Moving Average Window", layout=text_layout_med)
-        self._window_text = widgets.Text(value="4")
-        self._scenario_dd = widgets.Dropdown(
-            options=self._scenario_names,
-            value=self._scenario_names[0],
-            disabled=True,
-        )
-        self._scenario_dd.observe(self._on_scenario_change, names="value")
-        self._elem_class_dd = widgets.Dropdown(
-            options=self._element_classes,
-            value=self._element_classes[0],
-            disabled=True,
-        )
-        self._elem_class_dd.observe(self._on_elem_class_change, names="value")
-        self._elem_prop_dd = widgets.Dropdown(
-            options=self._element_props,
-            value=self._element_props[0],
-            disabled=True,
-        )
-        self._elem_prop_dd.observe(self._on_elem_prop_change, names="value")
-        self._prop_available_options_text = widgets.Text(disabled=True)
-        self._prop_options_text = widgets.Text(disabled=True)
-        self._num_elems_text = widgets.Text(disabled=True)
-        self._elem_regex_text = widgets.Text(disabled=True, placeholder="p1udt.*")
-        self._elem_names_select = widgets.SelectMultiple(
-            options=[""],
-            value=[""],
-            rows=5,
-            description="Elements",
-            disabled=True,
-        )
-        self._start_time_text = widgets.Text(disabled=True)
-        self._end_time_text = widgets.Text(disabled=True)
-        self._resolution_text = widgets.Text(disabled=True)
-        self._load_dataframe_btn = widgets.Button(
-            description="Load DataFrame",
-            layout=button_layout,
-            disabled=True,
-            tooltip="Load DataFrame into `app.df`",
-        )
-        self._load_dataframe_btn.on_click(self._on_load_dataframe_click)
-        self._plot_btn = widgets.Button(description="Plot", layout=button_layout)
-        self._plot_btn.on_click(self._on_plot_click)
-        self._aggregation_label = widgets.HTML("Aggregation", layout=text_layout_short)
-        self._aggregation_dd = widgets.Dropdown(
-            options=["None", "Min/Mean/Max", "Sum"],
-            value="None",
-            disabled=True,
-        )
-        self._real_only_cb = widgets.Checkbox(
-            value=True, description="Exclude imaginary parts", disabled=True
-        )
-        self._reset_btn = widgets.Button(description="Reset", layout=button_layout)
-        self._reset_btn.on_click(self._on_reset_click)
-        self._plot_output = widgets.Output()
-
-    def _display_widgets(self):
-        box = widgets.VBox(
-            (
-                self._main_label,
-                widgets.HBox((self._load_project_btn, self._project_path_text)),
-                widgets.HBox((self._scenario_label, self._scenario_dd)),
-                widgets.HBox((self._elem_class_label, self._elem_class_dd)),
-                widgets.HBox(
-                    (
-                        self._elem_prop_label,
-                        self._elem_prop_dd,
-                        self._prop_available_options_label,
-                        self._prop_available_options_text,
-                    )
-                ),
-                widgets.HBox((self._prop_options_label, self._prop_options_text)),
-                widgets.HBox(
-                    (
-                        widgets.VBox(
-                            (
-                                widgets.HBox((self._num_elems_label, self._num_elems_text)),
-                                widgets.HBox((self._elem_regex_label, self._elem_regex_text)),
-                            )
-                        ),
-                        self._elem_names_select,
-                    )
-                ),
-                widgets.HBox(
-                    (
-                        widgets.VBox(
-                            (
-                                widgets.HBox((self._start_time_label, self._start_time_text)),
-                                widgets.HBox((self._end_time_label, self._end_time_text)),
-                                widgets.HBox((self._resolution_label, self._resolution_text)),
-                            ),
-                        ),
-                        widgets.VBox(
-                            (
-                                self._moving_avg_cb,
-                                widgets.HBox((self._window_label, self._window_text)),
-                            )
-                        ),
-                    )
-                ),
-                widgets.HBox(
-                    (
-                        self._plot_btn,
-                        self._load_dataframe_btn,
-                        self._aggregation_label,
-                        self._aggregation_dd,
-                        self._real_only_cb,
-                    )
-                ),
-                self._reset_btn,
-            )
-        )
-        display(box)
-
-    def _enable_project_actions(self):
-        self._elem_class_dd.disabled = False
-        self._elem_prop_dd.disabled = False
-        self._prop_options_text.disabled = False
-        self._load_dataframe_btn.disabled = False
-        self._plot_btn.disabled = False
-        self._scenario_dd.disabled = False
-        self._aggregation_dd.disabled = False
-        self._real_only_cb.disabled = False
-        self._elem_regex_text.disabled = False
-        self._elem_names_select.disabled = False
-        self._start_time_text.disabled = False
-        self._end_time_text.disabled = False
-        self._moving_avg_cb.disabled = False
-        self._assign_widgets()
-
-    def _assign_widgets(self):
-        self._scenario_names[:] = [x.name for x in self._results.scenarios]
-        self._scenario_dd.options = self._scenario_names
-        self._scenario_dd.value = self._scenario_names[0]
-        self._on_elem_class_change(None)
-        self._on_elem_prop_change(None)
-
-    def _on_scenario_change(self, _):
-        self._scenario = [x for x in self._results.scenarios if x.name == self._scenario_dd.value][
-            0
-        ]
-        self._element_classes[:] = self._scenario.list_element_classes()
-        self._elem_class_dd.options = self._element_classes
-        self._elem_class_dd.value = self._element_classes[0]
-
-    def _on_elem_class_change(self, _):
-        elem_class = self._elem_class_dd.value
-        self._element_props[:] = self._scenario.list_element_properties(elem_class)
-        self._elem_prop_dd.options = self._element_props
-        if self._element_props:
-            self._elem_prop_dd.value = self._element_props[0]
-        elem_names = self._scenario.list_element_names(elem_class)
-        elem_names.insert(0, "All")
-        self._elem_names_select.options = elem_names
-        self._elem_names_select.value = [elem_names[0]]
-        self._num_elems_text.value = str(len(elem_names))
-
-    def _on_elem_prop_change(self, _):
-        elem_class = self._elem_class_dd.value
-        elem_prop = self._elem_prop_dd.value
-        options = self._scenario.list_element_property_options(elem_class, elem_prop)
-        self._prop_available_options_text.value = ", ".join(options)
-        self._prop_options_text.value = ""
-        self._prop_options_text.placeholder = ", ".join(f"{x}='X'" for x in options)
-        self._elem_regex_text.value = ""
-
-    def _on_load_project_click(self, _):
-        path = self._project_path_text.value
-        if path == "":
-            print("Project Path cannot be empty.", file=sys.stderr)
-            return
-
-        self._results = PyDssResults(path)
-        self._timestamps = self._results.scenarios[0].get_timestamps()
-        self._start_time_text.value = str(self._timestamps.iloc[0])
-        self._end_time_text.value = str(self._timestamps.iloc[-1])
-        self._resolution_text.value = str(self._timestamps.iloc[1] - self._timestamps.iloc[0])
-        self._enable_project_actions()
-
-    def _filter_dataframe_by_time(self):
-        filter_required = False
-        start_time = pd.Timestamp(self._start_time_text.value)
-        end_time = pd.Timestamp(self._end_time_text.value)
-        p_start = self._timestamps.iloc[0]
-        p_end = self._timestamps.iloc[-1]
-        if start_time < p_start or start_time > p_end:
-            print(
-                f"Error: start_time={start_time} must be between {p_start} and {p_end}",
-                file=sys.stderr,
-            )
-            raise BadInputError("invalid start time")
-        if start_time != p_start:
-            filter_required = True
-        if end_time > p_end or end_time < p_start:
-            print(
-                f"Error: end_time={end_time} must be between {p_start} and {p_end}",
-                file=sys.stderr,
-            )
-            raise BadInputError("invalid end time")
-        if end_time != p_end:
-            filter_required = True
-
-        if filter_required:
-            self._df = self._df.loc[start_time:end_time, :]
-
-    def _filter_dataframe_by_elem_names(self):
-        value = self._elem_names_select.value
-        if value != ("All",):
-            values = set(value)
-            columns = [
-                x for x in self._df.columns if self._scenario.get_name_from_column(x) in values
-            ]
-            self._df = self._df[columns]
-
-    def _filter_dataframe_by_elem_regex(self):
-        regex_str = self._elem_regex_text.value
-        if regex_str != "":
-            regex = re.compile(rf"{regex_str}")
-            columns = [x for x in self._df.columns if regex.search(x) is not None]
-            self._df = self._df[columns]
-
-    def _get_elem_property_options(self):
-        options = {}
-        options_text = self._prop_options_text.value.strip()
-        if options_text != "":
-            for option_pair in options_text.split(","):
-                fields = option_pair.split("=")
-                if len(fields) != 2:
-                    print(
-                        f"Invalid option pair: '{option_pair}'. Must be 'option=value'",
-                        file=sys.stderr,
-                    )
-                    return
-                key = fields[0].strip()
-                val = fields[1].strip()
-                options[key] = ast.literal_eval(val)
-        return options
-
-    def _assign_dataframe(self):
-        elem_class = self._elem_class_dd.value
-        elem_prop = self._elem_prop_dd.value
-        real_only = self._real_only_cb.value
-
-        self._df = self._scenario.get_full_dataframe(
-            elem_class,
-            elem_prop,
-            real_only=real_only,
-            **self._get_elem_property_options(),
-        )
-        try:
-            self._filter_dataframe_by_time()
-        except BadInputError:
-            return
-
-        self._filter_dataframe_by_elem_names()
-        self._filter_dataframe_by_elem_regex()
-
-        if self._aggregation_dd.value == "Min/Mean/Max":
-            columns = list(self._df.columns)
-            self._df["Max"] = self._df.max(axis=1)
-            self._df["Mean"] = self._df.mean(axis=1)
-            self._df["Min"] = self._df.min(axis=1)
-            self._df.drop(columns=columns, inplace=True)
-        elif self._aggregation_dd.value == "Sum":
-            columns = list(self._df.columns)
-            self._df["Sum"] = self._df.sum(axis=1)
-            self._df.drop(columns=columns, inplace=True)
-        else:
-            assert self._aggregation_dd.value == "None"
-
-        if self._moving_avg_cb.value:
-            try:
-                window = int(self._window_text.value)
-            except Exception:
-                print(f"window size {self._window_text.value} must be an integer")
-                return
-            self._df = self._df.rolling(window=window).mean()
-
-    def _on_load_dataframe_click(self, _):
-        self._assign_dataframe()
-
-    def _on_plot_click(self, _):
-        if self._first_plot:
-            # This is a hack. Not sure why, but the first plot isn't displayed unless I do this.
-            display(self._plot_output)
-            self._plot_output.clear_output()
-            self._first_plot = False
-
-        self._assign_dataframe()
-        title = f"{self._elem_class_dd.value} {self._elem_prop_dd.value}"
-        # Potential bug: pandas reports a warning about a fragmented dataframe as a result of
-        # plotly code whenever the dataframe has more than 100 columns.
-        # Seems to be caused by plotly but am not sure.
-        fig = self._df.plot(title=title)
-        with self._plot_output:
-            fig.show()
-        display(self._plot_output)
-        self._plot_output.clear_output(wait=True)
-        # TODO: Something about this function is incorrect.
-        # The notebook accumulates blank space every time it is called.
-
-    def _on_reset_click(self, _):
-        self._plot_output.clear_output()
-        for val in self.__dict__.values():
-            if isinstance(val, widgets.Widget):
-                val.close_all()
-        self._make_widgets()
-        self._display_widgets()
-        self._enable_project_actions()
-        self._first_plot = True
-
-
-class BadInputError(Exception):
-    """Raise on bad user input."""
+import ast
+import copy
+import os
+import re
+import sys
+from collections import defaultdict
+
+import ipywidgets as widgets
+import pandas as pd
+from IPython.display import display
+
+try:
+    import plotly
+except ImportError:
+    print("plotly is required to run the DataViewer. Please run 'pip install plotly'")
+    sys.exit(1)
+
+
+from pydss.pydss_results import PyDssResults
+
+
+class DataViewer:
+    """Provides a UI for viewing pydss results."""
+
+    DEFAULTS = {
+        "project_path": os.environ.get("PYDSS_PROJECT_PATH"),
+    }
+
+    def __init__(self, **kwargs):
+        pd.options.plotting.backend = "plotly"
+        self._results = None
+        self._project_path = None
+        self._scenario = None
+        self._scenario_names = [""]
+        self._element_classes = [""]
+        self._element_props = [""]
+        self._timestamps = None
+        self._df = None
+        self._defaults = copy.deepcopy(self.DEFAULTS)
+        if kwargs:
+            self._defaults.update(kwargs)
+        self._make_widgets()
+        self._display_widgets()
+        if self._project_path_text.value != "":
+            self._on_load_project_click(None)
+        self._first_plot = True
+
+    @property
+    def results(self):
+        """Return the current PyDssResults object."""
+        return self._results
+
+    @property
+    def scenarios(self):
+        """Return the current PyDssScenarioResults objects."""
+        return self._results.scenarios
+
+    @property
+    def df(self):
+        """Return the current DataFrame."""
+        return self._df
+
+    def _make_widgets(self):
+        self._main_label = widgets.HTML("<b>Pydss Data Viewer</b>")
+        text_layout_long = widgets.Layout(width="400px")
+        text_layout_med = widgets.Layout(width="200px")
+        text_layout_short = widgets.Layout(width="100px")
+        button_layout = widgets.Layout(width="200px")
+        path = "" if self._defaults["project_path"] is None else self._defaults["project_path"]
+        self._project_path_text = widgets.Text(
+            path,
+            description="Project Path",
+            layout=text_layout_long,
+            placeholder="pydss_projects/project1/",
+        )
+        self._load_project_btn = widgets.Button(description="Load project", layout=button_layout)
+        self._load_project_btn.on_click(self._on_load_project_click)
+        self._scenario_label = widgets.HTML("Scenario", layout=text_layout_med)
+        self._elem_class_label = widgets.HTML("Element Class", layout=text_layout_med)
+        self._elem_prop_label = widgets.HTML("Element Property", layout=text_layout_med)
+        self._prop_available_options_label = widgets.HTML("Options")
+        self._prop_options_label = widgets.HTML("Element Property Options", layout=text_layout_med)
+        self._num_elems_label = widgets.HTML("Number of Elements", layout=text_layout_med)
+        self._elem_regex_label = widgets.HTML("Element Name Regex", layout=text_layout_med)
+        self._start_time_label = widgets.HTML("Start Time", layout=text_layout_med)
+        self._end_time_label = widgets.HTML("End Time", layout=text_layout_med)
+        self._resolution_label = widgets.HTML("Resolution", layout=text_layout_med)
+        self._moving_avg_cb = widgets.Checkbox(
+            value=False, description="Moving Average", disabled=True
+        )
+        self._window_label = widgets.HTML("Moving Average Window", layout=text_layout_med)
+        self._window_text = widgets.Text(value="4")
+        self._scenario_dd = widgets.Dropdown(
+            options=self._scenario_names,
+            value=self._scenario_names[0],
+            disabled=True,
+        )
+        self._scenario_dd.observe(self._on_scenario_change, names="value")
+        self._elem_class_dd = widgets.Dropdown(
+            options=self._element_classes,
+            value=self._element_classes[0],
+            disabled=True,
+        )
+        self._elem_class_dd.observe(self._on_elem_class_change, names="value")
+        self._elem_prop_dd = widgets.Dropdown(
+            options=self._element_props,
+            value=self._element_props[0],
+            disabled=True,
+        )
+        self._elem_prop_dd.observe(self._on_elem_prop_change, names="value")
+        self._prop_available_options_text = widgets.Text(disabled=True)
+        self._prop_options_text = widgets.Text(disabled=True)
+        self._num_elems_text = widgets.Text(disabled=True)
+        self._elem_regex_text = widgets.Text(disabled=True, placeholder="p1udt.*")
+        self._elem_names_select = widgets.SelectMultiple(
+            options=[""],
+            value=[""],
+            rows=5,
+            description="Elements",
+            disabled=True,
+        )
+        self._start_time_text = widgets.Text(disabled=True)
+        self._end_time_text = widgets.Text(disabled=True)
+        self._resolution_text = widgets.Text(disabled=True)
+        self._load_dataframe_btn = widgets.Button(
+            description="Load DataFrame",
+            layout=button_layout,
+            disabled=True,
+            tooltip="Load DataFrame into `app.df`",
+        )
+        self._load_dataframe_btn.on_click(self._on_load_dataframe_click)
+        self._plot_btn = widgets.Button(description="Plot", layout=button_layout)
+        self._plot_btn.on_click(self._on_plot_click)
+        self._aggregation_label = widgets.HTML("Aggregation", layout=text_layout_short)
+        self._aggregation_dd = widgets.Dropdown(
+            options=["None", "Min/Mean/Max", "Sum"],
+            value="None",
+            disabled=True,
+        )
+        self._real_only_cb = widgets.Checkbox(
+            value=True, description="Exclude imaginary parts", disabled=True
+        )
+        self._reset_btn = widgets.Button(description="Reset", layout=button_layout)
+        self._reset_btn.on_click(self._on_reset_click)
+        self._plot_output = widgets.Output()
+
+    def _display_widgets(self):
+        box = widgets.VBox(
+            (
+                self._main_label,
+                widgets.HBox((self._load_project_btn, self._project_path_text)),
+                widgets.HBox((self._scenario_label, self._scenario_dd)),
+                widgets.HBox((self._elem_class_label, self._elem_class_dd)),
+                widgets.HBox(
+                    (
+                        self._elem_prop_label,
+                        self._elem_prop_dd,
+                        self._prop_available_options_label,
+                        self._prop_available_options_text,
+                    )
+                ),
+                widgets.HBox((self._prop_options_label, self._prop_options_text)),
+                widgets.HBox(
+                    (
+                        widgets.VBox(
+                            (
+                                widgets.HBox((self._num_elems_label, self._num_elems_text)),
+                                widgets.HBox((self._elem_regex_label, self._elem_regex_text)),
+                            )
+                        ),
+                        self._elem_names_select,
+                    )
+                ),
+                widgets.HBox(
+                    (
+                        widgets.VBox(
+                            (
+                                widgets.HBox((self._start_time_label, self._start_time_text)),
+                                widgets.HBox((self._end_time_label, self._end_time_text)),
+                                widgets.HBox((self._resolution_label, self._resolution_text)),
+                            ),
+                        ),
+                        widgets.VBox(
+                            (
+                                self._moving_avg_cb,
+                                widgets.HBox((self._window_label, self._window_text)),
+                            )
+                        ),
+                    )
+                ),
+                widgets.HBox(
+                    (
+                        self._plot_btn,
+                        self._load_dataframe_btn,
+                        self._aggregation_label,
+                        self._aggregation_dd,
+                        self._real_only_cb,
+                    )
+                ),
+                self._reset_btn,
+            )
+        )
+        display(box)
+
+    def _enable_project_actions(self):
+        self._elem_class_dd.disabled = False
+        self._elem_prop_dd.disabled = False
+        self._prop_options_text.disabled = False
+        self._load_dataframe_btn.disabled = False
+        self._plot_btn.disabled = False
+        self._scenario_dd.disabled = False
+        self._aggregation_dd.disabled = False
+        self._real_only_cb.disabled = False
+        self._elem_regex_text.disabled = False
+        self._elem_names_select.disabled = False
+        self._start_time_text.disabled = False
+        self._end_time_text.disabled = False
+        self._moving_avg_cb.disabled = False
+        self._assign_widgets()
+
+    def _assign_widgets(self):
+        self._scenario_names[:] = [x.name for x in self._results.scenarios]
+        self._scenario_dd.options = self._scenario_names
+        self._scenario_dd.value = self._scenario_names[0]
+        self._on_elem_class_change(None)
+        self._on_elem_prop_change(None)
+
+    def _on_scenario_change(self, _):
+        self._scenario = [x for x in self._results.scenarios if x.name == self._scenario_dd.value][
+            0
+        ]
+        self._element_classes[:] = self._scenario.list_element_classes()
+        self._elem_class_dd.options = self._element_classes
+        self._elem_class_dd.value = self._element_classes[0]
+
+    def _on_elem_class_change(self, _):
+        elem_class = self._elem_class_dd.value
+        self._element_props[:] = self._scenario.list_element_properties(elem_class)
+        self._elem_prop_dd.options = self._element_props
+        if self._element_props:
+            self._elem_prop_dd.value = self._element_props[0]
+        elem_names = self._scenario.list_element_names(elem_class)
+        elem_names.insert(0, "All")
+        self._elem_names_select.options = elem_names
+        self._elem_names_select.value = [elem_names[0]]
+        self._num_elems_text.value = str(len(elem_names))
+
+    def _on_elem_prop_change(self, _):
+        elem_class = self._elem_class_dd.value
+        elem_prop = self._elem_prop_dd.value
+        options = self._scenario.list_element_property_options(elem_class, elem_prop)
+        self._prop_available_options_text.value = ", ".join(options)
+        self._prop_options_text.value = ""
+        self._prop_options_text.placeholder = ", ".join(f"{x}='X'" for x in options)
+        self._elem_regex_text.value = ""
+
+    def _on_load_project_click(self, _):
+        path = self._project_path_text.value
+        if path == "":
+            print("Project Path cannot be empty.", file=sys.stderr)
+            return
+
+        self._results = PyDssResults(path)
+        self._timestamps = self._results.scenarios[0].get_timestamps()
+        self._start_time_text.value = str(self._timestamps.iloc[0])
+        self._end_time_text.value = str(self._timestamps.iloc[-1])
+        self._resolution_text.value = str(self._timestamps.iloc[1] - self._timestamps.iloc[0])
+        self._enable_project_actions()
+
+    def _filter_dataframe_by_time(self):
+        filter_required = False
+        start_time = pd.Timestamp(self._start_time_text.value)
+        end_time = pd.Timestamp(self._end_time_text.value)
+        p_start = self._timestamps.iloc[0]
+        p_end = self._timestamps.iloc[-1]
+        if start_time < p_start or start_time > p_end:
+            print(
+                f"Error: start_time={start_time} must be between {p_start} and {p_end}",
+                file=sys.stderr,
+            )
+            raise BadInputError("invalid start time")
+        if start_time != p_start:
+            filter_required = True
+        if end_time > p_end or end_time < p_start:
+            print(
+                f"Error: end_time={end_time} must be between {p_start} and {p_end}",
+                file=sys.stderr,
+            )
+            raise BadInputError("invalid end time")
+        if end_time != p_end:
+            filter_required = True
+
+        if filter_required:
+            self._df = self._df.loc[start_time:end_time, :]
+
+    def _filter_dataframe_by_elem_names(self):
+        value = self._elem_names_select.value
+        if value != ("All",):
+            values = set(value)
+            columns = [
+                x for x in self._df.columns if self._scenario.get_name_from_column(x) in values
+            ]
+            self._df = self._df[columns]
+
+    def _filter_dataframe_by_elem_regex(self):
+        regex_str = self._elem_regex_text.value
+        if regex_str != "":
+            regex = re.compile(rf"{regex_str}")
+            columns = [x for x in self._df.columns if regex.search(x) is not None]
+            self._df = self._df[columns]
+
+    def _get_elem_property_options(self):
+        options = {}
+        options_text = self._prop_options_text.value.strip()
+        if options_text != "":
+            for option_pair in options_text.split(","):
+                fields = option_pair.split("=")
+                if len(fields) != 2:
+                    print(
+                        f"Invalid option pair: '{option_pair}'. Must be 'option=value'",
+                        file=sys.stderr,
+                    )
+                    return
+                key = fields[0].strip()
+                val = fields[1].strip()
+                options[key] = ast.literal_eval(val)
+        return options
+
+    def _assign_dataframe(self):
+        elem_class = self._elem_class_dd.value
+        elem_prop = self._elem_prop_dd.value
+        real_only = self._real_only_cb.value
+
+        self._df = self._scenario.get_full_dataframe(
+            elem_class,
+            elem_prop,
+            real_only=real_only,
+            **self._get_elem_property_options(),
+        )
+        try:
+            self._filter_dataframe_by_time()
+        except BadInputError:
+            return
+
+        self._filter_dataframe_by_elem_names()
+        self._filter_dataframe_by_elem_regex()
+
+        if self._aggregation_dd.value == "Min/Mean/Max":
+            columns = list(self._df.columns)
+            self._df["Max"] = self._df.max(axis=1)
+            self._df["Mean"] = self._df.mean(axis=1)
+            self._df["Min"] = self._df.min(axis=1)
+            self._df.drop(columns=columns, inplace=True)
+        elif self._aggregation_dd.value == "Sum":
+            columns = list(self._df.columns)
+            self._df["Sum"] = self._df.sum(axis=1)
+            self._df.drop(columns=columns, inplace=True)
+        else:
+            assert self._aggregation_dd.value == "None"
+
+        if self._moving_avg_cb.value:
+            try:
+                window = int(self._window_text.value)
+            except Exception:
+                print(f"window size {self._window_text.value} must be an integer")
+                return
+            self._df = self._df.rolling(window=window).mean()
+
+    def _on_load_dataframe_click(self, _):
+        self._assign_dataframe()
+
+    def _on_plot_click(self, _):
+        if self._first_plot:
+            # This is a hack. Not sure why, but the first plot isn't displayed unless I do this.
+            display(self._plot_output)
+            self._plot_output.clear_output()
+            self._first_plot = False
+
+        self._assign_dataframe()
+        title = f"{self._elem_class_dd.value} {self._elem_prop_dd.value}"
+        # Potential bug: pandas reports a warning about a fragmented dataframe as a result of
+        # plotly code whenever the dataframe has more than 100 columns.
+        # Seems to be caused by plotly but am not sure.
+        fig = self._df.plot(title=title)
+        with self._plot_output:
+            fig.show()
+        display(self._plot_output)
+        self._plot_output.clear_output(wait=True)
+        # TODO: Something about this function is incorrect.
+        # The notebook accumulates blank space every time it is called.
+
+    def _on_reset_click(self, _):
+        self._plot_output.clear_output()
+        for val in self.__dict__.values():
+            if isinstance(val, widgets.Widget):
+                val.close_all()
+        self._make_widgets()
+        self._display_widgets()
+        self._enable_project_actions()
+        self._first_plot = True
+
+
+class BadInputError(Exception):
+    """Raise on bad user input."""
```

### Comparing `nrel_pydss-3.1.3/src/pydss/cli/add_post_process.py` & `nrel_pydss-3.1.4/src/pydss/cli/add_post_process.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-"""CLI to create a new pydss project"""
-
-
-import click
-
-from pydss.pydss_project import PyDssProject
-
-
-@click.argument("config-file")
-@click.argument("script")
-@click.argument("scenario-name")
-@click.argument("project-path")
-@click.command()
-def add_post_process(project_path, scenario_name, script, config_file):
-    """Add post-process script to pydss scenario."""
-    project = PyDssProject.load_project(project_path)
-    scenario = project.get_scenario(scenario_name)
-    pp_info = {"script": script, "config_file": config_file}
-    scenario.add_post_process(pp_info)
-    project.serialize()
+"""CLI to create a new pydss project"""
+
+
+import click
+
+from pydss.pydss_project import PyDssProject
+
+
+@click.argument("config-file")
+@click.argument("script")
+@click.argument("scenario-name")
+@click.argument("project-path")
+@click.command()
+def add_post_process(project_path, scenario_name, script, config_file):
+    """Add post-process script to pydss scenario."""
+    project = PyDssProject.load_project(project_path)
+    scenario = project.get_scenario(scenario_name)
+    pp_info = {"script": script, "config_file": config_file}
+    scenario.add_post_process(pp_info)
+    project.serialize()
```

### Comparing `nrel_pydss-3.1.3/src/pydss/cli/add_scenario.py` & `nrel_pydss-3.1.4/src/pydss/cli/add_scenario.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,58 +1,58 @@
-from pathlib import Path
-import toml
-import click
-import os
-
-from pydss.simulation_input_models import MappedControllers
-from pydss.pydss_project import PyDssScenario
-from pydss.common import CONTROLLER_TYPES, ControllerType
-
-def build_scenario(project_path:str, scenario_name:str, controller_mapping:str):
-    project_path = Path(project_path)
-    controller_mapping_path = Path(controller_mapping)    
-    assert project_path.exists(), "Provided project path does not exist"
-    assert (project_path / "Scenarios").exists(), "provided project is not a valid pydss project"
-    assert controller_mapping_path.exists(), "rovided controller_mapping file does not exist"
-    assert controller_mapping_path.suffix.lower() == ".toml", "controller_mapping should be a TOML file"
-    
-    controller_map = toml.load(controller_mapping_path)
-    mapped_controllers = MappedControllers(**controller_map)
-    acceptable_controller_types = CONTROLLER_TYPES
-    controllers = {}
-    for controller in mapped_controllers.mapping:
-        settings_path_obj = Path(controller.controller_file)
-        assert controller.controller_type in ControllerType, \
-        f"{controller.controller_type} is not a valid contoller. Options are {acceptable_controller_types}"
-        assert settings_path_obj.exists(), \
-            f"file for controller type {controller.controller_type} does not exist"
-        controller_data = toml.load(settings_path_obj)
-        if controller_data:
-            if controller.controller_type in controllers:
-                msg= f"Multiple keys files for the same controller type {controller.controller_type}." \
-                "Each controller type can only be attached to a single file."
-                raise ValueError(msg)
-            controllers[controller.controller_type] = toml.load(settings_path_obj)
-    scenario_dir = project_path / "Scenarios" / scenario_name
-    scenario_obj = PyDssScenario(
-            [scenario_name], controllers=controllers, export_modes=None
-        )
-    scenario_obj.serialize(str(scenario_dir))
-
-@click.argument("project-path", type=click.Path(exists=True))
-@click.option(
-    "-s", "--scenario_name",
-    required=True,
-    help="name of the new scenario",
-)
-@click.option(
-    "-c", "--controller-mapping",
-    required=True,
-    default=None,
-    type=click.Path(exists=True),
-    help="JSON file that maps controller type to controller definition files",
-)
-@click.command()
-def add_scenario(project_path:str, scenario_name:str, controller_mapping:str):
-    """Add a new scenario to an existing project"""
-    build_scenario(project_path, scenario_name, controller_mapping)
-
+from pathlib import Path
+import toml
+import click
+import os
+
+from pydss.simulation_input_models import MappedControllers
+from pydss.pydss_project import PyDssScenario
+from pydss.common import CONTROLLER_TYPES, ControllerType
+
+def build_scenario(project_path:str, scenario_name:str, controller_mapping:str):
+    project_path = Path(project_path)
+    controller_mapping_path = Path(controller_mapping)    
+    assert project_path.exists(), "Provided project path does not exist"
+    assert (project_path / "Scenarios").exists(), "provided project is not a valid pydss project"
+    assert controller_mapping_path.exists(), "rovided controller_mapping file does not exist"
+    assert controller_mapping_path.suffix.lower() == ".toml", "controller_mapping should be a TOML file"
+    
+    controller_map = toml.load(controller_mapping_path)
+    mapped_controllers = MappedControllers(**controller_map)
+    acceptable_controller_types = CONTROLLER_TYPES
+    controllers = {}
+    for controller in mapped_controllers.mapping:
+        settings_path_obj = Path(controller.controller_file)
+        assert controller.controller_type in ControllerType, \
+        f"{controller.controller_type} is not a valid contoller. Options are {acceptable_controller_types}"
+        assert settings_path_obj.exists(), \
+            f"file for controller type {controller.controller_type} does not exist"
+        controller_data = toml.load(settings_path_obj)
+        if controller_data:
+            if controller.controller_type in controllers:
+                msg= f"Multiple keys files for the same controller type {controller.controller_type}." \
+                "Each controller type can only be attached to a single file."
+                raise ValueError(msg)
+            controllers[controller.controller_type] = toml.load(settings_path_obj)
+    scenario_dir = project_path / "Scenarios" / scenario_name
+    scenario_obj = PyDssScenario(
+            [scenario_name], controllers=controllers, export_modes=None
+        )
+    scenario_obj.serialize(str(scenario_dir))
+
+@click.argument("project-path", type=click.Path(exists=True))
+@click.option(
+    "-s", "--scenario_name",
+    required=True,
+    help="name of the new scenario",
+)
+@click.option(
+    "-c", "--controller-mapping",
+    required=True,
+    default=None,
+    type=click.Path(exists=True),
+    help="JSON file that maps controller type to controller definition files",
+)
+@click.command()
+def add_scenario(project_path:str, scenario_name:str, controller_mapping:str):
+    """Add a new scenario to an existing project"""
+    build_scenario(project_path, scenario_name, controller_mapping)
+
```

### Comparing `nrel_pydss-3.1.3/src/pydss/cli/controllers.py` & `nrel_pydss-3.1.4/src/pydss/cli/controllers.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,64 +1,64 @@
-"""
-CLI to add controllers to the local registry
-"""
-
-import sys
-import os
-
-import click
-
-
-from pydss.common import CONTROLLER_TYPES
-from pydss.utils.utils import load_data
-from pydss.registry import Registry
-
-@click.group()
-def controllers():
-    """Manage registered pydss controllers."""
-
-
-@click.argument("filename")
-@click.argument("controller_type")
-@click.command()
-def register(controller_type, filename):
-    """Register a controller in the local registry."""
-    if controller_type not in CONTROLLER_TYPES:
-        print(f"controller_type must be one of {CONTROLLER_TYPES}")
-        sys.exit(1)
-    if not os.path.exists(filename):
-        print(f"{filename} does not exist")
-        sys.exit(1)
-
-    registry = Registry()
-    for name in load_data(filename):
-        data = {"name": name, "filename": filename}
-        registry.register_controller(controller_type, data)
-        print(f"Registered {controller_type} {name}")
-
-
-@click.argument("name")
-@click.argument("controller_type")
-@click.command()
-def unregister(controller_type, name):
-    """Unregister a controller."""
-    Registry().unregister_controller(controller_type, name)
-    print(f"Unregistered {controller_type} {name}")
-
-
-@click.command()
-def show():
-    """Show the registered controllers."""
-    Registry().show_controllers()
-
-
-@click.command()
-def reset_defaults():
-    """Reset defaults."""
-    Registry().reset_defaults(controllers_only=True)
-    print("Reset pydss defaults")
-
-
-controllers.add_command(register)
-controllers.add_command(unregister)
-controllers.add_command(show)
-controllers.add_command(reset_defaults)
+"""
+CLI to add controllers to the local registry
+"""
+
+import sys
+import os
+
+import click
+
+
+from pydss.common import CONTROLLER_TYPES
+from pydss.utils.utils import load_data
+from pydss.registry import Registry
+
+@click.group()
+def controllers():
+    """Manage registered pydss controllers."""
+
+
+@click.argument("filename")
+@click.argument("controller_type")
+@click.command()
+def register(controller_type, filename):
+    """Register a controller in the local registry."""
+    if controller_type not in CONTROLLER_TYPES:
+        print(f"controller_type must be one of {CONTROLLER_TYPES}")
+        sys.exit(1)
+    if not os.path.exists(filename):
+        print(f"{filename} does not exist")
+        sys.exit(1)
+
+    registry = Registry()
+    for name in load_data(filename):
+        data = {"name": name, "filename": filename}
+        registry.register_controller(controller_type, data)
+        print(f"Registered {controller_type} {name}")
+
+
+@click.argument("name")
+@click.argument("controller_type")
+@click.command()
+def unregister(controller_type, name):
+    """Unregister a controller."""
+    Registry().unregister_controller(controller_type, name)
+    print(f"Unregistered {controller_type} {name}")
+
+
+@click.command()
+def show():
+    """Show the registered controllers."""
+    Registry().show_controllers()
+
+
+@click.command()
+def reset_defaults():
+    """Reset defaults."""
+    Registry().reset_defaults(controllers_only=True)
+    print("Reset pydss defaults")
+
+
+controllers.add_command(register)
+controllers.add_command(unregister)
+controllers.add_command(show)
+controllers.add_command(reset_defaults)
```

### Comparing `nrel_pydss-3.1.3/src/pydss/cli/convert.py` & `nrel_pydss-3.1.4/src/pydss/cli/convert.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,55 +1,55 @@
-"""CLI to convert legacy formats"""
-
-import os
-
-import click
-
-from pydss.config_data import convert_config_data_to_toml
-from pydss.export_list_reader import ExportListReader
-from pydss.utils.utils import dump_data
-
-
-@click.group("convert")
-def convert():
-    """Convert input files to new formats."""
-
-
-@click.argument(
-    "filenames",
-    nargs=-1,
-)
-@click.option(
-    "-n", "--name",
-    help="new filename; default is to use the basename of the XLSX file",
-)
-@click.command()
-def excel_to_toml(filenames, name=None):
-    """Convert an Excel configuration file to TOML."""
-    for filename in filenames:
-        convert_config_data_to_toml(filename, name)
-
-
-@click.argument(
-    "filenames",
-    nargs=-1,
-)
-@click.option(
-    "-n", "--name",
-    help="new filename; default is Exports.toml",
-)
-@click.command()
-def simulation_file(filenames, name=None):
-    """Convert a legacy simulation TOML file to the new format."""
-    for filename in filenames:
-        dirname = os.path.dirname(filename)
-        if name is None:
-            new_filename = os.path.join(dirname, "Exports.toml")
-        else:
-            new_filename = name
-        reader = ExportListReader(filename)
-        dump_data(reader.serialize(), new_filename)
-        print(f"Converted {filename} to {new_filename}")
-
-
-convert.add_command(excel_to_toml)
-convert.add_command(simulation_file)
+"""CLI to convert legacy formats"""
+
+import os
+
+import click
+
+from pydss.config_data import convert_config_data_to_toml
+from pydss.export_list_reader import ExportListReader
+from pydss.utils.utils import dump_data
+
+
+@click.group("convert")
+def convert():
+    """Convert input files to new formats."""
+
+
+@click.argument(
+    "filenames",
+    nargs=-1,
+)
+@click.option(
+    "-n", "--name",
+    help="new filename; default is to use the basename of the XLSX file",
+)
+@click.command()
+def excel_to_toml(filenames, name=None):
+    """Convert an Excel configuration file to TOML."""
+    for filename in filenames:
+        convert_config_data_to_toml(filename, name)
+
+
+@click.argument(
+    "filenames",
+    nargs=-1,
+)
+@click.option(
+    "-n", "--name",
+    help="new filename; default is Exports.toml",
+)
+@click.command()
+def simulation_file(filenames, name=None):
+    """Convert a legacy simulation TOML file to the new format."""
+    for filename in filenames:
+        dirname = os.path.dirname(filename)
+        if name is None:
+            new_filename = os.path.join(dirname, "Exports.toml")
+        else:
+            new_filename = name
+        reader = ExportListReader(filename)
+        dump_data(reader.serialize(), new_filename)
+        print(f"Converted {filename} to {new_filename}")
+
+
+convert.add_command(excel_to_toml)
+convert.add_command(simulation_file)
```

### Comparing `nrel_pydss-3.1.3/src/pydss/cli/edit_scenario.py` & `nrel_pydss-3.1.4/src/pydss/cli/edit_scenario.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,55 +1,55 @@
-"""CLI to edit an existing project or scenario"""
-
-import click
-
-from pydss.common import CONTROLLER_TYPES
-from pydss.pydss_project import update_pydss_controllers
-
-
-@click.option(
-    "-p", "--project-path",
-    required=True,
-    help="project path",
-)
-@click.option(
-    "-s", "--scenario",
-    required=True,
-    help="Project name (should exist)",
-)
-@click.group()
-def edit_scenario(project_path=None, scenario=None):
-    """Edit scenario in a pydss project."""
-
-
-@click.option(
-    "-c", "--controller",
-    required=True,
-    help="controller name",
-)
-@click.option(
-    "-f", "--dss-file",
-    required=True,
-    help="OpenDSS file containing elements",
-)
-@click.option(
-    "-t", "--controller-type",
-    required=True,
-    type=click.Choice(CONTROLLER_TYPES),
-    help="controller type",
-)
-@click.command()
-@click.pass_context
-def update_controllers(ctx, controller_type=None, controller=None, dss_file=None):
-    """Update a scenario's controllers from an OpenDSS file."""
-    project_path = ctx.parent.params["project_path"]
-    scenario = ctx.parent.params["scenario"]
-    update_pydss_controllers(
-        project_path=project_path,
-        scenario=scenario,
-        controller_type=controller_type,
-        controller=controller,
-        dss_file=dss_file
-    )
-
-
-edit_scenario.add_command(update_controllers)
+"""CLI to edit an existing project or scenario"""
+
+import click
+
+from pydss.common import CONTROLLER_TYPES
+from pydss.pydss_project import update_pydss_controllers
+
+
+@click.option(
+    "-p", "--project-path",
+    required=True,
+    help="project path",
+)
+@click.option(
+    "-s", "--scenario",
+    required=True,
+    help="Project name (should exist)",
+)
+@click.group()
+def edit_scenario(project_path=None, scenario=None):
+    """Edit scenario in a pydss project."""
+
+
+@click.option(
+    "-c", "--controller",
+    required=True,
+    help="controller name",
+)
+@click.option(
+    "-f", "--dss-file",
+    required=True,
+    help="OpenDSS file containing elements",
+)
+@click.option(
+    "-t", "--controller-type",
+    required=True,
+    type=click.Choice(CONTROLLER_TYPES),
+    help="controller type",
+)
+@click.command()
+@click.pass_context
+def update_controllers(ctx, controller_type=None, controller=None, dss_file=None):
+    """Update a scenario's controllers from an OpenDSS file."""
+    project_path = ctx.parent.params["project_path"]
+    scenario = ctx.parent.params["scenario"]
+    update_pydss_controllers(
+        project_path=project_path,
+        scenario=scenario,
+        controller_type=controller_type,
+        controller=controller,
+        dss_file=dss_file
+    )
+
+
+edit_scenario.add_command(update_controllers)
```

### Comparing `nrel_pydss-3.1.3/src/pydss/cli/export.py` & `nrel_pydss-3.1.4/src/pydss/cli/export.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,65 +1,65 @@
-"""
-CLI to export data from a pydss project
-"""
-
-import sys
-import os
-
-from loguru import logger
-import click
-
-
-from pydss.pydss_results import PyDssResults
-from pydss.utils.utils import get_cli_string
-
-
-# TODO Make command to list scenarios.
-
-@click.argument(
-    "project-path",
-)
-@click.option(
-    "-f", "--fmt",
-    default="csv",
-    help="Output file format (csv or h5)."
-)
-@click.option(
-    "-c", "--compress",
-    is_flag=True,
-    default=False,
-    show_default=True,
-    help="Compress output files.",
-)
-@click.option(
-    "-o", "--output-dir",
-    help="Output directory. Default is project exports directory.",
-)
-@click.option(
-    "--verbose",
-    is_flag=True,
-    default=False,
-    show_default=True,
-    help="Enable verbose log output."
-)
-@click.command()
-def export(project_path, fmt="csv", compress=False, output_dir=None, verbose=False):
-    """Export data from a pydss project."""
-    if not os.path.exists(project_path):
-        sys.exit(1)
-
-    filename = "pydss_export.log"
-    console_level = "INFO"
-    file_level = "INFO"
-    if verbose:
-        console_level = "DEBUG"
-        file_level = "DEBUG"
-
-    logger.level(console_level)
-    if filename:
-        logger.add(filename)
-
-    logger.info("CLI: [%s]", get_cli_string())
-
-    results = PyDssResults(project_path)
-    for scenario in results.scenarios:
-        scenario.export_data(output_dir, fmt=fmt, compress=compress)
+"""
+CLI to export data from a pydss project
+"""
+
+import sys
+import os
+
+from loguru import logger
+import click
+
+
+from pydss.pydss_results import PyDssResults
+from pydss.utils.utils import get_cli_string
+
+
+# TODO Make command to list scenarios.
+
+@click.argument(
+    "project-path",
+)
+@click.option(
+    "-f", "--fmt",
+    default="csv",
+    help="Output file format (csv or h5)."
+)
+@click.option(
+    "-c", "--compress",
+    is_flag=True,
+    default=False,
+    show_default=True,
+    help="Compress output files.",
+)
+@click.option(
+    "-o", "--output-dir",
+    help="Output directory. Default is project exports directory.",
+)
+@click.option(
+    "--verbose",
+    is_flag=True,
+    default=False,
+    show_default=True,
+    help="Enable verbose log output."
+)
+@click.command()
+def export(project_path, fmt="csv", compress=False, output_dir=None, verbose=False):
+    """Export data from a pydss project."""
+    if not os.path.exists(project_path):
+        sys.exit(1)
+
+    filename = "pydss_export.log"
+    console_level = "INFO"
+    file_level = "INFO"
+    if verbose:
+        console_level = "DEBUG"
+        file_level = "DEBUG"
+
+    logger.level(console_level)
+    if filename:
+        logger.add(filename)
+
+    logger.info("CLI: [%s]", get_cli_string())
+
+    results = PyDssResults(project_path)
+    for scenario in results.scenarios:
+        scenario.export_data(output_dir, fmt=fmt, compress=compress)
```

### Comparing `nrel_pydss-3.1.3/src/pydss/cli/pydss.py` & `nrel_pydss-3.1.4/src/pydss/cli/pydss.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,44 +1,44 @@
-"""Main CLI command for pydss."""
-
-from loguru import logger
-import click
-
-from pydss.cli.create_project import create_project
-from pydss.cli.add_post_process import add_post_process
-from pydss.cli.controllers import controllers
-from pydss.cli.convert import convert
-from pydss.cli.export import export
-from pydss.cli.extract import extract, extract_element_files
-from pydss.cli.run import run
-from pydss.cli.edit_scenario import edit_scenario
-
-from pydss.cli.reports import reports
-from pydss.cli.add_scenario import add_scenario
-
-server_dependencies_installed = True
-
-try:
-    from pydss.cli.run_server import serve
-except ImportError:
-    server_dependencies_installed = False
-    logger.warning(
-        "Server dependencies not installed. Use 'pip install NREL-pydss[server]' to install additional dependencies"
-    )
-
-@click.group()
-def cli():
-    """Pydss commands"""
-
-cli.add_command(create_project)
-cli.add_command(add_post_process)
-cli.add_command(export)
-cli.add_command(extract)
-cli.add_command(extract_element_files)
-cli.add_command(run)
-cli.add_command(add_scenario)
-cli.add_command(edit_scenario)
-cli.add_command(convert)
-cli.add_command(controllers)
-cli.add_command(reports)
-if server_dependencies_installed:
+"""Main CLI command for pydss."""
+
+from loguru import logger
+import click
+
+from pydss.cli.create_project import create_project
+from pydss.cli.add_post_process import add_post_process
+from pydss.cli.controllers import controllers
+from pydss.cli.convert import convert
+from pydss.cli.export import export
+from pydss.cli.extract import extract, extract_element_files
+from pydss.cli.run import run
+from pydss.cli.edit_scenario import edit_scenario
+
+from pydss.cli.reports import reports
+from pydss.cli.add_scenario import add_scenario
+
+server_dependencies_installed = True
+
+try:
+    from pydss.cli.run_server import serve
+except ImportError:
+    server_dependencies_installed = False
+    logger.warning(
+        "Server dependencies not installed. Use 'pip install NREL-pydss[server]' to install additional dependencies"
+    )
+
+@click.group()
+def cli():
+    """Pydss commands"""
+
+cli.add_command(create_project)
+cli.add_command(add_post_process)
+cli.add_command(export)
+cli.add_command(extract)
+cli.add_command(extract_element_files)
+cli.add_command(run)
+cli.add_command(add_scenario)
+cli.add_command(edit_scenario)
+cli.add_command(convert)
+cli.add_command(controllers)
+cli.add_command(reports)
+if server_dependencies_installed:
     cli.add_command(serve)
```

### Comparing `nrel_pydss-3.1.3/src/pydss/cli/reports.py` & `nrel_pydss-3.1.4/src/pydss/cli/reports.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,129 +1,129 @@
-"""
-CLI to run a pydss project
-"""
-
-import click
-import json
-import os
-
-from terminaltables import SingleTable
-from os.path import normpath, basename
-
-@click.argument(
-    "project-path",
-)
-
-@click.option(
-    "-l", "--list-reports",
-    help="List all reports for a given project path",
-    is_flag=True,
-    default=False,
-    show_default=True,
-)
-
-@click.option(
-    "-i", "--index",
-    help="View report by index (use -l flag to see list of available reports)",
-    default=0,
-    show_default=True,
-)
-
-@click.option(
-    "-s", "--scenario",
-    required=False,
-    help="Pydss scenario name.",
-)
-
-@click.option(
-    "-r", "--report",
-    required=False,
-    help="Pydss report name.",
-)
-@click.command()
-
-def reports(project_path, list_reports=False, scenario=None, report=None, index=0):
-    """Explore and print pydss reports."""
-    assert not (list_reports and index), "Both 'list' and 'index' options cannot be set to true at the same time"
-    assert os.path.exists(project_path), "The provided project path {} does not exist".format(project_path)
-    logsPath = os.path.join(project_path, "Logs")
-    assert os.path.exists(logsPath), "No Logs folder in the provided project path.".format(project_path)
-    print(logsPath)
-    reportList = getAvailableReports(logsPath)
-    project = basename(normpath(project_path))
-    if list_reports:
-        Table = SingleTable(reportList, title="Available pydss reports")
-        print("")
-        print(Table.table)
-    elif index:
-        idx, projectName, ScenarioName, ReportName = reportList[index]
-        printReport(logsPath, projectName, ScenarioName, ReportName)
-    elif project:
-        for dx, projectName, ScenarioName, ReportName in reportList[1:]:
-            if projectName == project:
-                if scenario is None or scenario == ScenarioName:
-                    if report is None or report == ReportName:
-                        printReport(logsPath, projectName, ScenarioName, ReportName)
-
-def printReport(logsPath, project, scenario, report):
-    fileName = "{}__{}__reports.log".format(project, scenario)
-    filePath = os.path.join(logsPath, fileName)
-    assert os.path.exists(filePath), "Report {} for project: {} / scenario: {} does not exist".format(
-        report, project, scenario
-    )
-
-    tableData = []
-    Keys = {}
-    with open(os.path.join(logsPath, fileName), "r") as f:
-        for l in f:
-            data = json.loads(l.strip())
-            if "Report" not in data:
-                print("Skipping {}. Not a valid pydss report.".format(fileName))
-                return None
-            elif data["Report"] == report:
-                if report not in Keys:
-                    Keys[report] = list(data.keys())
-                    Keys[report] = [x for x in Keys[report] if x != "Report"]
-                values = []
-                for k in Keys[report]:
-                    values.append(data[k])
-                tableData.append(values)
-    tableData.insert(0, Keys[report])
-    Table = SingleTable(tableData, title="{} report (Project: {}, Scenario: {})".format(
-        report, project, scenario
-    ))
-    print("")
-    print(Table.table)
-    return
-
-def getAvailableReports(logsPath):
-    logFiles = list(filter(lambda x: '.log' in x, os.listdir(logsPath)))
-    reportFiles = [x for x in logFiles if "__reports" in x]
-    headings = ["#", "Project", "Scenario", "Report"]
-    reportList = [headings]
-    reportNumber = 0
-    for report in reportFiles:
-        project, scenario, _ = report.split("__")
-        print(reportNumber, project, scenario, report)
-        reportTypes = getReportTypes(logsPath, report)
-        if reportTypes is not None:
-            for reportTypes in reportTypes:
-                reportNumber += 1
-                reportList.append([reportNumber, project, scenario, reportTypes])
-    return reportList
-
-
-def getReportTypes(logsPath, reportFile):
-    fileName = os.path.join(logsPath, reportFile)
-    print(fileName)
-    f = open(fileName, "r")
-    lines = f.readlines()
-    reportTypes = []
-    for l in lines:
-        data = json.loads(l.strip())
-        if "Report" not in data:
-            print("Skipping {}. Not a valid pydss report.".format(fileName))
-            return None
-        else:
-            if data["Report"] not in reportTypes:
-                reportTypes.append(data["Report"])
-    return reportTypes
+"""
+CLI to run a pydss project
+"""
+
+import click
+import json
+import os
+
+from terminaltables import SingleTable
+from os.path import normpath, basename
+
+@click.argument(
+    "project-path",
+)
+
+@click.option(
+    "-l", "--list-reports",
+    help="List all reports for a given project path",
+    is_flag=True,
+    default=False,
+    show_default=True,
+)
+
+@click.option(
+    "-i", "--index",
+    help="View report by index (use -l flag to see list of available reports)",
+    default=0,
+    show_default=True,
+)
+
+@click.option(
+    "-s", "--scenario",
+    required=False,
+    help="Pydss scenario name.",
+)
+
+@click.option(
+    "-r", "--report",
+    required=False,
+    help="Pydss report name.",
+)
+@click.command()
+
+def reports(project_path, list_reports=False, scenario=None, report=None, index=0):
+    """Explore and print pydss reports."""
+    assert not (list_reports and index), "Both 'list' and 'index' options cannot be set to true at the same time"
+    assert os.path.exists(project_path), "The provided project path {} does not exist".format(project_path)
+    logsPath = os.path.join(project_path, "Logs")
+    assert os.path.exists(logsPath), "No Logs folder in the provided project path.".format(project_path)
+    print(logsPath)
+    reportList = getAvailableReports(logsPath)
+    project = basename(normpath(project_path))
+    if list_reports:
+        Table = SingleTable(reportList, title="Available pydss reports")
+        print("")
+        print(Table.table)
+    elif index:
+        idx, projectName, ScenarioName, ReportName = reportList[index]
+        printReport(logsPath, projectName, ScenarioName, ReportName)
+    elif project:
+        for dx, projectName, ScenarioName, ReportName in reportList[1:]:
+            if projectName == project:
+                if scenario is None or scenario == ScenarioName:
+                    if report is None or report == ReportName:
+                        printReport(logsPath, projectName, ScenarioName, ReportName)
+
+def printReport(logsPath, project, scenario, report):
+    fileName = "{}__{}__reports.log".format(project, scenario)
+    filePath = os.path.join(logsPath, fileName)
+    assert os.path.exists(filePath), "Report {} for project: {} / scenario: {} does not exist".format(
+        report, project, scenario
+    )
+
+    tableData = []
+    Keys = {}
+    with open(os.path.join(logsPath, fileName), "r") as f:
+        for l in f:
+            data = json.loads(l.strip())
+            if "Report" not in data:
+                print("Skipping {}. Not a valid pydss report.".format(fileName))
+                return None
+            elif data["Report"] == report:
+                if report not in Keys:
+                    Keys[report] = list(data.keys())
+                    Keys[report] = [x for x in Keys[report] if x != "Report"]
+                values = []
+                for k in Keys[report]:
+                    values.append(data[k])
+                tableData.append(values)
+    tableData.insert(0, Keys[report])
+    Table = SingleTable(tableData, title="{} report (Project: {}, Scenario: {})".format(
+        report, project, scenario
+    ))
+    print("")
+    print(Table.table)
+    return
+
+def getAvailableReports(logsPath):
+    logFiles = list(filter(lambda x: '.log' in x, os.listdir(logsPath)))
+    reportFiles = [x for x in logFiles if "__reports" in x]
+    headings = ["#", "Project", "Scenario", "Report"]
+    reportList = [headings]
+    reportNumber = 0
+    for report in reportFiles:
+        project, scenario, _ = report.split("__")
+        print(reportNumber, project, scenario, report)
+        reportTypes = getReportTypes(logsPath, report)
+        if reportTypes is not None:
+            for reportTypes in reportTypes:
+                reportNumber += 1
+                reportList.append([reportNumber, project, scenario, reportTypes])
+    return reportList
+
+
+def getReportTypes(logsPath, reportFile):
+    fileName = os.path.join(logsPath, reportFile)
+    print(fileName)
+    f = open(fileName, "r")
+    lines = f.readlines()
+    reportTypes = []
+    for l in lines:
+        data = json.loads(l.strip())
+        if "Report" not in data:
+            print("Skipping {}. Not a valid pydss report.".format(fileName))
+            return None
+        else:
+            if data["Report"] not in reportTypes:
+                reportTypes.append(data["Report"])
+    return reportTypes
```

### Comparing `nrel_pydss-3.1.3/src/pydss/cli/run.py` & `nrel_pydss-3.1.4/src/pydss/cli/run.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,116 +1,116 @@
-"""
-CLI to run a pydss project
-"""
-
-from pathlib import Path
-import ast
-import sys
-
-from loguru import logger
-import click
-
-from pydss.utils.utils import get_cli_string, make_human_readable_size
-from pydss.common import SIMULATION_SETTINGS_FILENAME
-from pydss.pydss_project import PyDssProject
-
-@click.argument("project-path", type=click.Path(exists=True))
-@click.option(
-    "-o", "--options",
-    help="dict-formatted simulation settings that override the config file. " \
-            "Example:  pydss run ./project --options \"{\\\"Exports\\\": {\\\"Export Compression\\\": \\\"true\\\"}}\"",
-)
-
-@click.option(
-    "-s", "--simulations-file",
-    required=False,
-    default = SIMULATION_SETTINGS_FILENAME,
-    show_default=True,
-    help="scenario toml file to run (over rides default)",
-)
-
-@click.option(
-    "-t", "--tar-project",
-    is_flag=True,
-    default=False,
-    show_default=True,
-    help="Tar project files after successful execution."
-)
-@click.option(
-    "-z", "--zip-project",
-    is_flag=True,
-    default=False,
-    show_default=True,
-    help="Zip project files after successful execution."
-)
-@click.option(
-    "--verbose",
-    is_flag=True,
-    default=False,
-    show_default=True,
-    help="Enable verbose log output."
-)
-@click.option(
-    "--dry-run",
-    is_flag=True,
-    default=False,
-    show_default=True,
-    help="Dry run for getting estimated space."
-)
-@click.command()
-
-def run(project_path, options=None, tar_project=False, zip_project=False, verbose=False, simulations_file=None, dry_run=False):
-    """Run a pydss simulation."""
-    project_path = Path(project_path)
-    settings = PyDssProject.load_simulation_settings(project_path, simulations_file)
-    if verbose:
-        # Override the config file.
-        settings.logging.logging_level = "DEBUG"
-
-    filename = None
-    console_level = "INFO" 
-    file_level = "INFO"
-    if not settings.logging.enable_console:
-        console_level = "ERROR"
-    if verbose:
-        console_level = "DEBUG"
-        file_level = "DEBUG"
-    if settings.logging.enable_file:
-        logs_path = project_path / "Logs"
-        if not logs_path.exists():
-            logger.error("Logs path %s does not exist", logs_path)
-            sys.exit(1)
-        filename = logs_path / "pydss.log"
-
-    logger.level(console_level)
-    if filename:
-        logger.add(filename)
-
-    logger.info(f"CLI: [{get_cli_string()}]",)
-
-    if options is not None:
-        options = ast.literal_eval(options)
-        if not isinstance(options, dict):
-            logger.error("options are invalid: %s", options)
-            sys.exit(1)
-
-    project = PyDssProject.load_project(project_path, options=options, simulation_file=simulations_file)
-    project.run(tar_project=tar_project, zip_project=zip_project, dry_run=dry_run)
-
-    if dry_run:
-        maxlen = max([len(k) for k in project.estimated_space.keys()])
-        if len("ScenarioName") > maxlen:
-            maxlen = len("ScenarioName")
-        template = "{:<{width}}   {}\n".format("ScenarioName", "EstimatedSpace", width=maxlen)
-        
-        total_size = 0
-        for k, v in project.estimated_space.items():
-            total_size += v
-            vstr = make_human_readable_size(v)
-            template += "{:<{width}} : {}\n".format(k, vstr, width=maxlen)
-        template = template.strip()
-        logger.info(template)
-        logger.info("-"*30)
-        logger.info(f"TotalSpace: {make_human_readable_size(total_size)}")
-        logger.info("="*30)
-        logger.info("Note: compression may reduce the size by ~90% depending on the data.")
-
+"""
+CLI to run a pydss project
+"""
+
+from pathlib import Path
+import ast
+import sys
+
+from loguru import logger
+import click
+
+from pydss.utils.utils import get_cli_string, make_human_readable_size
+from pydss.common import SIMULATION_SETTINGS_FILENAME
+from pydss.pydss_project import PyDssProject
+
+@click.argument("project-path", type=click.Path(exists=True))
+@click.option(
+    "-o", "--options",
+    help="dict-formatted simulation settings that override the config file. " \
+            "Example:  pydss run ./project --options \"{\\\"Exports\\\": {\\\"Export Compression\\\": \\\"true\\\"}}\"",
+)
+
+@click.option(
+    "-s", "--simulations-file",
+    required=False,
+    default = SIMULATION_SETTINGS_FILENAME,
+    show_default=True,
+    help="scenario toml file to run (over rides default)",
+)
+
+@click.option(
+    "-t", "--tar-project",
+    is_flag=True,
+    default=False,
+    show_default=True,
+    help="Tar project files after successful execution."
+)
+@click.option(
+    "-z", "--zip-project",
+    is_flag=True,
+    default=False,
+    show_default=True,
+    help="Zip project files after successful execution."
+)
+@click.option(
+    "--verbose",
+    is_flag=True,
+    default=False,
+    show_default=True,
+    help="Enable verbose log output."
+)
+@click.option(
+    "--dry-run",
+    is_flag=True,
+    default=False,
+    show_default=True,
+    help="Dry run for getting estimated space."
+)
+@click.command()
+
+def run(project_path, options=None, tar_project=False, zip_project=False, verbose=False, simulations_file=None, dry_run=False):
+    """Run a pydss simulation."""
+    project_path = Path(project_path)
+    settings = PyDssProject.load_simulation_settings(project_path, simulations_file)
+    if verbose:
+        # Override the config file.
+        settings.logging.logging_level = "DEBUG"
+
+    filename = None
+    console_level = "INFO" 
+    file_level = "INFO"
+    if not settings.logging.enable_console:
+        console_level = "ERROR"
+    if verbose:
+        console_level = "DEBUG"
+        file_level = "DEBUG"
+    if settings.logging.enable_file:
+        logs_path = project_path / "Logs"
+        if not logs_path.exists():
+            logger.error("Logs path %s does not exist", logs_path)
+            sys.exit(1)
+        filename = logs_path / "pydss.log"
+
+    logger.level(console_level)
+    if filename:
+        logger.add(filename)
+
+    logger.info(f"CLI: [{get_cli_string()}]",)
+
+    if options is not None:
+        options = ast.literal_eval(options)
+        if not isinstance(options, dict):
+            logger.error("options are invalid: %s", options)
+            sys.exit(1)
+
+    project = PyDssProject.load_project(project_path, options=options, simulation_file=simulations_file)
+    project.run(tar_project=tar_project, zip_project=zip_project, dry_run=dry_run)
+
+    if dry_run:
+        maxlen = max([len(k) for k in project.estimated_space.keys()])
+        if len("ScenarioName") > maxlen:
+            maxlen = len("ScenarioName")
+        template = "{:<{width}}   {}\n".format("ScenarioName", "EstimatedSpace", width=maxlen)
+        
+        total_size = 0
+        for k, v in project.estimated_space.items():
+            total_size += v
+            vstr = make_human_readable_size(v)
+            template += "{:<{width}} : {}\n".format(k, vstr, width=maxlen)
+        template = template.strip()
+        logger.info(template)
+        logger.info("-"*30)
+        logger.info(f"TotalSpace: {make_human_readable_size(total_size)}")
+        logger.info("="*30)
+        logger.info("Note: compression may reduce the size by ~90% depending on the data.")
+
```

### Comparing `nrel_pydss-3.1.3/src/pydss/cli/run_server.py` & `nrel_pydss-3.1.4/src/pydss/cli/run_server.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-"""
-CLI to run the pydss server
-"""
-
-from loguru import logger
-from aiohttp import web
-import click
-
-from pydss.api.server import pydss_server
-
-@click.option(
-    "-p", "--port",
-    default=9090,
-    show_default=True,
-    help="Socket port for the server",
-)
-
-@click.command()
-def serve(ip="127.0.0.1",port=9090):
-    """Run a pydss RESTful API server."""
-    FORMAT = '%(asctime)s - %(levelname)s - %(message)s'
-    logger.level("DEBUG")
-    pydss = pydss_server(ip, port)
+"""
+CLI to run the pydss server
+"""
+
+from loguru import logger
+from aiohttp import web
+import click
+
+from pydss.api.server import pydss_server
+
+@click.option(
+    "-p", "--port",
+    default=9090,
+    show_default=True,
+    help="Socket port for the server",
+)
+
+@click.command()
+def serve(ip="127.0.0.1",port=9090):
+    """Run a pydss RESTful API server."""
+    FORMAT = '%(asctime)s - %(levelname)s - %(message)s'
+    logger.level("DEBUG")
+    pydss = pydss_server(ip, port)
     web.run_app(pydss.app, port=port)
```

### Comparing `nrel_pydss-3.1.3/src/pydss/defaults/ExportLists/ExportMode-byClass.toml` & `nrel_pydss-3.1.4/src/pydss/defaults/ExportLists/ExportMode-byClass.toml`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,27 +1,27 @@
-[Loads]
-Publish = [ "Powers",]
-NoPublish = [ "VoltagesMagAng",]
-
-[PVSystems]
-Publish = [ "Powers", "VoltagesMagAng",]
-NoPublish = []
-
-[Storages]
-Publish = []
-NoPublish = [ "Powers", "VoltagesMagAng", "%stored",]
-
-[Buses]
-Publish = []
-NoPublish = [ "puVmagAngle", "Distance",]
-
-[Circuits]
-Publish = [ "TotalPower", "LineLosses", "Losses", "SubstationLosses",]
-NoPublish = []
-
-[Lines]
-Publish = []
-NoPublish = [ "CurrentsMagAng", "normamps",]
-
-[Transformers]
-Publish = []
-NoPublish = [ "CurrentsMagAng", "normamps", "taps",]
+[Loads]
+Publish = [ "Powers",]
+NoPublish = [ "VoltagesMagAng",]
+
+[PVSystems]
+Publish = [ "Powers", "VoltagesMagAng",]
+NoPublish = []
+
+[Storages]
+Publish = []
+NoPublish = [ "Powers", "VoltagesMagAng", "%stored",]
+
+[Buses]
+Publish = []
+NoPublish = [ "puVmagAngle", "Distance",]
+
+[Circuits]
+Publish = [ "TotalPower", "LineLosses", "Losses", "SubstationLosses",]
+NoPublish = []
+
+[Lines]
+Publish = []
+NoPublish = [ "CurrentsMagAng", "normamps",]
+
+[Transformers]
+Publish = []
+NoPublish = [ "CurrentsMagAng", "normamps", "taps",]
```

### Comparing `nrel_pydss-3.1.3/src/pydss/defaults/ExportLists/Exports.toml` & `nrel_pydss-3.1.4/src/pydss/defaults/ExportLists/Exports.toml`

 * *Files 27% similar despite different names*

```diff
@@ -1,41 +1,41 @@
-[[Generators]]
-property = "Powers"
-sample_interval = 1
-publish = true
-store_values_type = "all"
-
-[[Loads]]
-property = "Powers"
-sample_interval = 1
-publish = true
-store_values_type = "all"
-
-[[PVSystems]]
-property = "Powers"
-sample_interval = 1
-publish = true
-store_values_type = "all"
-
-[[Storages]]
-property = "Powers"
-sample_interval = 1
-publish = true
-store_values_type = "all"
-
-[[Buses]]
-property = "puVmagAngle"
-sample_interval = 1
-publish = true
-store_values_type = "all"
-
-[[Circuits]]
-property = "TotalPower"
-sample_interval = 1
-publish = true
-store_values_type = "all"
-
-[[Lines]]
-property = "VoltagesMagAng"
-sample_interval = 1
-publish = true
-store_values_type = "all"
+[[Generators]]
+property = "Powers"
+sample_interval = 1
+publish = true
+store_values_type = "all"
+
+[[Loads]]
+property = "Powers"
+sample_interval = 1
+publish = true
+store_values_type = "all"
+
+[[PVSystems]]
+property = "Powers"
+sample_interval = 1
+publish = true
+store_values_type = "all"
+
+[[Storages]]
+property = "Powers"
+sample_interval = 1
+publish = true
+store_values_type = "all"
+
+[[Buses]]
+property = "puVmagAngle"
+sample_interval = 1
+publish = true
+store_values_type = "all"
+
+[[Circuits]]
+property = "TotalPower"
+sample_interval = 1
+publish = true
+store_values_type = "all"
+
+[[Lines]]
+property = "VoltageMagAng"
+sample_interval = 1
+publish = true
+store_values_type = "all"
```

### Comparing `nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/PvDynamic.toml` & `nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/PvDynamic.toml`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,47 +1,47 @@
-["Generator.pvgui_mpx000460267"]
-DER_ID = "10"
-STEADY_STATE  = false
-RATED_POWER_AC_VA = 1000.0
-RATED_POWER_DC_WATTS = 1000.0
-MPPT_ENABLE = true
-RAMP_ENABLE = true
-VOLT_VAR_ENABLE = true
-LVRT_ENABLE = true
-HVRT_ENABLE = true
-LFRT_ENABLE = false
-DO_EXTRA_CALCULATIONS = true
-use_frequency_estimate = true
-jacFlag = true
-UPDATE_MODULE_PARAMETRS = false
-Np = 500
-Ns = 400
-Vdcmpp0 = 850.0
-Vdcmpp_max = 850.0
-Vdcmpp_min =  650.0
-UPDATE_INVERTER_PARAMETRS = false
-Vdcrated = 650.0
-Ioverload = 1.1
-Vrmsrated = 197.0
-Iramp_max_gradient_imag = 1.0
-Iramp_max_gradient_real = 1.0
-UPDATE_CIRCUIT_PARAMETRS = false
-Rf_actual = 0.002
-Lf_actual = 0.000025
-C_actual = 3e-05
-Z1_actual_real = 0.0019
-Z1_actual_imag = 0.0561
-R1_actual = 0.0019
-X1_actual = 0.0561
-UPDATE_CONTROLLER_PARAMETRS = false
-Kp_GCC = 12000.0
-Ki_GCC = 2000.0
-Kp_DC = -2.0
-Ki_DC = -10.0
-Kp_Q = 0.2
-Ki_Q = 10.0
-wp = 200000.0
-UPDATE_STEADYSTATE_PARAMETRS = false
-iaI0 = 0.001
-iaR0 = 1.0
-maI0 = 0.0
+["Generator.pvgui_mpx000460267"]
+DER_ID = "10"
+STEADY_STATE  = false
+RATED_POWER_AC_VA = 1000.0
+RATED_POWER_DC_WATTS = 1000.0
+MPPT_ENABLE = true
+RAMP_ENABLE = true
+VOLT_VAR_ENABLE = true
+LVRT_ENABLE = true
+HVRT_ENABLE = true
+LFRT_ENABLE = false
+DO_EXTRA_CALCULATIONS = true
+use_frequency_estimate = true
+jacFlag = true
+UPDATE_MODULE_PARAMETRS = false
+Np = 500
+Ns = 400
+Vdcmpp0 = 850.0
+Vdcmpp_max = 850.0
+Vdcmpp_min =  650.0
+UPDATE_INVERTER_PARAMETRS = false
+Vdcrated = 650.0
+Ioverload = 1.1
+Vrmsrated = 197.0
+Iramp_max_gradient_imag = 1.0
+Iramp_max_gradient_real = 1.0
+UPDATE_CIRCUIT_PARAMETRS = false
+Rf_actual = 0.002
+Lf_actual = 0.000025
+C_actual = 3e-05
+Z1_actual_real = 0.0019
+Z1_actual_imag = 0.0561
+R1_actual = 0.0019
+X1_actual = 0.0561
+UPDATE_CONTROLLER_PARAMETRS = false
+Kp_GCC = 12000.0
+Ki_GCC = 2000.0
+Kp_DC = -2.0
+Ki_DC = -10.0
+Kp_Q = 0.2
+Ki_Q = 10.0
+wp = 200000.0
+UPDATE_STEADYSTATE_PARAMETRS = false
+iaI0 = 0.001
+iaR0 = 1.0
+maI0 = 0.0
 maR0 = 0.89
```

### Comparing `nrel_pydss-3.1.3/src/pydss/defaults/pyControllerList/PvVoltageRideThru.toml` & `nrel_pydss-3.1.4/src/pydss/defaults/pyControllerList/PvVoltageRideThru.toml`

 * *Files 20% similar despite different names*

```diff
@@ -1,159 +1,152 @@
-00000000: 5b4e 4f5f 5652 545d 0d0a 2243 6174 6567  [NO_VRT].."Categ
-00000010: 6f72 7922 203d 2022 7465 7374 220d 0a22  ory" = "test".."
-00000020: 6b56 4122 203d 2034 2e30 0d0a 226d 6178  kVA" = 4.0.."max
-00000030: 4b57 2220 3d20 342e 300d 0a22 4b76 6172  KW" = 4.0.."Kvar
-00000040: 4c69 6d69 7422 093d 2031 2e37 360d 0a22  Limit".= 1.76.."
-00000050: 2550 4375 7469 6e22 203d 2031 302e 300d  %PCutin" = 10.0.
-00000060: 0a22 2550 4375 746f 7574 2220 3d20 3130  ."%PCutout" = 10
-00000070: 2e30 0d0a 2255 6361 6c63 4d6f 6465 2220  .0.."UcalcMode" 
-00000080: 3d20 224d 6178 220d 0a22 5072 696f 7269  = "Max".."Priori
-00000090: 7479 2220 3d20 2245 7175 616c 220d 0a22  ty" = "Equal".."
-000000a0: 456e 6162 6c65 2050 4620 6c69 6d69 7422  Enable PF limit"
-000000b0: 203d 2066 616c 7365 0d0a 2270 664d 696e   = false.."pfMin
-000000c0: 2209 3d20 302e 3935 0d0a 2246 6f6c 6c6f  ".= 0.95.."Follo
-000000d0: 7720 7374 616e 6461 7264 2220 3d20 2231  w standard" = "1
-000000e0: 3534 372d 3230 3033 220d 0a22 5269 6465  547-2003".."Ride
-000000f0: 2d74 6872 6f75 6768 2043 6174 6567 6f72  -through Categor
-00000100: 7922 093d 2022 4361 7465 676f 7279 2049  y".= "Category I
-00000110: 220d 0a22 4f56 3220 2d20 702e 752e 2220  ".."OV2 - p.u." 
-00000120: 3d20 312e 320d 0a22 4f56 3220 4354 202d  = 1.2.."OV2 CT -
-00000130: 2073 6563 2220 3d20 302e 3136 0d0a 224f   sec" = 0.16.."O
-00000140: 5631 202d 2070 2e75 2e22 203d 2031 2e32  V1 - p.u." = 1.2
-00000150: 0d0a 224f 5631 2043 5420 2d20 7365 6322  .."OV1 CT - sec"
-00000160: 203d 2032 2e30 0d0a 2255 5631 202d 2070   = 2.0.."UV1 - p
-00000170: 2e75 2e22 203d 2030 2e38 0d0a 2255 5631  .u." = 0.8.."UV1
-00000180: 2043 5420 2d20 7365 6322 203d 2032 2e30   CT - sec" = 2.0
-00000190: 0d0a 2255 5632 202d 2070 2e75 2e22 203d  .."UV2 - p.u." =
-000001a0: 2030 2e35 0d0a 2255 5632 2043 5420 2d20   0.5.."UV2 CT - 
-000001b0: 7365 6322 203d 2030 2e31 360d 0a22 5265  sec" = 0.16.."Re
-000001c0: 636f 6e6e 6563 7420 6465 6164 7469 6d65  connect deadtime
-000001d0: 202d 2073 6563 2220 3d20 3330 3030 2e30   - sec" = 3000.0
-000001e0: 0d0a 2252 6563 6f6e 6e65 6374 2050 6d61  .."Reconnect Pma
-000001f0: 7820 7469 6d65 202d 2073 6563 2220 3d20  x time - sec" = 
-00000200: 3330 302e 300d 0a22 5065 726d 6973 7369  300.0.."Permissi
-00000210: 7665 206f 7065 7261 7469 6f6e 2220 3d20  ve operation" = 
-00000220: 2243 7572 7265 6e74 206c 696d 6974 6564  "Current limited
-00000230: 220d 0a22 4d61 7920 7472 6970 206f 7065  ".."May trip ope
-00000240: 7261 7469 6f6e 2220 3d20 2254 7269 7022  ration" = "Trip"
-00000250: 0d0a 224d 756c 7469 706c 6520 6469 7374  .."Multiple dist
-00000260: 7572 6261 6e63 6573 2220 3d20 2254 7269  urbances" = "Tri
-00000270: 7022 0d0a 0d0a 5b31 3534 375f 4341 545f  p"....[1547_CAT_
-00000280: 495d 0d0a 2243 6174 6567 6f72 7922 203d  I].."Category" =
-00000290: 2022 7465 7374 220d 0a22 6b56 4122 203d   "test".."kVA" =
-000002a0: 2034 2e30 0d0a 226d 6178 4b57 2220 3d20   4.0.."maxKW" = 
-000002b0: 342e 300d 0a22 4b76 6172 4c69 6d69 7422  4.0.."KvarLimit"
-000002c0: 093d 2031 2e37 360d 0a22 2550 4375 7469  .= 1.76.."%PCuti
-000002d0: 6e22 203d 2030 2e30 0d0a 2225 5043 7574  n" = 0.0.."%PCut
-000002e0: 6f75 7422 203d 2030 2e30 0d0a 2255 6361  out" = 0.0.."Uca
-000002f0: 6c63 4d6f 6465 2220 3d20 224d 6178 220d  lcMode" = "Max".
-00000300: 0a22 5072 696f 7269 7479 2220 3d20 2245  ."Priority" = "E
-00000310: 7175 616c 220d 0a22 456e 6162 6c65 2050  qual".."Enable P
-00000320: 4620 6c69 6d69 7422 203d 2066 616c 7365  F limit" = false
-00000330: 0d0a 2270 664d 696e 2209 3d20 302e 3935  .."pfMin".= 0.95
-00000340: 0d0a 2246 6f6c 6c6f 7720 7374 616e 6461  .."Follow standa
-00000350: 7264 2220 3d20 2231 3534 372d 3230 3138  rd" = "1547-2018
-00000360: 220d 0a22 5269 6465 2d74 6872 6f75 6768  ".."Ride-through
-00000370: 2043 6174 6567 6f72 7922 093d 2022 4361   Category".= "Ca
-00000380: 7465 676f 7279 2049 220d 0a22 4f56 3220  tegory I".."OV2 
-00000390: 2d20 702e 752e 2220 3d20 312e 320d 0a22  - p.u." = 1.2.."
-000003a0: 4f56 3220 4354 202d 2073 6563 2220 3d20  OV2 CT - sec" = 
-000003b0: 302e 3136 0d0a 224f 5631 202d 2070 2e75  0.16.."OV1 - p.u
-000003c0: 2e22 203d 2031 2e31 0d0a 224f 5631 2043  ." = 1.1.."OV1 C
-000003d0: 5420 2d20 7365 6322 203d 2032 2e30 0d0a  T - sec" = 2.0..
-000003e0: 2255 5631 202d 2070 2e75 2e22 203d 2030  "UV1 - p.u." = 0
-000003f0: 2e37 0d0a 2255 5631 2043 5420 2d20 7365  .7.."UV1 CT - se
-00000400: 6322 203d 2032 2e30 0d0a 2255 5632 202d  c" = 2.0.."UV2 -
-00000410: 2070 2e75 2e22 203d 2030 2e34 350d 0a22   p.u." = 0.45.."
-00000420: 5556 3220 4354 202d 2073 6563 2220 3d20  UV2 CT - sec" = 
-00000430: 302e 3136 0d0a 2252 6563 6f6e 6e65 6374  0.16.."Reconnect
-00000440: 2064 6561 6474 696d 6520 2d20 7365 6322   deadtime - sec"
-00000450: 203d 2033 3030 302e 300d 0a22 5265 636f   = 3000.0.."Reco
-00000460: 6e6e 6563 7420 506d 6178 2074 696d 6520  nnect Pmax time 
-00000470: 2d20 7365 6322 203d 2033 3030 2e30 0d0a  - sec" = 300.0..
-00000480: 2250 6572 6d69 7373 6976 6520 6f70 6572  "Permissive oper
-00000490: 6174 696f 6e22 203d 2022 4375 7272 656e  ation" = "Curren
-000004a0: 7420 6c69 6d69 7465 6422 0d0a 224d 6179  t limited".."May
-000004b0: 2074 7269 7020 6f70 6572 6174 696f 6e22   trip operation"
-000004c0: 203d 2022 5472 6970 220d 0a22 4d75 6c74   = "Trip".."Mult
-000004d0: 6970 6c65 2064 6973 7475 7262 616e 6365  iple disturbance
-000004e0: 7322 203d 2022 5472 6970 220d 0a0d 0a0d  s" = "Trip".....
-000004f0: 0a5b 3135 3437 5f43 4154 5f49 495d 0d0a  .[1547_CAT_II]..
-00000500: 2243 6174 6567 6f72 7922 203d 2022 7465  "Category" = "te
-00000510: 7374 220d 0a22 6b56 4122 203d 2034 2e30  st".."kVA" = 4.0
-00000520: 0d0a 226d 6178 4b57 2220 3d20 342e 300d  .."maxKW" = 4.0.
-00000530: 0a22 4b76 6172 4c69 6d69 7422 093d 2031  ."KvarLimit".= 1
-00000540: 2e37 360d 0a22 2550 4375 7469 6e22 203d  .76.."%PCutin" =
-00000550: 2030 2e30 0d0a 2225 5043 7574 6f75 7422   0.0.."%PCutout"
-00000560: 203d 2030 2e30 0d0a 2255 6361 6c63 4d6f   = 0.0.."UcalcMo
-00000570: 6465 2220 3d20 224d 6178 220d 0a22 5072  de" = "Max".."Pr
-00000580: 696f 7269 7479 2220 3d20 2245 7175 616c  iority" = "Equal
-00000590: 220d 0a22 456e 6162 6c65 2050 4620 6c69  ".."Enable PF li
-000005a0: 6d69 7422 203d 2066 616c 7365 0d0a 2270  mit" = false.."p
-000005b0: 664d 696e 2209 3d20 302e 3935 0d0a 2246  fMin".= 0.95.."F
-000005c0: 6f6c 6c6f 7720 7374 616e 6461 7264 2220  ollow standard" 
-000005d0: 3d20 2231 3534 372d 3230 3138 220d 0a22  = "1547-2018".."
-000005e0: 5269 6465 2d74 6872 6f75 6768 2043 6174  Ride-through Cat
-000005f0: 6567 6f72 7922 093d 2022 4361 7465 676f  egory".= "Catego
-00000600: 7279 2049 4922 0d0a 224f 5632 202d 2070  ry II".."OV2 - p
-00000610: 2e75 2e22 203d 2031 2e32 0d0a 224f 5632  .u." = 1.2.."OV2
-00000620: 2043 5420 2d20 7365 6322 203d 2030 2e31   CT - sec" = 0.1
-00000630: 360d 0a22 4f56 3120 2d20 702e 752e 2220  6.."OV1 - p.u." 
-00000640: 3d20 312e 310d 0a22 4f56 3120 4354 202d  = 1.1.."OV1 CT -
-00000650: 2073 6563 2220 3d20 322e 300d 0a22 5556   sec" = 2.0.."UV
-00000660: 3120 2d20 702e 752e 2220 3d20 302e 370d  1 - p.u." = 0.7.
-00000670: 0a22 5556 3120 4354 202d 2073 6563 2220  ."UV1 CT - sec" 
-00000680: 3d20 3130 2e30 0d0a 2255 5632 202d 2070  = 10.0.."UV2 - p
-00000690: 2e75 2e22 203d 2030 2e34 350d 0a22 5556  .u." = 0.45.."UV
-000006a0: 3220 4354 202d 2073 6563 2220 3d20 302e  2 CT - sec" = 0.
-000006b0: 3136 0d0a 2252 6563 6f6e 6e65 6374 2064  16.."Reconnect d
-000006c0: 6561 6474 696d 6520 2d20 7365 6322 203d  eadtime - sec" =
-000006d0: 2033 3030 302e 300d 0a22 5265 636f 6e6e   3000.0.."Reconn
-000006e0: 6563 7420 506d 6178 2074 696d 6520 2d20  ect Pmax time - 
-000006f0: 7365 6322 203d 2033 3030 2e30 0d0a 2250  sec" = 300.0.."P
-00000700: 6572 6d69 7373 6976 6520 6f70 6572 6174  ermissive operat
-00000710: 696f 6e22 203d 2022 4375 7272 656e 7420  ion" = "Current 
-00000720: 6c69 6d69 7465 6422 0d0a 224d 6179 2074  limited".."May t
-00000730: 7269 7020 6f70 6572 6174 696f 6e22 203d  rip operation" =
-00000740: 2022 5472 6970 220d 0a22 4d75 6c74 6970   "Trip".."Multip
-00000750: 6c65 2064 6973 7475 7262 616e 6365 7322  le disturbances"
-00000760: 203d 2022 5472 6970 220d 0a0d 0a0d 0a5b   = "Trip"......[
-00000770: 3135 3437 5f43 4154 5f49 4949 5d0d 0a22  1547_CAT_III].."
-00000780: 4361 7465 676f 7279 2220 3d20 2274 6573  Category" = "tes
-00000790: 7422 0d0a 226b 5641 2220 3d20 342e 300d  t".."kVA" = 4.0.
-000007a0: 0a22 6d61 784b 5722 203d 2034 2e30 0d0a  ."maxKW" = 4.0..
-000007b0: 224b 7661 724c 696d 6974 2209 3d20 312e  "KvarLimit".= 1.
-000007c0: 3736 0d0a 2225 5043 7574 696e 2220 3d20  76.."%PCutin" = 
-000007d0: 300d 0a22 2550 4375 746f 7574 2220 3d20  0.."%PCutout" = 
-000007e0: 300d 0a22 5563 616c 634d 6f64 6522 203d  0.."UcalcMode" =
-000007f0: 2022 4d61 7822 0d0a 2250 7269 6f72 6974   "Max".."Priorit
-00000800: 7922 203d 2022 4571 7561 6c22 0d0a 2245  y" = "Equal".."E
-00000810: 6e61 626c 6520 5046 206c 696d 6974 2220  nable PF limit" 
-00000820: 3d20 6661 6c73 650d 0a22 7066 4d69 6e22  = false.."pfMin"
-00000830: 093d 2030 2e39 350d 0a22 466f 6c6c 6f77  .= 0.95.."Follow
-00000840: 2073 7461 6e64 6172 6422 203d 2022 3135   standard" = "15
-00000850: 3437 2d32 3031 3822 0d0a 2252 6964 652d  47-2018".."Ride-
-00000860: 7468 726f 7567 6820 4361 7465 676f 7279  through Category
-00000870: 2209 3d20 2243 6174 6567 6f72 7920 4949  ".= "Category II
-00000880: 4922 0d0a 224f 5632 202d 2070 2e75 2e22  I".."OV2 - p.u."
-00000890: 203d 2031 2e32 0d0a 224f 5632 2043 5420   = 1.2.."OV2 CT 
-000008a0: 2d20 7365 6322 203d 2030 2e31 360d 0a22  - sec" = 0.16.."
-000008b0: 4f56 3120 2d20 702e 752e 2220 3d20 312e  OV1 - p.u." = 1.
-000008c0: 310d 0a22 4f56 3120 4354 202d 2073 6563  1.."OV1 CT - sec
-000008d0: 2220 3d20 3133 2e30 0d0a 2255 5631 202d  " = 13.0.."UV1 -
-000008e0: 2070 2e75 2e22 203d 2030 2e38 380d 0a22   p.u." = 0.88.."
-000008f0: 5556 3120 4354 202d 2073 6563 2220 3d20  UV1 CT - sec" = 
-00000900: 3231 2e30 0d0a 2255 5632 202d 2070 2e75  21.0.."UV2 - p.u
-00000910: 2e22 203d 2030 2e35 0d0a 2255 5632 2043  ." = 0.5.."UV2 C
-00000920: 5420 2d20 7365 6322 203d 2032 2e30 0d0a  T - sec" = 2.0..
-00000930: 2252 6563 6f6e 6e65 6374 2064 6561 6474  "Reconnect deadt
-00000940: 696d 6520 2d20 7365 6322 203d 2033 3030  ime - sec" = 300
-00000950: 302e 300d 0a22 5265 636f 6e6e 6563 7420  0.0.."Reconnect 
-00000960: 506d 6178 2074 696d 6520 2d20 7365 6322  Pmax time - sec"
-00000970: 203d 2033 3030 2e30 0d0a 2250 6572 6d69   = 300.0.."Permi
-00000980: 7373 6976 6520 6f70 6572 6174 696f 6e22  ssive operation"
-00000990: 203d 2022 4375 7272 656e 7420 6c69 6d69   = "Current limi
-000009a0: 7465 6422 0d0a 224d 6179 2074 7269 7020  ted".."May trip 
-000009b0: 6f70 6572 6174 696f 6e22 203d 2022 5472  operation" = "Tr
-000009c0: 6970 220d 0a22 4d75 6c74 6970 6c65 2064  ip".."Multiple d
-000009d0: 6973 7475 7262 616e 6365 7322 203d 2022  isturbances" = "
-000009e0: 5472 6970 22                             Trip"
+00000000: 5b4e 4f5f 5652 545d 0a22 4361 7465 676f  [NO_VRT]."Catego
+00000010: 7279 2220 3d20 2274 6573 7422 0a22 6b56  ry" = "test"."kV
+00000020: 4122 203d 2034 2e30 0a22 6d61 784b 5722  A" = 4.0."maxKW"
+00000030: 203d 2034 2e30 0a22 4b76 6172 4c69 6d69   = 4.0."KvarLimi
+00000040: 7422 093d 2031 2e37 360a 2225 5043 7574  t".= 1.76."%PCut
+00000050: 696e 2220 3d20 3130 2e30 0a22 2550 4375  in" = 10.0."%PCu
+00000060: 746f 7574 2220 3d20 3130 2e30 0a22 5563  tout" = 10.0."Uc
+00000070: 616c 634d 6f64 6522 203d 2022 4d61 7822  alcMode" = "Max"
+00000080: 0a22 5072 696f 7269 7479 2220 3d20 2245  ."Priority" = "E
+00000090: 7175 616c 220a 2245 6e61 626c 6520 5046  qual"."Enable PF
+000000a0: 206c 696d 6974 2220 3d20 6661 6c73 650a   limit" = false.
+000000b0: 2270 664d 696e 2209 3d20 302e 3935 0a22  "pfMin".= 0.95."
+000000c0: 466f 6c6c 6f77 2073 7461 6e64 6172 6422  Follow standard"
+000000d0: 203d 2022 3135 3437 2d32 3030 3322 0a22   = "1547-2003"."
+000000e0: 5269 6465 2d74 6872 6f75 6768 2043 6174  Ride-through Cat
+000000f0: 6567 6f72 7922 093d 2022 4361 7465 676f  egory".= "Catego
+00000100: 7279 2049 220a 224f 5632 202d 2070 2e75  ry I"."OV2 - p.u
+00000110: 2e22 203d 2031 2e32 0a22 4f56 3220 4354  ." = 1.2."OV2 CT
+00000120: 202d 2073 6563 2220 3d20 302e 3136 0a22   - sec" = 0.16."
+00000130: 4f56 3120 2d20 702e 752e 2220 3d20 312e  OV1 - p.u." = 1.
+00000140: 320a 224f 5631 2043 5420 2d20 7365 6322  2."OV1 CT - sec"
+00000150: 203d 2032 2e30 0a22 5556 3120 2d20 702e   = 2.0."UV1 - p.
+00000160: 752e 2220 3d20 302e 380a 2255 5631 2043  u." = 0.8."UV1 C
+00000170: 5420 2d20 7365 6322 203d 2032 2e30 0a22  T - sec" = 2.0."
+00000180: 5556 3220 2d20 702e 752e 2220 3d20 302e  UV2 - p.u." = 0.
+00000190: 350a 2255 5632 2043 5420 2d20 7365 6322  5."UV2 CT - sec"
+000001a0: 203d 2030 2e31 360a 2252 6563 6f6e 6e65   = 0.16."Reconne
+000001b0: 6374 2064 6561 6474 696d 6520 2d20 7365  ct deadtime - se
+000001c0: 6322 203d 2033 3030 302e 300a 2252 6563  c" = 3000.0."Rec
+000001d0: 6f6e 6e65 6374 2050 6d61 7820 7469 6d65  onnect Pmax time
+000001e0: 202d 2073 6563 2220 3d20 3330 302e 300a   - sec" = 300.0.
+000001f0: 2250 6572 6d69 7373 6976 6520 6f70 6572  "Permissive oper
+00000200: 6174 696f 6e22 203d 2022 4375 7272 656e  ation" = "Curren
+00000210: 7420 6c69 6d69 7465 6422 0a22 4d61 7920  t limited"."May 
+00000220: 7472 6970 206f 7065 7261 7469 6f6e 2220  trip operation" 
+00000230: 3d20 2254 7269 7022 0a22 4d75 6c74 6970  = "Trip"."Multip
+00000240: 6c65 2064 6973 7475 7262 616e 6365 7322  le disturbances"
+00000250: 203d 2022 5472 6970 220a 0a5b 3135 3437   = "Trip"..[1547
+00000260: 5f43 4154 5f49 5d0a 2243 6174 6567 6f72  _CAT_I]."Categor
+00000270: 7922 203d 2022 7465 7374 220a 226b 5641  y" = "test"."kVA
+00000280: 2220 3d20 342e 300a 226d 6178 4b57 2220  " = 4.0."maxKW" 
+00000290: 3d20 342e 300a 224b 7661 724c 696d 6974  = 4.0."KvarLimit
+000002a0: 2209 3d20 312e 3736 0a22 2550 4375 7469  ".= 1.76."%PCuti
+000002b0: 6e22 203d 2030 2e30 0a22 2550 4375 746f  n" = 0.0."%PCuto
+000002c0: 7574 2220 3d20 302e 300a 2255 6361 6c63  ut" = 0.0."Ucalc
+000002d0: 4d6f 6465 2220 3d20 224d 6178 220a 2250  Mode" = "Max"."P
+000002e0: 7269 6f72 6974 7922 203d 2022 4571 7561  riority" = "Equa
+000002f0: 6c22 0a22 456e 6162 6c65 2050 4620 6c69  l"."Enable PF li
+00000300: 6d69 7422 203d 2066 616c 7365 0a22 7066  mit" = false."pf
+00000310: 4d69 6e22 093d 2030 2e39 350a 2246 6f6c  Min".= 0.95."Fol
+00000320: 6c6f 7720 7374 616e 6461 7264 2220 3d20  low standard" = 
+00000330: 2231 3534 372d 3230 3138 220a 2252 6964  "1547-2018"."Rid
+00000340: 652d 7468 726f 7567 6820 4361 7465 676f  e-through Catego
+00000350: 7279 2209 3d20 2243 6174 6567 6f72 7920  ry".= "Category 
+00000360: 4922 0a22 4f56 3220 2d20 702e 752e 2220  I"."OV2 - p.u." 
+00000370: 3d20 312e 320a 224f 5632 2043 5420 2d20  = 1.2."OV2 CT - 
+00000380: 7365 6322 203d 2030 2e31 360a 224f 5631  sec" = 0.16."OV1
+00000390: 202d 2070 2e75 2e22 203d 2031 2e31 0a22   - p.u." = 1.1."
+000003a0: 4f56 3120 4354 202d 2073 6563 2220 3d20  OV1 CT - sec" = 
+000003b0: 322e 300a 2255 5631 202d 2070 2e75 2e22  2.0."UV1 - p.u."
+000003c0: 203d 2030 2e37 0a22 5556 3120 4354 202d   = 0.7."UV1 CT -
+000003d0: 2073 6563 2220 3d20 322e 300a 2255 5632   sec" = 2.0."UV2
+000003e0: 202d 2070 2e75 2e22 203d 2030 2e34 350a   - p.u." = 0.45.
+000003f0: 2255 5632 2043 5420 2d20 7365 6322 203d  "UV2 CT - sec" =
+00000400: 2030 2e31 360a 2252 6563 6f6e 6e65 6374   0.16."Reconnect
+00000410: 2064 6561 6474 696d 6520 2d20 7365 6322   deadtime - sec"
+00000420: 203d 2033 3030 302e 300a 2252 6563 6f6e   = 3000.0."Recon
+00000430: 6e65 6374 2050 6d61 7820 7469 6d65 202d  nect Pmax time -
+00000440: 2073 6563 2220 3d20 3330 302e 300a 2250   sec" = 300.0."P
+00000450: 6572 6d69 7373 6976 6520 6f70 6572 6174  ermissive operat
+00000460: 696f 6e22 203d 2022 4375 7272 656e 7420  ion" = "Current 
+00000470: 6c69 6d69 7465 6422 0a22 4d61 7920 7472  limited"."May tr
+00000480: 6970 206f 7065 7261 7469 6f6e 2220 3d20  ip operation" = 
+00000490: 2254 7269 7022 0a22 4d75 6c74 6970 6c65  "Trip"."Multiple
+000004a0: 2064 6973 7475 7262 616e 6365 7322 203d   disturbances" =
+000004b0: 2022 5472 6970 220a 0a0a 5b31 3534 375f   "Trip"...[1547_
+000004c0: 4341 545f 4949 5d0a 2243 6174 6567 6f72  CAT_II]."Categor
+000004d0: 7922 203d 2022 7465 7374 220a 226b 5641  y" = "test"."kVA
+000004e0: 2220 3d20 342e 300a 226d 6178 4b57 2220  " = 4.0."maxKW" 
+000004f0: 3d20 342e 300a 224b 7661 724c 696d 6974  = 4.0."KvarLimit
+00000500: 2209 3d20 312e 3736 0a22 2550 4375 7469  ".= 1.76."%PCuti
+00000510: 6e22 203d 2030 2e30 0a22 2550 4375 746f  n" = 0.0."%PCuto
+00000520: 7574 2220 3d20 302e 300a 2255 6361 6c63  ut" = 0.0."Ucalc
+00000530: 4d6f 6465 2220 3d20 224d 6178 220a 2250  Mode" = "Max"."P
+00000540: 7269 6f72 6974 7922 203d 2022 4571 7561  riority" = "Equa
+00000550: 6c22 0a22 456e 6162 6c65 2050 4620 6c69  l"."Enable PF li
+00000560: 6d69 7422 203d 2066 616c 7365 0a22 7066  mit" = false."pf
+00000570: 4d69 6e22 093d 2030 2e39 350a 2246 6f6c  Min".= 0.95."Fol
+00000580: 6c6f 7720 7374 616e 6461 7264 2220 3d20  low standard" = 
+00000590: 2231 3534 372d 3230 3138 220a 2252 6964  "1547-2018"."Rid
+000005a0: 652d 7468 726f 7567 6820 4361 7465 676f  e-through Catego
+000005b0: 7279 2209 3d20 2243 6174 6567 6f72 7920  ry".= "Category 
+000005c0: 4949 220a 224f 5632 202d 2070 2e75 2e22  II"."OV2 - p.u."
+000005d0: 203d 2031 2e32 0a22 4f56 3220 4354 202d   = 1.2."OV2 CT -
+000005e0: 2073 6563 2220 3d20 302e 3136 0a22 4f56   sec" = 0.16."OV
+000005f0: 3120 2d20 702e 752e 2220 3d20 312e 310a  1 - p.u." = 1.1.
+00000600: 224f 5631 2043 5420 2d20 7365 6322 203d  "OV1 CT - sec" =
+00000610: 2032 2e30 0a22 5556 3120 2d20 702e 752e   2.0."UV1 - p.u.
+00000620: 2220 3d20 302e 370a 2255 5631 2043 5420  " = 0.7."UV1 CT 
+00000630: 2d20 7365 6322 203d 2031 302e 300a 2255  - sec" = 10.0."U
+00000640: 5632 202d 2070 2e75 2e22 203d 2030 2e34  V2 - p.u." = 0.4
+00000650: 350a 2255 5632 2043 5420 2d20 7365 6322  5."UV2 CT - sec"
+00000660: 203d 2030 2e31 360a 2252 6563 6f6e 6e65   = 0.16."Reconne
+00000670: 6374 2064 6561 6474 696d 6520 2d20 7365  ct deadtime - se
+00000680: 6322 203d 2033 3030 302e 300a 2252 6563  c" = 3000.0."Rec
+00000690: 6f6e 6e65 6374 2050 6d61 7820 7469 6d65  onnect Pmax time
+000006a0: 202d 2073 6563 2220 3d20 3330 302e 300a   - sec" = 300.0.
+000006b0: 2250 6572 6d69 7373 6976 6520 6f70 6572  "Permissive oper
+000006c0: 6174 696f 6e22 203d 2022 4375 7272 656e  ation" = "Curren
+000006d0: 7420 6c69 6d69 7465 6422 0a22 4d61 7920  t limited"."May 
+000006e0: 7472 6970 206f 7065 7261 7469 6f6e 2220  trip operation" 
+000006f0: 3d20 2254 7269 7022 0a22 4d75 6c74 6970  = "Trip"."Multip
+00000700: 6c65 2064 6973 7475 7262 616e 6365 7322  le disturbances"
+00000710: 203d 2022 5472 6970 220a 0a0a 5b31 3534   = "Trip"...[154
+00000720: 375f 4341 545f 4949 495d 0a22 4361 7465  7_CAT_III]."Cate
+00000730: 676f 7279 2220 3d20 2274 6573 7422 0a22  gory" = "test"."
+00000740: 6b56 4122 203d 2034 2e30 0a22 6d61 784b  kVA" = 4.0."maxK
+00000750: 5722 203d 2034 2e30 0a22 4b76 6172 4c69  W" = 4.0."KvarLi
+00000760: 6d69 7422 093d 2031 2e37 360a 2225 5043  mit".= 1.76."%PC
+00000770: 7574 696e 2220 3d20 300a 2225 5043 7574  utin" = 0."%PCut
+00000780: 6f75 7422 203d 2030 0a22 5563 616c 634d  out" = 0."UcalcM
+00000790: 6f64 6522 203d 2022 4d61 7822 0a22 5072  ode" = "Max"."Pr
+000007a0: 696f 7269 7479 2220 3d20 2245 7175 616c  iority" = "Equal
+000007b0: 220a 2245 6e61 626c 6520 5046 206c 696d  "."Enable PF lim
+000007c0: 6974 2220 3d20 6661 6c73 650a 2270 664d  it" = false."pfM
+000007d0: 696e 2209 3d20 302e 3935 0a22 466f 6c6c  in".= 0.95."Foll
+000007e0: 6f77 2073 7461 6e64 6172 6422 203d 2022  ow standard" = "
+000007f0: 3135 3437 2d32 3031 3822 0a22 5269 6465  1547-2018"."Ride
+00000800: 2d74 6872 6f75 6768 2043 6174 6567 6f72  -through Categor
+00000810: 7922 093d 2022 4361 7465 676f 7279 2049  y".= "Category I
+00000820: 4949 220a 224f 5632 202d 2070 2e75 2e22  II"."OV2 - p.u."
+00000830: 203d 2031 2e32 0a22 4f56 3220 4354 202d   = 1.2."OV2 CT -
+00000840: 2073 6563 2220 3d20 302e 3136 0a22 4f56   sec" = 0.16."OV
+00000850: 3120 2d20 702e 752e 2220 3d20 312e 310a  1 - p.u." = 1.1.
+00000860: 224f 5631 2043 5420 2d20 7365 6322 203d  "OV1 CT - sec" =
+00000870: 2031 332e 300a 2255 5631 202d 2070 2e75   13.0."UV1 - p.u
+00000880: 2e22 203d 2030 2e38 380a 2255 5631 2043  ." = 0.88."UV1 C
+00000890: 5420 2d20 7365 6322 203d 2032 312e 300a  T - sec" = 21.0.
+000008a0: 2255 5632 202d 2070 2e75 2e22 203d 2030  "UV2 - p.u." = 0
+000008b0: 2e35 0a22 5556 3220 4354 202d 2073 6563  .5."UV2 CT - sec
+000008c0: 2220 3d20 322e 300a 2252 6563 6f6e 6e65  " = 2.0."Reconne
+000008d0: 6374 2064 6561 6474 696d 6520 2d20 7365  ct deadtime - se
+000008e0: 6322 203d 2033 3030 302e 300a 2252 6563  c" = 3000.0."Rec
+000008f0: 6f6e 6e65 6374 2050 6d61 7820 7469 6d65  onnect Pmax time
+00000900: 202d 2073 6563 2220 3d20 3330 302e 300a   - sec" = 300.0.
+00000910: 2250 6572 6d69 7373 6976 6520 6f70 6572  "Permissive oper
+00000920: 6174 696f 6e22 203d 2022 4375 7272 656e  ation" = "Curren
+00000930: 7420 6c69 6d69 7465 6422 0a22 4d61 7920  t limited"."May 
+00000940: 7472 6970 206f 7065 7261 7469 6f6e 2220  trip operation" 
+00000950: 3d20 2254 7269 7022 0a22 4d75 6c74 6970  = "Trip"."Multip
+00000960: 6c65 2064 6973 7475 7262 616e 6365 7322  le disturbances"
+00000970: 203d 2022 5472 6970 22                    = "Trip"
```

### Comparing `nrel_pydss-3.1.3/src/pydss/modes/Dynamic.py` & `nrel_pydss-3.1.4/src/pydss/modes/Dynamic.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,71 +1,71 @@
-from datetime import timedelta
-import math
-
-from loguru import logger 
-
-from pydss.modes.solver_base import solver_base
-from pydss.simulation_input_models import ProjectModel
-
-
-class Dynamic(solver_base):
-    def __init__(self, dssInstance, settings: ProjectModel):
-        super().__init__(dssInstance, settings)
-        self.setMode('Dynamic')
-        self._dssInstance.utils.run_command('Set ControlMode={}'.format(settings.control_mode.value))
-        self._dssSolution.Number(1)
-        self._dssSolution.StepSize(self._sStepRes)
-        self._dssSolution.MaxControlIterations(settings.max_control_iterations)
-        self._dssSolution.DblHour(self._Hour + self._Second / 3600.0)
-        return
-
-    def setFrequency(self, frequency):
-        self._dssSolution.Frequency(frequency)
-        return
-
-    def getFrequency(self):
-        return self._dssSolution.Frequency()
-
-    def SimulationSteps(self):
-        Seconds = (self._EndTime - self._StartTime).total_seconds()
-        Steps = math.ceil(Seconds / self._sStepRes)
-        return Steps, self._StartTime, self._EndTime
-
-    def GetOpenDSSTime(self):
-        return self._dssSolution.DblHour()
-
-    def reset(self):
-        self.setMode('Dynamic')
-        self._dssSolution.Hour(self._Hour)
-        self._dssSolution.Seconds(self._Second)
-        self._dssSolution.Number(1)
-        self._dssSolution.StepSize(self._sStepRes)
-        self._dssSolution.MaxControlIterations(self._settings.project.max_control_iterations)
-        return
-
-    def SolveFor(self, mStartTime, mTimeStep):
-        Hour = int(mStartTime/60)
-        Min = mStartTime % 60
-        self._dssSolution.DblHour(Hour + Min / 60.0)
-        self._dssSolution.Number(mTimeStep)
-        self._dssSolution.Solve()
-        return self._dssSolution.Converged()
-
-    def IncStep(self):
-        self._dssSolution.StepSize(self._sStepRes)
-        self._dssSolution.Solve()
-        self._Time = self._Time + timedelta(seconds=self._sStepRes)
-        self._Hour = int(self._dssSolution.DblHour() // 1)
-        self._Second = (self._dssSolution.DblHour() % 1) * 60 * 60
-        logger.debug('OpenDSS time [h] - ' + str(self._dssSolution.DblHour()))
-        logger.debug('Pydss datetime - ' + str(self._Time))
-        return self._dssSolution.Converged()
-
-    def reSolve(self):
-        self._dssSolution.StepSize(0)
-        self._dssSolution.SolveNoControl()
-        return self._dssSolution.Converged()
-
-    def Solve(self):
-        self._dssSolution.StepSize(0)
-        self._dssSolution.Solve()
-        return self._dssSolution.Converged()
+from datetime import timedelta
+import math
+
+from loguru import logger 
+
+from pydss.modes.solver_base import solver_base
+from pydss.simulation_input_models import ProjectModel
+
+
+class Dynamic(solver_base):
+    def __init__(self, dssInstance, settings: ProjectModel):
+        super().__init__(dssInstance, settings)
+        self.setMode('Dynamic')
+        self._dssInstance.utils.run_command('Set ControlMode={}'.format(settings.control_mode.value))
+        self._dssSolution.Number(1)
+        self._dssSolution.StepSize(self._sStepRes)
+        self._dssSolution.MaxControlIterations(settings.max_control_iterations)
+        self._dssSolution.DblHour(self._Hour + self._Second / 3600.0)
+        return
+
+    def setFrequency(self, frequency):
+        self._dssSolution.Frequency(frequency)
+        return
+
+    def getFrequency(self):
+        return self._dssSolution.Frequency()
+
+    def SimulationSteps(self):
+        Seconds = (self._EndTime - self._StartTime).total_seconds()
+        Steps = math.ceil(Seconds / self._sStepRes)
+        return Steps, self._StartTime, self._EndTime
+
+    def GetOpenDSSTime(self):
+        return self._dssSolution.DblHour()
+
+    def reset(self):
+        self.setMode('Dynamic')
+        self._dssSolution.Hour(self._Hour)
+        self._dssSolution.Seconds(self._Second)
+        self._dssSolution.Number(1)
+        self._dssSolution.StepSize(self._sStepRes)
+        self._dssSolution.MaxControlIterations(self._settings.project.max_control_iterations)
+        return
+
+    def SolveFor(self, mStartTime, mTimeStep):
+        Hour = int(mStartTime/60)
+        Min = mStartTime % 60
+        self._dssSolution.DblHour(Hour + Min / 60.0)
+        self._dssSolution.Number(mTimeStep)
+        self._dssSolution.Solve()
+        return self._dssSolution.Converged()
+
+    def IncStep(self):
+        self._dssSolution.StepSize(self._sStepRes)
+        self._dssSolution.Solve()
+        self._Time = self._Time + timedelta(seconds=self._sStepRes)
+        self._Hour = int(self._dssSolution.DblHour() // 1)
+        self._Second = (self._dssSolution.DblHour() % 1) * 60 * 60
+        logger.debug('OpenDSS time [h] - ' + str(self._dssSolution.DblHour()))
+        logger.debug('Pydss datetime - ' + str(self._Time))
+        return self._dssSolution.Converged()
+
+    def reSolve(self):
+        self._dssSolution.StepSize(0)
+        self._dssSolution.SolveNoControl()
+        return self._dssSolution.Converged()
+
+    def Solve(self):
+        self._dssSolution.StepSize(0)
+        self._dssSolution.Solve()
+        return self._dssSolution.Converged()
```

### Comparing `nrel_pydss-3.1.3/src/pydss/modes/QSTS.py` & `nrel_pydss-3.1.4/src/pydss/modes/QSTS.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,64 +1,64 @@
-from datetime import timedelta
-
-from pydss.modes.solver_base import solver_base
-from pydss.simulation_input_models import ProjectModel
-from pydss.utils.dss_utils import get_load_shape_resolution_secs
-from pydss.utils.timing_utils import timer_stats_collector, track_timing
-
-
-class QSTS(solver_base):
-    def __init__(self, dssInstance, settings: ProjectModel):
-        super().__init__(dssInstance, settings)
-        self._dssSolution.Mode(2)
-        self._dssInstance.utils.run_command('Set ControlMode={}'.format(settings.control_mode.value))
-        self._dssSolution.Number(1)
-        self._dssSolution.StepSize(self._sStepRes)
-        self._dssSolution.MaxControlIterations(settings.max_control_iterations)
-
-        start_time_hours = self._Hour + self._Second / 3600.0
-        load_shape_resolutions_secs = get_load_shape_resolution_secs()
-        if load_shape_resolutions_secs == self._sStepRes:
-            # I don't know why this is needed in this case.
-            # The first data point gets skipped without it.
-            # FIXME
-            start_time_hours += self._sStepRes / 3600.0
-        self._dssSolution.DblHour(start_time_hours)
-        return
-
-    def SolveFor(self, mStartTime, mTimeStep):
-        Hour = int(mStartTime/60)
-        Min = mStartTime%60
-        self._dssSolution.DblHour(Hour + Min / 60.0)
-        self._dssSolution.Number(mTimeStep)
-        self._dssSolution.Solve()
-        return self._dssSolution.Converged()
-
-    def IncStep(self):
-        self._dssSolution.StepSize(self._sStepRes)
-        self._dssSolution.Solve()
-        self._Time = self._Time + timedelta(seconds=self._sStepRes)
-        self._Hour = int(self._dssSolution.DblHour() // 1)
-        self._Second = (self._dssSolution.DblHour() % 1) * 60 * 60
-        return self._dssSolution.Converged()
-
-    def reSolve(self):
-        self._dssSolution.StepSize(0)
-        self._dssSolution.SolveNoControl()
-        return self._dssSolution.Converged()
-
-    def Solve(self):
-        self._dssSolution.StepSize(0)
-        self._dssSolution.Solve()
-        return self._dssSolution.Converged()
-
-    def setMode(self, mode):
-        self._dssInstance.utils.run_command('Set Mode={}'.format(mode))
-        if mode.lower() == 'yearly':
-            self._dssSolution.Mode(2)
-            self._dssSolution.DblHour(self._Hour + self._Second / 3600.0)
-            self._dssSolution.Number(1)
-            self._dssSolution.StepSize(self._sStepRes)
-            self._dssSolution.MaxControlIterations(self._settings.project.max_control_iterations)
-
-    def reset(self):
-        pass
+from datetime import timedelta
+
+from pydss.modes.solver_base import solver_base
+from pydss.simulation_input_models import ProjectModel
+from pydss.utils.dss_utils import get_load_shape_resolution_secs
+from pydss.utils.timing_utils import timer_stats_collector, track_timing
+
+
+class QSTS(solver_base):
+    def __init__(self, dssInstance, settings: ProjectModel):
+        super().__init__(dssInstance, settings)
+        self._dssSolution.Mode(2)
+        self._dssInstance.utils.run_command('Set ControlMode={}'.format(settings.control_mode.value))
+        self._dssSolution.Number(1)
+        self._dssSolution.StepSize(self._sStepRes)
+        self._dssSolution.MaxControlIterations(settings.max_control_iterations)
+
+        start_time_hours = self._Hour + self._Second / 3600.0
+        load_shape_resolutions_secs = get_load_shape_resolution_secs()
+        if load_shape_resolutions_secs == self._sStepRes:
+            # I don't know why this is needed in this case.
+            # The first data point gets skipped without it.
+            # FIXME
+            start_time_hours += self._sStepRes / 3600.0
+        self._dssSolution.DblHour(start_time_hours)
+        return
+
+    def SolveFor(self, mStartTime, mTimeStep):
+        Hour = int(mStartTime/60)
+        Min = mStartTime%60
+        self._dssSolution.DblHour(Hour + Min / 60.0)
+        self._dssSolution.Number(mTimeStep)
+        self._dssSolution.Solve()
+        return self._dssSolution.Converged()
+
+    def IncStep(self):
+        self._dssSolution.StepSize(self._sStepRes)
+        self._dssSolution.Solve()
+        self._Time = self._Time + timedelta(seconds=self._sStepRes)
+        self._Hour = int(self._dssSolution.DblHour() // 1)
+        self._Second = (self._dssSolution.DblHour() % 1) * 60 * 60
+        return self._dssSolution.Converged()
+
+    def reSolve(self):
+        self._dssSolution.StepSize(0)
+        self._dssSolution.SolveNoControl()
+        return self._dssSolution.Converged()
+
+    def Solve(self):
+        self._dssSolution.StepSize(0)
+        self._dssSolution.Solve()
+        return self._dssSolution.Converged()
+
+    def setMode(self, mode):
+        self._dssInstance.utils.run_command('Set Mode={}'.format(mode))
+        if mode.lower() == 'yearly':
+            self._dssSolution.Mode(2)
+            self._dssSolution.DblHour(self._Hour + self._Second / 3600.0)
+            self._dssSolution.Number(1)
+            self._dssSolution.StepSize(self._sStepRes)
+            self._dssSolution.MaxControlIterations(self._settings.project.max_control_iterations)
+
+    def reset(self):
+        pass
```

### Comparing `nrel_pydss-3.1.3/src/pydss/modes/solver_base.py` & `nrel_pydss-3.1.4/src/pydss/modes/solver_base.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,99 +1,99 @@
-from datetime import timedelta
-import math
-import abc
-
-from loguru import logger
-
-from pydss.common import DATE_FORMAT
-from pydss.simulation_input_models import ProjectModel
-
-class solver_base(abc.ABC):
-    def __init__(self, dssInstance, settings: ProjectModel):
-
-        self._settings = settings
-
-        self._Time = settings.start_time
-        self._Loadshape_init_time = settings.loadshape_start_time
-        time_offset_days = (self._Time - self._Loadshape_init_time).days
-        time_offset_seconds = (self._Time - self._Loadshape_init_time).seconds
-
-        self._StartTime = self._Time
-        self._EndTime = self._Time + timedelta(minutes=settings.simulation_duration_min)
-
-        StartDay = time_offset_days
-        StartTimeMin = time_offset_seconds / 60.0
-        sStepResolution = settings.step_resolution_sec
-
-        self.StartDay = self._StartTime.timetuple().tm_yday
-        self.EndDay = self._EndTime.timetuple().tm_yday
-
-        self._sStepRes = sStepResolution
-        self._dssInstance = dssInstance
-        self._dssSolution = dssInstance.Solution
-
-        self._Hour = StartDay * 24
-        self._Second = StartTimeMin * 60.0
-
-        #self._dssSolution.DblHour()
-        self.reSolve()
-        logger.info("%s solver setup complete", settings.simulation_type)
-
-    def setFrequency(self, frequency):
-        self._dssSolution.Frequency(frequency)
-        return
-
-    def getFrequency(self):
-        return self._dssSolution.Frequency()
-
-    def SimulationSteps(self):
-        Seconds = (self._EndTime - self._StartTime).total_seconds()
-        Steps = math.ceil(Seconds / self._sStepRes)
-        return Steps, self._StartTime, self._EndTime
-
-    def GetTotalSeconds(self):
-        return (self._Time - self._StartTime).total_seconds()
-
-    def GetDateTime(self):
-        return self._Time
-
-    def GetStepResolutionSeconds(self):
-        return self._sStepRes
-
-    def GetStepSizeSec(self):
-        return self._sStepRes
-
-    def getMode(self):
-        return self._dssSolution.ModeID()
-
-    def setMode(self, mode):
-        return self._dssInstance.utils.run_command('Set Mode={}'.format(mode))
-
-    def GetOpenDSSTime(self):
-        return self._dssSolution.DblHour()
-
-    def get_simulation_end_time(self):
-        return self._EndTime
-
-    @property
-    def MaxIterations(self):
-        return self._settings.max_control_iterations
-
-    @abc.abstractmethod
-    def SolveFor(self, mStartTime, mTimeStep):
-        """Run SolveFor"""
-
-    @abc.abstractmethod
-    def reset(self):
-        """Reset the solver"""
-
-    @abc.abstractmethod
-    def reSolve(self):
-        """Run a SolveNoControl"""
-
-    @abc.abstractmethod
-    def Solve(self):
-        """Run a Solve"""
-
-    @abc.abstractmethod
-    def IncStep(self):
-        """Increment the simulation time step"""
+from datetime import timedelta
+import math
+import abc
+
+from loguru import logger
+
+from pydss.common import DATE_FORMAT
+from pydss.simulation_input_models import ProjectModel
+
+class solver_base(abc.ABC):
+    def __init__(self, dssInstance, settings: ProjectModel):
+
+        self._settings = settings
+
+        self._Time = settings.start_time
+        self._Loadshape_init_time = settings.loadshape_start_time
+        time_offset_days = (self._Time - self._Loadshape_init_time).days
+        time_offset_seconds = (self._Time - self._Loadshape_init_time).seconds
+
+        self._StartTime = self._Time
+        self._EndTime = self._Time + timedelta(minutes=settings.simulation_duration_min)
+
+        StartDay = time_offset_days
+        StartTimeMin = time_offset_seconds / 60.0
+        sStepResolution = settings.step_resolution_sec
+
+        self.StartDay = self._StartTime.timetuple().tm_yday
+        self.EndDay = self._EndTime.timetuple().tm_yday
+
+        self._sStepRes = sStepResolution
+        self._dssInstance = dssInstance
+        self._dssSolution = dssInstance.Solution
+
+        self._Hour = StartDay * 24
+        self._Second = StartTimeMin * 60.0
+
+        #self._dssSolution.DblHour()
+        self.reSolve()
+        logger.info("%s solver setup complete", settings.simulation_type)
+
+    def setFrequency(self, frequency):
+        self._dssSolution.Frequency(frequency)
+        return
+
+    def getFrequency(self):
+        return self._dssSolution.Frequency()
+
+    def SimulationSteps(self):
+        Seconds = (self._EndTime - self._StartTime).total_seconds()
+        Steps = math.ceil(Seconds / self._sStepRes)
+        return Steps, self._StartTime, self._EndTime
+
+    def GetTotalSeconds(self):
+        return (self._Time - self._StartTime).total_seconds()
+
+    def GetDateTime(self):
+        return self._Time
+
+    def GetStepResolutionSeconds(self):
+        return self._sStepRes
+
+    def GetStepSizeSec(self):
+        return self._sStepRes
+
+    def getMode(self):
+        return self._dssSolution.ModeID()
+
+    def setMode(self, mode):
+        return self._dssInstance.utils.run_command('Set Mode={}'.format(mode))
+
+    def GetOpenDSSTime(self):
+        return self._dssSolution.DblHour()
+
+    def get_simulation_end_time(self):
+        return self._EndTime
+
+    @property
+    def MaxIterations(self):
+        return self._settings.max_control_iterations
+
+    @abc.abstractmethod
+    def SolveFor(self, mStartTime, mTimeStep):
+        """Run SolveFor"""
+
+    @abc.abstractmethod
+    def reset(self):
+        """Reset the solver"""
+
+    @abc.abstractmethod
+    def reSolve(self):
+        """Run a SolveNoControl"""
+
+    @abc.abstractmethod
+    def Solve(self):
+        """Run a Solve"""
+
+    @abc.abstractmethod
+    def IncStep(self):
+        """Increment the simulation time step"""
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/enumerations.py` & `nrel_pydss-3.1.4/src/pydss/pyControllers/enumerations.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,224 +1,224 @@
-from enum import Enum
-
-class VoltageCalcModes(str, Enum):
-    """
-    Voltage calculation modes for the controller
-    
-    **MAX** - *The the maximum voltage from the available phases as the contoller input*
-    
-    **AVG** - *The the average voltage from the available phases as the contoller input*
-    
-    **MIN** - *The the minimum voltage from the available phases as the contoller input*
-    
-    **A** - *The voltage from phase A as the contoller input*
-    
-    **B** - *The voltage from phase B as the contoller input*
-    
-    **C** - *The voltage from phase C as the contoller input*
-    """
-    MAX = "Max"
-    AVG = "Avg"
-    MIN = "Min"
-    A = "1"
-    B = "2"
-    C = "3"
-
-class PvStandard(str, Enum):
-    """
-    PV standards for the controller
-    
-    **IEEE_1547_2003** - *For legacy PV systems, use the IEEE 1574-2003 standard*
-    
-    **IEEE_1547_2018** - *For legacy PV systems with smart controls, use the IEEE 1574-2018 standard*
-    """
-    IEEE_1547_2003 = "1547-2003" 
-    IEEE_1547_2018 = "1547-2018"
-
-class RideThroughCategory(str, Enum):
-    """
-    PV ride-through catregories
-    
-    **CATEGORY_I**  - *Intended to meet minimum Bulk EPS reliability needs and to be achievable by all DER technologies, including rotating machines*
-    
-    **CATEGORY_II**  - *Designed to align with the requirements in NERC PRC-024-2*
-    
-    **CATEGORY_III** - *Designed to meet the needs of low-inertia or highly-penetrated grids*
-    """
-    CATEGORY_I = "Category I"
-    CATEGORY_II = "Category II" 
-    CATEGORY_III = "Category III" 
-
-class PermissiveOperation(str, Enum):
-    """
-    Possible behaviors for permissive operation
-    
-    **CURRENT_LIMITED** - *Current injection into the grid is limited by the inverter during low or high voltage conditions outside the continuous operating range*
-    
-    **MOMENTARY_SUCESSION** - *No current is injected into the grid by the inverter during low or high voltage conditions outside the continuous operating range*
-    """
-    CURRENT_LIMITED = "Current limited"
-    MOMENTARY_SUCESSION = "Momentary sucession"
-
-class MayTripOperation(str, Enum):
-    """
-    Possible behaviors for my trip region
-    
-    **TRIP** - *PV system disconnects from the grid. Can not reconnect for atleast 300 seconds.*
-    
-    **PERMISSIVE_OPERATION** - *Option for the DER to either continue to exchange current with or to cease toenergize an EPS*
-    """
-    TRIP = "Trip" 
-    PERMISSIVE_OPERATION ="Permissive operation" 
-    
-class MultipleDisturbances(str, Enum):
-    """
-    Possible behaviors for multiple disturbamces
-    
-    **TRIP** - *PV system disconnects from the grid. Can not reconnect for atleast 300 seconds.*
-    
-    **PERMISSIVE_OPERATION** - *Option for the DER to either continue to exchange current with or to cease toenergize an EPS*
-    """
-    TRIP = "Trip" 
-    PERMISSIVE_OPERATION = "Permissive operation" 
-    
-class CategoryI(float, Enum):
-    """
-    Variable defination for Category I
-    
-    **OV2_PU** - *Upper bound for the over-voltage region*
-     
-    **OV2_CT_SEC** - *Trip time if voltage exceeds OV2_PU (seconds)*
-    
-    **OV1_PU** - *Lower bound for the over-voltage region* 
-    
-    **OV1_CT_SEC** - *Trip time if voltage exceeds OV1_CT_SEC (seconds)*
-    
-    **UV1_PU** - *Upper bound for the under-voltage region* 
-    
-    **UV1_CT_SEC** - *Trip time if voltage is less than UV1_PU (seconds)*
-    
-    **UV2_PU** - *Lower bound for the under-voltage region*
-    
-    **UV2_CT_SEC** - *Trip time if voltage is less than UV1_PU (seconds)*
-    """
-    OV2_PU = 1.2 
-    OV2_CT_SEC = 0.16
-    OV1_PU = 1.1
-    OV1_CT_SEC = 2.0
-    UV1_PU = 0.7
-    UV1_CT_SEC = 2.0
-    UV2_PU = 0.45
-    UV2_CT_SEC = 0.16
-
-class CategoryII(float, Enum):
-    """
-    Variable defination for Category II
-    
-    **OV2_PU** - *Upper bound for the over-voltage region*
-     
-    **OV2_CT_SEC** - *Trip time if voltage exceeds OV2_PU (seconds)*
-    
-    **OV1_PU** - *Lower bound for the over-voltage region* 
-    
-    **OV1_CT_SEC** - *Trip time if voltage exceeds OV1_CT_SEC (seconds)*
-    
-    **UV1_PU** - *Upper bound for the under-voltage region* 
-    
-    **UV1_CT_SEC** - *Trip time if voltage is less than UV1_PU (seconds)*
-    
-    **UV2_PU** - *Lower bound for the under-voltage region*
-    
-    **UV2_CT_SEC** - *Trip time if voltage is less than UV1_PU (seconds)*
-    """
-    OV2_PU = 1.2
-    OV2_CT_SEC = 0.16
-    OV1_PU = 1.1
-    OV1_CT_SEC = 2.0
-    UV1_PU = 0.7
-    UV1_CT_SEC = 10.0
-    UV2_PU = 0.45
-    UV2_CT_SEC = 0.16
-
-class CategoryIII(float, Enum):
-    """
-    Variable defination for Category III
-    
-    **OV2_PU** - *Upper bound for the over-voltage region*
-     
-    **OV2_CT_SEC** - *Trip time if voltage exceeds OV2_PU (seconds)*
-    
-    **OV1_PU** - *Lower bound for the over-voltage region* 
-    
-    **OV1_CT_SEC** - *Trip time if voltage exceeds OV1_CT_SEC (seconds)*
-    
-    **UV1_PU** - *Upper bound for the under-voltage region* 
-    
-    **UV1_CT_SEC** - *Trip time if voltage is less than UV1_PU (seconds)*
-    
-    **UV2_PU** - *Lower bound for the under-voltage region*
-    
-    **UV2_CT_SEC** - *Trip time if voltage is less than UV1_PU (seconds)*
-    """
-    OV2_PU = 1.2
-    OV2_CT_SEC = 0.16
-    OV1_PU = 1.1
-    OV1_CT_SEC = 13.0
-    UV1_PU = 0.88
-    UV1_CT_SEC = 21.0
-    UV2_PU = 0.5
-    UV2_CT_SEC = 2.0
-
-class SmartControls(str, Enum):
-    """
-    Supported smart control algorithms
-    
-    **NONE** - *No contol algorithm*
-     
-    **CONSTANT_POWER_FACTOR** - *Constant power factor implmentation*
-    
-    **VARIABLE_POWER_FACTOR** - *Variable power factor implmentation* 
-    
-    **VOLT_VAR** - *Volt / Var algorithm implementation*
-    
-    **VOLT_WATT** - *Volt / Watt algorithm implementation* 
-    
-    **TRIP** - *Over voltage trip implementation*
-    
-    """
-    
-    NONE = 'None'           
-    CONSTANT_POWER_FACTOR ='cpf'            
-    VARIABLE_POWER_FACTOR ='vpf'            
-    VOLT_VAR ='VVar'           
-    VOLT_WATT ='vwatt'             
-    TRIP = 'trip'
-    
-class ControlPriority(str, Enum):
-    """
-    Variable to prooritize at inverter capability limit
-    
-    **VAR** - *Var priority*
-     
-    **WATT** - *Watt priority*
-    
-    **PF** - *Powerfactor priority*
-        
-    """
-    
-    VAR = 'Var'           
-    WATT ='Watt' 
-    PF = "PF"           
-
-
-class VoltWattCurtailmentStrategy(str, Enum):
-    """
-    Curtailment strategy for volt / watt algorithm
-    
-    **AVAILABLE_POWER** - *Curtailment is based on available power of the inverter*
-     
-    **RAETED_POWER** - *Curtailment is based on rated power of the inverter*
-        
-    """
-    
-    AVAILABLE_POWER = 'Available Power'           
-    RAETED_POWER ='Rated Power' 
+from enum import Enum
+
+class VoltageCalcModes(str, Enum):
+    """
+    Voltage calculation modes for the controller
+    
+    **MAX** - *The the maximum voltage from the available phases as the contoller input*
+    
+    **AVG** - *The the average voltage from the available phases as the contoller input*
+    
+    **MIN** - *The the minimum voltage from the available phases as the contoller input*
+    
+    **A** - *The voltage from phase A as the contoller input*
+    
+    **B** - *The voltage from phase B as the contoller input*
+    
+    **C** - *The voltage from phase C as the contoller input*
+    """
+    MAX = "Max"
+    AVG = "Avg"
+    MIN = "Min"
+    A = "1"
+    B = "2"
+    C = "3"
+
+class PvStandard(str, Enum):
+    """
+    PV standards for the controller
+    
+    **IEEE_1547_2003** - *For legacy PV systems, use the IEEE 1574-2003 standard*
+    
+    **IEEE_1547_2018** - *For legacy PV systems with smart controls, use the IEEE 1574-2018 standard*
+    """
+    IEEE_1547_2003 = "1547-2003" 
+    IEEE_1547_2018 = "1547-2018"
+
+class RideThroughCategory(str, Enum):
+    """
+    PV ride-through catregories
+    
+    **CATEGORY_I**  - *Intended to meet minimum Bulk EPS reliability needs and to be achievable by all DER technologies, including rotating machines*
+    
+    **CATEGORY_II**  - *Designed to align with the requirements in NERC PRC-024-2*
+    
+    **CATEGORY_III** - *Designed to meet the needs of low-inertia or highly-penetrated grids*
+    """
+    CATEGORY_I = "Category I"
+    CATEGORY_II = "Category II" 
+    CATEGORY_III = "Category III" 
+
+class PermissiveOperation(str, Enum):
+    """
+    Possible behaviors for permissive operation
+    
+    **CURRENT_LIMITED** - *Current injection into the grid is limited by the inverter during low or high voltage conditions outside the continuous operating range*
+    
+    **MOMENTARY_SUCESSION** - *No current is injected into the grid by the inverter during low or high voltage conditions outside the continuous operating range*
+    """
+    CURRENT_LIMITED = "Current limited"
+    MOMENTARY_SUCESSION = "Momentary sucession"
+
+class MayTripOperation(str, Enum):
+    """
+    Possible behaviors for my trip region
+    
+    **TRIP** - *PV system disconnects from the grid. Can not reconnect for atleast 300 seconds.*
+    
+    **PERMISSIVE_OPERATION** - *Option for the DER to either continue to exchange current with or to cease toenergize an EPS*
+    """
+    TRIP = "Trip" 
+    PERMISSIVE_OPERATION ="Permissive operation" 
+    
+class MultipleDisturbances(str, Enum):
+    """
+    Possible behaviors for multiple disturbamces
+    
+    **TRIP** - *PV system disconnects from the grid. Can not reconnect for atleast 300 seconds.*
+    
+    **PERMISSIVE_OPERATION** - *Option for the DER to either continue to exchange current with or to cease toenergize an EPS*
+    """
+    TRIP = "Trip" 
+    PERMISSIVE_OPERATION = "Permissive operation" 
+    
+class CategoryI(float, Enum):
+    """
+    Variable defination for Category I
+    
+    **OV2_PU** - *Upper bound for the over-voltage region*
+     
+    **OV2_CT_SEC** - *Trip time if voltage exceeds OV2_PU (seconds)*
+    
+    **OV1_PU** - *Lower bound for the over-voltage region* 
+    
+    **OV1_CT_SEC** - *Trip time if voltage exceeds OV1_CT_SEC (seconds)*
+    
+    **UV1_PU** - *Upper bound for the under-voltage region* 
+    
+    **UV1_CT_SEC** - *Trip time if voltage is less than UV1_PU (seconds)*
+    
+    **UV2_PU** - *Lower bound for the under-voltage region*
+    
+    **UV2_CT_SEC** - *Trip time if voltage is less than UV1_PU (seconds)*
+    """
+    OV2_PU = 1.2 
+    OV2_CT_SEC = 0.16
+    OV1_PU = 1.1
+    OV1_CT_SEC = 2.0
+    UV1_PU = 0.7
+    UV1_CT_SEC = 2.0
+    UV2_PU = 0.45
+    UV2_CT_SEC = 0.16
+
+class CategoryII(float, Enum):
+    """
+    Variable defination for Category II
+    
+    **OV2_PU** - *Upper bound for the over-voltage region*
+     
+    **OV2_CT_SEC** - *Trip time if voltage exceeds OV2_PU (seconds)*
+    
+    **OV1_PU** - *Lower bound for the over-voltage region* 
+    
+    **OV1_CT_SEC** - *Trip time if voltage exceeds OV1_CT_SEC (seconds)*
+    
+    **UV1_PU** - *Upper bound for the under-voltage region* 
+    
+    **UV1_CT_SEC** - *Trip time if voltage is less than UV1_PU (seconds)*
+    
+    **UV2_PU** - *Lower bound for the under-voltage region*
+    
+    **UV2_CT_SEC** - *Trip time if voltage is less than UV1_PU (seconds)*
+    """
+    OV2_PU = 1.2
+    OV2_CT_SEC = 0.16
+    OV1_PU = 1.1
+    OV1_CT_SEC = 2.0
+    UV1_PU = 0.7
+    UV1_CT_SEC = 10.0
+    UV2_PU = 0.45
+    UV2_CT_SEC = 0.16
+
+class CategoryIII(float, Enum):
+    """
+    Variable defination for Category III
+    
+    **OV2_PU** - *Upper bound for the over-voltage region*
+     
+    **OV2_CT_SEC** - *Trip time if voltage exceeds OV2_PU (seconds)*
+    
+    **OV1_PU** - *Lower bound for the over-voltage region* 
+    
+    **OV1_CT_SEC** - *Trip time if voltage exceeds OV1_CT_SEC (seconds)*
+    
+    **UV1_PU** - *Upper bound for the under-voltage region* 
+    
+    **UV1_CT_SEC** - *Trip time if voltage is less than UV1_PU (seconds)*
+    
+    **UV2_PU** - *Lower bound for the under-voltage region*
+    
+    **UV2_CT_SEC** - *Trip time if voltage is less than UV1_PU (seconds)*
+    """
+    OV2_PU = 1.2
+    OV2_CT_SEC = 0.16
+    OV1_PU = 1.1
+    OV1_CT_SEC = 13.0
+    UV1_PU = 0.88
+    UV1_CT_SEC = 21.0
+    UV2_PU = 0.5
+    UV2_CT_SEC = 2.0
+
+class SmartControls(str, Enum):
+    """
+    Supported smart control algorithms
+    
+    **NONE** - *No contol algorithm*
+     
+    **CONSTANT_POWER_FACTOR** - *Constant power factor implmentation*
+    
+    **VARIABLE_POWER_FACTOR** - *Variable power factor implmentation* 
+    
+    **VOLT_VAR** - *Volt / Var algorithm implementation*
+    
+    **VOLT_WATT** - *Volt / Watt algorithm implementation* 
+    
+    **TRIP** - *Over voltage trip implementation*
+    
+    """
+    
+    NONE = 'None'           
+    CONSTANT_POWER_FACTOR ='cpf'            
+    VARIABLE_POWER_FACTOR ='vpf'            
+    VOLT_VAR ='VVar'           
+    VOLT_WATT ='vwatt'             
+    TRIP = 'trip'
+    
+class ControlPriority(str, Enum):
+    """
+    Variable to prooritize at inverter capability limit
+    
+    **VAR** - *Var priority*
+     
+    **WATT** - *Watt priority*
+    
+    **PF** - *Powerfactor priority*
+        
+    """
+    
+    VAR = 'Var'           
+    WATT ='Watt' 
+    PF = "PF"           
+
+
+class VoltWattCurtailmentStrategy(str, Enum):
+    """
+    Curtailment strategy for volt / watt algorithm
+    
+    **AVAILABLE_POWER** - *Curtailment is based on available power of the inverter*
+     
+    **RAETED_POWER** - *Curtailment is based on rated power of the inverter*
+        
+    """
+    
+    AVAILABLE_POWER = 'Available Power'           
+    RAETED_POWER ='Rated Power'
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/pyController.py` & `nrel_pydss-3.1.4/src/pydss/pyControllers/pyController.py`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-from os.path import dirname, basename, isfile
-import glob
-
-from  pydss.pyControllers import Controllers
-
-modules = glob.glob(Controllers.__path__[0]+"/*.py")
-pythonFiles = [ basename(f)[:-3] for f in modules if isfile(f) and not f.endswith('__init__.py') ]
-
-from pydss.dssElement import dssElement
-ControllerTypes = {}
-
-for file in pythonFiles:
-    exec('from pydss.pyControllers.Controllers import {}'.format(file))
-    exec('ControllerTypes["{}"] = {}.{}'.format(file, file, file))
-
-def Create(ElmName, ControllerType, Settings, ElmObjectList, dssInstance, dssSolver):
-
-    assert (ControllerType in ControllerTypes), "Definition for '{}' controller not found. \n " \
-                                                "Please define the controller in ~pydss\pyControllers\Controllers".format(
-        ControllerType
-    )
-
-    assert (ElmName in ElmObjectList), "'{}' does not exist in the pydss master object dictionary.".format(ElmName)
-    relObject = ElmObjectList[ElmName]
-    
-    # except:
-    #     Index = dssInstance.Circuit.SetActiveElement(ElmName)
-    #     if int(Index) >= 0:
-    #         ElmObjectList[ElmName] = dssElement(dssInstance)
-    #         relObject = ElmObjectList[ElmName]
-    # else:
-    #     return -1
-
-    ObjectController = ControllerTypes[ControllerType](relObject, Settings, dssInstance, ElmObjectList, dssSolver)
-    return ObjectController
+from os.path import dirname, basename, isfile
+import glob
+
+from  pydss.pyControllers import Controllers
+
+modules = glob.glob(Controllers.__path__[0]+"/*.py")
+pythonFiles = [ basename(f)[:-3] for f in modules if isfile(f) and not f.endswith('__init__.py') ]
+
+from pydss.dssElement import dssElement
+ControllerTypes = {}
+
+for file in pythonFiles:
+    exec('from pydss.pyControllers.Controllers import {}'.format(file))
+    exec('ControllerTypes["{}"] = {}.{}'.format(file, file, file))
+
+def Create(ElmName, ControllerType, Settings, ElmObjectList, dssInstance, dssSolver):
+
+    assert (ControllerType in ControllerTypes), "Definition for '{}' controller not found. \n " \
+                                                "Please define the controller in ~pydss\pyControllers\Controllers".format(
+        ControllerType
+    )
+
+    assert (ElmName in ElmObjectList), "'{}' does not exist in the pydss master object dictionary.".format(ElmName)
+    relObject = ElmObjectList[ElmName]
+    
+    # except:
+    #     Index = dssInstance.Circuit.SetActiveElement(ElmName)
+    #     if int(Index) >= 0:
+    #         ElmObjectList[ElmName] = dssElement(dssInstance)
+    #         relObject = ElmObjectList[ElmName]
+    # else:
+    #     return -1
+
+    ObjectController = ControllerTypes[ControllerType](relObject, Settings, dssInstance, ElmObjectList, dssSolver)
+    return ObjectController
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/FaultController.py` & `nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/FaultController.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,68 +1,68 @@
-from  pydss.pyControllers.pyControllerAbstract import ControllerAbstract
-
-class FaultController(ControllerAbstract):
-    """The class is used to induce faults on bus for dynamic simulation studies. Subclass of the :class:`pydss.pyControllers.pyControllerAbstract.ControllerAbstract` abstract class. 
-
-    :param FaultObj: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'Fault' element
-    :type FaultObj: class:`pydss.dssElement.dssElement`
-    :param Settings: A dictionary that defines the settings for the faul controller.
-    :type Settings: dict
-    :param dssInstance: An :class:`opendssdirect` instance
-    :type dssInstance: :class:`opendssdirect` instance
-    :param ElmObjectList: Dictionary of all dssElement, dssBus and dssCircuit ojects
-    :type ElmObjectList: dict
-    :param dssSolver: An instance of one of the classes defined in :mod:`pydss.SolveMode`.
-    :type dssSolver: :mod:`pydss.SolveMode`
-    :raises: AssertionError  if 'FaultObj' is not a wrapped OpenDSS Fault element
-
-    """
-    def __init__(self, FaultObj, Settings, dssInstance, ElmObjectList, dssSolver):
-        """Constructor method
-        """
-
-        super(FaultController, self).__init__(FaultObj, Settings, dssInstance, ElmObjectList, dssSolver)
-        self.P_old = 0
-        self.Time = -1
-        self.__Locked = False
-        self.__dssSolver = dssSolver
-        self.__FaultObj = FaultObj
-        self.__FaultEnabled = False
-        self.__Settings = Settings
-        nPhases = len(Settings['Bus1'].split('.')) - 1
-        FaultObj.SetParameter('bus1', Settings['Bus1'])
-        FaultObj.SetParameter('bus2', Settings['Bus2'])
-        FaultObj.SetParameter('phases', nPhases)
-        FaultObj.SetParameter('r', Settings['Fault resistance'])
-        self.__Class, Name = self.__FaultObj.GetInfo()
-        assert (self.__Class.lower() == 'fault'), 'FaultController works only with an OpenDSS Fault element'
-        self.__Name = 'pyCont_' + self.__Class + '_' + Name
-        self.__init_time = dssSolver.GetDateTime()
-        self.__Settings['Fault end time (sec)'] = self.__Settings['Fault start time (sec)'] + \
-                                                  self.__Settings['Fault duration (sec)']
-        return
-
-    def Update(self, Priority, Time, UpdateResults):
-        """Induces and removes a fault as the simulation runs as per user defined settings. 
-        """
-        time = self.__dssSolver.GetTotalSeconds() - self.__dssSolver.GetStepSizeSec()
-        self.__FaultObj.SetParameter('enabled', 'yes')
-        if time >= self.__Settings['Fault start time (sec)'] and time < self.__Settings['Fault end time (sec)'] and\
-            self.__FaultEnabled==False:
-            self.__FaultObj.SetParameter('enabled', 'yes')
-            self.__FaultEnabled = True
-        elif time >= self.__Settings['Fault end time (sec)'] and self.__FaultEnabled==True:
-            self.__FaultObj.SetParameter('enabled', 'no')
-            self.__FaultEnabled = False
-        return 0
-        
-    def Name(self):
-        return self.__Name
-
-    def ControlledElement(self):
-        return "{}.{}".format(self.__Class, self.__Name)
-
-    def debugInfo(self):
-        return [self.__Settings['Control{}'.format(i+1)] for i in range(3)]
-
-
-
+from  pydss.pyControllers.pyControllerAbstract import ControllerAbstract
+
+class FaultController(ControllerAbstract):
+    """The class is used to induce faults on bus for dynamic simulation studies. Subclass of the :class:`pydss.pyControllers.pyControllerAbstract.ControllerAbstract` abstract class. 
+
+    :param FaultObj: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'Fault' element
+    :type FaultObj: class:`pydss.dssElement.dssElement`
+    :param Settings: A dictionary that defines the settings for the faul controller.
+    :type Settings: dict
+    :param dssInstance: An :class:`opendssdirect` instance
+    :type dssInstance: :class:`opendssdirect` instance
+    :param ElmObjectList: Dictionary of all dssElement, dssBus and dssCircuit ojects
+    :type ElmObjectList: dict
+    :param dssSolver: An instance of one of the classes defined in :mod:`pydss.SolveMode`.
+    :type dssSolver: :mod:`pydss.SolveMode`
+    :raises: AssertionError  if 'FaultObj' is not a wrapped OpenDSS Fault element
+
+    """
+    def __init__(self, FaultObj, Settings, dssInstance, ElmObjectList, dssSolver):
+        """Constructor method
+        """
+
+        super(FaultController, self).__init__(FaultObj, Settings, dssInstance, ElmObjectList, dssSolver)
+        self.P_old = 0
+        self.Time = -1
+        self.__Locked = False
+        self.__dssSolver = dssSolver
+        self.__FaultObj = FaultObj
+        self.__FaultEnabled = False
+        self.__Settings = Settings
+        nPhases = len(Settings['Bus1'].split('.')) - 1
+        FaultObj.SetParameter('bus1', Settings['Bus1'])
+        FaultObj.SetParameter('bus2', Settings['Bus2'])
+        FaultObj.SetParameter('phases', nPhases)
+        FaultObj.SetParameter('r', Settings['Fault resistance'])
+        self.__Class, Name = self.__FaultObj.GetInfo()
+        assert (self.__Class.lower() == 'fault'), 'FaultController works only with an OpenDSS Fault element'
+        self.__Name = 'pyCont_' + self.__Class + '_' + Name
+        self.__init_time = dssSolver.GetDateTime()
+        self.__Settings['Fault end time (sec)'] = self.__Settings['Fault start time (sec)'] + \
+                                                  self.__Settings['Fault duration (sec)']
+        return
+
+    def Update(self, Priority, Time, UpdateResults):
+        """Induces and removes a fault as the simulation runs as per user defined settings. 
+        """
+        time = self.__dssSolver.GetTotalSeconds() - self.__dssSolver.GetStepSizeSec()
+        self.__FaultObj.SetParameter('enabled', 'yes')
+        if time >= self.__Settings['Fault start time (sec)'] and time < self.__Settings['Fault end time (sec)'] and\
+            self.__FaultEnabled==False:
+            self.__FaultObj.SetParameter('enabled', 'yes')
+            self.__FaultEnabled = True
+        elif time >= self.__Settings['Fault end time (sec)'] and self.__FaultEnabled==True:
+            self.__FaultObj.SetParameter('enabled', 'no')
+            self.__FaultEnabled = False
+        return 0
+        
+    def Name(self):
+        return self.__Name
+
+    def ControlledElement(self):
+        return "{}.{}".format(self.__Class, self.__Name)
+
+    def debugInfo(self):
+        return [self.__Settings['Control{}'.format(i+1)] for i in range(3)]
+
+
+
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/GenController.py` & `nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/GenController.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,148 +1,148 @@
-from  pydss.pyControllers.pyControllerAbstract import ControllerAbstract
-import math
-import abc
-
-class GenController(ControllerAbstract):
-    """Implementation of smart control modes of modern inverter systems. Subclass of the :class:`pydss.pyControllers.pyControllerAbstract.ControllerAbstract` abstract class.
-
-        :param PvObj: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'PVSystem' element
-        :type FaultObj: class:`pydss.dssElement.dssElement`
-        :param Settings: A dictionary that defines the settings for the PvController.
-        :type Settings: dict
-        :param dssInstance: An :class:`opendssdirect` instance
-        :type dssInstance: :class:`opendssdirect`
-        :param ElmObjectList: Dictionary of all dssElement, dssBus and dssCircuit objects
-        :type ElmObjectList: dict
-        :param dssSolver: An instance of one of the classed defined in :mod:`pydss.SolveMode`.
-        :type dssSolver: :mod:`pydss.SolveMode`
-        :raises: AssertionError if 'PvObj' is not a wrapped OpenDSS PVSystem element
-
-    """
-
-    def __init__(self, GenObj, Settings, dssInstance, ElmObjectList, dssSolver):
-        """Constructor method
-        """
-        super(GenController, self).__init__(GenObj, Settings, dssInstance, ElmObjectList, dssSolver)
-        self.TimeChange = False
-        self.Time = (-1, 0)
-
-        self.oldPcalc = 0
-        self.oldQcalc = 0
-
-        self.__vDisconnected = False
-        self.__pDisconnected = False
-
-        self.__ElmObjectList = ElmObjectList
-        self.ControlDict = {
-            'None'           : lambda: 0,
-            'VVar'           : self.VVARcontrol,
-        }
-
-        self.__ControlledElm = GenObj
-        self.ceClass, self.ceName = self.__ControlledElm.GetInfo()
-
-        assert (self.ceClass.lower()=='generator'), 'GenController works only with an OpenDSS generator element'
-        self.__Name = 'pyCont_' + self.ceClass + '_' +  self.ceName
-        if '_' in  self.ceName:
-            self.Phase =  self.ceName.split('_')[1]
-        else:
-            self.Phase = None
-        self.__ElmObjectList = ElmObjectList
-        self.__ControlledElm = GenObj
-        self.__dssInstance = dssInstance
-        self.__dssSolver = dssSolver
-        self.__Settings = Settings
-
-        self.__BaseKV = float(GenObj.GetParameter('kv'))
-        self.__Srated = float(GenObj.GetParameter('kVA'))
-        self.__Prated = float(GenObj.GetParameter('kW'))
-        self.__Qrated = float(GenObj.GetParameter('maxkvar'))
-        GenObj.SetParameter('minkvar', -self.__Qrated)
-
-        if Settings['Model as PVsystem']:
-            GenObj.SetParameter('model', 3)
-
-
-        self.__dampCoef = Settings['DampCoef']
-
-        self.__PFrated = Settings['PFlim']
-        self.Pmppt = 100
-        self.pf = 1
-
-        self.update = [self.ControlDict[Settings['Control' + str(i)]] for i in [1, 2, 3]]
-        self.QlimPU = min(self.__Qrated / self.__Srated, 1.0)
-        return
-
-
-    def Name(self):
-        return self.__Name
-
-    def ControlledElement(self):
-        return "{}.{}".format(self.ceClass, self.ceName)
-
-    def debugInfo(self):
-        return [self.__Settings['Control{}'.format(i+1)] for i in range(3)]
-
-    def Update(self, Priority, Time, Update):
-        self.TimeChange = self.Time != (Priority, Time)
-        self.Time = (Priority, Time)
-        if not self.TimeChange:
-            self.Itr += 1
-        else:
-            self.Itr = 0
-        return self.update[Priority]()
-
-    def VVARcontrol(self):
-        """Volt / var control implementation
-        """
-        uMin = self.__Settings['uMin']
-        uMax = self.__Settings['uMax']
-        pfLim = self.__Settings['PFlim']
-        uDbMin = self.__Settings['uDbMin']
-        uDbMax = self.__Settings['uDbMax']
-        Priority = self.__Settings['Priority']
-
-        uIn = max(self.__ControlledElm.sBus[0].GetVariable('puVmagAngle')[::2])
-
-        m1 = self.QlimPU / (uMin - uDbMin)
-        m2 = self.QlimPU / (uDbMax - uMax)
-        c1 = self.QlimPU * uDbMin / (uDbMin - uMin)
-        c2 = self.QlimPU * uDbMax / (uMax - uDbMax)
-
-        phases = int(self.__ControlledElm.GetParameter('phases'))
-        S = self.__ControlledElm.GetVariable('Powers')[: 2*phases]
-        Ppv = abs(sum(S[::2]))
-        Pcalc = Ppv / self.__Srated
-        Qpv = -sum(S[1::2])
-        Qpv = Qpv / self.__Srated
-
-        Qcalc = 0
-        if uIn <= uMin:
-            Qcalc = self.QlimPU
-        elif uIn <= uDbMin and uIn > uMin:
-            Qcalc = uIn * m1 + c1
-        elif uIn <= uDbMax and uIn > uDbMin:
-            Qcalc = 0
-        elif uIn <= uMax and uIn > uDbMax:
-            Qcalc = uIn * m2 + c2
-        elif uIn >= uMax:
-            Qcalc = -self.QlimPU
-
-        # adding heavy ball term to improve convergence
-        a = 0.7 + 0.5 * self.__dampCoef * (1 - self.Itr / self.__dssSolver.MaxIterations)
-        b = 0.1 / self.__dampCoef
-        Qcalc = Qpv + (Qcalc - Qpv) * a + (Qpv - self.oldQcalc) * b
-        dQ = Qcalc - Qpv
-
-        #Qcalc = Qpv + (Qcalc - Qpv)# + (Qpv - self.oldQcalc) * 0.1 / self.__dampCoef
-        #dQ = Qcalc - Qpv
-
-        Plim = 1
-        if Priority == 'Var':
-            Plim = (1 - Qcalc ** 2) ** 0.5
-        elif Priority == 'Watt':
-            Qlim = (1 - Ppv/self.__Prated ** 2) ** 0.5
-
-        self.__ControlledElm.SetParameter('kW', self.__Prated * Plim)
-        self.__ControlledElm.SetParameter('kvar', self.__Srated * Qcalc)
-        return abs(dQ)
+from  pydss.pyControllers.pyControllerAbstract import ControllerAbstract
+import math
+import abc
+
+class GenController(ControllerAbstract):
+    """Implementation of smart control modes of modern inverter systems. Subclass of the :class:`pydss.pyControllers.pyControllerAbstract.ControllerAbstract` abstract class.
+
+        :param PvObj: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'PVSystem' element
+        :type FaultObj: class:`pydss.dssElement.dssElement`
+        :param Settings: A dictionary that defines the settings for the PvController.
+        :type Settings: dict
+        :param dssInstance: An :class:`opendssdirect` instance
+        :type dssInstance: :class:`opendssdirect`
+        :param ElmObjectList: Dictionary of all dssElement, dssBus and dssCircuit objects
+        :type ElmObjectList: dict
+        :param dssSolver: An instance of one of the classed defined in :mod:`pydss.SolveMode`.
+        :type dssSolver: :mod:`pydss.SolveMode`
+        :raises: AssertionError if 'PvObj' is not a wrapped OpenDSS PVSystem element
+
+    """
+
+    def __init__(self, GenObj, Settings, dssInstance, ElmObjectList, dssSolver):
+        """Constructor method
+        """
+        super(GenController, self).__init__(GenObj, Settings, dssInstance, ElmObjectList, dssSolver)
+        self.TimeChange = False
+        self.Time = (-1, 0)
+
+        self.oldPcalc = 0
+        self.oldQcalc = 0
+
+        self.__vDisconnected = False
+        self.__pDisconnected = False
+
+        self.__ElmObjectList = ElmObjectList
+        self.ControlDict = {
+            'None'           : lambda: 0,
+            'VVar'           : self.VVARcontrol,
+        }
+
+        self.__ControlledElm = GenObj
+        self.ceClass, self.ceName = self.__ControlledElm.GetInfo()
+
+        assert (self.ceClass.lower()=='generator'), 'GenController works only with an OpenDSS generator element'
+        self.__Name = 'pyCont_' + self.ceClass + '_' +  self.ceName
+        if '_' in  self.ceName:
+            self.Phase =  self.ceName.split('_')[1]
+        else:
+            self.Phase = None
+        self.__ElmObjectList = ElmObjectList
+        self.__ControlledElm = GenObj
+        self.__dssInstance = dssInstance
+        self.__dssSolver = dssSolver
+        self.__Settings = Settings
+
+        self.__BaseKV = float(GenObj.GetParameter('kv'))
+        self.__Srated = float(GenObj.GetParameter('kVA'))
+        self.__Prated = float(GenObj.GetParameter('kW'))
+        self.__Qrated = float(GenObj.GetParameter('maxkvar'))
+        GenObj.SetParameter('minkvar', -self.__Qrated)
+
+        if Settings['Model as PVsystem']:
+            GenObj.SetParameter('model', 3)
+
+
+        self.__dampCoef = Settings['DampCoef']
+
+        self.__PFrated = Settings['PFlim']
+        self.Pmppt = 100
+        self.pf = 1
+
+        self.update = [self.ControlDict[Settings['Control' + str(i)]] for i in [1, 2, 3]]
+        self.QlimPU = min(self.__Qrated / self.__Srated, 1.0)
+        return
+
+
+    def Name(self):
+        return self.__Name
+
+    def ControlledElement(self):
+        return "{}.{}".format(self.ceClass, self.ceName)
+
+    def debugInfo(self):
+        return [self.__Settings['Control{}'.format(i+1)] for i in range(3)]
+
+    def Update(self, Priority, Time, Update):
+        self.TimeChange = self.Time != (Priority, Time)
+        self.Time = (Priority, Time)
+        if not self.TimeChange:
+            self.Itr += 1
+        else:
+            self.Itr = 0
+        return self.update[Priority]()
+
+    def VVARcontrol(self):
+        """Volt / var control implementation
+        """
+        uMin = self.__Settings['uMin']
+        uMax = self.__Settings['uMax']
+        pfLim = self.__Settings['PFlim']
+        uDbMin = self.__Settings['uDbMin']
+        uDbMax = self.__Settings['uDbMax']
+        Priority = self.__Settings['Priority']
+
+        uIn = max(self.__ControlledElm.sBus[0].GetVariable('puVmagAngle')[::2])
+
+        m1 = self.QlimPU / (uMin - uDbMin)
+        m2 = self.QlimPU / (uDbMax - uMax)
+        c1 = self.QlimPU * uDbMin / (uDbMin - uMin)
+        c2 = self.QlimPU * uDbMax / (uMax - uDbMax)
+
+        phases = int(self.__ControlledElm.GetParameter('phases'))
+        S = self.__ControlledElm.GetVariable('Powers')[: 2*phases]
+        Ppv = abs(sum(S[::2]))
+        Pcalc = Ppv / self.__Srated
+        Qpv = -sum(S[1::2])
+        Qpv = Qpv / self.__Srated
+
+        Qcalc = 0
+        if uIn <= uMin:
+            Qcalc = self.QlimPU
+        elif uIn <= uDbMin and uIn > uMin:
+            Qcalc = uIn * m1 + c1
+        elif uIn <= uDbMax and uIn > uDbMin:
+            Qcalc = 0
+        elif uIn <= uMax and uIn > uDbMax:
+            Qcalc = uIn * m2 + c2
+        elif uIn >= uMax:
+            Qcalc = -self.QlimPU
+
+        # adding heavy ball term to improve convergence
+        a = 0.7 + 0.5 * self.__dampCoef * (1 - self.Itr / self.__dssSolver.MaxIterations)
+        b = 0.1 / self.__dampCoef
+        Qcalc = Qpv + (Qcalc - Qpv) * a + (Qpv - self.oldQcalc) * b
+        dQ = Qcalc - Qpv
+
+        #Qcalc = Qpv + (Qcalc - Qpv)# + (Qpv - self.oldQcalc) * 0.1 / self.__dampCoef
+        #dQ = Qcalc - Qpv
+
+        Plim = 1
+        if Priority == 'Var':
+            Plim = (1 - Qcalc ** 2) ** 0.5
+        elif Priority == 'Watt':
+            Qlim = (1 - Ppv/self.__Prated ** 2) ** 0.5
+
+        self.__ControlledElm.SetParameter('kW', self.__Prated * Plim)
+        self.__ControlledElm.SetParameter('kvar', self.__Srated * Qcalc)
+        return abs(dQ)
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/MotorStall.py` & `nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/MotorStall.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,134 +1,134 @@
-#Algebraic model for Type D motor - residential air conditioner
-
-import scipy.signal as signal
-import numpy as np
-import math
-
-from pydss.pyControllers.models import MotorStallSettings
-from pydss.pyControllers.pyControllerAbstract import ControllerAbstract
-
-class MotorStall(ControllerAbstract):   
-    def __init__(self, motor_obj, settings, dss_instance, elm_object_list, dss_solver):
-        super(MotorStall, self).__init__(motor_obj, settings, dss_instance, elm_object_list, dss_solver)
-
-        self._class, self._name = motor_obj.GetInfo()
-        assert self._class == "Load", "Motor stall model can only be used with load models"
-        assert motor_obj.NumPhases == 1, "Motor stall model can only be used with single phase loads"
-        
-        self.name = "Controller-{}-{}".format(self._class, self._name)
-        self._controlled_element = motor_obj
-        self._settings = MotorStallSettings(**settings)
-        self._dss_solver = dss_solver
-        self.mode = 3
-        self.model_mode = self._controlled_element.SetParameter('model', self.mode) # 3 - motor, 1 - Standard constant P+jQ
-        self._controlled_element.SetParameter('vminpu', 0.0)
-
-        self.kw_rated = self._controlled_element.GetParameter('kw')
-        self.kvar_rated = self._controlled_element.GetParameter('kvar')
-        self.kva_rated = (self.kw_rated**2 + self.kvar_rated**2)**0.5
-            
-        self.stall_time_start = 0
-        self.stall = False
-
-        self.r_stall_pu = self._settings.r_stall_pu
-        self.va_base = 100 * 1e6
-        self.kvbase = self._controlled_element.sBus[0].GetVariable("kVBase")
-        self.i_base = 1e3 * self.kw_rated / 1e3 * self.kvbase
-
-        self.dt = dss_solver.GetStepResolutionSeconds()
-        self.h = signal.TransferFunction([1], [self._settings.t_th, 1])
-        self.r = signal.TransferFunction([1], [self._settings.t_restart, 1])
-        
-        self.u = [0, 0]
-        self.t_arr = [0, self.dt]
-        self.x = 0
-        
-        self.i2r = 0
-        
-        self.p_stall = 0
-        self.q_stall = 0
-        
-        return
-    
-    def Name(self):
-        return self.name
-
-    def ControlledElement(self):
-        return "{}.{}".format(self._class, self._name)
-
-    def debugInfo(self):
-        return 
-
-    def Update(self, Priority, time, update_results):
-        self.t = self._dss_solver.GetTotalSeconds()
-        if self.i_base:    
-            self.current_pu = self._controlled_element.GetVariable('CurrentsMagAng')[0] / self.i_base
-            self.voltage = self._controlled_element.GetVariable('VoltagesMagAng')[0]
-            self.p = self._controlled_element.GetVariable('Powers')[0]
-            self.q = self._controlled_element.GetVariable('Powers')[1]
-            self.voltage_pu = self._controlled_element.sBus[0].GetVariable("puVmagAngle")[0]
-
-            if Priority == 0:
-                
-                i2r = self.current_pu ** 2 * self.r_stall_pu       
-                self.i2r = max(self.i2r, i2r)  
-                self.t = np.array([self.t_arr [-1], self.t])
-                self.u = np.array([self.u[-1], self.i2r])
-                tout, yout, xout = signal.lsim(self.h, self.u, self.t_arr, self.x)
-                self.x = xout[-1]
-                i2r_calc = yout[-1] / 50.0
-                
-            
-                comp_lf = self.p / self.kw_rated
-                comp_pf = 0.75 #self.p / (self.p**2 + self.q**2)**0.5
-                
-                v_stall_adj = self._settings.v_stall*(1 + self._settings.lf_adj * (comp_lf-1))
-                v_break_adj = self._settings.v_break*(1 + self._settings.lf_adj * (comp_lf-1))
-                
-                p0 = 1 - self._settings.k_p1 * (1-v_break_adj)**self._settings.n_p1
-                q0 = ((1 - comp_pf**2)**0.5 / comp_pf)-self._settings.k_q1*(1-v_break_adj)**self._settings.n_q1
-                
-                p = self.p / self.kw_rated
-                q = self.q / self.kvar_rated
-                
-                if self.voltage_pu > v_break_adj and not self.stall: 
-                    p = p0 + self._settings.k_p1*(self.voltage_pu-v_break_adj)**self._settings.n_p1
-                    q = q0 + self._settings.k_q1*(self.voltage_pu-v_break_adj)**self._settings.n_q1
-                    self._controlled_element.SetParameter('kw', self.kw_rated * p) 
-                    self._controlled_element.SetParameter('kvar', self.kvar_rated * q) 
-
-                elif self.voltage_pu <= v_break_adj and not self.stall:   
-                    p = p0 + self._settings.k_p2 * (v_break_adj - self.voltage_pu)**self._settings.n_p2
-                    q = q0 + self._settings.k_q2 * (v_break_adj - self.voltage_pu)**self._settings.n_q2
-                    
-                    self._controlled_element.SetParameter('kw', self.kw_rated * p ) 
-                    self._controlled_element.SetParameter('kvar', self.kvar_rated * q) 
-
-                    if self.voltage_pu < v_stall_adj and not self.stall:
-                        self.p_stall = self._controlled_element.GetParameter('kw')
-                        self.q_stall = self._controlled_element.GetParameter('kvar')
-                        self.stall_time_start = self._dss_solver.GetTotalSeconds()
-                        self.stall = True
-                
-                if self.voltage_pu > v_stall_adj and self.stall:
-                    self.stall_time = self._dss_solver.GetTotalSeconds() - self.stall_time_start
-                    if self.stall_time < self._settings.t_stall:
-                        self._controlled_element.SetParameter('kw', self.p_stall)
-                        self._controlled_element.SetParameter('kvar', self.q_stall)
-                    else:
-                        if i2r_calc < self._settings.t_th1t:
-                            Kth = 1
-                        elif i2r_calc > self._settings.t_th2t:
-                            Kth = 0
-                        else:
-                            m = 1 / (self._settings.t_th1t - self._settings.t_th2t)
-                            c = - m * self._settings.t_th2t
-                            Kth = m * i2r_calc + c
-
-                        self._controlled_element.SetParameter('kw',  self.p_stall * Kth ) 
-                        self._controlled_element.SetParameter('kvar', self.q_stall * Kth ) 
-    
-            self.model_mode_old = self.model_mode
-        
-        return 0
-
+#Algebraic model for Type D motor - residential air conditioner
+
+import scipy.signal as signal
+import numpy as np
+import math
+
+from pydss.pyControllers.models import MotorStallSettings
+from pydss.pyControllers.pyControllerAbstract import ControllerAbstract
+
+class MotorStall(ControllerAbstract):   
+    def __init__(self, motor_obj, settings, dss_instance, elm_object_list, dss_solver):
+        super(MotorStall, self).__init__(motor_obj, settings, dss_instance, elm_object_list, dss_solver)
+
+        self._class, self._name = motor_obj.GetInfo()
+        assert self._class == "Load", "Motor stall model can only be used with load models"
+        assert motor_obj.NumPhases == 1, "Motor stall model can only be used with single phase loads"
+        
+        self.name = "Controller-{}-{}".format(self._class, self._name)
+        self._controlled_element = motor_obj
+        self._settings = MotorStallSettings(**settings)
+        self._dss_solver = dss_solver
+        self.mode = 3
+        self.model_mode = self._controlled_element.SetParameter('model', self.mode) # 3 - motor, 1 - Standard constant P+jQ
+        self._controlled_element.SetParameter('vminpu', 0.0)
+
+        self.kw_rated = self._controlled_element.GetParameter('kw')
+        self.kvar_rated = self._controlled_element.GetParameter('kvar')
+        self.kva_rated = (self.kw_rated**2 + self.kvar_rated**2)**0.5
+            
+        self.stall_time_start = 0
+        self.stall = False
+
+        self.r_stall_pu = self._settings.r_stall_pu
+        self.va_base = 100 * 1e6
+        self.kvbase = self._controlled_element.sBus[0].GetVariable("kVBase")
+        self.i_base = 1e3 * self.kw_rated / 1e3 * self.kvbase
+
+        self.dt = dss_solver.GetStepResolutionSeconds()
+        self.h = signal.TransferFunction([1], [self._settings.t_th, 1])
+        self.r = signal.TransferFunction([1], [self._settings.t_restart, 1])
+        
+        self.u = [0, 0]
+        self.t_arr = [0, self.dt]
+        self.x = 0
+        
+        self.i2r = 0
+        
+        self.p_stall = 0
+        self.q_stall = 0
+        
+        return
+    
+    def Name(self):
+        return self.name
+
+    def ControlledElement(self):
+        return "{}.{}".format(self._class, self._name)
+
+    def debugInfo(self):
+        return 
+
+    def Update(self, Priority, time, update_results):
+        self.t = self._dss_solver.GetTotalSeconds()
+        if self.i_base:    
+            self.current_pu = self._controlled_element.GetVariable('CurrentsMagAng')[0] / self.i_base
+            self.voltage = self._controlled_element.GetVariable('VoltagesMagAng')[0]
+            self.p = self._controlled_element.GetVariable('Powers')[0]
+            self.q = self._controlled_element.GetVariable('Powers')[1]
+            self.voltage_pu = self._controlled_element.sBus[0].GetVariable("puVmagAngle")[0]
+
+            if Priority == 0:
+                
+                i2r = self.current_pu ** 2 * self.r_stall_pu       
+                self.i2r = max(self.i2r, i2r)  
+                self.t = np.array([self.t_arr [-1], self.t])
+                self.u = np.array([self.u[-1], self.i2r])
+                tout, yout, xout = signal.lsim(self.h, self.u, self.t_arr, self.x)
+                self.x = xout[-1]
+                i2r_calc = yout[-1] / 50.0
+                
+            
+                comp_lf = self.p / self.kw_rated
+                comp_pf = 0.75 #self.p / (self.p**2 + self.q**2)**0.5
+                
+                v_stall_adj = self._settings.v_stall*(1 + self._settings.lf_adj * (comp_lf-1))
+                v_break_adj = self._settings.v_break*(1 + self._settings.lf_adj * (comp_lf-1))
+                
+                p0 = 1 - self._settings.k_p1 * (1-v_break_adj)**self._settings.n_p1
+                q0 = ((1 - comp_pf**2)**0.5 / comp_pf)-self._settings.k_q1*(1-v_break_adj)**self._settings.n_q1
+                
+                p = self.p / self.kw_rated
+                q = self.q / self.kvar_rated
+                
+                if self.voltage_pu > v_break_adj and not self.stall: 
+                    p = p0 + self._settings.k_p1*(self.voltage_pu-v_break_adj)**self._settings.n_p1
+                    q = q0 + self._settings.k_q1*(self.voltage_pu-v_break_adj)**self._settings.n_q1
+                    self._controlled_element.SetParameter('kw', self.kw_rated * p) 
+                    self._controlled_element.SetParameter('kvar', self.kvar_rated * q) 
+
+                elif self.voltage_pu <= v_break_adj and not self.stall:   
+                    p = p0 + self._settings.k_p2 * (v_break_adj - self.voltage_pu)**self._settings.n_p2
+                    q = q0 + self._settings.k_q2 * (v_break_adj - self.voltage_pu)**self._settings.n_q2
+                    
+                    self._controlled_element.SetParameter('kw', self.kw_rated * p ) 
+                    self._controlled_element.SetParameter('kvar', self.kvar_rated * q) 
+
+                    if self.voltage_pu < v_stall_adj and not self.stall:
+                        self.p_stall = self._controlled_element.GetParameter('kw')
+                        self.q_stall = self._controlled_element.GetParameter('kvar')
+                        self.stall_time_start = self._dss_solver.GetTotalSeconds()
+                        self.stall = True
+                
+                if self.voltage_pu > v_stall_adj and self.stall:
+                    self.stall_time = self._dss_solver.GetTotalSeconds() - self.stall_time_start
+                    if self.stall_time < self._settings.t_stall:
+                        self._controlled_element.SetParameter('kw', self.p_stall)
+                        self._controlled_element.SetParameter('kvar', self.q_stall)
+                    else:
+                        if i2r_calc < self._settings.t_th1t:
+                            Kth = 1
+                        elif i2r_calc > self._settings.t_th2t:
+                            Kth = 0
+                        else:
+                            m = 1 / (self._settings.t_th1t - self._settings.t_th2t)
+                            c = - m * self._settings.t_th2t
+                            Kth = m * i2r_calc + c
+
+                        self._controlled_element.SetParameter('kw',  self.p_stall * Kth ) 
+                        self._controlled_element.SetParameter('kvar', self.q_stall * Kth ) 
+    
+            self.model_mode_old = self.model_mode
+        
+        return 0
+
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/MotorStallBackup.py` & `nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/MotorStallBackup.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,196 +1,196 @@
-#Algebraic model for Type D motor - Residential air conditioner
-'''
-author: Kapil Duwadi
-Version: 1.0
-'''
-
-from pydss.pyControllers.pyControllerAbstract import ControllerAbstract
-import matplotlib.pyplot as plt
-import scipy.signal as signal
-import numpy as np
-import random
-import math
-import os
-
-class MotorStallBackup(ControllerAbstract):
-    """The controller locks a regulator in the event of reverse power flow. Subclass of the :class:`pydss.pyControllers.
-    pyControllerAbstract.ControllerAbstract` abstract class.
-
-        :param RegulatorObj: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'Regulator' element
-        :type FaultObj: class:`pydss.dssElement.dssElement`
-        :param Settings: A dictionary that defines the settings for the PvController.
-        :type Settings: dict
-        :param dssInstance: An :class:`opendssdirect` instance
-        :type dssInstance: :class:`opendssdirect`
-        :param ElmObjectList: Dictionary of all dssElement, dssBus and dssCircuit objects
-        :type ElmObjectList: dict
-        :param dssSolver: An instance of one of the classed defined in :mod:`pydss.SolveMode`.
-        :type dssSolver: :mod:`pydss.SolveMode`
-        :raises: AssertionError if 'RegulatorObj' is not a wrapped OpenDSS Regulator element
-
-    """
-        
-    def __init__(self, MotorObj, Settings, dssInstance, ElmObjectList, dssSolver):
-        super(MotorStallBackup, self).__init__(MotorObj, Settings, dssInstance, ElmObjectList, dssSolver)
-        self._class, self._name = MotorObj.GetInfo()
-        assert self._class == "Load", "Motor stall model can only be used with load models"
-        assert MotorObj.NumPhases == 1, "Motor stall model can only be used with single phase loads"
-        
-        self.name = "Controller-{}-{}".format(self._class, self._name)
-        self._ControlledElm = MotorObj
-        self.__Settings = Settings
-        self.__dssSolver = dssSolver
-        self.mode = 3
-        self.model_mode = self._ControlledElm.SetParameter('model', self.mode) # 3 - motor, 1 - Standard constant P+jQ
-        self._ControlledElm.SetParameter('vminpu', 0.0)
-
-        self.kw = self.__Settings['ratedKW']
-        S = self.kw / self.__Settings['ratedPF']
-        self.kvar = math.sqrt(S**2 - self.kw**2)
-        mode = 2
-        
-        if mode == 1:
-            self.kw_rated = self._ControlledElm.SetParameter('kw', self.kw)
-            self.kvar_rated = self._ControlledElm.SetParameter('kvar', self.kvar)        
-        elif mode == 2:
-            self.kw_rated = self._ControlledElm.GetParameter('kw')
-            self.kvar_rated = self._ControlledElm.GetParameter('kvar')
-        self.kva_rated = (self.kw_rated**2 + self.kvar_rated**2)**0.5
-            
-        self.stall_time_start = 0
-        self.stall = False
-        self.disconnected =False
-        self.Tdisconnect_start = 0
-        
-        self.R_stall_pu = self.__Settings['R_stall_pu']
-        self.kvbase = self._ControlledElm.sBus[0].GetVariable("kVBase")
-        self.Ibase = 1e3 * self.kva_rated /1e3 * self.kvbase
-
-        self.dt = dssSolver.GetStepResolutionSeconds()
-        self.H = signal.TransferFunction([1], [self.__Settings['Tth'], 1])
-        self.R = signal.TransferFunction([1], [self.__Settings['Trestart'], 1])
-        
-        self.U = [35.029919199211, 35.029919199211]
-        self.T = [0, self.dt]
-        self.X = 0
-
-        self.rU = [0, 0]
-        self.rT = [0, self.dt]
-        self.rX = 0
-        self.v = 0
-        
-        self.voltage_pu_old = 1.0
-        # Delete after debug #
-        
-        self.file = open(r"C:\Users\alatif\Desktop\NAERM\models\17735\motor.csv", "w")
-        self.file.write("CompLF,CompPF,V_stall_adj,V_break_adj,P0,Q0,p_run_pu,q_run_pu,p_stall_pu,q_stall_pu\n")
-        
-        return
-    
-    def Name(self):
-        return self.name
-
-    def ControlledElement(self):
-        return "{}.{}".format(self._class, self._name)
-
-    def debugInfo(self):
-        return 
-
-    def Update(self, Priority, Time, UpdateResults):
-        
-        self.t = self.__dssSolver.GetTotalSeconds()
-        self.current_pu = self._ControlledElm.GetVariable('CurrentsMagAng')[0] / self.Ibase
-        self.voltage = self._ControlledElm.GetVariable('VoltagesMagAng')[0]
-        self.p = self._ControlledElm.GetVariable('Powers')[0]
-        self.q = self._ControlledElm.GetVariable('Powers')[1]
-        self.voltage_pu = self._ControlledElm.sBus[0].GetVariable("puVmagAngle")[0]
-
-        if Priority == 0:
-            
-            u = self.current_pu ** 2 * self.R_stall_pu       
-          
-            CompLF = 1.0 #self.p / self.kw_rated
-            CompPF = self.p / (self.p**2 + self.q**2)**0.5
-            
-            V_stall_adj = self.__Settings['Vstall']*(1 + self.__Settings['LFadj'] * (CompLF-1))
-            V_break_adj = self.__Settings['Vbreak']*(1 + self.__Settings['LFadj'] * (CompLF-1))
-            
-            P0 = 1 - self.__Settings['Kp1'] * (1-V_break_adj)**self.__Settings['Np1']
-            Q0 = ((1 - CompPF**2)**0.5 / CompPF)-self.__Settings['Kq1']*(1-V_break_adj)**self.__Settings['Nq1']
-            
-            
-            p_run_pu = P0 + self.__Settings['Kp1']*(self.voltage_pu-V_break_adj)**self.__Settings['Np1']
-            q_run_pu = Q0 + self.__Settings['Kq1']*(self.voltage_pu-V_break_adj)**self.__Settings['Nq1']
-            
-            p_stall_pu = P0 + self.__Settings['Kp2'] * (V_break_adj - self.voltage_pu)**self.__Settings['Np2']
-            q_stall_pu = Q0 + self.__Settings['Kq2'] * (V_break_adj - self.voltage_pu)**self.__Settings['Nq2']
-
-            self.file.write(f"{CompLF},{CompPF},{V_stall_adj},{V_break_adj},{P0},{Q0},{p_run_pu},{q_run_pu},{p_stall_pu},{q_stall_pu}\n")
-            self.file.flush()
-
-            if self.voltage_pu < V_stall_adj and self.stall and self.model_mode == self.mode:
-                self.stall_time = self.__dssSolver.GetTotalSeconds() - self.stall_time_start
-                if self.stall_time > self.__Settings['Tstall']:
-                    self._ControlledElm.SetParameter('kw', self.kw_rated * self.__Settings['Pfault'])
-                    self._ControlledElm.SetParameter('kvar', self.kvar_rated * self.__Settings['Qfault'])
-                    self.model_mode = self._ControlledElm.SetParameter('model', 2)
-                      
-            if self.voltage_pu < V_stall_adj and not self.stall:
-                self.stall_time_start = self.__dssSolver.GetTotalSeconds()
-                self.stall = True
-            
-            u = 1 if self.model_mode == 2 else 0
-            
-            self.T = np.array([self.T[-1], self.t])
-            self.U = np.array([self.U[-1], u])
-            tout, yout, xout = signal.lsim(self.H, self.U, self.T, self.X)
-            self.X = xout[-1]
-            theeta = yout[-1]
-            
-            if theeta < self.__Settings['Tth1t']:
-                Kth = 1
-            elif theeta > self.__Settings['Tth2t']:
-                Kth = 0
-            else:
-                m = 1 / (self.__Settings['Tth1t'] - self.__Settings['Tth2t'])
-                c = - m * self.__Settings['Tth2t']
-                Kth = m * theeta + c
-            
-            if self.model_mode == 2:
-                p_stall = self.kw_rated * self.__Settings['Pfault'] 
-                q_stall = self.kvar_rated * self.__Settings['Qfault'] 
-                
-                
-                if p_stall * Kth > self.kw_rated:
-                    self._ControlledElm.SetParameter('kw', p_stall * Kth)
-                else:
-                    self._ControlledElm.SetParameter('kw', self.kw_rated * p_run_pu)
-                
-                #TODO: uncommect the line below and comment the if statements below to allow q to fall below self.kvar_rated
-                #self._ControlledElm.SetParameter('kvar', q_stall * Kth) 
-                
-                if q_stall * Kth > self.kvar_rated:
-                    self._ControlledElm.SetParameter('kvar', self.p * Kth)
-                else:
-                    self._ControlledElm.SetParameter('kvar', self.kvar_rated* q_run_pu)
-
-            if Kth==0 and self.voltage_pu > self.__Settings['Vrstrt']:
-                self.v = 1
-                
-            self.rT = np.array([self.rT[-1], self.t])
-            self.rU = np.array([self.rU[-1], self.v])
-            _, yout_r, xout_r = signal.lsim(self.R, self.rU, self.rT, self.rX)
-            self.rX = xout_r[-1]
-            reconnect = yout_r[-1]
-
-            set_p =  self._ControlledElm.GetParameter("kw")
-            set_q =  self._ControlledElm.GetParameter("kvar")
-
-            if set_p == self.kw_rated and set_q== self.kvar_rated and self.stall and reconnect>0:
-                self.model_mode = self._ControlledElm.SetParameter('model', self.mode)
-                self.stall = False
-                
-            self.voltage_pu_old = self.voltage_pu
-        self.model_mode_old = self.model_mode
-        return 0
-
+#Algebraic model for Type D motor - Residential air conditioner
+'''
+author: Kapil Duwadi
+Version: 1.0
+'''
+
+from pydss.pyControllers.pyControllerAbstract import ControllerAbstract
+import matplotlib.pyplot as plt
+import scipy.signal as signal
+import numpy as np
+import random
+import math
+import os
+
+class MotorStallBackup(ControllerAbstract):
+    """The controller locks a regulator in the event of reverse power flow. Subclass of the :class:`pydss.pyControllers.
+    pyControllerAbstract.ControllerAbstract` abstract class.
+
+        :param RegulatorObj: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'Regulator' element
+        :type FaultObj: class:`pydss.dssElement.dssElement`
+        :param Settings: A dictionary that defines the settings for the PvController.
+        :type Settings: dict
+        :param dssInstance: An :class:`opendssdirect` instance
+        :type dssInstance: :class:`opendssdirect`
+        :param ElmObjectList: Dictionary of all dssElement, dssBus and dssCircuit objects
+        :type ElmObjectList: dict
+        :param dssSolver: An instance of one of the classed defined in :mod:`pydss.SolveMode`.
+        :type dssSolver: :mod:`pydss.SolveMode`
+        :raises: AssertionError if 'RegulatorObj' is not a wrapped OpenDSS Regulator element
+
+    """
+        
+    def __init__(self, MotorObj, Settings, dssInstance, ElmObjectList, dssSolver):
+        super(MotorStallBackup, self).__init__(MotorObj, Settings, dssInstance, ElmObjectList, dssSolver)
+        self._class, self._name = MotorObj.GetInfo()
+        assert self._class == "Load", "Motor stall model can only be used with load models"
+        assert MotorObj.NumPhases == 1, "Motor stall model can only be used with single phase loads"
+        
+        self.name = "Controller-{}-{}".format(self._class, self._name)
+        self._ControlledElm = MotorObj
+        self.__Settings = Settings
+        self.__dssSolver = dssSolver
+        self.mode = 3
+        self.model_mode = self._ControlledElm.SetParameter('model', self.mode) # 3 - motor, 1 - Standard constant P+jQ
+        self._ControlledElm.SetParameter('vminpu', 0.0)
+
+        self.kw = self.__Settings['ratedKW']
+        S = self.kw / self.__Settings['ratedPF']
+        self.kvar = math.sqrt(S**2 - self.kw**2)
+        mode = 2
+        
+        if mode == 1:
+            self.kw_rated = self._ControlledElm.SetParameter('kw', self.kw)
+            self.kvar_rated = self._ControlledElm.SetParameter('kvar', self.kvar)        
+        elif mode == 2:
+            self.kw_rated = self._ControlledElm.GetParameter('kw')
+            self.kvar_rated = self._ControlledElm.GetParameter('kvar')
+        self.kva_rated = (self.kw_rated**2 + self.kvar_rated**2)**0.5
+            
+        self.stall_time_start = 0
+        self.stall = False
+        self.disconnected =False
+        self.Tdisconnect_start = 0
+        
+        self.R_stall_pu = self.__Settings['R_stall_pu']
+        self.kvbase = self._ControlledElm.sBus[0].GetVariable("kVBase")
+        self.Ibase = 1e3 * self.kva_rated /1e3 * self.kvbase
+
+        self.dt = dssSolver.GetStepResolutionSeconds()
+        self.H = signal.TransferFunction([1], [self.__Settings['Tth'], 1])
+        self.R = signal.TransferFunction([1], [self.__Settings['Trestart'], 1])
+        
+        self.U = [35.029919199211, 35.029919199211]
+        self.T = [0, self.dt]
+        self.X = 0
+
+        self.rU = [0, 0]
+        self.rT = [0, self.dt]
+        self.rX = 0
+        self.v = 0
+        
+        self.voltage_pu_old = 1.0
+        # Delete after debug #
+        
+        self.file = open(r"C:\Users\alatif\Desktop\NAERM\models\17735\motor.csv", "w")
+        self.file.write("CompLF,CompPF,V_stall_adj,V_break_adj,P0,Q0,p_run_pu,q_run_pu,p_stall_pu,q_stall_pu\n")
+        
+        return
+    
+    def Name(self):
+        return self.name
+
+    def ControlledElement(self):
+        return "{}.{}".format(self._class, self._name)
+
+    def debugInfo(self):
+        return 
+
+    def Update(self, Priority, Time, UpdateResults):
+        
+        self.t = self.__dssSolver.GetTotalSeconds()
+        self.current_pu = self._ControlledElm.GetVariable('CurrentsMagAng')[0] / self.Ibase
+        self.voltage = self._ControlledElm.GetVariable('VoltagesMagAng')[0]
+        self.p = self._ControlledElm.GetVariable('Powers')[0]
+        self.q = self._ControlledElm.GetVariable('Powers')[1]
+        self.voltage_pu = self._ControlledElm.sBus[0].GetVariable("puVmagAngle")[0]
+
+        if Priority == 0:
+            
+            u = self.current_pu ** 2 * self.R_stall_pu       
+          
+            CompLF = 1.0 #self.p / self.kw_rated
+            CompPF = self.p / (self.p**2 + self.q**2)**0.5
+            
+            V_stall_adj = self.__Settings['Vstall']*(1 + self.__Settings['LFadj'] * (CompLF-1))
+            V_break_adj = self.__Settings['Vbreak']*(1 + self.__Settings['LFadj'] * (CompLF-1))
+            
+            P0 = 1 - self.__Settings['Kp1'] * (1-V_break_adj)**self.__Settings['Np1']
+            Q0 = ((1 - CompPF**2)**0.5 / CompPF)-self.__Settings['Kq1']*(1-V_break_adj)**self.__Settings['Nq1']
+            
+            
+            p_run_pu = P0 + self.__Settings['Kp1']*(self.voltage_pu-V_break_adj)**self.__Settings['Np1']
+            q_run_pu = Q0 + self.__Settings['Kq1']*(self.voltage_pu-V_break_adj)**self.__Settings['Nq1']
+            
+            p_stall_pu = P0 + self.__Settings['Kp2'] * (V_break_adj - self.voltage_pu)**self.__Settings['Np2']
+            q_stall_pu = Q0 + self.__Settings['Kq2'] * (V_break_adj - self.voltage_pu)**self.__Settings['Nq2']
+
+            self.file.write(f"{CompLF},{CompPF},{V_stall_adj},{V_break_adj},{P0},{Q0},{p_run_pu},{q_run_pu},{p_stall_pu},{q_stall_pu}\n")
+            self.file.flush()
+
+            if self.voltage_pu < V_stall_adj and self.stall and self.model_mode == self.mode:
+                self.stall_time = self.__dssSolver.GetTotalSeconds() - self.stall_time_start
+                if self.stall_time > self.__Settings['Tstall']:
+                    self._ControlledElm.SetParameter('kw', self.kw_rated * self.__Settings['Pfault'])
+                    self._ControlledElm.SetParameter('kvar', self.kvar_rated * self.__Settings['Qfault'])
+                    self.model_mode = self._ControlledElm.SetParameter('model', 2)
+                      
+            if self.voltage_pu < V_stall_adj and not self.stall:
+                self.stall_time_start = self.__dssSolver.GetTotalSeconds()
+                self.stall = True
+            
+            u = 1 if self.model_mode == 2 else 0
+            
+            self.T = np.array([self.T[-1], self.t])
+            self.U = np.array([self.U[-1], u])
+            tout, yout, xout = signal.lsim(self.H, self.U, self.T, self.X)
+            self.X = xout[-1]
+            theeta = yout[-1]
+            
+            if theeta < self.__Settings['Tth1t']:
+                Kth = 1
+            elif theeta > self.__Settings['Tth2t']:
+                Kth = 0
+            else:
+                m = 1 / (self.__Settings['Tth1t'] - self.__Settings['Tth2t'])
+                c = - m * self.__Settings['Tth2t']
+                Kth = m * theeta + c
+            
+            if self.model_mode == 2:
+                p_stall = self.kw_rated * self.__Settings['Pfault'] 
+                q_stall = self.kvar_rated * self.__Settings['Qfault'] 
+                
+                
+                if p_stall * Kth > self.kw_rated:
+                    self._ControlledElm.SetParameter('kw', p_stall * Kth)
+                else:
+                    self._ControlledElm.SetParameter('kw', self.kw_rated * p_run_pu)
+                
+                #TODO: uncommect the line below and comment the if statements below to allow q to fall below self.kvar_rated
+                #self._ControlledElm.SetParameter('kvar', q_stall * Kth) 
+                
+                if q_stall * Kth > self.kvar_rated:
+                    self._ControlledElm.SetParameter('kvar', self.p * Kth)
+                else:
+                    self._ControlledElm.SetParameter('kvar', self.kvar_rated* q_run_pu)
+
+            if Kth==0 and self.voltage_pu > self.__Settings['Vrstrt']:
+                self.v = 1
+                
+            self.rT = np.array([self.rT[-1], self.t])
+            self.rU = np.array([self.rU[-1], self.v])
+            _, yout_r, xout_r = signal.lsim(self.R, self.rU, self.rT, self.rX)
+            self.rX = xout_r[-1]
+            reconnect = yout_r[-1]
+
+            set_p =  self._ControlledElm.GetParameter("kw")
+            set_q =  self._ControlledElm.GetParameter("kvar")
+
+            if set_p == self.kw_rated and set_q== self.kvar_rated and self.stall and reconnect>0:
+                self.model_mode = self._ControlledElm.SetParameter('model', self.mode)
+                self.stall = False
+                
+            self.voltage_pu_old = self.voltage_pu
+        self.model_mode_old = self.model_mode
+        return 0
+
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/MotorStallSimple.py` & `nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/MotorStallSimple.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,80 +1,80 @@
-#Algebraic model for Type D motor - Residential air conditioner
-'''
-author: Aadil Latif
-Version: 1.0
-'''
-
-from pydss.pyControllers.pyControllerAbstract import ControllerAbstract
-from pydss.pyControllers.models import MotorStallSimpleSettings
-
-class MotorStallSimple(ControllerAbstract):
-
-    def __init__(self, motor_obj, settings, dss_instance, elm_object_list, dss_solver):
-        super(MotorStallSimple, self).__init__(motor_obj, settings, dss_instance, elm_object_list, dss_solver)
-        self._class, self._name = motor_obj.GetInfo()
-        self.name = "Controller-{}-{}".format(self._class, self._name)
-        self._controlled_element = motor_obj
-        self._settings = MotorStallSimpleSettings(**settings)
-        self._dss_solver = dss_solver
-
-        self._controlled_element.SetParameter('model', 3)
-        self._controlled_element.SetParameter('vminpu', 0.0)
-        self._controlled_element.SetParameter('vlowpu', 0.0)
-        self.kw = self._controlled_element.GetParameter('kw')
-        self.kvar = self._controlled_element.GetParameter('kvar')
-
-        self.stall_time_start = 0
-        self.stall = False
-        self.disconnected =False
-        self.t_disconnect_start = 0
-        
-        return
-
-
-    def Name(self):
-        return self.name
-
-    def ControlledElement(self):
-        return "{}.{}".format(self._class, self._name)
-
-    def debugInfo(self):
-        return [] #[self._settings['Control{}'.format(i+1)] for i in range(3)]
-
-
-    def Update(self, priority, time, _):
-        assert priority in [0, 1, 2], "Valid control priorities can range from 0-2."
-        v_e_mags = 1.0
-        if priority == 0:
-            v_base = self._controlled_element.sBus[0].GetVariable('kVBase') * 1000
-            v_e_mags = max(self._controlled_element.GetVariable('VoltagesMagAng')[::2])/ v_base
-            if v_e_mags < self._settings.v_stall and not self.stall and not self.disconnected:                
-                self._controlled_element.SetParameter('kw', self.kw * self._settings.p_fault)
-                self._controlled_element.SetParameter('kvar', self.kvar * self._settings.q_fault )
-                self._controlled_element.SetParameter('model', 2)
-                self.stall = True
-                self.stall_time_start = self._dss_solver.GetTotalSeconds()
-                return 0.1
-            return 0
-        if priority == 1:
-            if self.stall:
-                
-                self.stall_time = self._dss_solver.GetTotalSeconds() - self.stall_time_start
-                if self.stall_time > self._settings.t_protection:
-                    self.stall = False
-                    self.disconnected = True
-                    self._controlled_element.SetParameter('kw', 0)
-                    self._controlled_element.SetParameter('kvar', 0)
-                    self.t_disconnect_start = self._dss_solver.GetTotalSeconds()
-                return 0 
-            return 0
-        if priority == 2:
-            if self.disconnected:
-                time = self._dss_solver.GetTotalSeconds() - self.t_disconnect_start
-                if time > self._settings.t_reconnect and v_e_mags > self._settings.v_stall:
-                    self.disconnected = False
-                    self._controlled_element.SetParameter('kw', self.kw)
-                    self._controlled_element.SetParameter('kvar', self.kvar)
-                    self._controlled_element.SetParameter('model', 3)
-                    self._controlled_element.SetParameter('vminpu', 0.0)
-        return 0
-
+#Algebraic model for Type D motor - Residential air conditioner
+'''
+author: Aadil Latif
+Version: 1.0
+'''
+
+from pydss.pyControllers.pyControllerAbstract import ControllerAbstract
+from pydss.pyControllers.models import MotorStallSimpleSettings
+
+class MotorStallSimple(ControllerAbstract):
+
+    def __init__(self, motor_obj, settings, dss_instance, elm_object_list, dss_solver):
+        super(MotorStallSimple, self).__init__(motor_obj, settings, dss_instance, elm_object_list, dss_solver)
+        self._class, self._name = motor_obj.GetInfo()
+        self.name = "Controller-{}-{}".format(self._class, self._name)
+        self._controlled_element = motor_obj
+        self._settings = MotorStallSimpleSettings(**settings)
+        self._dss_solver = dss_solver
+
+        self._controlled_element.SetParameter('model', 3)
+        self._controlled_element.SetParameter('vminpu', 0.0)
+        self._controlled_element.SetParameter('vlowpu', 0.0)
+        self.kw = self._controlled_element.GetParameter('kw')
+        self.kvar = self._controlled_element.GetParameter('kvar')
+
+        self.stall_time_start = 0
+        self.stall = False
+        self.disconnected =False
+        self.t_disconnect_start = 0
+        
+        return
+
+
+    def Name(self):
+        return self.name
+
+    def ControlledElement(self):
+        return "{}.{}".format(self._class, self._name)
+
+    def debugInfo(self):
+        return [] #[self._settings['Control{}'.format(i+1)] for i in range(3)]
+
+
+    def Update(self, priority, time, _):
+        assert priority in [0, 1, 2], "Valid control priorities can range from 0-2."
+        v_e_mags = 1.0
+        if priority == 0:
+            v_base = self._controlled_element.sBus[0].GetVariable('kVBase') * 1000
+            v_e_mags = max(self._controlled_element.GetVariable('VoltagesMagAng')[::2])/ v_base
+            if v_e_mags < self._settings.v_stall and not self.stall and not self.disconnected:                
+                self._controlled_element.SetParameter('kw', self.kw * self._settings.p_fault)
+                self._controlled_element.SetParameter('kvar', self.kvar * self._settings.q_fault )
+                self._controlled_element.SetParameter('model', 2)
+                self.stall = True
+                self.stall_time_start = self._dss_solver.GetTotalSeconds()
+                return 0.1
+            return 0
+        if priority == 1:
+            if self.stall:
+                
+                self.stall_time = self._dss_solver.GetTotalSeconds() - self.stall_time_start
+                if self.stall_time > self._settings.t_protection:
+                    self.stall = False
+                    self.disconnected = True
+                    self._controlled_element.SetParameter('kw', 0)
+                    self._controlled_element.SetParameter('kvar', 0)
+                    self.t_disconnect_start = self._dss_solver.GetTotalSeconds()
+                return 0 
+            return 0
+        if priority == 2:
+            if self.disconnected:
+                time = self._dss_solver.GetTotalSeconds() - self.t_disconnect_start
+                if time > self._settings.t_reconnect and v_e_mags > self._settings.v_stall:
+                    self.disconnected = False
+                    self._controlled_element.SetParameter('kw', self.kw)
+                    self._controlled_element.SetParameter('kvar', self.kvar)
+                    self._controlled_element.SetParameter('model', 3)
+                    self._controlled_element.SetParameter('vminpu', 0.0)
+        return 0
+
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/PvController.py` & `nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/PvController.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,311 +1,311 @@
-from collections import namedtuple
-import math
-
-from pydss.pyControllers.enumerations import SmartControls, ControlPriority, VoltWattCurtailmentStrategy, VoltageCalcModes
-from pydss.pyControllers.pyControllerAbstract import ControllerAbstract
-from pydss.pyControllers.models import PvControllerModel
-
-
-class PvController(ControllerAbstract):
-    """Implementation of smart control modes of modern inverter systems. Subclass of the :class:`pydss.pyControllers.pyControllerAbstract.ControllerAbstract` abstract class.
-
-        :param pv_obj: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'PVSystem' element
-        :type FaultObj: class:`pydss.dssElement.dssElement`
-        :param settings: A dictionary that defines the settings for the PvController.
-        :type settings: dict
-        :param dss_instance: An :class:`opendssdirect` instance
-        :type dss_instance: :class:`opendssdirect`
-        :param element_object_list: Dictionary of all dssElement, dssBus and dssCircuit objects
-        :type element_object_list: dict
-        :param dss_solver: An instance of one of the classed defined in :mod:`pydss.SolveMode`.
-        :type dss_solver: :mod:`pydss.SolveMode`
-        :raises: Assertionerror if 'pv_obj' is not a wrapped OpenDSS PVSystem element
-
-    """
-
-    def __init__(self, pv_obj, settings, dss_instance, element_object_list, dss_solver):
-        """Constructor method
-        """
-        
-        super(PvController, self).__init__(pv_obj, settings, dss_instance, element_object_list, dss_solver)
-        self.time_change = False
-        self.time = (-1, 0)
-        self.old_q_pv = 0
-        self.old_p_calc = 0
-
-        self._v_disconnected = False
-        self._p_disconnected = False
-
-        self._element_object_list = element_object_list
-        self.control_dict = {
-            SmartControls.NONE : lambda: 0,
-            SmartControls.CONSTANT_POWER_FACTOR : self.constant_powerfactor_control,
-            SmartControls.VARIABLE_POWER_FACTOR : self.variable_powerfactor_control,
-            SmartControls.VOLT_VAR : self.volt_var_control,
-            SmartControls.VOLT_WATT : self.volt_watt_control,
-            SmartControls.TRIP : self.cutoff_control,
-        }
-
-        self._controlled_element = pv_obj
-        self.ce_class, self.ce_name = self._controlled_element.GetInfo()
-
-        assert (self.ce_class.lower()=='pvsystem'), 'PvController works only with an OpenDSS PVSystem element'
-        self._name = 'pyCont_' + self.ce_class + '_' +  self.ce_name
-        if '_' in  self.ce_name:
-            self.phase =  self.ce_name.split('_')[1]
-        else:
-            self.phase = None
-        self._element_object_list = element_object_list
-        self._controlled_element = pv_obj
-        self._dss_instance = dss_instance
-        self._dss_solver = dss_solver
-        self._settings = PvControllerModel(**settings)
-
-        self._base_kv = float(pv_obj.GetParameter('kv'))
-        self._s_rated = float(pv_obj.GetParameter('kVA'))
-        self._p_rated = float(pv_obj.GetParameter('Pmpp'))
-        self._q_rated = float(pv_obj.GetParameter('kvarMax'))
-        self._cutin = float(pv_obj.SetParameter('%cutin', 0)) / 100
-        self._cutout = float(pv_obj.SetParameter('%cutout', 0)) / 100
-        self._damp_coef = self._settings.damp_coef
-        self._pf_rated = self._settings.pf_lim
-        self.p_mppt = 100
-        self.pf = 1
-
-        self.update = []
-            
-        for i in range(1, 4):
-            controller_type = getattr(self._settings, 'control' + str(i))
-            self.update.append(self.control_dict[controller_type])
-
-
-        if self._settings.priority == ControlPriority.VAR:
-            pv_obj.SetParameter('Wattpriority', "False")
-        elif self._settings.priority == ControlPriority.WATT:
-            pv_obj.SetParameter('Wattpriority', "True")
-        #pv_obj.SetParameter('VarFollowInverter', "False")
-
-        #self.q_lim_pu = self._q_rated / self._s_rated if self._q_rated < self._s_rated else 1
-
-        self.q_lim_pu = min(self._q_rated / self._s_rated, self._settings.q_lim_pu, 1.0)
-        self.itr = 0
-        return
-
-    def Name(self):
-        return self._name
-
-    def ControlledElement(self):
-        return "{}.{}".format(self.ce_class, self.ce_name)
-
-    def debugInfo(self):
-        return [getattr(self._settings, 'control' + str(i)) for i in range(3)]
-
-    def Update(self, priority, time, update):
-        self.time_change = self.time != (priority, time)
-        self.time = (priority, time)
-        p_pv = -sum(self._controlled_element.GetVariable('Powers')[::2]) / self._p_rated
-
-        if self.time_change:
-            self.itr = 0
-        else:
-            self.itr += 1
-
-        if self._p_disconnected:
-            if p_pv < self._cutin:
-                return 0
-            else:
-                self._p_disconnected = False
-        else:
-            if p_pv < self._cutout:
-                self._p_disconnected = True
-                self._controlled_element.SetParameter('pf', 1)
-                return 0
-        return self.update[priority]()
-
-    def volt_watt_control(self):
-        """Volt / Watt  control implementation
-        """
-        u_min_c = self._settings.u_min_c
-        u_max_c = self._settings.u_max_c
-        p_min  = self._settings.p_min_vw / 100
-
-        u_in = max(self._controlled_element.sBus[0].GetVariable('puVmagAngle')[::2])
-        p_pv = -sum(self._controlled_element.GetVariable('Powers')[::2]) / self._s_rated
-        q_pv = -sum(self._controlled_element.GetVariable('Powers')[1::2]) / self._s_rated
-
-
-        #p_pvoutPU = p_pv / self._p_rated
-
-        p_lim = (1 - q_pv ** 2) ** 0.5 if self._settings.vw_type == VoltWattCurtailmentStrategy.AVAILABLE_POWER else 1
-        m = (1 - p_min) / (u_min_c - u_max_c)
-        #m = (p_lim - p_min) / (u_min_c - u_max_c)
-        c = ((p_min * u_min_c) - u_max_c) / (u_min_c - u_max_c)
-
-        if u_in < u_min_c:
-            p_calc = p_lim
-        elif u_in < u_max_c and u_in > u_min_c:
-            p_calc = min(m * u_in + c, p_lim)
-        else:
-            p_calc = p_min
-
-        if p_pv > p_calc or (p_pv > 0 and self.p_mppt < 100):
-            # adding heavy ball term to improve convergence
-            dp = (p_pv - p_calc) * 0.5 / self._damp_coef + (self.old_p_calc - p_pv) * 0.1 / self._damp_coef
-            p_calc = p_pv - dp
-            self.p_mppt = min(self.p_mppt * p_calc / p_pv, 100)
-            self._controlled_element.SetParameter('%Pmpp', self.p_mppt)
-            self.pf = math.cos(math.atan(q_pv / p_calc))
-            if q_pv < 0:
-                self.pf = -self.pf
-            self._controlled_element.SetParameter('pf', self.pf)
-        else:
-            dp = 0
-
-        error = abs(dp)
-        self.old_p_calc = p_pv
-        return error
-
-    def cutoff_control(self):
-        """Over voltage trip implementation
-        """
-        u_in = max(self._controlled_element.sBus[0].GetVariable('puVmagAngle')[::2])
-        u_cut = self._settings['%u_cutoff']
-        if u_in >= u_cut:
-            self._controlled_element.SetParameter('%Pmpp', 0)
-            self._controlled_element.SetParameter('pf', 1)
-            if self._v_disconnected:
-                return 0
-            else:
-                self._v_disconnected = True
-                return self._p_rated
-
-        if self.time_change and self._v_disconnected and u_in < u_cut:
-            self._controlled_element.SetParameter('%Pmpp', self.p_mppt)
-            self._controlled_element.SetParameter('pf', self.pf)
-            self._v_disconnected = False
-            return self._p_rated
-
-        return 0
-
-    def constant_powerfactor_control(self):
-        """Constant power factor implementation
-        """
-        pf_set = self._settings.pf
-        pf_act = self._controlled_element.GetParameter('pf')
-        p_pv = abs(sum(self._controlled_element.GetVariable('Powers')[::2])) / self._s_rated
-        q_pv = -sum(self._controlled_element.GetVariable('Powers')[1::2]) / self._s_rated
-
-        if self._settings.priority == ControlPriority.PF:
-           # if self.time_change:
-            p_lim = pf_set * 100
-            self._controlled_element.SetParameter('%Pmpp', p_lim)
-           # else:
-        else:
-            if self._settings.priority == ControlPriority.VAR:
-                #add code for var priority here
-                p_lim = 0
-            else:
-                p_lim = 1
-            if self.time_change:
-                self.p_mppt = 100
-            else:
-                self.p_mppt = p_lim  * self._s_rated
-
-        error = abs(pf_set + pf_act)
-        self._controlled_element.SetParameter('pf', str(-pf_set))
-        return error
-
-    def variable_powerfactor_control(self):
-        """Variable power factor control implementation
-        """
-        p_min = self._settings.p_min
-        p_max = self._settings.p_max
-        pf_min = self._settings.pf_min
-        pf_max = self._settings.pf_max
-        self._dss_solver.reSolve()
-        p_calc = abs(sum(-(float(x)) for x in self._controlled_element.GetVariable('Powers')[0::2]) ) / self._s_rated
-        if p_calc > 0:
-            if p_calc < p_min:
-                pf = pf_max
-            elif p_calc > p_max:
-                pf = pf_min
-            else:
-                m = (pf_max - pf_min) / (p_min - p_max)
-                c = (pf_min * p_min - pf_max * p_max) / (p_min - p_max)
-                pf = p_calc * m + c
-        else:
-            pf = pf_max
-
-        self._controlled_element.SetParameter('irradiance', 1)
-        self._controlled_element.SetParameter('pf', str(-pf))
-        self._dss_solver.reSolve()
-
-        for i in range(10):
-            error = pf + float(self._controlled_element.GetParameter('pf'))
-            if abs(error) < 1E-4:
-                break
-            p_irr = float(self._controlled_element.GetParameter('irradiance'))
-            self._controlled_element.SetParameter('pf', str(-pf))
-            self._controlled_element.SetParameter('irradiance', p_irr * (1 + error*1.5))
-            self._dss_solver.reSolve()
-
-        return 0
-
-    def volt_var_control(self):
-        """Volt / var control implementation
-        """
-
-        u_mag = self._controlled_element.GetVariable('VoltagesMagAng')[::2]
-        u_mag = [i for i in u_mag if i != 0]
-        kv_base = self._controlled_element.sBus[0].GetVariable('kVBase') * 1000
-
-        if self._settings.voltage_calc_mode == VoltageCalcModes.MAX:
-            u_in = max(u_mag) / kv_base
-        elif self._settings.voltage_calc_mode == VoltageCalcModes.AVG:
-            u_in = sum(u_mag) / (len(u_mag) * kv_base)
-        elif self._settings.voltage_calc_mode == VoltageCalcModes.MIN:
-            u_in = min(u_mag) / kv_base
-        elif self._settings.voltage_calc_mode == VoltageCalcModes.A:
-            u_in = u_mag[0] / kv_base
-        elif self._settings.voltage_calc_mode == VoltageCalcModes.B:
-            u_in = u_mag[1] / kv_base
-        elif self._settings.voltage_calc_mode == VoltageCalcModes.C:
-            u_in = u_mag[3] / kv_base
-        else:
-            u_in = max(u_mag) / kv_base
-
-        p_pv = abs(sum(self._controlled_element.GetVariable('Powers')[::2]))
-        p_calc = p_pv / self._s_rated
-        q_pv = -sum(self._controlled_element.GetVariable('Powers')[1::2])
-        q_pv = q_pv / self._s_rated
-
-        q_calc = 0
-        if u_in <= self._settings.u_min:
-            q_calc = self.q_lim_pu
-        elif u_in <= self._settings.u_db_min and u_in > self._settings.u_min:
-            m1 = self.q_lim_pu / (self._settings.u_min - self._settings.u_db_min)
-            c1 = self.q_lim_pu * self._settings.u_db_min / (self._settings.u_db_min - self._settings.u_min)
-            q_calc = u_in * m1 + c1
-        elif u_in <= self._settings.u_db_max and u_in > self._settings.u_db_min:
-            q_calc = 0
-        elif u_in <= self._settings.u_max and u_in > self._settings.u_db_max:
-            m2 = self.q_lim_pu / (self._settings.u_db_max - self._settings.u_max)
-            c2 = self.q_lim_pu * self._settings.u_db_max / (self._settings.u_max - self._settings.u_db_max)
-            q_calc = u_in * m2 + c2
-        elif u_in >= self._settings.u_max:
-            q_calc = -self.q_lim_pu
-
-        q_calc = q_pv + (q_calc - q_pv) * 0.5 / self._damp_coef + (q_pv - self.old_q_pv) * 0.1 / self._damp_coef
-
-        if p_calc > 0:
-            if self._controlled_element.NumPhases == 2:
-                self._controlled_element.SetParameter('kvar', q_calc * self._s_rated * 1.3905768334328491495461135972974)
-            else:
-                self._controlled_element.SetParameter('kvar', q_calc * self._s_rated)
-        else:
-            pass
-
-        error = abs(q_pv- self.old_q_pv)
-        self.old_q_pv = q_pv
-
-        return error
+from collections import namedtuple
+import math
+
+from pydss.pyControllers.enumerations import SmartControls, ControlPriority, VoltWattCurtailmentStrategy, VoltageCalcModes
+from pydss.pyControllers.pyControllerAbstract import ControllerAbstract
+from pydss.pyControllers.models import PvControllerModel
+
+
+class PvController(ControllerAbstract):
+    """Implementation of smart control modes of modern inverter systems. Subclass of the :class:`pydss.pyControllers.pyControllerAbstract.ControllerAbstract` abstract class.
+
+        :param pv_obj: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'PVSystem' element
+        :type FaultObj: class:`pydss.dssElement.dssElement`
+        :param settings: A dictionary that defines the settings for the PvController.
+        :type settings: dict
+        :param dss_instance: An :class:`opendssdirect` instance
+        :type dss_instance: :class:`opendssdirect`
+        :param element_object_list: Dictionary of all dssElement, dssBus and dssCircuit objects
+        :type element_object_list: dict
+        :param dss_solver: An instance of one of the classed defined in :mod:`pydss.SolveMode`.
+        :type dss_solver: :mod:`pydss.SolveMode`
+        :raises: Assertionerror if 'pv_obj' is not a wrapped OpenDSS PVSystem element
+
+    """
+
+    def __init__(self, pv_obj, settings, dss_instance, element_object_list, dss_solver):
+        """Constructor method
+        """
+        
+        super(PvController, self).__init__(pv_obj, settings, dss_instance, element_object_list, dss_solver)
+        self.time_change = False
+        self.time = (-1, 0)
+        self.old_q_pv = 0
+        self.old_p_calc = 0
+
+        self._v_disconnected = False
+        self._p_disconnected = False
+
+        self._element_object_list = element_object_list
+        self.control_dict = {
+            SmartControls.NONE : lambda: 0,
+            SmartControls.CONSTANT_POWER_FACTOR : self.constant_powerfactor_control,
+            SmartControls.VARIABLE_POWER_FACTOR : self.variable_powerfactor_control,
+            SmartControls.VOLT_VAR : self.volt_var_control,
+            SmartControls.VOLT_WATT : self.volt_watt_control,
+            SmartControls.TRIP : self.cutoff_control,
+        }
+
+        self._controlled_element = pv_obj
+        self.ce_class, self.ce_name = self._controlled_element.GetInfo()
+
+        assert (self.ce_class.lower()=='pvsystem'), 'PvController works only with an OpenDSS PVSystem element'
+        self._name = 'pyCont_' + self.ce_class + '_' +  self.ce_name
+        if '_' in  self.ce_name:
+            self.phase =  self.ce_name.split('_')[1]
+        else:
+            self.phase = None
+        self._element_object_list = element_object_list
+        self._controlled_element = pv_obj
+        self._dss_instance = dss_instance
+        self._dss_solver = dss_solver
+        self._settings = PvControllerModel(**settings)
+
+        self._base_kv = float(pv_obj.GetParameter('kv'))
+        self._s_rated = float(pv_obj.GetParameter('kVA'))
+        self._p_rated = float(pv_obj.GetParameter('Pmpp'))
+        self._q_rated = float(pv_obj.GetParameter('kvarMax'))
+        self._cutin = float(pv_obj.SetParameter('%cutin', 0)) / 100
+        self._cutout = float(pv_obj.SetParameter('%cutout', 0)) / 100
+        self._damp_coef = self._settings.damp_coef
+        self._pf_rated = self._settings.pf_lim
+        self.p_mppt = 100
+        self.pf = 1
+
+        self.update = []
+            
+        for i in range(1, 4):
+            controller_type = getattr(self._settings, 'control' + str(i))
+            self.update.append(self.control_dict[controller_type])
+
+
+        if self._settings.priority == ControlPriority.VAR:
+            pv_obj.SetParameter('Wattpriority', "False")
+        elif self._settings.priority == ControlPriority.WATT:
+            pv_obj.SetParameter('Wattpriority', "True")
+        #pv_obj.SetParameter('VarFollowInverter', "False")
+
+        #self.q_lim_pu = self._q_rated / self._s_rated if self._q_rated < self._s_rated else 1
+
+        self.q_lim_pu = min(self._q_rated / self._s_rated, self._settings.q_lim_pu, 1.0)
+        self.itr = 0
+        return
+
+    def Name(self):
+        return self._name
+
+    def ControlledElement(self):
+        return "{}.{}".format(self.ce_class, self.ce_name)
+
+    def debugInfo(self):
+        return [getattr(self._settings, 'control' + str(i)) for i in range(3)]
+
+    def Update(self, priority, time, update):
+        self.time_change = self.time != (priority, time)
+        self.time = (priority, time)
+        p_pv = -sum(self._controlled_element.GetVariable('Powers')[::2]) / self._p_rated
+
+        if self.time_change:
+            self.itr = 0
+        else:
+            self.itr += 1
+
+        if self._p_disconnected:
+            if p_pv < self._cutin:
+                return 0
+            else:
+                self._p_disconnected = False
+        else:
+            if p_pv < self._cutout:
+                self._p_disconnected = True
+                self._controlled_element.SetParameter('pf', 1)
+                return 0
+        return self.update[priority]()
+
+    def volt_watt_control(self):
+        """Volt / Watt  control implementation
+        """
+        u_min_c = self._settings.u_min_c
+        u_max_c = self._settings.u_max_c
+        p_min  = self._settings.p_min_vw / 100
+
+        u_in = max(self._controlled_element.sBus[0].GetVariable('puVmagAngle')[::2])
+        p_pv = -sum(self._controlled_element.GetVariable('Powers')[::2]) / self._s_rated
+        q_pv = -sum(self._controlled_element.GetVariable('Powers')[1::2]) / self._s_rated
+
+
+        #p_pvoutPU = p_pv / self._p_rated
+
+        p_lim = (1 - q_pv ** 2) ** 0.5 if self._settings.vw_type == VoltWattCurtailmentStrategy.AVAILABLE_POWER else 1
+        m = (1 - p_min) / (u_min_c - u_max_c)
+        #m = (p_lim - p_min) / (u_min_c - u_max_c)
+        c = ((p_min * u_min_c) - u_max_c) / (u_min_c - u_max_c)
+
+        if u_in < u_min_c:
+            p_calc = p_lim
+        elif u_in < u_max_c and u_in > u_min_c:
+            p_calc = min(m * u_in + c, p_lim)
+        else:
+            p_calc = p_min
+
+        if p_pv > p_calc or (p_pv > 0 and self.p_mppt < 100):
+            # adding heavy ball term to improve convergence
+            dp = (p_pv - p_calc) * 0.5 / self._damp_coef + (self.old_p_calc - p_pv) * 0.1 / self._damp_coef
+            p_calc = p_pv - dp
+            self.p_mppt = min(self.p_mppt * p_calc / p_pv, 100)
+            self._controlled_element.SetParameter('%Pmpp', self.p_mppt)
+            self.pf = math.cos(math.atan(q_pv / p_calc))
+            if q_pv < 0:
+                self.pf = -self.pf
+            self._controlled_element.SetParameter('pf', self.pf)
+        else:
+            dp = 0
+
+        error = abs(dp)
+        self.old_p_calc = p_pv
+        return error
+
+    def cutoff_control(self):
+        """Over voltage trip implementation
+        """
+        u_in = max(self._controlled_element.sBus[0].GetVariable('puVmagAngle')[::2])
+        u_cut = self._settings['%u_cutoff']
+        if u_in >= u_cut:
+            self._controlled_element.SetParameter('%Pmpp', 0)
+            self._controlled_element.SetParameter('pf', 1)
+            if self._v_disconnected:
+                return 0
+            else:
+                self._v_disconnected = True
+                return self._p_rated
+
+        if self.time_change and self._v_disconnected and u_in < u_cut:
+            self._controlled_element.SetParameter('%Pmpp', self.p_mppt)
+            self._controlled_element.SetParameter('pf', self.pf)
+            self._v_disconnected = False
+            return self._p_rated
+
+        return 0
+
+    def constant_powerfactor_control(self):
+        """Constant power factor implementation
+        """
+        pf_set = self._settings.pf
+        pf_act = self._controlled_element.GetParameter('pf')
+        p_pv = abs(sum(self._controlled_element.GetVariable('Powers')[::2])) / self._s_rated
+        q_pv = -sum(self._controlled_element.GetVariable('Powers')[1::2]) / self._s_rated
+
+        if self._settings.priority == ControlPriority.PF:
+           # if self.time_change:
+            p_lim = pf_set * 100
+            self._controlled_element.SetParameter('%Pmpp', p_lim)
+           # else:
+        else:
+            if self._settings.priority == ControlPriority.VAR:
+                #add code for var priority here
+                p_lim = 0
+            else:
+                p_lim = 1
+            if self.time_change:
+                self.p_mppt = 100
+            else:
+                self.p_mppt = p_lim  * self._s_rated
+
+        error = abs(pf_set + pf_act)
+        self._controlled_element.SetParameter('pf', str(-pf_set))
+        return error
+
+    def variable_powerfactor_control(self):
+        """Variable power factor control implementation
+        """
+        p_min = self._settings.p_min
+        p_max = self._settings.p_max
+        pf_min = self._settings.pf_min
+        pf_max = self._settings.pf_max
+        self._dss_solver.reSolve()
+        p_calc = abs(sum(-(float(x)) for x in self._controlled_element.GetVariable('Powers')[0::2]) ) / self._s_rated
+        if p_calc > 0:
+            if p_calc < p_min:
+                pf = pf_max
+            elif p_calc > p_max:
+                pf = pf_min
+            else:
+                m = (pf_max - pf_min) / (p_min - p_max)
+                c = (pf_min * p_min - pf_max * p_max) / (p_min - p_max)
+                pf = p_calc * m + c
+        else:
+            pf = pf_max
+
+        self._controlled_element.SetParameter('irradiance', 1)
+        self._controlled_element.SetParameter('pf', str(-pf))
+        self._dss_solver.reSolve()
+
+        for i in range(10):
+            error = pf + float(self._controlled_element.GetParameter('pf'))
+            if abs(error) < 1E-4:
+                break
+            p_irr = float(self._controlled_element.GetParameter('irradiance'))
+            self._controlled_element.SetParameter('pf', str(-pf))
+            self._controlled_element.SetParameter('irradiance', p_irr * (1 + error*1.5))
+            self._dss_solver.reSolve()
+
+        return 0
+
+    def volt_var_control(self):
+        """Volt / var control implementation
+        """
+
+        u_mag = self._controlled_element.GetVariable('VoltagesMagAng')[::2]
+        u_mag = [i for i in u_mag if i != 0]
+        kv_base = self._controlled_element.sBus[0].GetVariable('kVBase') * 1000
+
+        if self._settings.voltage_calc_mode == VoltageCalcModes.MAX:
+            u_in = max(u_mag) / kv_base
+        elif self._settings.voltage_calc_mode == VoltageCalcModes.AVG:
+            u_in = sum(u_mag) / (len(u_mag) * kv_base)
+        elif self._settings.voltage_calc_mode == VoltageCalcModes.MIN:
+            u_in = min(u_mag) / kv_base
+        elif self._settings.voltage_calc_mode == VoltageCalcModes.A:
+            u_in = u_mag[0] / kv_base
+        elif self._settings.voltage_calc_mode == VoltageCalcModes.B:
+            u_in = u_mag[1] / kv_base
+        elif self._settings.voltage_calc_mode == VoltageCalcModes.C:
+            u_in = u_mag[3] / kv_base
+        else:
+            u_in = max(u_mag) / kv_base
+
+        p_pv = abs(sum(self._controlled_element.GetVariable('Powers')[::2]))
+        p_calc = p_pv / self._s_rated
+        q_pv = -sum(self._controlled_element.GetVariable('Powers')[1::2])
+        q_pv = q_pv / self._s_rated
+
+        q_calc = 0
+        if u_in <= self._settings.u_min:
+            q_calc = self.q_lim_pu
+        elif u_in <= self._settings.u_db_min and u_in > self._settings.u_min:
+            m1 = self.q_lim_pu / (self._settings.u_min - self._settings.u_db_min)
+            c1 = self.q_lim_pu * self._settings.u_db_min / (self._settings.u_db_min - self._settings.u_min)
+            q_calc = u_in * m1 + c1
+        elif u_in <= self._settings.u_db_max and u_in > self._settings.u_db_min:
+            q_calc = 0
+        elif u_in <= self._settings.u_max and u_in > self._settings.u_db_max:
+            m2 = self.q_lim_pu / (self._settings.u_db_max - self._settings.u_max)
+            c2 = self.q_lim_pu * self._settings.u_db_max / (self._settings.u_max - self._settings.u_db_max)
+            q_calc = u_in * m2 + c2
+        elif u_in >= self._settings.u_max:
+            q_calc = -self.q_lim_pu
+
+        q_calc = q_pv + (q_calc - q_pv) * 0.5 / self._damp_coef + (q_pv - self.old_q_pv) * 0.1 / self._damp_coef
+
+        if p_calc > 0:
+            if self._controlled_element.NumPhases == 2:
+                self._controlled_element.SetParameter('kvar', q_calc * self._s_rated * 1.3905768334328491495461135972974)
+            else:
+                self._controlled_element.SetParameter('kvar', q_calc * self._s_rated)
+        else:
+            pass
+
+        error = abs(q_pv- self.old_q_pv)
+        self.old_q_pv = q_pv
+
+        return error
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/PvDynamic.py` & `nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/PvDynamic.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,302 +1,302 @@
-from  pydss.pyControllers.pyControllerAbstract import ControllerAbstract
-
-try:
-    from pvder.simulation_utilities import SimulationResults
-    from pvder.dynamic_simulation import DynamicSimulation
-    from pvder.simulation_events import SimulationEvents
-    from pvder.grid_components import Grid
-    from pvder.DER_wrapper import DERModel
-except ImportError:
-    raise ImportError("""
-        This controller requires installation of the PVDER module. 
-        Use 'pip install pvder' to install the module and try running the simulation again.
-        """ 
-    )
-#from pvder import utility_functions
-
-import numpy as np
-import math
-
-class PvDynamic(ControllerAbstract):
-
-    DEBUG_SIMULATION = False
-    DEBUG_VOLTAGES = False
-    DEBUG_CURRENTS = False
-    DEBUG_POWER = False
-    DEBUG_CONTROLLERS = True
-    DEBUG_PLL = False
-    DEBUG_SOLVER = True
-
-    def __init__(self, VSCObj, Settings, dssInstance, ElmObjectList, dssSolver):
-        super(PvDynamic, self).__init__(VSCObj, Settings, dssInstance, ElmObjectList, dssSolver)
-        self.solver = dssSolver
-        self.p_old = 0
-        self.time = 0
-        self.dt = dssSolver.GetStepResolutionSeconds()
-        self._settings = Settings
-        self._controlled_element = VSCObj
-        self._e_class, self._e_name = self._controlled_element.GetInfo()
-        self._name = 'pyCont_' + self._e_class + '_' + self._e_name
-        self.n_phases = self._controlled_element.NumPhases
-        self.freq = dssInstance.Solution.Frequency()
-
-        self.events1 = SimulationEvents(verbosity = 'DEBUG')
-        #self.grid1 = Grid(events=self.events1, unbalance_ratio_b=1.0, unbalance_ratio_c=1.0)
-        self.voltage_base = Grid.Vbase#self._controlled_element.sBus[0].GetVariable('kVBase') * 1000
-        stand_alone = False
-        steady_state = Settings["STEADY_STATE"]
-        rated_power_ac_va = Settings["RATED_POWER_AC_VA"]
-        rated_power_dc_watts = Settings["RATED_POWER_DC_WATTS"]
-        der_verbosity = 'DEBUG'
-        config_file = r"C:\Users\alatif\Documents\GitHub\pvder\config_der.json"
-        if self.n_phases == 1:
-            self._Va = self.Voltages()
-            self._Vrms = abs(self._Va )/math.sqrt(2)
-            #print(f"\nmodel inputs: \nVa - {self._Va}, \nVrms - {self._Vrms}\nFreq - {self.freq}\nvBase - {self.voltage_base}\n")
-            self._pv_model = DERModel(
-                modelType= 'SinglePhase',
-                powerRating = rated_power_dc_watts,
-                Sinverter_rated = rated_power_ac_va,
-                events=self.events1,
-                configFile=config_file,
-                Vrmsrated = self._Vrms,
-                gridVoltagePhaseA = self._Va,
-                gridFrequency=2 * math.pi * self.freq,
-                derId=Settings["DER_ID"],
-                standAlone = stand_alone,
-                steadyStateInitialization=steady_state,
-                verbosity = der_verbosity
-                )
-            
-        elif self.n_phases == 3:
-            self._Va, self._Vb, self._Vc = self.Voltages()
-            self._Vrms = abs(self._Va ) / math.sqrt(2)
-            self._pv_model = DERModel(
-                modelType= 'ThreePhaseUnbalanced',
-                powerRating = 250000,
-                Sinverter_rated = 250000,
-                events=self.events1,
-                configFile=config_file,
-                Vrmsrated = self._Vrms,
-                gridVoltagePhaseA = self._Va,
-                gridVoltagePhaseB = self._Vb,
-                gridVoltagePhaseC = self._Vc,
-                gridFrequency=2 * math.pi * self.freq,
-                derId=Settings["DER_ID"],
-                standAlone = stand_alone,
-                steadyStateInitialization=steady_state,
-                verbosity = der_verbosity
-                )
-    
-        self.sim1 = DynamicSimulation(
-            PV_model=self._pv_model.DER_model,
-            events = self.events1,
-            verbosity = 'INFO',
-            solverType='odeint',
-            LOOP_MODE=True
-            )
-        
-        self.results1 = SimulationResults(
-            simulation = self.sim1,
-            PER_UNIT=True,
-            verbosity = 'INFO'
-            )
-        
-        # self._pv_model.show_parameter_dictionaries()
-        # self._pv_model.show_parameter_types()
-        
-        self.update_model_parameters()
-
-        self._pv_model.DER_model.MPPT_ENABLE = Settings["MPPT_ENABLE"]
-        self._pv_model.DER_model.RAMP_ENABLE = Settings["RAMP_ENABLE"]
-        self._pv_model.DER_model.VOLT_VAR_ENABLE = Settings["VOLT_VAR_ENABLE"]
-        self._pv_model.DER_model.LVRT_ENABLE = Settings["LVRT_ENABLE"]
-        self._pv_model.DER_model.HVRT_ENABLE = Settings["HVRT_ENABLE"]
-        self._pv_model.DER_model.LFRT_ENABLE = Settings["LFRT_ENABLE"]
-        self._pv_model.DER_model.DO_EXTRA_CALCULATIONS = Settings["DO_EXTRA_CALCULATIONS"]
-        self._pv_model.DER_model.use_frequency_estimate=Settings["use_frequency_estimate"]
-        self.sim1.jacFlag = Settings["jacFlag"]
-        self.sim1.DEBUG_SIMULATION = self.DEBUG_SIMULATION
-        self.sim1.DEBUG_VOLTAGES = self.DEBUG_VOLTAGES
-        self.sim1.DEBUG_CURRENTS = self.DEBUG_CURRENTS
-        self.sim1.DEBUG_POWER = self.DEBUG_POWER
-        self.sim1.DEBUG_CONTROLLERS  = self.DEBUG_CONTROLLERS
-        self.sim1.DEBUG_PLL = self.DEBUG_PLL
-        self.sim1.PER_UNIT = True
-        self.sim1.DEBUG_SOLVER  = self.DEBUG_SOLVER
-        self.sim1.tStop = dssSolver.get_simulation_end_time()
-        self.sim1.tInc = self.dt
-        self._pv_model._del_t_frequency_estimate = self.sim1.tInc 
-
-        self.results = []
-    
-        return
-
-    def update_model_parameters(self):
-        make_changes = any([
-            self._settings["UPDATE_MODULE_PARAMETRS"],
-            self._settings["UPDATE_INVERTER_PARAMETRS"],
-            self._settings["UPDATE_CIRCUIT_PARAMETRS"],
-            self._settings["UPDATE_CONTROLLER_PARAMETRS"],
-            self._settings["UPDATE_STEADYSTATE_PARAMETRS"]
-        ])
-  
-        module_parameters = {
-            'Np': self._settings["Np"],
-            'Ns': self._settings["Ns"],
-            'Vdcmpp0': self._settings["Vdcmpp0"],
-            'Vdcmpp_max': self._settings["Vdcmpp_max"],
-            'Vdcmpp_min': self._settings["Vdcmpp_min"],
-        }
-        inverter_ratings = {
-            'Vdcrated': self._settings["Vdcrated"],
-            'Ioverload': self._settings["Ioverload"],
-            'Vrmsrated': self._settings["Vrmsrated"],
-            'Iramp_max_gradient_imag': self._settings["Iramp_max_gradient_imag"],
-            'Iramp_max_gradient_real': self._settings["Iramp_max_gradient_real"],
-        }   
-        circuit_parameters = {
-            'Rf_actual': self._settings["Rf_actual"],
-            'Lf_actual': self._settings["Lf_actual"],
-            'C_actual': self._settings["C_actual"],
-            'Z1_actual': self._settings["Z1_actual_real"] +self._settings["Z1_actual_imag"] *1j,
-            'R1_actual': self._settings["R1_actual"],
-            'X1_actual': self._settings["X1_actual"],
-        }
-        controller_gains = {
-            'Kp_GCC': self._settings["Kp_GCC"],
-            'Ki_GCC': self._settings["Ki_GCC"],
-            'Kp_DC': self._settings["Kp_DC"],
-            'Ki_DC': self._settings["Ki_DC"],
-            'Kp_Q': self._settings["Kp_Q"],
-            'Ki_Q': self._settings["Ki_Q"],
-            'wp': self._settings["wp"]
-        }
-        steadystate_values = {
-            'iaI0': self._settings["iaI0"],
-            'iaR0': self._settings["iaR0"],
-            'maI0': self._settings["maI0"],
-            'maR0': self._settings["maR0"],
-        }
-        
-        if make_changes:
-            self._pv_model.DER_model.initialize_parameter_dict(
-                parameter_ID=self.ControlledElement(),
-                source_parameter_ID=self._settings["DER_ID"]
-                )
-            
-            if self._settings["UPDATE_MODULE_PARAMETRS"]:
-                self._pv_model.DER_model.update_parameter_dict(
-                    parameter_ID=self.ControlledElement(),
-                    parameter_type='module_parameters',
-                    parameter_dict= module_parameters
-                    ) 
-            if self._settings["UPDATE_INVERTER_PARAMETRS"]:
-                self._pv_model.DER_model.update_parameter_dict(
-                    parameter_ID=self.ControlledElement(),
-                    parameter_type='inverter_ratings',
-                    parameter_dict= inverter_ratings
-                    )
-            if self._settings["UPDATE_CIRCUIT_PARAMETRS"]:
-                self._pv_model.DER_model.update_parameter_dict(
-                    parameter_ID=self.ControlledElement(),
-                    parameter_type='circuit_parameters',
-                    parameter_dict= circuit_parameters
-                    )
-            if self._settings["UPDATE_CONTROLLER_PARAMETRS"]:
-                self._pv_model.DER_model.update_parameter_dict(
-                    parameter_ID=self.ControlledElement(),
-                    parameter_type='controller_gains',
-                    parameter_dict= controller_gains
-                    )
-            if self._settings["UPDATE_STEADYSTATE_PARAMETRS"]:
-                self._pv_model.DER_model.update_parameter_dict(
-                    parameter_ID=self.ControlledElement(),
-                    parameter_type='steadystate_values',
-                    parameter_dict= steadystate_values
-                    )
-            self._pv_model.DER_model.modify_DER_parameters(parameter_ID=self.ControlledElement())
-            params = self._pv_model.DER_model.get_parameter_dictionary(parameter_type='all',parameter_ID=self.ControlledElement())
-        return
-
-    def Name(self):
-        return self.Name
-
-    def ControlledElement(self):
-        return "{}.{}".format(self._e_class, self._e_name)
-
-    def run_dynamic_model(self):
-        sim_time_sec = self.solver.GetTotalSeconds()
-        t_sim = [sim_time_sec,sim_time_sec + self.dt]
-        if self.n_phases == 1:
-            self._Va =  self.Voltages()
-
-            self.sim1.run_simulation(
-                gridVoltagePhaseA=self._Va / self.voltage_base , 
-                y0= self.sim1.y0 , 
-                t=t_sim
-                )
-        elif self.n_phases == 3:
-            self._Va, self._Vb, self._Vc = self.Voltages()
-            self.sim1.run_simulation(
-                gridVoltagePhaseA=self._Va / self.voltage_base , 
-                gridVoltagePhaseB=self._Vb / self.voltage_base , 
-                gridVoltagePhaseC=self._Vc / self.voltage_base , 
-                y0=self.sim1.y0, 
-                t=t_sim
-                )
-        
-
-        S_PCC = self._pv_model.DER_model.S_PCC * self._pv_model.DER_model.Sbase / 1000
-        #print(f"kW {S_PCC.real}\nkvar {S_PCC.imag}")
-        self._controlled_element.SetParameter("kw", S_PCC.real )
-        self._controlled_element.SetParameter("kvar", S_PCC.imag)
-        self.results.append({
-            "Vdc" : self._pv_model.DER_model.Vdc * self._pv_model.DER_model.Vbase,
-            "vta" : self._pv_model.DER_model.vta * self._pv_model.DER_model.Vbase,
-            "Vtrms" : self._pv_model.DER_model.Vtrms * self._pv_model.DER_model.Vbase,
-            "Vrms" : self._pv_model.DER_model.Vrms * self._pv_model.DER_model.Vbase,
-            "Irms" : self._pv_model.DER_model.Irms * self._pv_model.DER_model.Ibase,
-            "Ppv" : self._pv_model.DER_model.Ppv * self._pv_model.DER_model.Sbase  /1000,
-            "S" : self._pv_model.DER_model.S * self._pv_model.DER_model.Sbase,
-            "S_PCC" : self._pv_model.DER_model.S_PCC * self._pv_model.DER_model.Sbase,
-            "active_power [kVA]": np.real(self._pv_model.DER_model.S_PCC * self._pv_model.DER_model.Sbase) / 1000.0,
-            "reactive_power [kVA]": np.imag(self._pv_model.DER_model.S_PCC * self._pv_model.DER_model.Sbase) / 1000.0,
-            "grid_voltage [p.u.]": self._Va / self.voltage_base,
-        })
-                  
-    def Update(self, Priority, Time, UpdateResults):
-        if Priority == 0 :
-            self.run_dynamic_model()
-            # if self.solver.is_last_timestep:
-            #     import matplotlib.pyplot as plt
-            #     import pandas as pd
-                
-            #     fig, axs = plt.subplots(3, 2)
-
-            #     self.results = pd.DataFrame(self.results, index=self.solver.all_timestamps)
-            #     print(self.results)
-            #     self.results[["Ppv", "reactive_power [kVA]", "active_power [kVA]"]].plot(ax=axs[0, 0])
-            #     self.results[["Vdc", "Vrms", "vta", "Vtrms"]].plot(ax=axs[0, 1])
-            #     self.results[["Irms"]].plot(ax=axs[1, 0])
-            #     self.results[["S", "S_PCC"]].plot(ax=axs[1, 1])
-            #     self.results[["grid_voltage [p.u.]"]].plot(ax=axs[2, 0])
-            #     self.results.plot.scatter(x='grid_voltage [p.u.]',  y='reactive_power [kVA]', c='DarkBlue', ax=axs[2, 1])
-                
-            #     plt.show()
-            #     print(self.results)
-        return 0
-
-    def debugInfo(self):
-        return [self._Settings['Control{}'.format(i+1)] for i in range(3)]
-
-    def Voltages(self):
-        self._V0 = self._controlled_element.GetVariable('Voltages', convert=False)[: 2 * self.n_phases]
-        self._V0 = np.array(self._V0)
-        self._V0 = self._V0[0::2] + 1j * self._V0[1::2]
-        if self.n_phases == 1:
-            return self._V0[0]
-        elif self.n_phases == 3:  
-            return self._V0[0], self._V0[1], self._V0[2]
-        else:
+from  pydss.pyControllers.pyControllerAbstract import ControllerAbstract
+
+try:
+    from pvder.simulation_utilities import SimulationResults
+    from pvder.dynamic_simulation import DynamicSimulation
+    from pvder.simulation_events import SimulationEvents
+    from pvder.grid_components import Grid
+    from pvder.DER_wrapper import DERModel
+except ImportError:
+    raise ImportError("""
+        This controller requires installation of the PVDER module. 
+        Use 'pip install pvder' to install the module and try running the simulation again.
+        """ 
+    )
+#from pvder import utility_functions
+
+import numpy as np
+import math
+
+class PvDynamic(ControllerAbstract):
+
+    DEBUG_SIMULATION = False
+    DEBUG_VOLTAGES = False
+    DEBUG_CURRENTS = False
+    DEBUG_POWER = False
+    DEBUG_CONTROLLERS = True
+    DEBUG_PLL = False
+    DEBUG_SOLVER = True
+
+    def __init__(self, VSCObj, Settings, dssInstance, ElmObjectList, dssSolver):
+        super(PvDynamic, self).__init__(VSCObj, Settings, dssInstance, ElmObjectList, dssSolver)
+        self.solver = dssSolver
+        self.p_old = 0
+        self.time = 0
+        self.dt = dssSolver.GetStepResolutionSeconds()
+        self._settings = Settings
+        self._controlled_element = VSCObj
+        self._e_class, self._e_name = self._controlled_element.GetInfo()
+        self._name = 'pyCont_' + self._e_class + '_' + self._e_name
+        self.n_phases = self._controlled_element.NumPhases
+        self.freq = dssInstance.Solution.Frequency()
+
+        self.events1 = SimulationEvents(verbosity = 'DEBUG')
+        #self.grid1 = Grid(events=self.events1, unbalance_ratio_b=1.0, unbalance_ratio_c=1.0)
+        self.voltage_base = Grid.Vbase#self._controlled_element.sBus[0].GetVariable('kVBase') * 1000
+        stand_alone = False
+        steady_state = Settings["STEADY_STATE"]
+        rated_power_ac_va = Settings["RATED_POWER_AC_VA"]
+        rated_power_dc_watts = Settings["RATED_POWER_DC_WATTS"]
+        der_verbosity = 'DEBUG'
+        config_file = r"C:\Users\alatif\Documents\GitHub\pvder\config_der.json"
+        if self.n_phases == 1:
+            self._Va = self.Voltages()
+            self._Vrms = abs(self._Va )/math.sqrt(2)
+            #print(f"\nmodel inputs: \nVa - {self._Va}, \nVrms - {self._Vrms}\nFreq - {self.freq}\nvBase - {self.voltage_base}\n")
+            self._pv_model = DERModel(
+                modelType= 'SinglePhase',
+                powerRating = rated_power_dc_watts,
+                Sinverter_rated = rated_power_ac_va,
+                events=self.events1,
+                configFile=config_file,
+                Vrmsrated = self._Vrms,
+                gridVoltagePhaseA = self._Va,
+                gridFrequency=2 * math.pi * self.freq,
+                derId=Settings["DER_ID"],
+                standAlone = stand_alone,
+                steadyStateInitialization=steady_state,
+                verbosity = der_verbosity
+                )
+            
+        elif self.n_phases == 3:
+            self._Va, self._Vb, self._Vc = self.Voltages()
+            self._Vrms = abs(self._Va ) / math.sqrt(2)
+            self._pv_model = DERModel(
+                modelType= 'ThreePhaseUnbalanced',
+                powerRating = 250000,
+                Sinverter_rated = 250000,
+                events=self.events1,
+                configFile=config_file,
+                Vrmsrated = self._Vrms,
+                gridVoltagePhaseA = self._Va,
+                gridVoltagePhaseB = self._Vb,
+                gridVoltagePhaseC = self._Vc,
+                gridFrequency=2 * math.pi * self.freq,
+                derId=Settings["DER_ID"],
+                standAlone = stand_alone,
+                steadyStateInitialization=steady_state,
+                verbosity = der_verbosity
+                )
+    
+        self.sim1 = DynamicSimulation(
+            PV_model=self._pv_model.DER_model,
+            events = self.events1,
+            verbosity = 'INFO',
+            solverType='odeint',
+            LOOP_MODE=True
+            )
+        
+        self.results1 = SimulationResults(
+            simulation = self.sim1,
+            PER_UNIT=True,
+            verbosity = 'INFO'
+            )
+        
+        # self._pv_model.show_parameter_dictionaries()
+        # self._pv_model.show_parameter_types()
+        
+        self.update_model_parameters()
+
+        self._pv_model.DER_model.MPPT_ENABLE = Settings["MPPT_ENABLE"]
+        self._pv_model.DER_model.RAMP_ENABLE = Settings["RAMP_ENABLE"]
+        self._pv_model.DER_model.VOLT_VAR_ENABLE = Settings["VOLT_VAR_ENABLE"]
+        self._pv_model.DER_model.LVRT_ENABLE = Settings["LVRT_ENABLE"]
+        self._pv_model.DER_model.HVRT_ENABLE = Settings["HVRT_ENABLE"]
+        self._pv_model.DER_model.LFRT_ENABLE = Settings["LFRT_ENABLE"]
+        self._pv_model.DER_model.DO_EXTRA_CALCULATIONS = Settings["DO_EXTRA_CALCULATIONS"]
+        self._pv_model.DER_model.use_frequency_estimate=Settings["use_frequency_estimate"]
+        self.sim1.jacFlag = Settings["jacFlag"]
+        self.sim1.DEBUG_SIMULATION = self.DEBUG_SIMULATION
+        self.sim1.DEBUG_VOLTAGES = self.DEBUG_VOLTAGES
+        self.sim1.DEBUG_CURRENTS = self.DEBUG_CURRENTS
+        self.sim1.DEBUG_POWER = self.DEBUG_POWER
+        self.sim1.DEBUG_CONTROLLERS  = self.DEBUG_CONTROLLERS
+        self.sim1.DEBUG_PLL = self.DEBUG_PLL
+        self.sim1.PER_UNIT = True
+        self.sim1.DEBUG_SOLVER  = self.DEBUG_SOLVER
+        self.sim1.tStop = dssSolver.get_simulation_end_time()
+        self.sim1.tInc = self.dt
+        self._pv_model._del_t_frequency_estimate = self.sim1.tInc 
+
+        self.results = []
+    
+        return
+
+    def update_model_parameters(self):
+        make_changes = any([
+            self._settings["UPDATE_MODULE_PARAMETRS"],
+            self._settings["UPDATE_INVERTER_PARAMETRS"],
+            self._settings["UPDATE_CIRCUIT_PARAMETRS"],
+            self._settings["UPDATE_CONTROLLER_PARAMETRS"],
+            self._settings["UPDATE_STEADYSTATE_PARAMETRS"]
+        ])
+  
+        module_parameters = {
+            'Np': self._settings["Np"],
+            'Ns': self._settings["Ns"],
+            'Vdcmpp0': self._settings["Vdcmpp0"],
+            'Vdcmpp_max': self._settings["Vdcmpp_max"],
+            'Vdcmpp_min': self._settings["Vdcmpp_min"],
+        }
+        inverter_ratings = {
+            'Vdcrated': self._settings["Vdcrated"],
+            'Ioverload': self._settings["Ioverload"],
+            'Vrmsrated': self._settings["Vrmsrated"],
+            'Iramp_max_gradient_imag': self._settings["Iramp_max_gradient_imag"],
+            'Iramp_max_gradient_real': self._settings["Iramp_max_gradient_real"],
+        }   
+        circuit_parameters = {
+            'Rf_actual': self._settings["Rf_actual"],
+            'Lf_actual': self._settings["Lf_actual"],
+            'C_actual': self._settings["C_actual"],
+            'Z1_actual': self._settings["Z1_actual_real"] +self._settings["Z1_actual_imag"] *1j,
+            'R1_actual': self._settings["R1_actual"],
+            'X1_actual': self._settings["X1_actual"],
+        }
+        controller_gains = {
+            'Kp_GCC': self._settings["Kp_GCC"],
+            'Ki_GCC': self._settings["Ki_GCC"],
+            'Kp_DC': self._settings["Kp_DC"],
+            'Ki_DC': self._settings["Ki_DC"],
+            'Kp_Q': self._settings["Kp_Q"],
+            'Ki_Q': self._settings["Ki_Q"],
+            'wp': self._settings["wp"]
+        }
+        steadystate_values = {
+            'iaI0': self._settings["iaI0"],
+            'iaR0': self._settings["iaR0"],
+            'maI0': self._settings["maI0"],
+            'maR0': self._settings["maR0"],
+        }
+        
+        if make_changes:
+            self._pv_model.DER_model.initialize_parameter_dict(
+                parameter_ID=self.ControlledElement(),
+                source_parameter_ID=self._settings["DER_ID"]
+                )
+            
+            if self._settings["UPDATE_MODULE_PARAMETRS"]:
+                self._pv_model.DER_model.update_parameter_dict(
+                    parameter_ID=self.ControlledElement(),
+                    parameter_type='module_parameters',
+                    parameter_dict= module_parameters
+                    ) 
+            if self._settings["UPDATE_INVERTER_PARAMETRS"]:
+                self._pv_model.DER_model.update_parameter_dict(
+                    parameter_ID=self.ControlledElement(),
+                    parameter_type='inverter_ratings',
+                    parameter_dict= inverter_ratings
+                    )
+            if self._settings["UPDATE_CIRCUIT_PARAMETRS"]:
+                self._pv_model.DER_model.update_parameter_dict(
+                    parameter_ID=self.ControlledElement(),
+                    parameter_type='circuit_parameters',
+                    parameter_dict= circuit_parameters
+                    )
+            if self._settings["UPDATE_CONTROLLER_PARAMETRS"]:
+                self._pv_model.DER_model.update_parameter_dict(
+                    parameter_ID=self.ControlledElement(),
+                    parameter_type='controller_gains',
+                    parameter_dict= controller_gains
+                    )
+            if self._settings["UPDATE_STEADYSTATE_PARAMETRS"]:
+                self._pv_model.DER_model.update_parameter_dict(
+                    parameter_ID=self.ControlledElement(),
+                    parameter_type='steadystate_values',
+                    parameter_dict= steadystate_values
+                    )
+            self._pv_model.DER_model.modify_DER_parameters(parameter_ID=self.ControlledElement())
+            params = self._pv_model.DER_model.get_parameter_dictionary(parameter_type='all',parameter_ID=self.ControlledElement())
+        return
+
+    def Name(self):
+        return self.Name
+
+    def ControlledElement(self):
+        return "{}.{}".format(self._e_class, self._e_name)
+
+    def run_dynamic_model(self):
+        sim_time_sec = self.solver.GetTotalSeconds()
+        t_sim = [sim_time_sec,sim_time_sec + self.dt]
+        if self.n_phases == 1:
+            self._Va =  self.Voltages()
+
+            self.sim1.run_simulation(
+                gridVoltagePhaseA=self._Va / self.voltage_base , 
+                y0= self.sim1.y0 , 
+                t=t_sim
+                )
+        elif self.n_phases == 3:
+            self._Va, self._Vb, self._Vc = self.Voltages()
+            self.sim1.run_simulation(
+                gridVoltagePhaseA=self._Va / self.voltage_base , 
+                gridVoltagePhaseB=self._Vb / self.voltage_base , 
+                gridVoltagePhaseC=self._Vc / self.voltage_base , 
+                y0=self.sim1.y0, 
+                t=t_sim
+                )
+        
+
+        S_PCC = self._pv_model.DER_model.S_PCC * self._pv_model.DER_model.Sbase / 1000
+        #print(f"kW {S_PCC.real}\nkvar {S_PCC.imag}")
+        self._controlled_element.SetParameter("kw", S_PCC.real )
+        self._controlled_element.SetParameter("kvar", S_PCC.imag)
+        self.results.append({
+            "Vdc" : self._pv_model.DER_model.Vdc * self._pv_model.DER_model.Vbase,
+            "vta" : self._pv_model.DER_model.vta * self._pv_model.DER_model.Vbase,
+            "Vtrms" : self._pv_model.DER_model.Vtrms * self._pv_model.DER_model.Vbase,
+            "Vrms" : self._pv_model.DER_model.Vrms * self._pv_model.DER_model.Vbase,
+            "Irms" : self._pv_model.DER_model.Irms * self._pv_model.DER_model.Ibase,
+            "Ppv" : self._pv_model.DER_model.Ppv * self._pv_model.DER_model.Sbase  /1000,
+            "S" : self._pv_model.DER_model.S * self._pv_model.DER_model.Sbase,
+            "S_PCC" : self._pv_model.DER_model.S_PCC * self._pv_model.DER_model.Sbase,
+            "active_power [kVA]": np.real(self._pv_model.DER_model.S_PCC * self._pv_model.DER_model.Sbase) / 1000.0,
+            "reactive_power [kVA]": np.imag(self._pv_model.DER_model.S_PCC * self._pv_model.DER_model.Sbase) / 1000.0,
+            "grid_voltage [p.u.]": self._Va / self.voltage_base,
+        })
+                  
+    def Update(self, Priority, Time, UpdateResults):
+        if Priority == 0 :
+            self.run_dynamic_model()
+            # if self.solver.is_last_timestep:
+            #     import matplotlib.pyplot as plt
+            #     import pandas as pd
+                
+            #     fig, axs = plt.subplots(3, 2)
+
+            #     self.results = pd.DataFrame(self.results, index=self.solver.all_timestamps)
+            #     print(self.results)
+            #     self.results[["Ppv", "reactive_power [kVA]", "active_power [kVA]"]].plot(ax=axs[0, 0])
+            #     self.results[["Vdc", "Vrms", "vta", "Vtrms"]].plot(ax=axs[0, 1])
+            #     self.results[["Irms"]].plot(ax=axs[1, 0])
+            #     self.results[["S", "S_PCC"]].plot(ax=axs[1, 1])
+            #     self.results[["grid_voltage [p.u.]"]].plot(ax=axs[2, 0])
+            #     self.results.plot.scatter(x='grid_voltage [p.u.]',  y='reactive_power [kVA]', c='DarkBlue', ax=axs[2, 1])
+                
+            #     plt.show()
+            #     print(self.results)
+        return 0
+
+    def debugInfo(self):
+        return [self._Settings['Control{}'.format(i+1)] for i in range(3)]
+
+    def Voltages(self):
+        self._V0 = self._controlled_element.GetVariable('Voltages', convert=False)[: 2 * self.n_phases]
+        self._V0 = np.array(self._V0)
+        self._V0 = self._V0[0::2] + 1j * self._V0[1::2]
+        if self.n_phases == 1:
+            return self._V0[0]
+        elif self.n_phases == 3:  
+            return self._V0[0], self._V0[1], self._V0[2]
+        else:
             raise Exception("Only single or three phase models can be used")
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/PvFrequencyRideThru.py` & `nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/PvFrequencyRideThru.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,462 +1,462 @@
-from ipaddress import v4_int_to_packed
-from  pydss.pyControllers.pyControllerAbstract import ControllerAbstract
-from shapely.geometry import MultiPoint, Polygon, Point, MultiPolygon
-from shapely.ops import triangulate, cascaded_union
-import matplotlib.pyplot as plt
-import datetime
-import math
-import os
-import pdb
-
-class PvFrequencyRideThru(ControllerAbstract):
-    """Implementation of IEEE1547-2003 and IEEE1547-2018 frequency ride-through standards using the OpenDSS Generator model. Subclass of the :class:`pydss.pyControllers.pyControllerAbstract.ControllerAbstract` abstract class.
-
-            :param PvObj: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'Generator' element
-            :type FaultObj: class:`pydss.dssElement.dssElement`
-            :param Settings: A dictionary that defines the settings for the PvController.
-            :type Settings: dict
-            :param dssInstance: An :class:`opendssdirect` instance
-            :type dssInstance: :class:`opendssdirect`
-            :param ElmObjectList: Dictionary of all dssElement, dssBus and dssCircuit objects
-            :type ElmObjectList: dict
-            :param dssSolver: An instance of one of the classed defined in :mod:`pydss.SolveMode`.
-            :type dssSolver: :mod:`pydss.SolveMode`
-            :raises: AssertionError if 'PvObj' is not a wrapped OpenDSS Generator element
-
-    """
-
-    def __init__(self, PvObj, Settings, dssInstance, ElmObjectList, dssSolver):
-        super(PvFrequencyRideThru, self).__init__(PvObj, Settings, dssInstance, ElmObjectList, dssSolver)
-
-        self.TimeChange = False
-        self.Time = (-1, 0)
-
-        self.oldPcalc = 0
-        self.oldQcalc = 0
-        self.Qpvpu = 0
-        self. __vDisconnected = False
-        self.__pDisconnected = False
-
-        self._ElmObjectList = ElmObjectList
-        self.ControlDict = {
-            'None': lambda: 0,
-        }
-
-        self._ControlledElm = PvObj
-        self.__ElmObjectList = ElmObjectList
-        self.__dssInstance = dssInstance
-        self.__dssSolver = dssSolver
-        self.__Settings = Settings
-
-        self.Class, self.Name = self._ControlledElm.GetInfo()
-        assert (self.Class.lower() == 'generator'), 'PvControllerGen works only with an OpenDSS Generator element'
-        self.__Name = 'pyCont_' + self.Class + '_' + self.Name
-        if '_' in self.Name:
-            self.Phase = self.Name.split('_')[1]
-        else:
-            self.Phase = None
-
-        # Initializing the model
-        PvObj.SetParameter('kvar', 0)
-        self.__Srated = float(PvObj.SetParameter('kva', Settings['kVA']))
-        self.__Prated = float(PvObj.SetParameter('kW', Settings['maxKW']))
-        self.__minQ = float(PvObj.SetParameter('minkvar', -Settings['KvarLimit']))
-        self.__maxQ = float(PvObj.SetParameter('maxkvar', Settings['KvarLimit']))
-
-        # MISC settings
-        self.__cutin = Settings['%PCutin']
-        self.__cutout = Settings['%PCutout']
-        self.__trip_deadtime_sec = Settings['Reconnect deadtime - sec']
-        self.__Time_to_Pmax_sec = Settings['Reconnect Pmax time - sec']
-        self.__Prated = Settings['maxKW']
-        self.__priority = Settings['Priority']
-        self.__enablePFlimit = Settings['Enable PF limit']
-        self.__minPF = Settings['pfMin']
-        self.__UcalcMode = Settings['UcalcMode']
-        # initialize deadtimes and other variables
-        self.__initializeRideThroughSettings()
-        # self.__rVs, self.__rTs = self.__CreateOperationRegions()
-        self.__CreateOperationRegions()
-        # For debugging only
-        self.useAvgFrequency = True
-        cycleAvg = 5 #same for voltage and frequency.  #see Table 3 of 1547-2018. Measurement window for frequency is 5 cycles
-        freq = dssSolver.getFrequency()
-        step = dssSolver.GetStepSizeSec()
-        hist_size = math.ceil(cycleAvg / (step * freq))
-        self.frequency = [60.0 for i in range(hist_size)]
-        self.reactive_power = [0.0 for i in range(hist_size)]
-        self.__VoltVioM = False
-        self.__VoltVioP = False
-        
-        self.region = [3, 3, 3]
-
-        self.frequency_hist = []
-        self.power_hist = []
-        self.timer_hist = []
-        self.timer_act_hist = []
-        
-        
-        self.u_ang = self._ControlledElm.GetVariable('VoltagesMagAng')[1::2][0]
-        self.df = 0
-        self.freq_hist = []
-        return
-
-    def Name(self):
-        return self.__Name
-
-    def ControlledElement(self):
-        return "{}.{}".format(self.Class, self.Name)
-
-    def debugInfo(self):
-        return [self.__Settings['Control{}'.format(i+1)] for i in range(3)]
-
-    def __initializeRideThroughSettings(self):
-        self.f_temp = 58.5
-        self.__isConnected = True
-        self.__Plimit = self.__Prated
-        self.__CutoffTime = self.__dssSolver.GetDateTime()
-        self.__ReconnStartTime = self.__dssSolver.GetDateTime() - datetime.timedelta(
-            seconds=int(self.__Time_to_Pmax_sec))
-
-        self.__TrippedPmaxDelay = 0
-        self.__NormOper = True
-        self.__NormOperStartTime = self.__dssSolver.GetDateTime()
-        self.__fViolationtime = 99999
-        self.__TrippedStartTime = self.__dssSolver.GetDateTime()
-        self.__TrippedDeadtime = 0
-        self.__faultCounter = 0
-        self.__isinContinuousRegion = True
-        self.__FaultwindowClearingStartTime = self.__dssSolver.GetDateTime()
-        self.__continuous_f_upper = 61.2
-        self.__continuous_f_lower = 58.8
-
-        return
-
-    def __CreateOperationRegions(self):
-        fMaxTheo = 100
-        tMax = 1e10
-        # #deprecated...
-        # V = [1.10, 0.88, 0.5, 1.2, 1.2, 1.2, 0.0, 0.0]
-        # T = [21, 10, 13, 13, 13, 1, 1]
-
-        #...deprecated
-
-        #define the over frequency shall-trip region
-        OFtripPoints = [
-            Point(fMaxTheo, self.__Settings['OF2 CT - sec']),
-            Point(self.__Settings['OF2 - Hz'], self.__Settings['OF2 CT - sec']),
-            Point(self.__Settings['OF2 - Hz'], self.__Settings['OF1 CT - sec']),
-            Point(self.__Settings['OF1 - Hz'], self.__Settings['OF1 CT - sec']),
-            Point(self.__Settings['OF1 - Hz'], tMax),
-            Point(fMaxTheo, tMax)
-        ]
-        OFtripRegion = Polygon([[p.y, p.x] for p in OFtripPoints])
-
-        #define the under frequency shall-trip region
-        UFtripPoints = [
-            Point(0, self.__Settings['UF2 CT - sec']),
-            Point(self.__Settings['UF2 - Hz'], self.__Settings['UF2 CT - sec']),
-            Point(self.__Settings['UF2 - Hz'], self.__Settings['UF1 CT - sec']),
-            Point(self.__Settings['UF1 - Hz'], self.__Settings['UF1 CT - sec']),
-            Point(self.__Settings['UF1 - Hz'], tMax),
-            Point(0, tMax)
-        ]
-        UFtripRegion = Polygon([[p.y, p.x] for p in UFtripPoints])
-
-        if self.__Settings['Ride-through Category'] in ['Category I', 'Category II', 'Category III']:
-
-            #check over frequency points
-            if self.__Settings['OF2 - Hz'] < 61.8 or self.__Settings['OF2 - Hz'] > 66:
-                #print("User defined setting outside of IEEE 1547 acceptable range.")
-                assert False
-
-            if self.__Settings['OF2 CT - sec'] < 0.16 or self.__Settings['OF2 CT - sec'] > 1000:
-                #print("User defined setting outside of IEEE 1547 acceptable range.")
-                assert False
-
-            if self.__Settings['OF1 - Hz'] < 61.0 or self.__Settings['OF1 - Hz'] > 66.0:
-                #print("User defined setting outside of IEEE 1547 acceptable range.")
-                assert False
-
-            if self.__Settings['OF1 CT - sec'] <180 or self.__Settings['OF1 CT - sec'] > 1000:
-                #print("User defined setting outside of IEEE 1547 acceptable range.")
-                assert False
-            #check under frequency points
-            if self.__Settings['UF2 - Hz'] > 57.0 or self.__Settings['UF2 - Hz'] < 50:
-                #print("User defined setting outside of IEEE 1547 acceptable range.")
-                assert False
-
-            if self.__Settings['UF2 CT - sec'] < 0.16 or self.__Settings['UF2 CT - sec'] > 1000:
-                #print("User defined setting outside of IEEE 1547 acceptable range.")
-                assert False
-
-            if self.__Settings['UF1 - Hz'] < 50 or self.__Settings['UF1 - Hz'] > 59.0:
-                #print("User defined setting outside of IEEE 1547 acceptable range.")
-                assert False
-
-            if self.__Settings['UF1 CT - sec'] < 180 or self.__Settings['UF1 CT - sec'] > 1000:
-                #print("User defined setting outside of IEEE 1547 acceptable range.")
-                assert False
-
-      
-            # F = [1.10, 0.88, 0.7, 1.20, 1.175, 1.15, 0.50, 0.5]
-            # T = [1.5,   0.7, 0.2, 0.50, 1.000, 0.16, 0.16]
-            self.__faultCounterMax = 2
-            self.__faultCounterClearingTimeSec = 20
-
-        else:
-            assert False
-
-        self._ControlledElm.SetParameter('Model', '7')
-        # self._ControlledElm.SetParameter('Vmaxpu', V[0])
-        # self._ControlledElm.SetParameter('Vminpu', V[1])
-
-
-        ContinuousPoints = [Point(self.__continuous_f_upper, 0), Point(self.__continuous_f_upper, tMax), Point(self.__continuous_f_lower, tMax), Point(self.__continuous_f_lower, 0)]
-        ContinuousRegion = Polygon([[p.y, p.x] for p in ContinuousPoints])
-
-        MandatoryPoints1 = [Point(61.8, 0), Point(61.8, 299), Point(self.__continuous_f_upper, 299), Point(self.__continuous_f_upper, 0)]
-        MandatoryRegion1 = Polygon([[p.y, p.x] for p in MandatoryPoints1])
-
-        MandatoryPoints2 = [Point(self.__continuous_f_lower, 0), Point(self.__continuous_f_lower, 299), Point(57.0, 299), Point(57.0, 0)]
-        MandatoryRegion2 = Polygon([[p.y, p.x] for p in MandatoryPoints2])
-
-        # PermissiveOVPoints = [Point(V[3], 0), Point(V[3], T[2]), Point(V[4], T[2]), Point(V[4], T[3]),
-        #                       Point(V[5], T[3]),
-        #                       Point(V[5], T[4]), Point(V[0], T[4]), Point(V[0], 0.0)]
-        # PermissiveOVRegion = Polygon([[p.y, p.x] for p in PermissiveOVPoints])
-
-        # PermissiveUVPoints = [Point(V[2], 0), Point(V[2], T[5]), Point(V[6], T[5]), Point(V[6], T[6]),
-        #                       Point(V[7], T[6]), Point(V[7], 0)]
-        # PermissiveUVRegion = Polygon([[p.y, p.x] for p in PermissiveUVPoints])
-
-        ActiveRegion = MultiPolygon([OFtripRegion, UFtripRegion, ContinuousRegion, 
-                                    MandatoryRegion1,MandatoryRegion2])
-
-
-        TotalPoints = [Point(fMaxTheo, 0), Point(fMaxTheo, tMax), Point(0, tMax), Point(0, 0)]
-        TotalRegion = Polygon([[p.y, p.x] for p in TotalPoints])
-        intersection = TotalRegion.intersection(ActiveRegion)
-        MayTripRegion = TotalRegion.difference(intersection) #everything not in the active region is the white "may trip" 
-
-        if self.__Settings['May trip operation'] == 'Trip':
-            self.CurrLimRegion = cascaded_union([MandatoryRegion1, MandatoryRegion2]) #Naming probably not appropriate with frequency variation.  
-            self.TripRegion = cascaded_union([OFtripRegion, UFtripRegion, MayTripRegion])
-        elif self.__Settings['May trip operation'] == 'Ride-Through':
-            self.CurrLimRegion = cascaded_union([MandatoryRegion1, MandatoryRegion2, MayTripRegion])
-            self.TripRegion = cascaded_union([OFtripRegion, UFtripRegion])
-        else:
-            assert False
-        self.ContinuousRegion = ContinuousRegion
-      
-        
-        return 
-
-    def calculate_frequency(self, priority, time):    
-        vsrc = self._ElmObjectList["Vsource.source"]
-        u_ang = vsrc.GetParameter("angle")
-        u_ang = u_ang * math.pi / 180
-        
-        if priority == 2:
-            if time <= 1:
-                bus_freq = self.__dssSolver.getFrequency()
-                self.u_ang = u_ang
-            else:
-                h = self.__dssSolver.GetStepSizeSec()
-                tau = h * 4
-                dphi = (u_ang - self.u_ang) / (2 * math.pi * h)
-                self.df = (dphi + self.df * (tau / h) ) / ( 1 + tau / h )    
-                base_freq = self.__dssSolver.getFrequency()
-                bus_freq = base_freq + self.df 
-                self.u_ang = u_ang
-
-            self.freq_hist.append(bus_freq)
-            return bus_freq
-
-
-    def Update(self, Priority, Time, Update):
-        Error = 0
-        self.TimeChange = self.Time != (Priority, Time)
-        self.Time = Time
-        
-        self.freq = self.calculate_frequency(Priority, Time)
-       
-        
-        if Priority == 0:
-            if self.Time == 0:
-                self.u_ang = self._ControlledElm.GetVariable('VoltagesMagAng')[1::2][0]
-            self.__isConnected = self.__Connect()
-  
-        if Priority == 2: 
-            
-            if self.Time == 719:
-                fig, ax = plt.subplots()
-                ax.plot(self.freq_hist[:-3])
-                plt.show()
-            
-        #     fIn = self.__UpdateViolatonTimers()
-        #     if self.__Settings["Follow standard"] == "1547-2018":
-        #         self.FrequencyRideThrough(fIn)
-        #     elif self.__Settings["Follow standard"] == "1547-2003":
-        #         self.Trip(fIn)
-        #     else:
-        #         raise Exception("Valid standard setting defined. Options are: 1547-2003, 1547-2018")
-            
-        #     P = -sum(self._ControlledElm.GetVariable('Powers')[::2])
-        #     self.power_hist.append(P)
-        #     self.frequency_hist.append(fIn)
-        #     self.timer_hist.append(self.__fViolationtime)
-        #     self.timer_act_hist.append(self.__dssSolver.GetTotalSeconds())
-
-        # if self.Time == 39 and Priority==2: # Time is the time step, 
-        #     import matplotlib.pyplot as plt
-        #     fig, (ax1, ax2) = plt.subplots(2,1)
-
-        #     models = [self.CurrLimRegion, self.TripRegion, MultiPolygon([self.ContinuousRegion])]                    
-        #     models = [i for i in models if i is not None]      
-            
-        #     colors = ["orange", "red", "green"]
-        #     for m, c in zip(models, colors):
-        #         for geom in m.geoms:    
-        #             xs, ys = geom.exterior.xy    
-        #             ax1.fill(xs, ys, alpha=0.35, fc=c, ec='none')
-
-
-        #     ax1.set_xlim(0, 2)
-        #     ax1.set_ylim(55, 65)
-        #     # ax1.scatter( self.timer_hist, self.frequency_hist)
-        #     ax1.scatter( self.timer_act_hist, self.frequency_hist)
-        #     ax3 = ax2.twinx()
-        #     ax2.set_ylabel('Power (kW) in green')
-        #     ax3.set_ylabel('Frequency in red')
-        #     ax2.plot(self.timer_act_hist[1:], self.power_hist[1:], c="green")
-        #     ax3.plot(self.timer_act_hist[1:], self.frequency_hist[1:], c="red")
-        #     fig.savefig(f"C:/Users/jkeen/Desktop/{self.__Name}_{self.__Settings['Ride-through Category']}_test.png")
-  
-        return Error
-
-    def Trip(self, fIn):
-        """ Implementation of the IEEE1587-2003 voltage ride-through requirements for inverter systems
-        """
-        if fIn < 59.3 or fIn > 60.5: # see page 19 of 1547-2003. #todo: more parameters are possible.  
-            if self.__isConnected:
-                self.__Trip(30.0, 0.4, False) #__Trip(self, Deadtime, Time2Pmax, forceTrip, permissive_to_trip=False)
-        return
-
-    def FrequencyRideThrough(self, fIn):
-        """ Implementation of the IEEE1587-2018 voltage ride-through requirements for inverter systems
-        """
-        self.__faultCounterClearingTimeSec = 1
-        Pm = Point(self.__fViolationtime, fIn)
-        if Pm.within(self.CurrLimRegion):
-            region = 0
-            isinContinuousRegion = False
-
-        elif Pm.within(self.TripRegion):
-            region = 2
-            isinContinuousRegion = False
-            if self.region == [3, 1, 1]: #Why? What is this region logic?
-                self.__Trip(self.__trip_deadtime_sec, self.__Time_to_Pmax_sec, False, True)
-            else: 
-                self.__Trip(self.__trip_deadtime_sec, self.__Time_to_Pmax_sec, False)
-        else:
-            isinContinuousRegion = True
-            region = 3
-            
-        self.region = self.region[1:] + self.region[:1]
-        self.region[0] = region
-
-        #if we were not originally in a continous region and we transitioned to a continous region, reset the fault timer counter
-        if isinContinuousRegion and not self.__isinContinuousRegion:
-            self.__FaultwindowClearingStartTime = self.__dssSolver.GetDateTime()
-        #Keep track of time under fault conditions.  
-        clearingTime = (self.__dssSolver.GetDateTime() - self.__FaultwindowClearingStartTime).total_seconds() 
-
-        #if we were in a continuous region and transition to fault region
-        if self.__isinContinuousRegion and not isinContinuousRegion:
-            if  clearingTime <= self.__faultCounterClearingTimeSec: # faultCounterClearingTimeSec is set to 1 in this function. Is this logic right??  
-                self.__faultCounter += 1
-                if self.__faultCounter > self.__faultCounterMax:
-                    if self.__Settings['Multiple disturbances'] == 'Trip':
-                        self.__Trip(self.__trip_deadtime_sec, self.__Time_to_Pmax_sec, True)
-                        self.__faultCounter = 0
-                    else:
-                        pass
-        if  clearingTime > self.__faultCounterClearingTimeSec and self.__faultCounter > 0:
-            self.__faultCounter = 0
-        self.__isinContinuousRegion = isinContinuousRegion
-        return
-
-    def __Connect(self):
-        if not self.__isConnected:
-            aIn = self._ControlledElm.GetVariable('VoltagesMagAng')[::2]
-            fIn = self.f_temp  #62.5 #should be some function of aIn
-            if self.useAvgFrequency:
-                self.frequency = self.frequency[1:] + self.frequency[:1] #WHY??
-                self.frequency[0] = fIn
-                fIn = sum(self.frequency) / len(self.frequency)
-            deadtime = (self.__dssSolver.GetDateTime() - self.__TrippedStartTime).total_seconds()
-            if fIn < self.__continuous_f_upper and fIn > self.continuous_f_lower and deadtime >= self.__TrippedDeadtime:
-                self._ControlledElm.SetParameter('enabled', True)
-                self.__isConnected = True
-                self._ControlledElm.SetParameter('kw', 0)
-                self.__ReconnStartTime = self.__dssSolver.GetDateTime()
-        else:
-            conntime = (self.__dssSolver.GetDateTime() - self.__ReconnStartTime).total_seconds()
-            self.__Plimit = conntime / self.__TrippedPmaxDelay * self.__Prated if conntime < self.__TrippedPmaxDelay \
-                else self.__Prated
-            self._ControlledElm.SetParameter('kw', self.__Plimit)
-        return self.__isConnected
-
-    def __Trip(self, Deadtime, Time2Pmax, forceTrip, permissive_to_trip=False):
-        #Why? Is this logic right?  They look the same? Change to: "if self.__isConnected or forceTrip or permissive_to_trip"
-        if self.__isConnected or forceTrip:
-
-            self._ControlledElm.SetParameter('enabled', False)
-
-            self.__isConnected = False
-            self.__TrippedStartTime = self.__dssSolver.GetDateTime()
-            self.__TrippedPmaxDelay = Time2Pmax
-            self.__TrippedDeadtime = Deadtime
-            
-        elif permissive_to_trip:
-    
-            self._ControlledElm.SetParameter('enabled', False)
-
-            self.__isConnected = False
-            self.__TrippedStartTime = self.__dssSolver.GetDateTime()
-            self.__TrippedPmaxDelay = Time2Pmax
-            self.__TrippedDeadtime = Deadtime
-        return
-
-    
-    def __UpdateViolatonTimers(self):
-        aIn = self._ControlledElm.GetVariable('VoltagesMagAng')[::2]
-        fIn = self.f_temp  #62.5 #should be some function of aIn
-
-        if self.useAvgFrequency:
-            self.frequency = self.frequency[1:] + self.frequency[:1] #WHY??  I think we're trying to insert new element at first position and remove last
-            self.frequency[0] = fIn
-            fIn = sum(self.frequency) / len(self.frequency)
-
-        #track how long we've been operating under normal or abnormal conditions
-        if fIn < self.__continuous_f_upper and fIn > self.__continuous_f_lower:
-            if not self.__NormOper:
-                self.__NormOper = True
-                self.__NormOperStartTime = self.__dssSolver.GetDateTime()
-                self.__NormOperTime = 0
-            else:
-                self.__NormOperTime = (self.__dssSolver.GetDateTime() - self.__NormOperStartTime).total_seconds()
-            self.__FreqVioM = False
-            # self.__VoltVioP = False
-
-        else: #not in continuous region
-            if not self.__FreqVioM:
-                self.__FreqVioM = True
-                self.__fViolationstartTime = self.__dssSolver.GetDateTime()
-                self.__fViolationtime = 0
-            else:
-                self.__fViolationtime = (self.__dssSolver.GetDateTime() - self.__fViolationstartTime).total_seconds()
-
-
-        return fIn
+from ipaddress import v4_int_to_packed
+from  pydss.pyControllers.pyControllerAbstract import ControllerAbstract
+from shapely.geometry import MultiPoint, Polygon, Point, MultiPolygon
+from shapely.ops import triangulate, cascaded_union
+import matplotlib.pyplot as plt
+import datetime
+import math
+import os
+import pdb
+
+class PvFrequencyRideThru(ControllerAbstract):
+    """Implementation of IEEE1547-2003 and IEEE1547-2018 frequency ride-through standards using the OpenDSS Generator model. Subclass of the :class:`pydss.pyControllers.pyControllerAbstract.ControllerAbstract` abstract class.
+
+            :param PvObj: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'Generator' element
+            :type FaultObj: class:`pydss.dssElement.dssElement`
+            :param Settings: A dictionary that defines the settings for the PvController.
+            :type Settings: dict
+            :param dssInstance: An :class:`opendssdirect` instance
+            :type dssInstance: :class:`opendssdirect`
+            :param ElmObjectList: Dictionary of all dssElement, dssBus and dssCircuit objects
+            :type ElmObjectList: dict
+            :param dssSolver: An instance of one of the classed defined in :mod:`pydss.SolveMode`.
+            :type dssSolver: :mod:`pydss.SolveMode`
+            :raises: AssertionError if 'PvObj' is not a wrapped OpenDSS Generator element
+
+    """
+
+    def __init__(self, PvObj, Settings, dssInstance, ElmObjectList, dssSolver):
+        super(PvFrequencyRideThru, self).__init__(PvObj, Settings, dssInstance, ElmObjectList, dssSolver)
+
+        self.TimeChange = False
+        self.Time = (-1, 0)
+
+        self.oldPcalc = 0
+        self.oldQcalc = 0
+        self.Qpvpu = 0
+        self. __vDisconnected = False
+        self.__pDisconnected = False
+
+        self._ElmObjectList = ElmObjectList
+        self.ControlDict = {
+            'None': lambda: 0,
+        }
+
+        self._ControlledElm = PvObj
+        self.__ElmObjectList = ElmObjectList
+        self.__dssInstance = dssInstance
+        self.__dssSolver = dssSolver
+        self.__Settings = Settings
+
+        self.Class, self.Name = self._ControlledElm.GetInfo()
+        assert (self.Class.lower() == 'generator'), 'PvControllerGen works only with an OpenDSS Generator element'
+        self.__Name = 'pyCont_' + self.Class + '_' + self.Name
+        if '_' in self.Name:
+            self.Phase = self.Name.split('_')[1]
+        else:
+            self.Phase = None
+
+        # Initializing the model
+        PvObj.SetParameter('kvar', 0)
+        self.__Srated = float(PvObj.SetParameter('kva', Settings['kVA']))
+        self.__Prated = float(PvObj.SetParameter('kW', Settings['maxKW']))
+        self.__minQ = float(PvObj.SetParameter('minkvar', -Settings['KvarLimit']))
+        self.__maxQ = float(PvObj.SetParameter('maxkvar', Settings['KvarLimit']))
+
+        # MISC settings
+        self.__cutin = Settings['%PCutin']
+        self.__cutout = Settings['%PCutout']
+        self.__trip_deadtime_sec = Settings['Reconnect deadtime - sec']
+        self.__Time_to_Pmax_sec = Settings['Reconnect Pmax time - sec']
+        self.__Prated = Settings['maxKW']
+        self.__priority = Settings['Priority']
+        self.__enablePFlimit = Settings['Enable PF limit']
+        self.__minPF = Settings['pfMin']
+        self.__UcalcMode = Settings['UcalcMode']
+        # initialize deadtimes and other variables
+        self.__initializeRideThroughSettings()
+        # self.__rVs, self.__rTs = self.__CreateOperationRegions()
+        self.__CreateOperationRegions()
+        # For debugging only
+        self.useAvgFrequency = True
+        cycleAvg = 5 #same for voltage and frequency.  #see Table 3 of 1547-2018. Measurement window for frequency is 5 cycles
+        freq = dssSolver.getFrequency()
+        step = dssSolver.GetStepSizeSec()
+        hist_size = math.ceil(cycleAvg / (step * freq))
+        self.frequency = [60.0 for i in range(hist_size)]
+        self.reactive_power = [0.0 for i in range(hist_size)]
+        self.__VoltVioM = False
+        self.__VoltVioP = False
+        
+        self.region = [3, 3, 3]
+
+        self.frequency_hist = []
+        self.power_hist = []
+        self.timer_hist = []
+        self.timer_act_hist = []
+        
+        
+        self.u_ang = self._ControlledElm.GetVariable('VoltagesMagAng')[1::2][0]
+        self.df = 0
+        self.freq_hist = []
+        return
+
+    def Name(self):
+        return self.__Name
+
+    def ControlledElement(self):
+        return "{}.{}".format(self.Class, self.Name)
+
+    def debugInfo(self):
+        return [self.__Settings['Control{}'.format(i+1)] for i in range(3)]
+
+    def __initializeRideThroughSettings(self):
+        self.f_temp = 58.5
+        self.__isConnected = True
+        self.__Plimit = self.__Prated
+        self.__CutoffTime = self.__dssSolver.GetDateTime()
+        self.__ReconnStartTime = self.__dssSolver.GetDateTime() - datetime.timedelta(
+            seconds=int(self.__Time_to_Pmax_sec))
+
+        self.__TrippedPmaxDelay = 0
+        self.__NormOper = True
+        self.__NormOperStartTime = self.__dssSolver.GetDateTime()
+        self.__fViolationtime = 99999
+        self.__TrippedStartTime = self.__dssSolver.GetDateTime()
+        self.__TrippedDeadtime = 0
+        self.__faultCounter = 0
+        self.__isinContinuousRegion = True
+        self.__FaultwindowClearingStartTime = self.__dssSolver.GetDateTime()
+        self.__continuous_f_upper = 61.2
+        self.__continuous_f_lower = 58.8
+
+        return
+
+    def __CreateOperationRegions(self):
+        fMaxTheo = 100
+        tMax = 1e10
+        # #deprecated...
+        # V = [1.10, 0.88, 0.5, 1.2, 1.2, 1.2, 0.0, 0.0]
+        # T = [21, 10, 13, 13, 13, 1, 1]
+
+        #...deprecated
+
+        #define the over frequency shall-trip region
+        OFtripPoints = [
+            Point(fMaxTheo, self.__Settings['OF2 CT - sec']),
+            Point(self.__Settings['OF2 - Hz'], self.__Settings['OF2 CT - sec']),
+            Point(self.__Settings['OF2 - Hz'], self.__Settings['OF1 CT - sec']),
+            Point(self.__Settings['OF1 - Hz'], self.__Settings['OF1 CT - sec']),
+            Point(self.__Settings['OF1 - Hz'], tMax),
+            Point(fMaxTheo, tMax)
+        ]
+        OFtripRegion = Polygon([[p.y, p.x] for p in OFtripPoints])
+
+        #define the under frequency shall-trip region
+        UFtripPoints = [
+            Point(0, self.__Settings['UF2 CT - sec']),
+            Point(self.__Settings['UF2 - Hz'], self.__Settings['UF2 CT - sec']),
+            Point(self.__Settings['UF2 - Hz'], self.__Settings['UF1 CT - sec']),
+            Point(self.__Settings['UF1 - Hz'], self.__Settings['UF1 CT - sec']),
+            Point(self.__Settings['UF1 - Hz'], tMax),
+            Point(0, tMax)
+        ]
+        UFtripRegion = Polygon([[p.y, p.x] for p in UFtripPoints])
+
+        if self.__Settings['Ride-through Category'] in ['Category I', 'Category II', 'Category III']:
+
+            #check over frequency points
+            if self.__Settings['OF2 - Hz'] < 61.8 or self.__Settings['OF2 - Hz'] > 66:
+                #print("User defined setting outside of IEEE 1547 acceptable range.")
+                assert False
+
+            if self.__Settings['OF2 CT - sec'] < 0.16 or self.__Settings['OF2 CT - sec'] > 1000:
+                #print("User defined setting outside of IEEE 1547 acceptable range.")
+                assert False
+
+            if self.__Settings['OF1 - Hz'] < 61.0 or self.__Settings['OF1 - Hz'] > 66.0:
+                #print("User defined setting outside of IEEE 1547 acceptable range.")
+                assert False
+
+            if self.__Settings['OF1 CT - sec'] <180 or self.__Settings['OF1 CT - sec'] > 1000:
+                #print("User defined setting outside of IEEE 1547 acceptable range.")
+                assert False
+            #check under frequency points
+            if self.__Settings['UF2 - Hz'] > 57.0 or self.__Settings['UF2 - Hz'] < 50:
+                #print("User defined setting outside of IEEE 1547 acceptable range.")
+                assert False
+
+            if self.__Settings['UF2 CT - sec'] < 0.16 or self.__Settings['UF2 CT - sec'] > 1000:
+                #print("User defined setting outside of IEEE 1547 acceptable range.")
+                assert False
+
+            if self.__Settings['UF1 - Hz'] < 50 or self.__Settings['UF1 - Hz'] > 59.0:
+                #print("User defined setting outside of IEEE 1547 acceptable range.")
+                assert False
+
+            if self.__Settings['UF1 CT - sec'] < 180 or self.__Settings['UF1 CT - sec'] > 1000:
+                #print("User defined setting outside of IEEE 1547 acceptable range.")
+                assert False
+
+      
+            # F = [1.10, 0.88, 0.7, 1.20, 1.175, 1.15, 0.50, 0.5]
+            # T = [1.5,   0.7, 0.2, 0.50, 1.000, 0.16, 0.16]
+            self.__faultCounterMax = 2
+            self.__faultCounterClearingTimeSec = 20
+
+        else:
+            assert False
+
+        self._ControlledElm.SetParameter('Model', '7')
+        # self._ControlledElm.SetParameter('Vmaxpu', V[0])
+        # self._ControlledElm.SetParameter('Vminpu', V[1])
+
+
+        ContinuousPoints = [Point(self.__continuous_f_upper, 0), Point(self.__continuous_f_upper, tMax), Point(self.__continuous_f_lower, tMax), Point(self.__continuous_f_lower, 0)]
+        ContinuousRegion = Polygon([[p.y, p.x] for p in ContinuousPoints])
+
+        MandatoryPoints1 = [Point(61.8, 0), Point(61.8, 299), Point(self.__continuous_f_upper, 299), Point(self.__continuous_f_upper, 0)]
+        MandatoryRegion1 = Polygon([[p.y, p.x] for p in MandatoryPoints1])
+
+        MandatoryPoints2 = [Point(self.__continuous_f_lower, 0), Point(self.__continuous_f_lower, 299), Point(57.0, 299), Point(57.0, 0)]
+        MandatoryRegion2 = Polygon([[p.y, p.x] for p in MandatoryPoints2])
+
+        # PermissiveOVPoints = [Point(V[3], 0), Point(V[3], T[2]), Point(V[4], T[2]), Point(V[4], T[3]),
+        #                       Point(V[5], T[3]),
+        #                       Point(V[5], T[4]), Point(V[0], T[4]), Point(V[0], 0.0)]
+        # PermissiveOVRegion = Polygon([[p.y, p.x] for p in PermissiveOVPoints])
+
+        # PermissiveUVPoints = [Point(V[2], 0), Point(V[2], T[5]), Point(V[6], T[5]), Point(V[6], T[6]),
+        #                       Point(V[7], T[6]), Point(V[7], 0)]
+        # PermissiveUVRegion = Polygon([[p.y, p.x] for p in PermissiveUVPoints])
+
+        ActiveRegion = MultiPolygon([OFtripRegion, UFtripRegion, ContinuousRegion, 
+                                    MandatoryRegion1,MandatoryRegion2])
+
+
+        TotalPoints = [Point(fMaxTheo, 0), Point(fMaxTheo, tMax), Point(0, tMax), Point(0, 0)]
+        TotalRegion = Polygon([[p.y, p.x] for p in TotalPoints])
+        intersection = TotalRegion.intersection(ActiveRegion)
+        MayTripRegion = TotalRegion.difference(intersection) #everything not in the active region is the white "may trip" 
+
+        if self.__Settings['May trip operation'] == 'Trip':
+            self.CurrLimRegion = cascaded_union([MandatoryRegion1, MandatoryRegion2]) #Naming probably not appropriate with frequency variation.  
+            self.TripRegion = cascaded_union([OFtripRegion, UFtripRegion, MayTripRegion])
+        elif self.__Settings['May trip operation'] == 'Ride-Through':
+            self.CurrLimRegion = cascaded_union([MandatoryRegion1, MandatoryRegion2, MayTripRegion])
+            self.TripRegion = cascaded_union([OFtripRegion, UFtripRegion])
+        else:
+            assert False
+        self.ContinuousRegion = ContinuousRegion
+      
+        
+        return 
+
+    def calculate_frequency(self, priority, time):    
+        vsrc = self._ElmObjectList["Vsource.source"]
+        u_ang = vsrc.GetParameter("angle")
+        u_ang = u_ang * math.pi / 180
+        
+        if priority == 2:
+            if time <= 1:
+                bus_freq = self.__dssSolver.getFrequency()
+                self.u_ang = u_ang
+            else:
+                h = self.__dssSolver.GetStepSizeSec()
+                tau = h * 4
+                dphi = (u_ang - self.u_ang) / (2 * math.pi * h)
+                self.df = (dphi + self.df * (tau / h) ) / ( 1 + tau / h )    
+                base_freq = self.__dssSolver.getFrequency()
+                bus_freq = base_freq + self.df 
+                self.u_ang = u_ang
+
+            self.freq_hist.append(bus_freq)
+            return bus_freq
+
+
+    def Update(self, Priority, Time, Update):
+        Error = 0
+        self.TimeChange = self.Time != (Priority, Time)
+        self.Time = Time
+        
+        self.freq = self.calculate_frequency(Priority, Time)
+       
+        
+        if Priority == 0:
+            if self.Time == 0:
+                self.u_ang = self._ControlledElm.GetVariable('VoltagesMagAng')[1::2][0]
+            self.__isConnected = self.__Connect()
+  
+        if Priority == 2: 
+            
+            if self.Time == 719:
+                fig, ax = plt.subplots()
+                ax.plot(self.freq_hist[:-3])
+                plt.show()
+            
+        #     fIn = self.__UpdateViolatonTimers()
+        #     if self.__Settings["Follow standard"] == "1547-2018":
+        #         self.FrequencyRideThrough(fIn)
+        #     elif self.__Settings["Follow standard"] == "1547-2003":
+        #         self.Trip(fIn)
+        #     else:
+        #         raise Exception("Valid standard setting defined. Options are: 1547-2003, 1547-2018")
+            
+        #     P = -sum(self._ControlledElm.GetVariable('Powers')[::2])
+        #     self.power_hist.append(P)
+        #     self.frequency_hist.append(fIn)
+        #     self.timer_hist.append(self.__fViolationtime)
+        #     self.timer_act_hist.append(self.__dssSolver.GetTotalSeconds())
+
+        # if self.Time == 39 and Priority==2: # Time is the time step, 
+        #     import matplotlib.pyplot as plt
+        #     fig, (ax1, ax2) = plt.subplots(2,1)
+
+        #     models = [self.CurrLimRegion, self.TripRegion, MultiPolygon([self.ContinuousRegion])]                    
+        #     models = [i for i in models if i is not None]      
+            
+        #     colors = ["orange", "red", "green"]
+        #     for m, c in zip(models, colors):
+        #         for geom in m.geoms:    
+        #             xs, ys = geom.exterior.xy    
+        #             ax1.fill(xs, ys, alpha=0.35, fc=c, ec='none')
+
+
+        #     ax1.set_xlim(0, 2)
+        #     ax1.set_ylim(55, 65)
+        #     # ax1.scatter( self.timer_hist, self.frequency_hist)
+        #     ax1.scatter( self.timer_act_hist, self.frequency_hist)
+        #     ax3 = ax2.twinx()
+        #     ax2.set_ylabel('Power (kW) in green')
+        #     ax3.set_ylabel('Frequency in red')
+        #     ax2.plot(self.timer_act_hist[1:], self.power_hist[1:], c="green")
+        #     ax3.plot(self.timer_act_hist[1:], self.frequency_hist[1:], c="red")
+        #     fig.savefig(f"C:/Users/jkeen/Desktop/{self.__Name}_{self.__Settings['Ride-through Category']}_test.png")
+  
+        return Error
+
+    def Trip(self, fIn):
+        """ Implementation of the IEEE1587-2003 voltage ride-through requirements for inverter systems
+        """
+        if fIn < 59.3 or fIn > 60.5: # see page 19 of 1547-2003. #todo: more parameters are possible.  
+            if self.__isConnected:
+                self.__Trip(30.0, 0.4, False) #__Trip(self, Deadtime, Time2Pmax, forceTrip, permissive_to_trip=False)
+        return
+
+    def FrequencyRideThrough(self, fIn):
+        """ Implementation of the IEEE1587-2018 voltage ride-through requirements for inverter systems
+        """
+        self.__faultCounterClearingTimeSec = 1
+        Pm = Point(self.__fViolationtime, fIn)
+        if Pm.within(self.CurrLimRegion):
+            region = 0
+            isinContinuousRegion = False
+
+        elif Pm.within(self.TripRegion):
+            region = 2
+            isinContinuousRegion = False
+            if self.region == [3, 1, 1]: #Why? What is this region logic?
+                self.__Trip(self.__trip_deadtime_sec, self.__Time_to_Pmax_sec, False, True)
+            else: 
+                self.__Trip(self.__trip_deadtime_sec, self.__Time_to_Pmax_sec, False)
+        else:
+            isinContinuousRegion = True
+            region = 3
+            
+        self.region = self.region[1:] + self.region[:1]
+        self.region[0] = region
+
+        #if we were not originally in a continous region and we transitioned to a continous region, reset the fault timer counter
+        if isinContinuousRegion and not self.__isinContinuousRegion:
+            self.__FaultwindowClearingStartTime = self.__dssSolver.GetDateTime()
+        #Keep track of time under fault conditions.  
+        clearingTime = (self.__dssSolver.GetDateTime() - self.__FaultwindowClearingStartTime).total_seconds() 
+
+        #if we were in a continuous region and transition to fault region
+        if self.__isinContinuousRegion and not isinContinuousRegion:
+            if  clearingTime <= self.__faultCounterClearingTimeSec: # faultCounterClearingTimeSec is set to 1 in this function. Is this logic right??  
+                self.__faultCounter += 1
+                if self.__faultCounter > self.__faultCounterMax:
+                    if self.__Settings['Multiple disturbances'] == 'Trip':
+                        self.__Trip(self.__trip_deadtime_sec, self.__Time_to_Pmax_sec, True)
+                        self.__faultCounter = 0
+                    else:
+                        pass
+        if  clearingTime > self.__faultCounterClearingTimeSec and self.__faultCounter > 0:
+            self.__faultCounter = 0
+        self.__isinContinuousRegion = isinContinuousRegion
+        return
+
+    def __Connect(self):
+        if not self.__isConnected:
+            aIn = self._ControlledElm.GetVariable('VoltagesMagAng')[::2]
+            fIn = self.f_temp  #62.5 #should be some function of aIn
+            if self.useAvgFrequency:
+                self.frequency = self.frequency[1:] + self.frequency[:1] #WHY??
+                self.frequency[0] = fIn
+                fIn = sum(self.frequency) / len(self.frequency)
+            deadtime = (self.__dssSolver.GetDateTime() - self.__TrippedStartTime).total_seconds()
+            if fIn < self.__continuous_f_upper and fIn > self.continuous_f_lower and deadtime >= self.__TrippedDeadtime:
+                self._ControlledElm.SetParameter('enabled', True)
+                self.__isConnected = True
+                self._ControlledElm.SetParameter('kw', 0)
+                self.__ReconnStartTime = self.__dssSolver.GetDateTime()
+        else:
+            conntime = (self.__dssSolver.GetDateTime() - self.__ReconnStartTime).total_seconds()
+            self.__Plimit = conntime / self.__TrippedPmaxDelay * self.__Prated if conntime < self.__TrippedPmaxDelay \
+                else self.__Prated
+            self._ControlledElm.SetParameter('kw', self.__Plimit)
+        return self.__isConnected
+
+    def __Trip(self, Deadtime, Time2Pmax, forceTrip, permissive_to_trip=False):
+        #Why? Is this logic right?  They look the same? Change to: "if self.__isConnected or forceTrip or permissive_to_trip"
+        if self.__isConnected or forceTrip:
+
+            self._ControlledElm.SetParameter('enabled', False)
+
+            self.__isConnected = False
+            self.__TrippedStartTime = self.__dssSolver.GetDateTime()
+            self.__TrippedPmaxDelay = Time2Pmax
+            self.__TrippedDeadtime = Deadtime
+            
+        elif permissive_to_trip:
+    
+            self._ControlledElm.SetParameter('enabled', False)
+
+            self.__isConnected = False
+            self.__TrippedStartTime = self.__dssSolver.GetDateTime()
+            self.__TrippedPmaxDelay = Time2Pmax
+            self.__TrippedDeadtime = Deadtime
+        return
+
+    
+    def __UpdateViolatonTimers(self):
+        aIn = self._ControlledElm.GetVariable('VoltagesMagAng')[::2]
+        fIn = self.f_temp  #62.5 #should be some function of aIn
+
+        if self.useAvgFrequency:
+            self.frequency = self.frequency[1:] + self.frequency[:1] #WHY??  I think we're trying to insert new element at first position and remove last
+            self.frequency[0] = fIn
+            fIn = sum(self.frequency) / len(self.frequency)
+
+        #track how long we've been operating under normal or abnormal conditions
+        if fIn < self.__continuous_f_upper and fIn > self.__continuous_f_lower:
+            if not self.__NormOper:
+                self.__NormOper = True
+                self.__NormOperStartTime = self.__dssSolver.GetDateTime()
+                self.__NormOperTime = 0
+            else:
+                self.__NormOperTime = (self.__dssSolver.GetDateTime() - self.__NormOperStartTime).total_seconds()
+            self.__FreqVioM = False
+            # self.__VoltVioP = False
+
+        else: #not in continuous region
+            if not self.__FreqVioM:
+                self.__FreqVioM = True
+                self.__fViolationstartTime = self.__dssSolver.GetDateTime()
+                self.__fViolationtime = 0
+            else:
+                self.__fViolationtime = (self.__dssSolver.GetDateTime() - self.__fViolationstartTime).total_seconds()
+
+
+        return fIn
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/PvVoltageRideThru.py` & `nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/PvVoltageRideThru.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,445 +1,445 @@
-from shapely.geometry import MultiPoint, Polygon, Point, MultiPolygon
-from shapely.ops import triangulate, unary_union
-import datetime
-import math
-
-from pydss.pyControllers.pyControllerAbstract import ControllerAbstract
-from pydss.pyControllers.models import PvVoltageRideThruModel
-from pydss.pyControllers.enumerations import PvStandard, VoltageCalcModes, RideThroughCategory, PermissiveOperation, MayTripOperation, MultipleDisturbances
-
-
-class PvVoltageRideThru(ControllerAbstract):
-    """Implementation of IEEE1547-2003 and IEEE1547-2018 voltage ride-through standards using the OpenDSS Generator model. Subclass of the :class:`pydss.pyControllers.pyControllerAbstract.ControllerAbstract` abstract class.
-
-            :param pv_object: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'Generator' element
-            :type FaultObj: class:`pydss.dssElement.dssElement`
-            :param settings: A dictionary that defines the settings for the PvController.
-            :type settings: dict
-            :param dss_instance: An :class:`opendssdirect` instance
-            :type dss_instance: :class:`opendssdirect`
-            :param elm_object_list: Dictionary of all dssElement, dssBus and dssCircuit objects
-            :type elm_object_list: dict
-            :param dss_solver: An instance of one of the classed defined in :mod:`pydss.SolveMode`.
-            :type dss_solver: :mod:`pydss.SolveMode`
-            :raises: Assertionerror if 'pv_object' is not a wrapped OpenDSS Generator element
-
-    """
-
-    def __init__(self, pv_object, settings, dss_instance, elm_object_list, dss_solver):
-        super(PvVoltageRideThru, self).__init__(pv_object, settings, dss_instance, elm_object_list, dss_solver)
-
-        self.model = PvVoltageRideThruModel(**settings)
-        
-        self.time_change = False
-        self.time = (-1, 0)
-
-        self.ControlDict = {
-            'None': lambda: 0,
-        }
-
-        self._controlled_element = pv_object
-        self.__dss_solver = dss_solver
-
-        self.object_type, self.object_name = self._controlled_element.GetInfo()
-        assert (self.object_type.lower() == 'generator'), 'PvControllerGen works only with an OpenDSS Generator element'
-        self._name = 'pyCont_' + self.object_type + '_' + self.object_name
-        if '_' in self.object_name:
-            self.phase = self.object_name.split('_')[1]
-        else:
-            self.phase = None
-
-        # Initializing the model
-        pv_object.SetParameter('kvar', 0)
-        pv_object.SetParameter('kva', self.model.kva)
-        self._p_rated = float(pv_object.SetParameter('kW', self.model.max_kw))
-
-        # MISC settings
-        self._trip_deadtime_sec = self.model.reconnect_deadtime_sec
-        self._time_to_p_max_sec = self.model.reconnect_pmax_time_sec
-        self._p_rated = self.model.max_kw
-        self._voltage_calc_mode = self.model.voltage_calc_mode
-        # initialize deadtimes and other variables
-        self._initialize_ride_through_settings()
-        if self.model.follow_standard == PvStandard.IEEE_1547_2003:
-            self._rvs = [1.10, 0.88]
-        else:
-            self._rvs, _= self._create_operation_regions()
-
-        # For debugging only
-        self.use_avg_voltage = False
-        cycle_average = 5
-        freq = dss_solver.getFrequency()
-        step = dss_solver.GetStepSizeSec()
-        hist_size = math.ceil(cycle_average / (step * freq))
-        self.voltage = [1.0 for i in range(hist_size)]
-        self.reactive_power = [0.0 for i in range(hist_size)]
-        self._voltage_violation_m = False
-        
-        self.region = [3, 3, 3]
-        
-        # self.voltage_hist = []
-        # self.power_hist = []
-        # self.timer_hist = []
-        # self.timer_act_hist = []
-        
-        return
-
-    def Name(self):
-        return self._name
-
-    def ControlledElement(self):
-        return "{}.{}".format(self.object_type, self.object_name)
-
-    def debugInfo(self):
-        return []
-
-    def _initialize_ride_through_settings(self):
-        self._is_connected = True
-        self._p_limit = self._p_rated
-        self._reconnect_start_time = self.__dss_solver.GetDateTime() - datetime.timedelta(
-            seconds=int(self._time_to_p_max_sec))
-
-        self._tripped_p_max_delay = 0
-        self._normal_operation = True
-        self._normal_operation_start_time = self.__dss_solver.GetDateTime()
-        self._u_violation_time = 99999
-        self._tripped_start_time = self.__dss_solver.GetDateTime()
-        self._tripped_dead_time = 0
-        self._fault_counter = 0
-        self._is_in_contioeous_region = True
-        self._fault_window_clearing_start_time = self.__dss_solver.GetDateTime()
-
-        return
-
-    def _create_operation_regions(self):
-        u_max_theo = 10
-        t_max = 1e10
-
-        ov_trip_points = [
-            Point(u_max_theo, self.model.ov_2_ct_sec),
-            Point(self.model.ov_2_pu, self.model.ov_2_ct_sec),
-            Point(self.model.ov_2_pu, self.model.ov_1_ct_sec),
-            Point(self.model.ov_1_pu, self.model.ov_1_ct_sec),
-            Point(self.model.ov_1_pu, t_max),
-            Point(u_max_theo, t_max)
-        ]
-        ov_trip_region = Polygon([[p.y, p.x] for p in ov_trip_points])
-
-        UVtripPoints = [
-            Point(0, self.model.uv_2_ct_sec),
-            Point(self.model.uv_2_pu, self.model.uv_2_ct_sec),
-            Point(self.model.uv_2_pu, self.model.uv_1_ct_sec),
-            Point(self.model.uv_1_pu, self.model.uv_1_ct_sec),
-            Point(self.model.uv_1_pu, t_max),
-            Point(0, t_max)
-        ]
-        uv_trip_region = Polygon([[p.y, p.x] for p in UVtripPoints])
-
-        if self.model.ride_through_category == RideThroughCategory.CATEGORY_I:
-            ov2pu_eq = 1.20
-            ov2sec_eq = 0.16
-            ov1pu_min = 1.1
-            ov1pu_max = 1.2
-            ov1sec_min = 1
-            ov1sec_max = 13
-            uv2pu_min = 0.0
-            uv2pu_max = 0.5
-            uv2sec_min = 0.16
-            uv2sec_max = 2.0
-            uv1pu_min = 0.0
-            uv1pu_max = 0.88
-            uv1sec_min = 2.0
-            uv1sec_max = 21
-    
-            V = [1.10, 0.88, 0.7, 1.20, 1.175, 1.15, 0.5, 0.5]
-            T = [1.5, 0.7, 0.2, 0.5, 1, 0.16, 0.16]
-            self._fault_counter_max = 2
-            self._fault_counter_clearing_time_sec = 20
-
-        elif self.model.ride_through_category == RideThroughCategory.CATEGORY_II:
-            ov2pu_eq = 1.20
-            ov2sec_eq = 0.16
-            ov1pu_min = 1.1
-            ov1pu_max = 1.2
-            ov1sec_min = 1
-            ov1sec_max = 13
-            uv2pu_min = 0.0
-            uv2pu_max = 0.45
-            uv2sec_min = 0.16
-            uv2sec_max = 2.0
-            uv1pu_min = 0.0
-            uv1pu_max = 0.88
-            uv1sec_min = 2.0
-            uv1sec_max = 21
-
-            V = [1.10, 0.88, 0.65, 1.20, 1.175, 1.15, 0.45, 0.30]
-            T = [5, 3, 0.2, 0.5, 1, 0.32, 0.16]
-            self._fault_counter_max = 2
-            self._fault_counter_clearing_time_sec = 10
-
-        elif self.model.ride_through_category == RideThroughCategory.CATEGORY_I:
-            ov2pu_eq = 1.20
-            ov2sec_eq = 0.16
-            ov1pu_min = 1.1
-            ov1pu_max = 1.2
-            ov1sec_min = 1
-            ov1sec_max = 13
-            uv2pu_min = 0.0
-            uv2pu_max = 0.5
-            uv2sec_min = 2.0
-            uv2sec_max = 21.0
-            uv1pu_min = 0.0
-            uv1pu_max = 0.88
-            uv1sec_min = 2.0
-            uv1sec_max = 21.0
-
-            V = [1.10, 0.88, 0.5, 1.2, 1.2, 1.2, 0.0, 0.0]
-            T = [21, 10, 13, 13, 13, 1, 1]
-            self._fault_counter_max = 3
-            self._fault_counter_clearing_time_sec = 5
-
-        #check overvoltage points
-        if self.model.ov_2_pu != ov2pu_eq:
-            #print("User defined setting outside of IEEE 1547 acceptable range.")
-            assert False
-
-        
-        if self.model.ov_2_ct_sec != ov2sec_eq:
-            #print("User defined setting outside of IEEE 1547 acceptable range.")
-            assert False
-
-        if self.model.ov_1_pu < ov1pu_min and self.model.ov_1_pu > ov1pu_max:
-            #print("User defined setting outside of IEEE 1547 acceptable range.")
-            assert False
-
-        if self.model.ov_1_ct_sec < ov1sec_min and self.model.ov_1_ct_sec > ov1sec_max:
-            #print("User defined setting outside of IEEE 1547 acceptable range.")
-            assert False
-        #check undervoltage points
-        if self.model.uv_2_pu < uv2pu_min and self.model.uv_2_pu > uv2pu_max:
-            #print("User defined setting outside of IEEE 1547 acceptable range.")
-            assert False
-
-        if self.model.uv_2_ct_sec < uv2sec_min and self.model.uv_2_ct_sec > uv2sec_max:
-            #print("User defined setting outside of IEEE 1547 acceptable range.")
-            assert False
-
-        if self.model.uv_1_pu < uv1pu_min and self.model.uv_1_pu > uv1pu_max:
-            #print("User defined setting outside of IEEE 1547 acceptable range.")
-            assert False
-
-        if self.model.uv_1_ct_sec <uv1sec_min and self.model.uv_1_ct_sec > uv1sec_max:
-            #print("User defined setting outside of IEEE 1547 acceptable range.")
-            assert False
-
-        self._controlled_element.SetParameter('Model', '7')
-        self._controlled_element.SetParameter('Vmaxpu', V[0])
-        self._controlled_element.SetParameter('Vminpu', V[1])
-
-        contineous_points = [Point(V[0], 0), Point(V[0], t_max), Point(V[1], t_max), Point(V[1], 0)]
-        contineous_region = Polygon([[p.y, p.x] for p in contineous_points])
-
-        mandatory_points = [Point(V[1], 0), Point(V[1], T[0]), Point(V[2], T[1]), Point(V[2], 0)]
-        mandatory_region = Polygon([[p.y, p.x] for p in mandatory_points])
-
-        permissive_ov_points = [Point(V[3], 0), Point(V[3], T[2]), Point(V[4], T[2]), Point(V[4], T[3]),
-                              Point(V[5], T[3]),
-                              Point(V[5], T[4]), Point(V[0], T[4]), Point(V[0], 0.0)]
-        permissive_ov_region = Polygon([[p.y, p.x] for p in permissive_ov_points])
-
-        permissive_uv_points = [Point(V[2], 0), Point(V[2], T[5]), Point(V[6], T[5]), Point(V[6], T[6]),
-                              Point(V[7], T[6]), Point(V[7], 0)]
-        permissive_uv_region = Polygon([[p.y, p.x] for p in permissive_uv_points])
-
-        active_region = MultiPolygon([ov_trip_region, uv_trip_region, contineous_region, mandatory_region,
-                                     permissive_ov_region, permissive_uv_region])
-
-        total_points = [Point(u_max_theo, 0), Point(u_max_theo, t_max), Point(0, t_max), Point(0, 0)]
-        total_region = Polygon([[p.y, p.x] for p in total_points])
-        intersection = total_region.intersection(active_region)
-        may_trip_region = total_region.difference(intersection)
-
-        if self.model.ride_through_category in [RideThroughCategory.CATEGORY_I, RideThroughCategory.CATEGORY_II]:
-            if self.model.permissive_operation ==  PermissiveOperation.CURRENT_LIMITED:
-                if self.model.may_trip_operation == MayTripOperation.PERMISSIVE_OPERATION:
-                    self.curr_lim_region = unary_union(
-                        [permissive_ov_region, permissive_uv_region, mandatory_region, may_trip_region])
-                    self.momentary_sucession_region = None
-                    self.trip_region = unary_union([ov_trip_region, uv_trip_region])
-                else:
-                    self.curr_lim_region = unary_union([permissive_ov_region, permissive_uv_region, mandatory_region])
-                    self.momentary_sucession_region = None
-                    self.trip_region = unary_union([ov_trip_region, uv_trip_region, may_trip_region])
-            else:
-                if self.model.may_trip_operation == MayTripOperation.PERMISSIVE_OPERATION:
-                    self.curr_lim_region = mandatory_region
-                    self.momentary_sucession_region = unary_union([permissive_ov_region, permissive_uv_region, may_trip_region])
-                    self.trip_region = unary_union([ov_trip_region, uv_trip_region])
-                else:
-                    self.curr_lim_region = mandatory_region
-                    self.momentary_sucession_region = unary_union([permissive_ov_region, permissive_uv_region])
-                    self.trip_region = unary_union([ov_trip_region, uv_trip_region, may_trip_region])
-        else:
-            if self.model.may_trip_operation == MayTripOperation.PERMISSIVE_OPERATION:
-                self.curr_lim_region = mandatory_region
-                self.momentary_sucession_region = unary_union([permissive_ov_region, permissive_uv_region, may_trip_region])
-                self.trip_region = unary_union([ov_trip_region, uv_trip_region])
-            else:
-                self.curr_lim_region = mandatory_region
-                self.momentary_sucession_region = unary_union([permissive_ov_region, permissive_uv_region])
-                self.trip_region = unary_union([ov_trip_region, uv_trip_region, may_trip_region])
-        self.normal_region = contineous_region
-        
-        
-        
-        return V, T
-
-    def Update(self, priority, time, update_results):
-
-        error = 0
-        self.time_change = self.time != (priority, time)
-        self.time = time
-        if priority == 0:
-            self._is_connected = self._connect()
-  
-        if priority == 2:
-            u_in = self._update_violaton_timers()
-            if self.model.follow_standard == PvStandard.IEEE_1547_2018:
-                self.voltage_ride_through(u_in)
-            elif self.model.follow_standard == PvStandard.IEEE_1547_2003:
-                self.trip(u_in)
-            else:
-                raise Exception("Valid standard setting defined. Options are: 1547-2003, 1547-2018")
-            
-            P = -sum(self._controlled_element.GetVariable('Powers')[::2])
-            # self.power_hist.append(P)
-            # self.voltage_hist.append(u_in)
-            # self.timer_hist.append(self._u_violation_time)
-            # self.timer_act_hist.append(self.__dss_solver.GetTotalSeconds())
-
-        return error
-
-    def trip(self, u_in):
-        """ Implementation of the IEEE1587-2003 voltage ride-through requirements for inverter systems
-        """
-        if u_in < 0.88:
-            if self._is_connected:
-                self._trip(30.0, 0.4, False)
-        return
-
-    def voltage_ride_through(self, u_in):
-        """ Implementation of the IEEE1587-2018 voltage ride-through requirements for inverter systems
-        """
-        self._fault_counter_clearing_time_sec = 1
-
-        Pm = Point(self._u_violation_time, u_in)
-        if Pm.within(self.curr_lim_region):
-            region = 0
-            is_in_contioeous_region = False
-        elif self.momentary_sucession_region and Pm.within(self.momentary_sucession_region):
-            region = 1
-            is_in_contioeous_region = False
-            self._trip(self.__dss_solver.GetStepSizeSec(), 0.4, False)
-        elif Pm.within(self.trip_region):
-            region = 2
-            is_in_contioeous_region = False
-            if self.region == [3, 1, 1]:
-                self._trip(self._trip_deadtime_sec, self._time_to_p_max_sec, False, True)
-            else: 
-                self._trip(self._trip_deadtime_sec, self._time_to_p_max_sec, False)
-        else:
-            is_in_contioeous_region = True
-            region = 3
-            
-        self.region = self.region[1:] + self.region[:1]
-        self.region[0] = region
-
-        if is_in_contioeous_region and not self._is_in_contioeous_region:
-            self._fault_window_clearing_start_time = self.__dss_solver.GetDateTime()
-        clearing_time = (self.__dss_solver.GetDateTime() - self._fault_window_clearing_start_time).total_seconds()
-
-        if self._is_in_contioeous_region and not is_in_contioeous_region:
-            if  clearing_time <= self._fault_counter_clearing_time_sec:
-                self._fault_counter += 1
-                if self._fault_counter > self._fault_counter_max:
-                    if self.model.multiple_disturdances == MultipleDisturbances.TRIP: 
-                        self._trip(self._trip_deadtime_sec, self._time_to_p_max_sec, True)
-                        self._fault_counter = 0
-                    else:
-                        pass
-        if  clearing_time > self._fault_counter_clearing_time_sec and self._fault_counter > 0:
-            self._fault_counter = 0
-        self._is_in_contioeous_region = is_in_contioeous_region
-        return
-
-    def _connect(self):
-        if not self._is_connected:
-            u_in = self._controlled_element.GetVariable('VoltagesMagAng')[::2]
-            u_base = self._controlled_element.sBus[0].GetVariable('kVBase') * 1000
-            u_in = max(u_in) / u_base if self._voltage_calc_mode == VoltageCalcModes.MAX else sum(u_in) / (u_base * len(u_in))
-            if self.use_avg_voltage:
-                self.voltage = self.voltage[1:] + self.voltage[:1]
-                self.voltage[0] = u_in
-                u_in = sum(self.voltage) / len(self.voltage)
-            deadtime = (self.__dss_solver.GetDateTime() - self._tripped_start_time).total_seconds()
-            if u_in < self._rvs[0] and u_in > self._rvs[1] and deadtime >= self._tripped_dead_time:
-                
-                self._controlled_element.SetParameter('enabled', True)
-                self._is_connected = True
-                self._controlled_element.SetParameter('kw', 0)
-                self._reconnect_start_time = self.__dss_solver.GetDateTime()
-        else:
-            conntime = (self.__dss_solver.GetDateTime() - self._reconnect_start_time).total_seconds()
-            self._p_limit = conntime / self._tripped_p_max_delay * self._p_rated if conntime < self._tripped_p_max_delay \
-                else self._p_rated
-            self._controlled_element.SetParameter('kw', self._p_limit)
-        return self._is_connected
-
-    def _trip(self, Deadtime, time2Pmax, forceTrip, permissive_to_trip=False):
-        
-        u_in = self._controlled_element.GetVariable('VoltagesMagAng')[::2]
-        u_base = self._controlled_element.sBus[0].GetVariable('kVBase') * 1000
-        u_in = max(u_in) / u_base if self._voltage_calc_mode == VoltageCalcModes.MAX else sum(u_in) / (u_base * len(u_in))
-
-        if self._is_connected or forceTrip:
-            self._controlled_element.SetParameter('enabled', False)
-
-            self._is_connected = False
-            self._tripped_start_time = self.__dss_solver.GetDateTime()
-            self._tripped_p_max_delay = time2Pmax
-            self._tripped_dead_time = Deadtime
-            
-        elif permissive_to_trip:
-            self._controlled_element.SetParameter('enabled', False)
-
-            self._is_connected = False
-            self._tripped_start_time = self.__dss_solver.GetDateTime()
-            self._tripped_p_max_delay = time2Pmax
-            self._tripped_dead_time = Deadtime
-        return
-
-    def _update_violaton_timers(self):
-        u_in = self._controlled_element.GetVariable('VoltagesMagAng')[::2]
-        u_base = self._controlled_element.sBus[0].GetVariable('kVBase') * 1000
-        u_in = max(u_in) / u_base if self._voltage_calc_mode == VoltageCalcModes.MAX else sum(u_in) / (u_base * len(u_in))
-        if self.use_avg_voltage:
-            self.voltage = self.voltage[1:] + self.voltage[:1]
-            self.voltage[0] = u_in
-            u_in = sum(self.voltage) / len(self.voltage)
-
-        if u_in < self._rvs[0] and u_in > self._rvs[1]:
-            if not self._normal_operation:
-                self._normal_operation = True
-                self._normal_operation_start_time = self.__dss_solver.GetDateTime()
-                self._normal_operation_time = 0
-            else:
-                self._normal_operation_time = (self.__dss_solver.GetDateTime() - self._normal_operation_start_time).total_seconds()
-            self._voltage_violation_m = False
-        else:
-            if not self._voltage_violation_m:
-                self._voltage_violation_m = True
-                self.__uViolationstarttime = self.__dss_solver.GetDateTime()
-                self._u_violation_time = 0
-            else:
-                self._u_violation_time = (self.__dss_solver.GetDateTime() - self.__uViolationstarttime).total_seconds()
-        return u_in
+from shapely.geometry import MultiPoint, Polygon, Point, MultiPolygon
+from shapely.ops import triangulate, unary_union
+import datetime
+import math
+
+from pydss.pyControllers.pyControllerAbstract import ControllerAbstract
+from pydss.pyControllers.models import PvVoltageRideThruModel
+from pydss.pyControllers.enumerations import PvStandard, VoltageCalcModes, RideThroughCategory, PermissiveOperation, MayTripOperation, MultipleDisturbances
+
+
+class PvVoltageRideThru(ControllerAbstract):
+    """Implementation of IEEE1547-2003 and IEEE1547-2018 voltage ride-through standards using the OpenDSS Generator model. Subclass of the :class:`pydss.pyControllers.pyControllerAbstract.ControllerAbstract` abstract class.
+
+            :param pv_object: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'Generator' element
+            :type FaultObj: class:`pydss.dssElement.dssElement`
+            :param settings: A dictionary that defines the settings for the PvController.
+            :type settings: dict
+            :param dss_instance: An :class:`opendssdirect` instance
+            :type dss_instance: :class:`opendssdirect`
+            :param elm_object_list: Dictionary of all dssElement, dssBus and dssCircuit objects
+            :type elm_object_list: dict
+            :param dss_solver: An instance of one of the classed defined in :mod:`pydss.SolveMode`.
+            :type dss_solver: :mod:`pydss.SolveMode`
+            :raises: Assertionerror if 'pv_object' is not a wrapped OpenDSS Generator element
+
+    """
+
+    def __init__(self, pv_object, settings, dss_instance, elm_object_list, dss_solver):
+        super(PvVoltageRideThru, self).__init__(pv_object, settings, dss_instance, elm_object_list, dss_solver)
+
+        self.model = PvVoltageRideThruModel(**settings)
+        
+        self.time_change = False
+        self.time = (-1, 0)
+
+        self.ControlDict = {
+            'None': lambda: 0,
+        }
+
+        self._controlled_element = pv_object
+        self.__dss_solver = dss_solver
+
+        self.object_type, self.object_name = self._controlled_element.GetInfo()
+        assert (self.object_type.lower() == 'generator'), 'PvControllerGen works only with an OpenDSS Generator element'
+        self._name = 'pyCont_' + self.object_type + '_' + self.object_name
+        if '_' in self.object_name:
+            self.phase = self.object_name.split('_')[1]
+        else:
+            self.phase = None
+
+        # Initializing the model
+        pv_object.SetParameter('kvar', 0)
+        pv_object.SetParameter('kva', self.model.kva)
+        self._p_rated = float(pv_object.SetParameter('kW', self.model.max_kw))
+
+        # MISC settings
+        self._trip_deadtime_sec = self.model.reconnect_deadtime_sec
+        self._time_to_p_max_sec = self.model.reconnect_pmax_time_sec
+        self._p_rated = self.model.max_kw
+        self._voltage_calc_mode = self.model.voltage_calc_mode
+        # initialize deadtimes and other variables
+        self._initialize_ride_through_settings()
+        if self.model.follow_standard == PvStandard.IEEE_1547_2003:
+            self._rvs = [1.10, 0.88]
+        else:
+            self._rvs, _= self._create_operation_regions()
+
+        # For debugging only
+        self.use_avg_voltage = False
+        cycle_average = 5
+        freq = dss_solver.getFrequency()
+        step = dss_solver.GetStepSizeSec()
+        hist_size = math.ceil(cycle_average / (step * freq))
+        self.voltage = [1.0 for i in range(hist_size)]
+        self.reactive_power = [0.0 for i in range(hist_size)]
+        self._voltage_violation_m = False
+        
+        self.region = [3, 3, 3]
+        
+        # self.voltage_hist = []
+        # self.power_hist = []
+        # self.timer_hist = []
+        # self.timer_act_hist = []
+        
+        return
+
+    def Name(self):
+        return self._name
+
+    def ControlledElement(self):
+        return "{}.{}".format(self.object_type, self.object_name)
+
+    def debugInfo(self):
+        return []
+
+    def _initialize_ride_through_settings(self):
+        self._is_connected = True
+        self._p_limit = self._p_rated
+        self._reconnect_start_time = self.__dss_solver.GetDateTime() - datetime.timedelta(
+            seconds=int(self._time_to_p_max_sec))
+
+        self._tripped_p_max_delay = 0
+        self._normal_operation = True
+        self._normal_operation_start_time = self.__dss_solver.GetDateTime()
+        self._u_violation_time = 99999
+        self._tripped_start_time = self.__dss_solver.GetDateTime()
+        self._tripped_dead_time = 0
+        self._fault_counter = 0
+        self._is_in_contioeous_region = True
+        self._fault_window_clearing_start_time = self.__dss_solver.GetDateTime()
+
+        return
+
+    def _create_operation_regions(self):
+        u_max_theo = 10
+        t_max = 1e10
+
+        ov_trip_points = [
+            Point(u_max_theo, self.model.ov_2_ct_sec),
+            Point(self.model.ov_2_pu, self.model.ov_2_ct_sec),
+            Point(self.model.ov_2_pu, self.model.ov_1_ct_sec),
+            Point(self.model.ov_1_pu, self.model.ov_1_ct_sec),
+            Point(self.model.ov_1_pu, t_max),
+            Point(u_max_theo, t_max)
+        ]
+        ov_trip_region = Polygon([[p.y, p.x] for p in ov_trip_points])
+
+        UVtripPoints = [
+            Point(0, self.model.uv_2_ct_sec),
+            Point(self.model.uv_2_pu, self.model.uv_2_ct_sec),
+            Point(self.model.uv_2_pu, self.model.uv_1_ct_sec),
+            Point(self.model.uv_1_pu, self.model.uv_1_ct_sec),
+            Point(self.model.uv_1_pu, t_max),
+            Point(0, t_max)
+        ]
+        uv_trip_region = Polygon([[p.y, p.x] for p in UVtripPoints])
+
+        if self.model.ride_through_category == RideThroughCategory.CATEGORY_I:
+            ov2pu_eq = 1.20
+            ov2sec_eq = 0.16
+            ov1pu_min = 1.1
+            ov1pu_max = 1.2
+            ov1sec_min = 1
+            ov1sec_max = 13
+            uv2pu_min = 0.0
+            uv2pu_max = 0.5
+            uv2sec_min = 0.16
+            uv2sec_max = 2.0
+            uv1pu_min = 0.0
+            uv1pu_max = 0.88
+            uv1sec_min = 2.0
+            uv1sec_max = 21
+    
+            V = [1.10, 0.88, 0.7, 1.20, 1.175, 1.15, 0.5, 0.5]
+            T = [1.5, 0.7, 0.2, 0.5, 1, 0.16, 0.16]
+            self._fault_counter_max = 2
+            self._fault_counter_clearing_time_sec = 20
+
+        elif self.model.ride_through_category == RideThroughCategory.CATEGORY_II:
+            ov2pu_eq = 1.20
+            ov2sec_eq = 0.16
+            ov1pu_min = 1.1
+            ov1pu_max = 1.2
+            ov1sec_min = 1
+            ov1sec_max = 13
+            uv2pu_min = 0.0
+            uv2pu_max = 0.45
+            uv2sec_min = 0.16
+            uv2sec_max = 2.0
+            uv1pu_min = 0.0
+            uv1pu_max = 0.88
+            uv1sec_min = 2.0
+            uv1sec_max = 21
+
+            V = [1.10, 0.88, 0.65, 1.20, 1.175, 1.15, 0.45, 0.30]
+            T = [5, 3, 0.2, 0.5, 1, 0.32, 0.16]
+            self._fault_counter_max = 2
+            self._fault_counter_clearing_time_sec = 10
+
+        elif self.model.ride_through_category == RideThroughCategory.CATEGORY_I:
+            ov2pu_eq = 1.20
+            ov2sec_eq = 0.16
+            ov1pu_min = 1.1
+            ov1pu_max = 1.2
+            ov1sec_min = 1
+            ov1sec_max = 13
+            uv2pu_min = 0.0
+            uv2pu_max = 0.5
+            uv2sec_min = 2.0
+            uv2sec_max = 21.0
+            uv1pu_min = 0.0
+            uv1pu_max = 0.88
+            uv1sec_min = 2.0
+            uv1sec_max = 21.0
+
+            V = [1.10, 0.88, 0.5, 1.2, 1.2, 1.2, 0.0, 0.0]
+            T = [21, 10, 13, 13, 13, 1, 1]
+            self._fault_counter_max = 3
+            self._fault_counter_clearing_time_sec = 5
+
+        #check overvoltage points
+        if self.model.ov_2_pu != ov2pu_eq:
+            #print("User defined setting outside of IEEE 1547 acceptable range.")
+            assert False
+
+        
+        if self.model.ov_2_ct_sec != ov2sec_eq:
+            #print("User defined setting outside of IEEE 1547 acceptable range.")
+            assert False
+
+        if self.model.ov_1_pu < ov1pu_min and self.model.ov_1_pu > ov1pu_max:
+            #print("User defined setting outside of IEEE 1547 acceptable range.")
+            assert False
+
+        if self.model.ov_1_ct_sec < ov1sec_min and self.model.ov_1_ct_sec > ov1sec_max:
+            #print("User defined setting outside of IEEE 1547 acceptable range.")
+            assert False
+        #check undervoltage points
+        if self.model.uv_2_pu < uv2pu_min and self.model.uv_2_pu > uv2pu_max:
+            #print("User defined setting outside of IEEE 1547 acceptable range.")
+            assert False
+
+        if self.model.uv_2_ct_sec < uv2sec_min and self.model.uv_2_ct_sec > uv2sec_max:
+            #print("User defined setting outside of IEEE 1547 acceptable range.")
+            assert False
+
+        if self.model.uv_1_pu < uv1pu_min and self.model.uv_1_pu > uv1pu_max:
+            #print("User defined setting outside of IEEE 1547 acceptable range.")
+            assert False
+
+        if self.model.uv_1_ct_sec <uv1sec_min and self.model.uv_1_ct_sec > uv1sec_max:
+            #print("User defined setting outside of IEEE 1547 acceptable range.")
+            assert False
+
+        self._controlled_element.SetParameter('Model', '7')
+        self._controlled_element.SetParameter('Vmaxpu', V[0])
+        self._controlled_element.SetParameter('Vminpu', V[1])
+
+        contineous_points = [Point(V[0], 0), Point(V[0], t_max), Point(V[1], t_max), Point(V[1], 0)]
+        contineous_region = Polygon([[p.y, p.x] for p in contineous_points])
+
+        mandatory_points = [Point(V[1], 0), Point(V[1], T[0]), Point(V[2], T[1]), Point(V[2], 0)]
+        mandatory_region = Polygon([[p.y, p.x] for p in mandatory_points])
+
+        permissive_ov_points = [Point(V[3], 0), Point(V[3], T[2]), Point(V[4], T[2]), Point(V[4], T[3]),
+                              Point(V[5], T[3]),
+                              Point(V[5], T[4]), Point(V[0], T[4]), Point(V[0], 0.0)]
+        permissive_ov_region = Polygon([[p.y, p.x] for p in permissive_ov_points])
+
+        permissive_uv_points = [Point(V[2], 0), Point(V[2], T[5]), Point(V[6], T[5]), Point(V[6], T[6]),
+                              Point(V[7], T[6]), Point(V[7], 0)]
+        permissive_uv_region = Polygon([[p.y, p.x] for p in permissive_uv_points])
+
+        active_region = MultiPolygon([ov_trip_region, uv_trip_region, contineous_region, mandatory_region,
+                                     permissive_ov_region, permissive_uv_region])
+
+        total_points = [Point(u_max_theo, 0), Point(u_max_theo, t_max), Point(0, t_max), Point(0, 0)]
+        total_region = Polygon([[p.y, p.x] for p in total_points])
+        intersection = total_region.intersection(active_region)
+        may_trip_region = total_region.difference(intersection)
+
+        if self.model.ride_through_category in [RideThroughCategory.CATEGORY_I, RideThroughCategory.CATEGORY_II]:
+            if self.model.permissive_operation ==  PermissiveOperation.CURRENT_LIMITED:
+                if self.model.may_trip_operation == MayTripOperation.PERMISSIVE_OPERATION:
+                    self.curr_lim_region = unary_union(
+                        [permissive_ov_region, permissive_uv_region, mandatory_region, may_trip_region])
+                    self.momentary_sucession_region = None
+                    self.trip_region = unary_union([ov_trip_region, uv_trip_region])
+                else:
+                    self.curr_lim_region = unary_union([permissive_ov_region, permissive_uv_region, mandatory_region])
+                    self.momentary_sucession_region = None
+                    self.trip_region = unary_union([ov_trip_region, uv_trip_region, may_trip_region])
+            else:
+                if self.model.may_trip_operation == MayTripOperation.PERMISSIVE_OPERATION:
+                    self.curr_lim_region = mandatory_region
+                    self.momentary_sucession_region = unary_union([permissive_ov_region, permissive_uv_region, may_trip_region])
+                    self.trip_region = unary_union([ov_trip_region, uv_trip_region])
+                else:
+                    self.curr_lim_region = mandatory_region
+                    self.momentary_sucession_region = unary_union([permissive_ov_region, permissive_uv_region])
+                    self.trip_region = unary_union([ov_trip_region, uv_trip_region, may_trip_region])
+        else:
+            if self.model.may_trip_operation == MayTripOperation.PERMISSIVE_OPERATION:
+                self.curr_lim_region = mandatory_region
+                self.momentary_sucession_region = unary_union([permissive_ov_region, permissive_uv_region, may_trip_region])
+                self.trip_region = unary_union([ov_trip_region, uv_trip_region])
+            else:
+                self.curr_lim_region = mandatory_region
+                self.momentary_sucession_region = unary_union([permissive_ov_region, permissive_uv_region])
+                self.trip_region = unary_union([ov_trip_region, uv_trip_region, may_trip_region])
+        self.normal_region = contineous_region
+        
+        
+        
+        return V, T
+
+    def Update(self, priority, time, update_results):
+
+        error = 0
+        self.time_change = self.time != (priority, time)
+        self.time = time
+        if priority == 0:
+            self._is_connected = self._connect()
+  
+        if priority == 2:
+            u_in = self._update_violaton_timers()
+            if self.model.follow_standard == PvStandard.IEEE_1547_2018:
+                self.voltage_ride_through(u_in)
+            elif self.model.follow_standard == PvStandard.IEEE_1547_2003:
+                self.trip(u_in)
+            else:
+                raise Exception("Valid standard setting defined. Options are: 1547-2003, 1547-2018")
+            
+            P = -sum(self._controlled_element.GetVariable('Powers')[::2])
+            # self.power_hist.append(P)
+            # self.voltage_hist.append(u_in)
+            # self.timer_hist.append(self._u_violation_time)
+            # self.timer_act_hist.append(self.__dss_solver.GetTotalSeconds())
+
+        return error
+
+    def trip(self, u_in):
+        """ Implementation of the IEEE1587-2003 voltage ride-through requirements for inverter systems
+        """
+        if u_in < 0.88:
+            if self._is_connected:
+                self._trip(30.0, 0.4, False)
+        return
+
+    def voltage_ride_through(self, u_in):
+        """ Implementation of the IEEE1587-2018 voltage ride-through requirements for inverter systems
+        """
+        self._fault_counter_clearing_time_sec = 1
+
+        Pm = Point(self._u_violation_time, u_in)
+        if Pm.within(self.curr_lim_region):
+            region = 0
+            is_in_contioeous_region = False
+        elif self.momentary_sucession_region and Pm.within(self.momentary_sucession_region):
+            region = 1
+            is_in_contioeous_region = False
+            self._trip(self.__dss_solver.GetStepSizeSec(), 0.4, False)
+        elif Pm.within(self.trip_region):
+            region = 2
+            is_in_contioeous_region = False
+            if self.region == [3, 1, 1]:
+                self._trip(self._trip_deadtime_sec, self._time_to_p_max_sec, False, True)
+            else: 
+                self._trip(self._trip_deadtime_sec, self._time_to_p_max_sec, False)
+        else:
+            is_in_contioeous_region = True
+            region = 3
+            
+        self.region = self.region[1:] + self.region[:1]
+        self.region[0] = region
+
+        if is_in_contioeous_region and not self._is_in_contioeous_region:
+            self._fault_window_clearing_start_time = self.__dss_solver.GetDateTime()
+        clearing_time = (self.__dss_solver.GetDateTime() - self._fault_window_clearing_start_time).total_seconds()
+
+        if self._is_in_contioeous_region and not is_in_contioeous_region:
+            if  clearing_time <= self._fault_counter_clearing_time_sec:
+                self._fault_counter += 1
+                if self._fault_counter > self._fault_counter_max:
+                    if self.model.multiple_disturdances == MultipleDisturbances.TRIP: 
+                        self._trip(self._trip_deadtime_sec, self._time_to_p_max_sec, True)
+                        self._fault_counter = 0
+                    else:
+                        pass
+        if  clearing_time > self._fault_counter_clearing_time_sec and self._fault_counter > 0:
+            self._fault_counter = 0
+        self._is_in_contioeous_region = is_in_contioeous_region
+        return
+
+    def _connect(self):
+        if not self._is_connected:
+            u_in = self._controlled_element.GetVariable('VoltagesMagAng')[::2]
+            u_base = self._controlled_element.sBus[0].GetVariable('kVBase') * 1000
+            u_in = max(u_in) / u_base if self._voltage_calc_mode == VoltageCalcModes.MAX else sum(u_in) / (u_base * len(u_in))
+            if self.use_avg_voltage:
+                self.voltage = self.voltage[1:] + self.voltage[:1]
+                self.voltage[0] = u_in
+                u_in = sum(self.voltage) / len(self.voltage)
+            deadtime = (self.__dss_solver.GetDateTime() - self._tripped_start_time).total_seconds()
+            if u_in < self._rvs[0] and u_in > self._rvs[1] and deadtime >= self._tripped_dead_time:
+                
+                self._controlled_element.SetParameter('enabled', True)
+                self._is_connected = True
+                self._controlled_element.SetParameter('kw', 0)
+                self._reconnect_start_time = self.__dss_solver.GetDateTime()
+        else:
+            conntime = (self.__dss_solver.GetDateTime() - self._reconnect_start_time).total_seconds()
+            self._p_limit = conntime / self._tripped_p_max_delay * self._p_rated if conntime < self._tripped_p_max_delay \
+                else self._p_rated
+            self._controlled_element.SetParameter('kw', self._p_limit)
+        return self._is_connected
+
+    def _trip(self, Deadtime, time2Pmax, forceTrip, permissive_to_trip=False):
+        
+        u_in = self._controlled_element.GetVariable('VoltagesMagAng')[::2]
+        u_base = self._controlled_element.sBus[0].GetVariable('kVBase') * 1000
+        u_in = max(u_in) / u_base if self._voltage_calc_mode == VoltageCalcModes.MAX else sum(u_in) / (u_base * len(u_in))
+
+        if self._is_connected or forceTrip:
+            self._controlled_element.SetParameter('enabled', False)
+
+            self._is_connected = False
+            self._tripped_start_time = self.__dss_solver.GetDateTime()
+            self._tripped_p_max_delay = time2Pmax
+            self._tripped_dead_time = Deadtime
+            
+        elif permissive_to_trip:
+            self._controlled_element.SetParameter('enabled', False)
+
+            self._is_connected = False
+            self._tripped_start_time = self.__dss_solver.GetDateTime()
+            self._tripped_p_max_delay = time2Pmax
+            self._tripped_dead_time = Deadtime
+        return
+
+    def _update_violaton_timers(self):
+        u_in = self._controlled_element.GetVariable('VoltagesMagAng')[::2]
+        u_base = self._controlled_element.sBus[0].GetVariable('kVBase') * 1000
+        u_in = max(u_in) / u_base if self._voltage_calc_mode == VoltageCalcModes.MAX else sum(u_in) / (u_base * len(u_in))
+        if self.use_avg_voltage:
+            self.voltage = self.voltage[1:] + self.voltage[:1]
+            self.voltage[0] = u_in
+            u_in = sum(self.voltage) / len(self.voltage)
+
+        if u_in < self._rvs[0] and u_in > self._rvs[1]:
+            if not self._normal_operation:
+                self._normal_operation = True
+                self._normal_operation_start_time = self.__dss_solver.GetDateTime()
+                self._normal_operation_time = 0
+            else:
+                self._normal_operation_time = (self.__dss_solver.GetDateTime() - self._normal_operation_start_time).total_seconds()
+            self._voltage_violation_m = False
+        else:
+            if not self._voltage_violation_m:
+                self._voltage_violation_m = True
+                self.__uViolationstarttime = self.__dss_solver.GetDateTime()
+                self._u_violation_time = 0
+            else:
+                self._u_violation_time = (self.__dss_solver.GetDateTime() - self.__uViolationstarttime).total_seconds()
+        return u_in
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/StorageController.py` & `nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/StorageController.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,553 +1,553 @@
-from  pydss.pyControllers.pyControllerAbstract import ControllerAbstract
-import calendar
-import math
-import ast
-
-class StorageController(ControllerAbstract):
-    """Numerous control implementation for a storage system from both behind-the-meter and front-of- meter applications. Subclass of the :class:`pydss.pyControllers.pyControllerAbstract.ControllerAbstract` abstract class.
-
-            :param StorageObj: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS Storage element
-            :type FaultObj: class:`pydss.dssElement.dssElement`
-            :param Settings: A dictionary that defines the settings for the PvController.
-            :type Settings: dict
-            :param dssInstance: An :class:`opendssdirect` instance
-            :type dssInstance: :class:`opendssdirect`
-            :param ElmObjectList: Dictionary of all dssElement, dssBus and dssCircuit objects
-            :type ElmObjectList: dict
-            :param dssSolver: An instance of one of the classed defined in :mod:`pydss.SolveMode`.
-            :type dssSolver: :mod:`pydss.SolveMode`
-            :raises: AssertionError if 'StorageObj' is not a wrapped OpenDSS Storage element
-
-    """
-    def __init__(self, StorageObj, Settings, dssInstance, ElmObjectList, dssSolver):
-        self.Time = -1
-        super(StorageController, self).__init__(StorageObj, Settings, dssInstance, ElmObjectList, dssSolver)
-
-        self.__ControlledElm = StorageObj
-        self.ceClass, self.ceName = self.__ControlledElm.GetInfo()
-        assert (self.ceClass.lower() == 'storage'), 'StorageController works only with an OpenDSS Storage element'
-        self.__Name = 'pyCont_' + self.ceClass + '_' + self.ceName
-
-        self.Time = (-1, 0)
-        self.__Pin = 0
-        self.Pbatt = 0
-        self.__PinOld = 0
-        self.PbattOld = 0
-        self.oldQcalc = 0
-        self.ExportOld = False
-        self.__ElmObjectList = ElmObjectList
-
-        self.ControlDict = {
-            'None'   : lambda: 0,
-            'PS'     : self.PeakShavingControl,
-            'CF'     : self.CapacityFirmimgControl,
-            'TT'     : self.TimeTriggeredControl,
-            'RT'     : self.RealTimeControl,
-            'SH'     : self.ScheduledControl,
-            'NETT'   : self.NonExportTimeTriggered,
-            'TOU'    : self.TimeOfUse,
-            'DemChg' : self.DemandCharge,
-            'CPF'    : self.ConstantPowerFactorControl,
-            'VPF'    : self.VariablePowerFactorControl,
-            'VVar'   : self.VoltVarControl,
-        }
-
-        self.__a = Settings['alpha']
-        self.__b = Settings['beta']
-        self.__ElmObjectList = ElmObjectList
-        self.__ControlledElm = StorageObj
-        self.__dssInstance = dssInstance
-        self.__dssSolver = dssSolver
-        self.__Settings = Settings
-
-        self.__Srated = float(StorageObj.GetParameter('kVA'))
-        self.__Prated = float(StorageObj.GetParameter('kWrated'))
-        self.__Pbatt = float(StorageObj.GetParameter('kW'))
-        self.__dampCoef = Settings['DampCoef']
-        self.update = [self.ControlDict[Settings['Control' + str(i)]] for i in [1, 2, 3]]
-        return
-
-    def Name(self):
-        return self.__Name
-
-    def ControlledElement(self):
-        return "{}.{}".format(self.ceClass, self.ceName)
-
-    def debugInfo(self):
-        return [self.__Settings['Control{}'.format(i+1)] for i in range(3)]
-
-    def Update(self, Priority, Time, Update):
-        self.TimeChange = self.Time != (Time, Priority)
-        self.Time = (Time, Priority)
-        return self.update[Priority]()
-
-    def SetSetting(self, Property, Value):
-        self.__Settings[Property] = Value
-        return
-
-    def GetSetting(self, Property):
-        return self.__Settings[Property]
-
-    def __parseRatePlan(self, tarrif):
-        self.touTarrif = ast.literal_eval(tarrif)
-        CurrDateTime = self.__dssSolver.GetDateTime()
-        DayOfYear = CurrDateTime.timetuple().tm_yday
-        weekno = CurrDateTime.weekday()
-        currentDay = calendar.day_name[weekno]
-
-        for period, touDetails in self.touTarrif.items():
-            if touDetails['ED'] < touDetails['SD']:
-                TOUday = [i for j in (range(1, touDetails['ED']), range(touDetails['SD'], 365)) for i in j]
-            else:
-                TOUday = range(touDetails['SD'], touDetails['ED'] + 1)
-            if DayOfYear in TOUday:
-                if currentDay in touDetails['TOW']:
-                    for st, et in zip(touDetails['ST'], touDetails['ET']):
-                        if et < st:
-                            TOUtime = [i for j in (range(1, et), range(st, 25)) for i in j]
-                        else:
-                            TOUtime = range(st, et)
-                        if CurrDateTime.hour in TOUtime:
-                            return True
-                else:
-                    pass
-        return False
-
-    def TimeOfUse(self):
-        """ Implementation of a time of use controller for behind the meter applications
-        """
-        dP = 0
-        Pub = self.__Settings['touLoadLim']
-        touCharge = self.__Settings['%touCharge']
-        tarrif = self.__Settings['touTarrifStructure']
-        isTOU = self.__parseRatePlan(tarrif)
-        Pbatt = float(self.__ControlledElm.GetParameter('kw'))
-        if self.__Settings['PowerMeaElem'] == 'Total':
-            Sin = self.__dssInstance.Circuit.TotalPower()
-            Pin = -sum(Sin[0:5:2])
-        else:
-            Sin = self.__ElmObjectList[self.__Settings['PowerMeaElem']].GetVariable('Powers')
-            Pin = sum(Sin[0:5:2])
-
-        if isTOU:
-            if Pin > Pub:
-                dP = Pin - Pub
-                Pbatt = Pbatt + dP * self.__a - (Pbatt - self.PbattOld) * self.__b
-            else:
-                Pbatt = 0
-        else:
-            Pbatt =  -touCharge * self.__Prated / 100
-
-        if Pbatt >= 0:
-            pctdischarge = Pbatt / (self.__Prated) * 100
-            pctdischarge = 100 if pctdischarge > 100 else pctdischarge
-            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
-            self.__ControlledElm.SetParameter('%Discharge', str(pctdischarge))
-        if Pbatt < 0:
-            pctcharge = -Pbatt / (self.__Prated) * 100
-            pctcharge = 100 if pctcharge > 100 else pctcharge
-            self.__ControlledElm.SetParameter('State', 'CHARGING')
-            self.__ControlledElm.SetParameter('%charge', str(pctcharge))
-
-        Error = abs(Pbatt - self.PbattOld)
-        self.PbattOld = Pbatt
-        self.dPold = dP
-        return Error
-
-
-    def DemandCharge(self):
-        """ Implementation of a demand charge controller for behind the meter applications
-        """
-
-        self.Demand = 0
-        dP = 0
-        DemandChgThreh = self.__Settings['DemandChgThreh[kWh]']
-        Pub = self.__Settings['touLoadLim']
-        touCharge = self.__Settings['%touCharge']
-        tarrif = self.__Settings['touTarrifStructure']
-        isTOU = self.__parseRatePlan(tarrif)
-        Pbatt = float(self.__ControlledElm.GetParameter('kw'))
-        if self.__Settings['PowerMeaElem'] == 'Total':
-            Sin = self.__dssInstance.Circuit.TotalPower()
-            Pin = -sum(Sin[0:5:2])
-        else:
-            Sin = self.__ElmObjectList[self.__Settings['PowerMeaElem']].GetVariable('Powers')
-            Pin = sum(Sin[0:5:2])
-        DateAndTime = self.__dssSolver.GetDateTime()
-        CurrMin = DateAndTime.minute
-        if isTOU:
-            i = CurrMin % 30
-            if i == 0:
-                self.__EnergyCounter = [0 for i in range(30)]
-            self.__EnergyCounter[i] = Pin
-            self.Demand = sum(self.__EnergyCounter) / (60 / self.__dssSolver.GetStepResolutionMinutes())
-
-            if self.Demand >= 0.9 * DemandChgThreh:
-                if Pin > Pub:
-                    dP = Pin - Pub
-                    Pbatt = Pbatt + (dP) * self.__a - (Pbatt - self.PbattOld) * self.__b
-                else:
-                    Pbatt = 0
-            else:
-                Pbatt = 0
-        else:
-            Pbatt = -touCharge * self.__Prated / 100
-
-        if Pbatt >= 0:
-            pctdischarge = Pbatt / (self.__Prated) * 100
-            pctdischarge =  100 if  pctdischarge > 100 else pctdischarge
-            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
-            self.__ControlledElm.SetParameter('%Discharge', str(pctdischarge))
-        if Pbatt < 0:
-            pctcharge = -Pbatt / (self.__Prated) * 100
-            pctcharge = 100 if pctcharge > 100 else pctcharge
-            self.__ControlledElm.SetParameter('State', 'CHARGING')
-            self.__ControlledElm.SetParameter('%charge', str(pctcharge))
-
-        Error = abs(Pbatt - self.PbattOld)
-        self.PbattOld = Pbatt
-        self.dPold = dP
-        return Error
-
-    def ScheduledControl(self):
-        """ Implementation of a fixed schedule controller. Used to implemented predefined dispatch signals
-        """
-        P_profile = self.__Settings['Schedule']
-        Days = self.__Settings['Days']
-        LenSchedule = len(P_profile)
-        TotalSeconds = 24*60*60*Days
-        TimeStepPerSample = int(TotalSeconds / LenSchedule)
-
-        CurrentTime = int(self.__dssInstance.Solution.Hour()) * 60 * 60 + \
-                      int(self.__dssInstance.Solution.Seconds())
-
-        Index = int(CurrentTime / TimeStepPerSample)
-        Pout = P_profile[Index]
-        if Pout > 0:
-            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
-            self.__ControlledElm.SetParameter('%Discharge', str(Pout * 100))
-        elif Pout < 0:
-            self.__ControlledElm.SetParameter('State', 'CHARGING')
-            self.__ControlledElm.SetParameter('%charge', str(-Pout * 100))
-        else:
-            self.__ControlledElm.SetParameter('State', 'IDLE')
-        return 0
-
-    def NonExportTimeTriggered(self):
-        """ Implementation of a smart non-export controller. Makes use of TOU window to optimize charging
-        """
-        # self.dPold = 0
-        dP = 0
-        Plb = self.__Settings['BaseLoadLim']
-        sTime = self.__Settings['ExpWindowStart']
-        eTime = self.__Settings['ExpWindowEnd']
-        KWHrated = float(self.__ControlledElm.GetParameter('kWhrated'))
-        perIdle = float(self.__ControlledElm.GetParameter('%IdlingkW'))
-        effDchg = float(self.__ControlledElm.GetParameter('%EffDischarge'))
-
-        Minutes = int(self.__dssInstance.Solution.Seconds() / 60)
-        Hour = self.__dssInstance.Solution.Hour() % 24
-
-        if sTime.hour < eTime.hour:
-            Twindow = ((eTime.hour * 60 + eTime.minute) - (sTime.hour * 60 + sTime.minute)) / 60
-            if (Hour * 60 + Minutes) > (sTime.hour * 60 + sTime.minute) and (Hour * 60 + Minutes) < (eTime.hour * 60 + eTime.minute):
-                Export = True
-            else:
-                Export = False
-        else:
-            Twindow = 24 - ((sTime.hour * 60 + sTime.minute) - (eTime.hour * 60 + eTime.minute)) / 60
-            if (Hour * 60 + Minutes) > (sTime.hour * 60 + sTime.minute) or \
-                    (Hour * 60 + Minutes) < (eTime.hour * 60 + eTime.minute):
-                Export = True
-            else:
-                Export = False
-
-        if self.ExportOld == False and Export == True:
-            perKWHstored = float(self.__ControlledElm.GetParameter('%stored'))
-            kWhrem = KWHrated * (perKWHstored / 100)
-            self.Pbatt = kWhrem / (Twindow) * effDchg / 100 - perIdle * self.__Prated / 100
-        self.ExportOld = Export
-
-        if Export:
-            pctcharge = self.Pbatt / (self.__Prated) * 100
-            rT = ((eTime.hour * 60 + eTime.minute) - (Hour * 60 + Minutes)) / 60
-            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
-            self.__ControlledElm.SetParameter('%Discharge', str(pctcharge))
-            Error = 0
-        else:
-            if self.__Settings['PowerMeaElem'] == 'Total':
-                Sin = self.__dssInstance.Circuit.TotalPower()
-                Pin = -sum(Sin[0:5:2])
-            else:
-                Sin = self.__ElmObjectList[self.__Settings['PowerMeaElem']].GetVariable('Powers')
-                Sin2 = Sin[:int(len(Sin) / 2)]
-                Pin = sum(Sin2[0::2])
-            Pbatt = float(self.__ControlledElm.GetParameter('kw'))
-            if Pin < Plb:
-                dP = Plb - Pin
-                Pbatt = Pbatt - (dP) * self.__a - (Pbatt - self.PbattOld) * self.__b
-            else:
-                Pbatt = 0
-
-            if Pbatt >= 0:
-                pctdischarge = Pbatt / (self.__Prated) * 100
-                self.__ControlledElm.SetParameter('State', 'DISCHARGING')
-                self.__ControlledElm.SetParameter('%Discharge', str(pctdischarge))
-            if Pbatt < 0:
-                pctcharge = -Pbatt / (self.__Prated) * 100
-                self.__ControlledElm.SetParameter('State', 'CHARGING')
-                self.__ControlledElm.SetParameter('%charge', str(pctcharge))
-
-            Error = abs(Pbatt - self.PbattOld)
-            self.PbattOld = Pbatt
-            self.dPold = dP
-        return Error
-
-    def PeakShavingControl(self):
-        """ Implementation of a peak shaving / base loading controller. Setting both peak shaving and base loading
-        limits to zero will make the storage work in "SELF CONSUMPTION" mode
-        """
-        Pub = self.__Settings['PS_ub']
-        Plb = self.__Settings['PS_lb']
-        IdlingkWPercent = float(self.__ControlledElm.GetParameter('%IdlingkW'))
-        IdlingkW = -IdlingkWPercent/100*self.__Prated
-        if self.__Settings['PowerMeaElem'] == 'Total':
-            Sin = self.__dssInstance.Circuit.TotalPower()
-            Pin = -sum(Sin[0:5:2])
-        else:
-            Sin = self.__ElmObjectList[self.__Settings['PowerMeaElem']].GetVariable('Powers')
-            Pin = sum(Sin[0:int(len(Sin)/2):2])
-
-        Pbatt = float(self.__ControlledElm.GetParameter('kw'))
-
-        if Pin > Pub:
-            dP = Pin - Pub
-            Pbatt = Pbatt + dP * self.__dampCoef
-        elif Pin < Plb:
-            dP = Pin - Plb
-            Pbatt = Pbatt + dP * self.__dampCoef
-        else:
-            Pbatt = Pbatt * self.__dampCoef
-
-        if Pbatt >= 0:
-            pctdischarge = min(100, Pbatt / self.__Prated* 100)
-            pctdischarge = 100 if pctdischarge > 100 else pctdischarge
-            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
-            self.__ControlledElm.SetParameter('%Discharge', str(pctdischarge))
-
-        elif Pbatt < 0:
-            pctcharge = min(100, -Pbatt / (self.__Prated)* 100)
-            pctcharge = 100 if pctcharge > 100 else pctcharge
-            self.__ControlledElm.SetParameter('State', 'CHARGING')
-            self.__ControlledElm.SetParameter('%charge', str(pctcharge))
-
-        Error = abs(Pbatt - self.PbattOld) / self.__Srated
-        self.PbattOld = Pbatt
-        return Error
-
-    def RealTimeControl(self):
-        kWOut = self.__Settings['%kWOut']
-        if kWOut > 0:
-            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
-            self.__ControlledElm.SetParameter('%Discharge', str(kWOut))
-        elif kWOut < 0:
-            self.__ControlledElm.SetParameter('State', 'CHARGING')
-            self.__ControlledElm.SetParameter('%charge', str(-kWOut))
-        else:
-            self.__ControlledElm.SetParameter('State', 'IDLE')
-        return 0
-
-    def TimeTriggeredControl(self):
-        HrCharge = self.__Settings['HrCharge']
-        HrDischarge = self.__Settings['HrDischarge']
-        rateCharge = self.__Settings['%rateCharge']
-        rateDischarge = self.__Settings['%rateDischarge']
-
-        HrC = int(HrCharge)
-        MnC = int((HrCharge - HrC) * 60)
-        HrD = int(HrDischarge)
-        MnD = int((HrDischarge - HrD) * 60)
-
-        Minutes = int(self.__dssInstance.Solution.Seconds()/60)
-        Hour = self.__dssInstance.Solution.Hour() % 24
-        if Hour == HrC and Minutes == MnC:
-            self.__ControlledElm.SetParameter('State', 'CHARGING')
-            self.__ControlledElm.SetParameter('%charge', str(rateCharge))
-        elif Hour == HrD and Minutes == MnD:
-            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
-            self.__ControlledElm.SetParameter('%Discharge', str(rateDischarge))
-        return 0
-
-    def CapacityFirmimgControl(self):
-        """ Implementation of a capacity firming algorithm
-        """
-        dPub = self.__Settings['CF_dP_ub']
-        dPlb = self.__Settings['CF_dP_lb']
-
-        if self.Time[0] == 0:
-            self.__PinOld = self.__Pin
-
-        if self.__Settings['PowerMeaElem'] == 'Total':
-            Sin = self.__dssInstance.Circuit.TotalPower()
-            Pin = -sum(Sin[0:5:2])
-        else:
-            Sin = self.__ElmObjectList[self.__Settings['PowerMeaElem']].GetVariable('Powers')
-            Pin = sum(Sin[0:5:2])
-        Pbatt = -float(self.__ControlledElm.GetVariable('Powers')[0]) * 3
-        ramp = (Pin - self.__PinOld)
-        if self.Time[0] > 1:
-            if ramp >= dPub:
-                dPbatt = self.__dampCoef * (ramp - dPub)
-                Pbatt += dPbatt
-            elif ramp <= dPlb:
-                dPbatt = self.__dampCoef * (ramp - dPlb)
-                Pbatt += dPbatt
-            else:
-                dPbatt = 0
-                Pbatt = 0
-            if Pbatt > 0:
-                pctdischarge = Pbatt / self.__Prated * 100
-                self.__ControlledElm.SetParameter('State', 'DISCHARGING')
-                self.__ControlledElm.SetParameter('%Discharge', str(pctdischarge))
-            elif Pbatt < 0:
-                pctcharge = -Pbatt / self.__Prated * 100
-                self.__ControlledElm.SetParameter('State', 'CHARGING')
-                self.__ControlledElm.SetParameter('%charge', str(pctcharge))
-            elif Pbatt == 0:
-                self.__ControlledElm.SetParameter('State', 'IDLING')
-            self.__Pin = Pin
-        else:
-            self.__PinOld = Pin
-            self.__Pin = Pin
-            return 0
-
-        Error = abs(dPbatt)
-        return Error
-
-    def ConstantPowerFactorControl(self):
-        """ Implementation of a constant power factor algorithm. In all cases of reactive power support, active power
-        will be prioritized over reactive power.
-        """
-        PF = self.__Settings['pf']
-        self.__dssSolver.reSolve()
-        Pcalc = float(self.__ControlledElm.GetParameter('kw')) / self.__Prated
-
-        if Pcalc > 0:
-            Qcalc = float(self.__ControlledElm.GetParameter('kvar')) / self.__Prated
-
-            Scalc = (Pcalc ** 2 + Qcalc ** 2) ** (0.5)
-            if Scalc > 1:
-                Scaler = (1 - (Scalc - 1) / Scalc)
-                Pcalc = Pcalc * Scaler
-            if Pcalc > 0:
-                self.__ControlledElm.SetParameter('%Discharge', Pcalc * 100)
-            elif Pcalc < 0:
-                self.__ControlledElm.SetParameter('%charge', -Pcalc * 100)
-
-            self.__ControlledElm.SetParameter('pf', str(-PF))
-        else:
-            self.__ControlledElm.SetParameter('pf', str(1))
-
-        return 0
-
-    def VariablePowerFactorControl(self):
-        """ Implementation of a variable power factor algorithm. In all cases of reactive power support, active power
-            will be prioritized over reactive power.
-        """
-
-        pMin = self.__Settings['Pmin']
-        pMax = self.__Settings['Pmax']
-        pfMin = self.__Settings['pfMin']
-        pfMax = self.__Settings['pfMax']
-
-        self.__dssSolver.reSolve()
-        Pcalc = float(self.__ControlledElm.GetParameter('kw')) / self.__Prated
-
-        if Pcalc > 0:
-            if Pcalc < pMin:
-                PF = pfMax
-            elif Pcalc > pMax:
-                PF = pfMin
-            else:
-                m = (pfMax - pfMin) / (pMin - pMax)
-                c = (pfMin * pMin - pfMax * pMax) / (pMin - pMax)
-                PF = Pcalc * m + c
-        else:
-            PF = pfMax
-
-        Qcalc =  (Pcalc / PF) * math.sin(math.acos(PF))
-
-        Scalc = (Pcalc ** 2 + Qcalc ** 2) ** (0.5)
-        if Scalc > 1:
-            Scaler = (1 - (Scalc - 1) / Scalc)
-            Qcalc = Qcalc * Scaler
-            PF = math.cos(math.atan(Qcalc/Pcalc))
-
-        self.__ControlledElm.SetParameter('pf', str(-PF))
-        if Pcalc > 0:
-            self.__ControlledElm.SetParameter('%Discharge', Pcalc*100)
-        elif Pcalc < 0:
-            self.__ControlledElm.SetParameter('%charge', -Pcalc*100)
-        return 0
-
-    def VoltVarControl(self):
-        """ Implementation of a Volt / var algorithm. Enables the storage to stack multiple services. In all cases of
-        reactive power support, active power will be prioritized over reactive power.
-        """
-        uMin = self.__Settings['uMin']
-        uMax = self.__Settings['uMax']
-        uDbMin = self.__Settings['uDbMin']
-        uDbMax = self.__Settings['uDbMax']
-        QlimPU = self.__Settings['QlimPU']
-        PFlim = self.__Settings['PFlim']
-
-        uIn = max(self.__ControlledElm.sBus[0].GetVariable('puVmagAngle')[::2])
-
-        m1 = QlimPU / (uMin-uDbMin)
-        m2 = QlimPU / (uDbMax-uMax)
-        c1 = QlimPU * uDbMin / (uDbMin-uMin)
-        c2 = QlimPU * uDbMax / (uMax-uDbMax)
-
-        Ppv = float(self.__ControlledElm.GetParameter('kw'))
-        Pcalc = Ppv / self.__Srated
-        Qpv = sum(self.__ControlledElm.GetVariable('Powers')[1::2])
-        Qpv = Qpv / self.__Srated
-
-        Qcalc = 0
-        if uIn <= uMin:
-            Qcalc = QlimPU
-        elif uIn <= uDbMin and uIn > uMin:
-            Qcalc = uIn * m1 + c1
-        elif uIn <= uDbMax and uIn > uDbMin:
-            Qcalc = 0
-        elif uIn <= uMax and uIn > uDbMax:
-            Qcalc = uIn * m2 + c2
-        elif uIn >= uMax:
-            Qcalc = -QlimPU
-
-        # adding heavy ball term to improve convergence
-        Qcalc = Qpv + (Qcalc - Qpv) * 0.5 / self.__dampCoef + (Qpv - self.oldQcalc) * 0.1 / self.__dampCoef
-        Qlim = (1 - Pcalc ** 2) ** 0.5 if abs(Pcalc) < 1 else 0 # note - this is watt priority
-        if self.__Settings['Enable PF limit']:
-            Qlim = min(Qlim, abs(Pcalc * math.tan(math.acos(PFlim))))
-        if abs(Qcalc) > Qlim:
-            Qcalc = Qlim if Qcalc > 0 else -Qlim
-
-        dQ = abs(Qcalc - Qpv)
-        pct = min((Qcalc**2 + Pcalc**2) ** 0.5 * self.__Srated / self.__Prated * 100, 100)
-        pf = math.cos(math.atan(Qcalc / Pcalc)) if Pcalc != 0 else 1
-        pf = -pf if Qcalc * Pcalc < 0 else pf
-        if Pcalc > 0:
-            self.__ControlledElm.SetParameter('pf', pf)
-            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
-            self.__ControlledElm.SetParameter('%Discharge', str(pct))
-        elif Pcalc < 0:
-            self.__ControlledElm.SetParameter('pf', pf)
-            self.__ControlledElm.SetParameter('State', 'CHARGING')
-            self.__ControlledElm.SetParameter('%charge', str(pct))
-        else:
-            dQ = 0
-
-        Error = abs(dQ)
-        self.oldQcalc = Qcalc
-        return Error
+from  pydss.pyControllers.pyControllerAbstract import ControllerAbstract
+import calendar
+import math
+import ast
+
+class StorageController(ControllerAbstract):
+    """Numerous control implementation for a storage system from both behind-the-meter and front-of- meter applications. Subclass of the :class:`pydss.pyControllers.pyControllerAbstract.ControllerAbstract` abstract class.
+
+            :param StorageObj: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS Storage element
+            :type FaultObj: class:`pydss.dssElement.dssElement`
+            :param Settings: A dictionary that defines the settings for the PvController.
+            :type Settings: dict
+            :param dssInstance: An :class:`opendssdirect` instance
+            :type dssInstance: :class:`opendssdirect`
+            :param ElmObjectList: Dictionary of all dssElement, dssBus and dssCircuit objects
+            :type ElmObjectList: dict
+            :param dssSolver: An instance of one of the classed defined in :mod:`pydss.SolveMode`.
+            :type dssSolver: :mod:`pydss.SolveMode`
+            :raises: AssertionError if 'StorageObj' is not a wrapped OpenDSS Storage element
+
+    """
+    def __init__(self, StorageObj, Settings, dssInstance, ElmObjectList, dssSolver):
+        self.Time = -1
+        super(StorageController, self).__init__(StorageObj, Settings, dssInstance, ElmObjectList, dssSolver)
+
+        self.__ControlledElm = StorageObj
+        self.ceClass, self.ceName = self.__ControlledElm.GetInfo()
+        assert (self.ceClass.lower() == 'storage'), 'StorageController works only with an OpenDSS Storage element'
+        self.__Name = 'pyCont_' + self.ceClass + '_' + self.ceName
+
+        self.Time = (-1, 0)
+        self.__Pin = 0
+        self.Pbatt = 0
+        self.__PinOld = 0
+        self.PbattOld = 0
+        self.oldQcalc = 0
+        self.ExportOld = False
+        self.__ElmObjectList = ElmObjectList
+
+        self.ControlDict = {
+            'None'   : lambda: 0,
+            'PS'     : self.PeakShavingControl,
+            'CF'     : self.CapacityFirmimgControl,
+            'TT'     : self.TimeTriggeredControl,
+            'RT'     : self.RealTimeControl,
+            'SH'     : self.ScheduledControl,
+            'NETT'   : self.NonExportTimeTriggered,
+            'TOU'    : self.TimeOfUse,
+            'DemChg' : self.DemandCharge,
+            'CPF'    : self.ConstantPowerFactorControl,
+            'VPF'    : self.VariablePowerFactorControl,
+            'VVar'   : self.VoltVarControl,
+        }
+
+        self.__a = Settings['alpha']
+        self.__b = Settings['beta']
+        self.__ElmObjectList = ElmObjectList
+        self.__ControlledElm = StorageObj
+        self.__dssInstance = dssInstance
+        self.__dssSolver = dssSolver
+        self.__Settings = Settings
+
+        self.__Srated = float(StorageObj.GetParameter('kVA'))
+        self.__Prated = float(StorageObj.GetParameter('kWrated'))
+        self.__Pbatt = float(StorageObj.GetParameter('kW'))
+        self.__dampCoef = Settings['DampCoef']
+        self.update = [self.ControlDict[Settings['Control' + str(i)]] for i in [1, 2, 3]]
+        return
+
+    def Name(self):
+        return self.__Name
+
+    def ControlledElement(self):
+        return "{}.{}".format(self.ceClass, self.ceName)
+
+    def debugInfo(self):
+        return [self.__Settings['Control{}'.format(i+1)] for i in range(3)]
+
+    def Update(self, Priority, Time, Update):
+        self.TimeChange = self.Time != (Time, Priority)
+        self.Time = (Time, Priority)
+        return self.update[Priority]()
+
+    def SetSetting(self, Property, Value):
+        self.__Settings[Property] = Value
+        return
+
+    def GetSetting(self, Property):
+        return self.__Settings[Property]
+
+    def __parseRatePlan(self, tarrif):
+        self.touTarrif = ast.literal_eval(tarrif)
+        CurrDateTime = self.__dssSolver.GetDateTime()
+        DayOfYear = CurrDateTime.timetuple().tm_yday
+        weekno = CurrDateTime.weekday()
+        currentDay = calendar.day_name[weekno]
+
+        for period, touDetails in self.touTarrif.items():
+            if touDetails['ED'] < touDetails['SD']:
+                TOUday = [i for j in (range(1, touDetails['ED']), range(touDetails['SD'], 365)) for i in j]
+            else:
+                TOUday = range(touDetails['SD'], touDetails['ED'] + 1)
+            if DayOfYear in TOUday:
+                if currentDay in touDetails['TOW']:
+                    for st, et in zip(touDetails['ST'], touDetails['ET']):
+                        if et < st:
+                            TOUtime = [i for j in (range(1, et), range(st, 25)) for i in j]
+                        else:
+                            TOUtime = range(st, et)
+                        if CurrDateTime.hour in TOUtime:
+                            return True
+                else:
+                    pass
+        return False
+
+    def TimeOfUse(self):
+        """ Implementation of a time of use controller for behind the meter applications
+        """
+        dP = 0
+        Pub = self.__Settings['touLoadLim']
+        touCharge = self.__Settings['%touCharge']
+        tarrif = self.__Settings['touTarrifStructure']
+        isTOU = self.__parseRatePlan(tarrif)
+        Pbatt = float(self.__ControlledElm.GetParameter('kw'))
+        if self.__Settings['PowerMeaElem'] == 'Total':
+            Sin = self.__dssInstance.Circuit.TotalPower()
+            Pin = -sum(Sin[0:5:2])
+        else:
+            Sin = self.__ElmObjectList[self.__Settings['PowerMeaElem']].GetVariable('Powers')
+            Pin = sum(Sin[0:5:2])
+
+        if isTOU:
+            if Pin > Pub:
+                dP = Pin - Pub
+                Pbatt = Pbatt + dP * self.__a - (Pbatt - self.PbattOld) * self.__b
+            else:
+                Pbatt = 0
+        else:
+            Pbatt =  -touCharge * self.__Prated / 100
+
+        if Pbatt >= 0:
+            pctdischarge = Pbatt / (self.__Prated) * 100
+            pctdischarge = 100 if pctdischarge > 100 else pctdischarge
+            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
+            self.__ControlledElm.SetParameter('%Discharge', str(pctdischarge))
+        if Pbatt < 0:
+            pctcharge = -Pbatt / (self.__Prated) * 100
+            pctcharge = 100 if pctcharge > 100 else pctcharge
+            self.__ControlledElm.SetParameter('State', 'CHARGING')
+            self.__ControlledElm.SetParameter('%charge', str(pctcharge))
+
+        Error = abs(Pbatt - self.PbattOld)
+        self.PbattOld = Pbatt
+        self.dPold = dP
+        return Error
+
+
+    def DemandCharge(self):
+        """ Implementation of a demand charge controller for behind the meter applications
+        """
+
+        self.Demand = 0
+        dP = 0
+        DemandChgThreh = self.__Settings['DemandChgThreh[kWh]']
+        Pub = self.__Settings['touLoadLim']
+        touCharge = self.__Settings['%touCharge']
+        tarrif = self.__Settings['touTarrifStructure']
+        isTOU = self.__parseRatePlan(tarrif)
+        Pbatt = float(self.__ControlledElm.GetParameter('kw'))
+        if self.__Settings['PowerMeaElem'] == 'Total':
+            Sin = self.__dssInstance.Circuit.TotalPower()
+            Pin = -sum(Sin[0:5:2])
+        else:
+            Sin = self.__ElmObjectList[self.__Settings['PowerMeaElem']].GetVariable('Powers')
+            Pin = sum(Sin[0:5:2])
+        DateAndTime = self.__dssSolver.GetDateTime()
+        CurrMin = DateAndTime.minute
+        if isTOU:
+            i = CurrMin % 30
+            if i == 0:
+                self.__EnergyCounter = [0 for i in range(30)]
+            self.__EnergyCounter[i] = Pin
+            self.Demand = sum(self.__EnergyCounter) / (60 / self.__dssSolver.GetStepResolutionMinutes())
+
+            if self.Demand >= 0.9 * DemandChgThreh:
+                if Pin > Pub:
+                    dP = Pin - Pub
+                    Pbatt = Pbatt + (dP) * self.__a - (Pbatt - self.PbattOld) * self.__b
+                else:
+                    Pbatt = 0
+            else:
+                Pbatt = 0
+        else:
+            Pbatt = -touCharge * self.__Prated / 100
+
+        if Pbatt >= 0:
+            pctdischarge = Pbatt / (self.__Prated) * 100
+            pctdischarge =  100 if  pctdischarge > 100 else pctdischarge
+            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
+            self.__ControlledElm.SetParameter('%Discharge', str(pctdischarge))
+        if Pbatt < 0:
+            pctcharge = -Pbatt / (self.__Prated) * 100
+            pctcharge = 100 if pctcharge > 100 else pctcharge
+            self.__ControlledElm.SetParameter('State', 'CHARGING')
+            self.__ControlledElm.SetParameter('%charge', str(pctcharge))
+
+        Error = abs(Pbatt - self.PbattOld)
+        self.PbattOld = Pbatt
+        self.dPold = dP
+        return Error
+
+    def ScheduledControl(self):
+        """ Implementation of a fixed schedule controller. Used to implemented predefined dispatch signals
+        """
+        P_profile = self.__Settings['Schedule']
+        Days = self.__Settings['Days']
+        LenSchedule = len(P_profile)
+        TotalSeconds = 24*60*60*Days
+        TimeStepPerSample = int(TotalSeconds / LenSchedule)
+
+        CurrentTime = int(self.__dssInstance.Solution.Hour()) * 60 * 60 + \
+                      int(self.__dssInstance.Solution.Seconds())
+
+        Index = int(CurrentTime / TimeStepPerSample)
+        Pout = P_profile[Index]
+        if Pout > 0:
+            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
+            self.__ControlledElm.SetParameter('%Discharge', str(Pout * 100))
+        elif Pout < 0:
+            self.__ControlledElm.SetParameter('State', 'CHARGING')
+            self.__ControlledElm.SetParameter('%charge', str(-Pout * 100))
+        else:
+            self.__ControlledElm.SetParameter('State', 'IDLE')
+        return 0
+
+    def NonExportTimeTriggered(self):
+        """ Implementation of a smart non-export controller. Makes use of TOU window to optimize charging
+        """
+        # self.dPold = 0
+        dP = 0
+        Plb = self.__Settings['BaseLoadLim']
+        sTime = self.__Settings['ExpWindowStart']
+        eTime = self.__Settings['ExpWindowEnd']
+        KWHrated = float(self.__ControlledElm.GetParameter('kWhrated'))
+        perIdle = float(self.__ControlledElm.GetParameter('%IdlingkW'))
+        effDchg = float(self.__ControlledElm.GetParameter('%EffDischarge'))
+
+        Minutes = int(self.__dssInstance.Solution.Seconds() / 60)
+        Hour = self.__dssInstance.Solution.Hour() % 24
+
+        if sTime.hour < eTime.hour:
+            Twindow = ((eTime.hour * 60 + eTime.minute) - (sTime.hour * 60 + sTime.minute)) / 60
+            if (Hour * 60 + Minutes) > (sTime.hour * 60 + sTime.minute) and (Hour * 60 + Minutes) < (eTime.hour * 60 + eTime.minute):
+                Export = True
+            else:
+                Export = False
+        else:
+            Twindow = 24 - ((sTime.hour * 60 + sTime.minute) - (eTime.hour * 60 + eTime.minute)) / 60
+            if (Hour * 60 + Minutes) > (sTime.hour * 60 + sTime.minute) or \
+                    (Hour * 60 + Minutes) < (eTime.hour * 60 + eTime.minute):
+                Export = True
+            else:
+                Export = False
+
+        if self.ExportOld == False and Export == True:
+            perKWHstored = float(self.__ControlledElm.GetParameter('%stored'))
+            kWhrem = KWHrated * (perKWHstored / 100)
+            self.Pbatt = kWhrem / (Twindow) * effDchg / 100 - perIdle * self.__Prated / 100
+        self.ExportOld = Export
+
+        if Export:
+            pctcharge = self.Pbatt / (self.__Prated) * 100
+            rT = ((eTime.hour * 60 + eTime.minute) - (Hour * 60 + Minutes)) / 60
+            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
+            self.__ControlledElm.SetParameter('%Discharge', str(pctcharge))
+            Error = 0
+        else:
+            if self.__Settings['PowerMeaElem'] == 'Total':
+                Sin = self.__dssInstance.Circuit.TotalPower()
+                Pin = -sum(Sin[0:5:2])
+            else:
+                Sin = self.__ElmObjectList[self.__Settings['PowerMeaElem']].GetVariable('Powers')
+                Sin2 = Sin[:int(len(Sin) / 2)]
+                Pin = sum(Sin2[0::2])
+            Pbatt = float(self.__ControlledElm.GetParameter('kw'))
+            if Pin < Plb:
+                dP = Plb - Pin
+                Pbatt = Pbatt - (dP) * self.__a - (Pbatt - self.PbattOld) * self.__b
+            else:
+                Pbatt = 0
+
+            if Pbatt >= 0:
+                pctdischarge = Pbatt / (self.__Prated) * 100
+                self.__ControlledElm.SetParameter('State', 'DISCHARGING')
+                self.__ControlledElm.SetParameter('%Discharge', str(pctdischarge))
+            if Pbatt < 0:
+                pctcharge = -Pbatt / (self.__Prated) * 100
+                self.__ControlledElm.SetParameter('State', 'CHARGING')
+                self.__ControlledElm.SetParameter('%charge', str(pctcharge))
+
+            Error = abs(Pbatt - self.PbattOld)
+            self.PbattOld = Pbatt
+            self.dPold = dP
+        return Error
+
+    def PeakShavingControl(self):
+        """ Implementation of a peak shaving / base loading controller. Setting both peak shaving and base loading
+        limits to zero will make the storage work in "SELF CONSUMPTION" mode
+        """
+        Pub = self.__Settings['PS_ub']
+        Plb = self.__Settings['PS_lb']
+        IdlingkWPercent = float(self.__ControlledElm.GetParameter('%IdlingkW'))
+        IdlingkW = -IdlingkWPercent/100*self.__Prated
+        if self.__Settings['PowerMeaElem'] == 'Total':
+            Sin = self.__dssInstance.Circuit.TotalPower()
+            Pin = -sum(Sin[0:5:2])
+        else:
+            Sin = self.__ElmObjectList[self.__Settings['PowerMeaElem']].GetVariable('Powers')
+            Pin = sum(Sin[0:int(len(Sin)/2):2])
+
+        Pbatt = float(self.__ControlledElm.GetParameter('kw'))
+
+        if Pin > Pub:
+            dP = Pin - Pub
+            Pbatt = Pbatt + dP * self.__dampCoef
+        elif Pin < Plb:
+            dP = Pin - Plb
+            Pbatt = Pbatt + dP * self.__dampCoef
+        else:
+            Pbatt = Pbatt * self.__dampCoef
+
+        if Pbatt >= 0:
+            pctdischarge = min(100, Pbatt / self.__Prated* 100)
+            pctdischarge = 100 if pctdischarge > 100 else pctdischarge
+            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
+            self.__ControlledElm.SetParameter('%Discharge', str(pctdischarge))
+
+        elif Pbatt < 0:
+            pctcharge = min(100, -Pbatt / (self.__Prated)* 100)
+            pctcharge = 100 if pctcharge > 100 else pctcharge
+            self.__ControlledElm.SetParameter('State', 'CHARGING')
+            self.__ControlledElm.SetParameter('%charge', str(pctcharge))
+
+        Error = abs(Pbatt - self.PbattOld) / self.__Srated
+        self.PbattOld = Pbatt
+        return Error
+
+    def RealTimeControl(self):
+        kWOut = self.__Settings['%kWOut']
+        if kWOut > 0:
+            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
+            self.__ControlledElm.SetParameter('%Discharge', str(kWOut))
+        elif kWOut < 0:
+            self.__ControlledElm.SetParameter('State', 'CHARGING')
+            self.__ControlledElm.SetParameter('%charge', str(-kWOut))
+        else:
+            self.__ControlledElm.SetParameter('State', 'IDLE')
+        return 0
+
+    def TimeTriggeredControl(self):
+        HrCharge = self.__Settings['HrCharge']
+        HrDischarge = self.__Settings['HrDischarge']
+        rateCharge = self.__Settings['%rateCharge']
+        rateDischarge = self.__Settings['%rateDischarge']
+
+        HrC = int(HrCharge)
+        MnC = int((HrCharge - HrC) * 60)
+        HrD = int(HrDischarge)
+        MnD = int((HrDischarge - HrD) * 60)
+
+        Minutes = int(self.__dssInstance.Solution.Seconds()/60)
+        Hour = self.__dssInstance.Solution.Hour() % 24
+        if Hour == HrC and Minutes == MnC:
+            self.__ControlledElm.SetParameter('State', 'CHARGING')
+            self.__ControlledElm.SetParameter('%charge', str(rateCharge))
+        elif Hour == HrD and Minutes == MnD:
+            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
+            self.__ControlledElm.SetParameter('%Discharge', str(rateDischarge))
+        return 0
+
+    def CapacityFirmimgControl(self):
+        """ Implementation of a capacity firming algorithm
+        """
+        dPub = self.__Settings['CF_dP_ub']
+        dPlb = self.__Settings['CF_dP_lb']
+
+        if self.Time[0] == 0:
+            self.__PinOld = self.__Pin
+
+        if self.__Settings['PowerMeaElem'] == 'Total':
+            Sin = self.__dssInstance.Circuit.TotalPower()
+            Pin = -sum(Sin[0:5:2])
+        else:
+            Sin = self.__ElmObjectList[self.__Settings['PowerMeaElem']].GetVariable('Powers')
+            Pin = sum(Sin[0:5:2])
+        Pbatt = -float(self.__ControlledElm.GetVariable('Powers')[0]) * 3
+        ramp = (Pin - self.__PinOld)
+        if self.Time[0] > 1:
+            if ramp >= dPub:
+                dPbatt = self.__dampCoef * (ramp - dPub)
+                Pbatt += dPbatt
+            elif ramp <= dPlb:
+                dPbatt = self.__dampCoef * (ramp - dPlb)
+                Pbatt += dPbatt
+            else:
+                dPbatt = 0
+                Pbatt = 0
+            if Pbatt > 0:
+                pctdischarge = Pbatt / self.__Prated * 100
+                self.__ControlledElm.SetParameter('State', 'DISCHARGING')
+                self.__ControlledElm.SetParameter('%Discharge', str(pctdischarge))
+            elif Pbatt < 0:
+                pctcharge = -Pbatt / self.__Prated * 100
+                self.__ControlledElm.SetParameter('State', 'CHARGING')
+                self.__ControlledElm.SetParameter('%charge', str(pctcharge))
+            elif Pbatt == 0:
+                self.__ControlledElm.SetParameter('State', 'IDLING')
+            self.__Pin = Pin
+        else:
+            self.__PinOld = Pin
+            self.__Pin = Pin
+            return 0
+
+        Error = abs(dPbatt)
+        return Error
+
+    def ConstantPowerFactorControl(self):
+        """ Implementation of a constant power factor algorithm. In all cases of reactive power support, active power
+        will be prioritized over reactive power.
+        """
+        PF = self.__Settings['pf']
+        self.__dssSolver.reSolve()
+        Pcalc = float(self.__ControlledElm.GetParameter('kw')) / self.__Prated
+
+        if Pcalc > 0:
+            Qcalc = float(self.__ControlledElm.GetParameter('kvar')) / self.__Prated
+
+            Scalc = (Pcalc ** 2 + Qcalc ** 2) ** (0.5)
+            if Scalc > 1:
+                Scaler = (1 - (Scalc - 1) / Scalc)
+                Pcalc = Pcalc * Scaler
+            if Pcalc > 0:
+                self.__ControlledElm.SetParameter('%Discharge', Pcalc * 100)
+            elif Pcalc < 0:
+                self.__ControlledElm.SetParameter('%charge', -Pcalc * 100)
+
+            self.__ControlledElm.SetParameter('pf', str(-PF))
+        else:
+            self.__ControlledElm.SetParameter('pf', str(1))
+
+        return 0
+
+    def VariablePowerFactorControl(self):
+        """ Implementation of a variable power factor algorithm. In all cases of reactive power support, active power
+            will be prioritized over reactive power.
+        """
+
+        pMin = self.__Settings['Pmin']
+        pMax = self.__Settings['Pmax']
+        pfMin = self.__Settings['pfMin']
+        pfMax = self.__Settings['pfMax']
+
+        self.__dssSolver.reSolve()
+        Pcalc = float(self.__ControlledElm.GetParameter('kw')) / self.__Prated
+
+        if Pcalc > 0:
+            if Pcalc < pMin:
+                PF = pfMax
+            elif Pcalc > pMax:
+                PF = pfMin
+            else:
+                m = (pfMax - pfMin) / (pMin - pMax)
+                c = (pfMin * pMin - pfMax * pMax) / (pMin - pMax)
+                PF = Pcalc * m + c
+        else:
+            PF = pfMax
+
+        Qcalc =  (Pcalc / PF) * math.sin(math.acos(PF))
+
+        Scalc = (Pcalc ** 2 + Qcalc ** 2) ** (0.5)
+        if Scalc > 1:
+            Scaler = (1 - (Scalc - 1) / Scalc)
+            Qcalc = Qcalc * Scaler
+            PF = math.cos(math.atan(Qcalc/Pcalc))
+
+        self.__ControlledElm.SetParameter('pf', str(-PF))
+        if Pcalc > 0:
+            self.__ControlledElm.SetParameter('%Discharge', Pcalc*100)
+        elif Pcalc < 0:
+            self.__ControlledElm.SetParameter('%charge', -Pcalc*100)
+        return 0
+
+    def VoltVarControl(self):
+        """ Implementation of a Volt / var algorithm. Enables the storage to stack multiple services. In all cases of
+        reactive power support, active power will be prioritized over reactive power.
+        """
+        uMin = self.__Settings['uMin']
+        uMax = self.__Settings['uMax']
+        uDbMin = self.__Settings['uDbMin']
+        uDbMax = self.__Settings['uDbMax']
+        QlimPU = self.__Settings['QlimPU']
+        PFlim = self.__Settings['PFlim']
+
+        uIn = max(self.__ControlledElm.sBus[0].GetVariable('puVmagAngle')[::2])
+
+        m1 = QlimPU / (uMin-uDbMin)
+        m2 = QlimPU / (uDbMax-uMax)
+        c1 = QlimPU * uDbMin / (uDbMin-uMin)
+        c2 = QlimPU * uDbMax / (uMax-uDbMax)
+
+        Ppv = float(self.__ControlledElm.GetParameter('kw'))
+        Pcalc = Ppv / self.__Srated
+        Qpv = sum(self.__ControlledElm.GetVariable('Powers')[1::2])
+        Qpv = Qpv / self.__Srated
+
+        Qcalc = 0
+        if uIn <= uMin:
+            Qcalc = QlimPU
+        elif uIn <= uDbMin and uIn > uMin:
+            Qcalc = uIn * m1 + c1
+        elif uIn <= uDbMax and uIn > uDbMin:
+            Qcalc = 0
+        elif uIn <= uMax and uIn > uDbMax:
+            Qcalc = uIn * m2 + c2
+        elif uIn >= uMax:
+            Qcalc = -QlimPU
+
+        # adding heavy ball term to improve convergence
+        Qcalc = Qpv + (Qcalc - Qpv) * 0.5 / self.__dampCoef + (Qpv - self.oldQcalc) * 0.1 / self.__dampCoef
+        Qlim = (1 - Pcalc ** 2) ** 0.5 if abs(Pcalc) < 1 else 0 # note - this is watt priority
+        if self.__Settings['Enable PF limit']:
+            Qlim = min(Qlim, abs(Pcalc * math.tan(math.acos(PFlim))))
+        if abs(Qcalc) > Qlim:
+            Qcalc = Qlim if Qcalc > 0 else -Qlim
+
+        dQ = abs(Qcalc - Qpv)
+        pct = min((Qcalc**2 + Pcalc**2) ** 0.5 * self.__Srated / self.__Prated * 100, 100)
+        pf = math.cos(math.atan(Qcalc / Pcalc)) if Pcalc != 0 else 1
+        pf = -pf if Qcalc * Pcalc < 0 else pf
+        if Pcalc > 0:
+            self.__ControlledElm.SetParameter('pf', pf)
+            self.__ControlledElm.SetParameter('State', 'DISCHARGING')
+            self.__ControlledElm.SetParameter('%Discharge', str(pct))
+        elif Pcalc < 0:
+            self.__ControlledElm.SetParameter('pf', pf)
+            self.__ControlledElm.SetParameter('State', 'CHARGING')
+            self.__ControlledElm.SetParameter('%charge', str(pct))
+        else:
+            dQ = 0
+
+        Error = abs(dQ)
+        self.oldQcalc = Qcalc
+        return Error
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/Controllers/Settings/PvControllers.toml` & `nrel_pydss-3.1.4/src/pydss/pyControllers/Controllers/Settings/PvControllers.toml`

 * *Files 27% similar despite different names*

```diff
@@ -1,213 +1,203 @@
-00000000: 5b4e 4f5f 5652 545d 0d0a 2243 6174 6567  [NO_VRT].."Categ
-00000010: 6f72 7922 203d 2022 7465 7374 220d 0a22  ory" = "test".."
-00000020: 6b56 4122 203d 2034 2e30 0d0a 226d 6178  kVA" = 4.0.."max
-00000030: 4b57 2220 3d20 342e 300d 0a22 4b76 6172  KW" = 4.0.."Kvar
-00000040: 4c69 6d69 7422 093d 2031 2e37 360d 0a22  Limit".= 1.76.."
-00000050: 2550 4375 7469 6e22 203d 2031 302e 300d  %PCutin" = 10.0.
-00000060: 0a22 2550 4375 746f 7574 2220 3d20 3130  ."%PCutout" = 10
-00000070: 2e30 0d0a 2255 6361 6c63 4d6f 6465 2220  .0.."UcalcMode" 
-00000080: 3d20 224d 6178 220d 0a22 5072 696f 7269  = "Max".."Priori
-00000090: 7479 2220 3d20 2245 7175 616c 220d 0a22  ty" = "Equal".."
-000000a0: 456e 6162 6c65 2050 4620 6c69 6d69 7422  Enable PF limit"
-000000b0: 203d 2066 616c 7365 0d0a 2270 664d 696e   = false.."pfMin
-000000c0: 2209 3d20 302e 3935 0d0a 2246 6f6c 6c6f  ".= 0.95.."Follo
-000000d0: 7720 7374 616e 6461 7264 2220 3d20 2231  w standard" = "1
-000000e0: 3534 372d 3230 3033 220d 0a22 5269 6465  547-2003".."Ride
-000000f0: 2d74 6872 6f75 6768 2043 6174 6567 6f72  -through Categor
-00000100: 7922 093d 2022 4361 7465 676f 7279 2049  y".= "Category I
-00000110: 220d 0a22 4f56 3220 2d20 702e 752e 2220  ".."OV2 - p.u." 
-00000120: 3d20 312e 320d 0a22 4f56 3220 4354 202d  = 1.2.."OV2 CT -
-00000130: 2073 6563 2220 3d20 302e 3136 0d0a 224f   sec" = 0.16.."O
-00000140: 5631 202d 2070 2e75 2e22 203d 2031 2e32  V1 - p.u." = 1.2
-00000150: 0d0a 224f 5631 2043 5420 2d20 7365 6322  .."OV1 CT - sec"
-00000160: 203d 2032 2e30 0d0a 2255 5631 202d 2070   = 2.0.."UV1 - p
-00000170: 2e75 2e22 203d 2030 2e38 0d0a 2255 5631  .u." = 0.8.."UV1
-00000180: 2043 5420 2d20 7365 6322 203d 2032 2e30   CT - sec" = 2.0
-00000190: 0d0a 2255 5632 202d 2070 2e75 2e22 203d  .."UV2 - p.u." =
-000001a0: 2030 2e35 0d0a 2255 5632 2043 5420 2d20   0.5.."UV2 CT - 
-000001b0: 7365 6322 203d 2030 2e31 360d 0a22 5265  sec" = 0.16.."Re
-000001c0: 636f 6e6e 6563 7420 6465 6164 7469 6d65  connect deadtime
-000001d0: 202d 2073 6563 2220 3d20 3330 3030 2e30   - sec" = 3000.0
-000001e0: 0d0a 2252 6563 6f6e 6e65 6374 2050 6d61  .."Reconnect Pma
-000001f0: 7820 7469 6d65 202d 2073 6563 2220 3d20  x time - sec" = 
-00000200: 3330 302e 300d 0a22 5065 726d 6973 7369  300.0.."Permissi
-00000210: 7665 206f 7065 7261 7469 6f6e 2220 3d20  ve operation" = 
-00000220: 2243 7572 7265 6e74 206c 696d 6974 6564  "Current limited
-00000230: 220d 0a22 4d61 7920 7472 6970 206f 7065  ".."May trip ope
-00000240: 7261 7469 6f6e 2220 3d20 2254 7269 7022  ration" = "Trip"
-00000250: 0d0a 224d 756c 7469 706c 6520 6469 7374  .."Multiple dist
-00000260: 7572 6261 6e63 6573 2220 3d20 2254 7269  urbances" = "Tri
-00000270: 7022 0d0a 0d0a 5b31 3534 375f 4341 545f  p"....[1547_CAT_
-00000280: 495d 0d0a 2243 6174 6567 6f72 7922 203d  I].."Category" =
-00000290: 2022 7465 7374 220d 0a22 6b56 4122 203d   "test".."kVA" =
-000002a0: 2034 2e30 0d0a 226d 6178 4b57 2220 3d20   4.0.."maxKW" = 
-000002b0: 342e 300d 0a22 4b76 6172 4c69 6d69 7422  4.0.."KvarLimit"
-000002c0: 093d 2031 2e37 360d 0a22 2550 4375 7469  .= 1.76.."%PCuti
-000002d0: 6e22 203d 2030 2e30 0d0a 2225 5043 7574  n" = 0.0.."%PCut
-000002e0: 6f75 7422 203d 2030 2e30 0d0a 2255 6361  out" = 0.0.."Uca
-000002f0: 6c63 4d6f 6465 2220 3d20 224d 6178 220d  lcMode" = "Max".
-00000300: 0a22 5072 696f 7269 7479 2220 3d20 2245  ."Priority" = "E
-00000310: 7175 616c 220d 0a22 456e 6162 6c65 2050  qual".."Enable P
-00000320: 4620 6c69 6d69 7422 203d 2066 616c 7365  F limit" = false
-00000330: 0d0a 2270 664d 696e 2209 3d20 302e 3935  .."pfMin".= 0.95
-00000340: 0d0a 2246 6f6c 6c6f 7720 7374 616e 6461  .."Follow standa
-00000350: 7264 2220 3d20 2231 3534 372d 3230 3138  rd" = "1547-2018
-00000360: 220d 0a22 5269 6465 2d74 6872 6f75 6768  ".."Ride-through
-00000370: 2043 6174 6567 6f72 7922 093d 2022 4361   Category".= "Ca
-00000380: 7465 676f 7279 2049 220d 0a22 4f56 3220  tegory I".."OV2 
-00000390: 2d20 702e 752e 2220 3d20 312e 320d 0a22  - p.u." = 1.2.."
-000003a0: 4f56 3220 4354 202d 2073 6563 2220 3d20  OV2 CT - sec" = 
-000003b0: 302e 3136 0d0a 224f 5631 202d 2070 2e75  0.16.."OV1 - p.u
-000003c0: 2e22 203d 2031 2e31 0d0a 224f 5631 2043  ." = 1.1.."OV1 C
-000003d0: 5420 2d20 7365 6322 203d 2032 2e30 0d0a  T - sec" = 2.0..
-000003e0: 2255 5631 202d 2070 2e75 2e22 203d 2030  "UV1 - p.u." = 0
-000003f0: 2e37 0d0a 2255 5631 2043 5420 2d20 7365  .7.."UV1 CT - se
-00000400: 6322 203d 2032 2e30 0d0a 2255 5632 202d  c" = 2.0.."UV2 -
-00000410: 2070 2e75 2e22 203d 2030 2e34 350d 0a22   p.u." = 0.45.."
-00000420: 5556 3220 4354 202d 2073 6563 2220 3d20  UV2 CT - sec" = 
-00000430: 302e 3136 0d0a 2252 6563 6f6e 6e65 6374  0.16.."Reconnect
-00000440: 2064 6561 6474 696d 6520 2d20 7365 6322   deadtime - sec"
-00000450: 203d 2033 3030 302e 300d 0a22 5265 636f   = 3000.0.."Reco
-00000460: 6e6e 6563 7420 506d 6178 2074 696d 6520  nnect Pmax time 
-00000470: 2d20 7365 6322 203d 2033 3030 2e30 0d0a  - sec" = 300.0..
-00000480: 2250 6572 6d69 7373 6976 6520 6f70 6572  "Permissive oper
-00000490: 6174 696f 6e22 203d 2022 4375 7272 656e  ation" = "Curren
-000004a0: 7420 6c69 6d69 7465 6422 0d0a 224d 6179  t limited".."May
-000004b0: 2074 7269 7020 6f70 6572 6174 696f 6e22   trip operation"
-000004c0: 203d 2022 5472 6970 220d 0a22 4d75 6c74   = "Trip".."Mult
-000004d0: 6970 6c65 2064 6973 7475 7262 616e 6365  iple disturbance
-000004e0: 7322 203d 2022 5472 6970 220d 0a0d 0a0d  s" = "Trip".....
-000004f0: 0a5b 3135 3437 5f43 4154 5f49 495d 0d0a  .[1547_CAT_II]..
-00000500: 2243 6174 6567 6f72 7922 203d 2022 7465  "Category" = "te
-00000510: 7374 220d 0a22 6b56 4122 203d 2034 2e30  st".."kVA" = 4.0
-00000520: 0d0a 226d 6178 4b57 2220 3d20 342e 300d  .."maxKW" = 4.0.
-00000530: 0a22 4b76 6172 4c69 6d69 7422 093d 2031  ."KvarLimit".= 1
-00000540: 2e37 360d 0a22 2550 4375 7469 6e22 203d  .76.."%PCutin" =
-00000550: 2030 2e30 0d0a 2225 5043 7574 6f75 7422   0.0.."%PCutout"
-00000560: 203d 2030 2e30 0d0a 2255 6361 6c63 4d6f   = 0.0.."UcalcMo
-00000570: 6465 2220 3d20 224d 6178 220d 0a22 5072  de" = "Max".."Pr
-00000580: 696f 7269 7479 2220 3d20 2245 7175 616c  iority" = "Equal
-00000590: 220d 0a22 456e 6162 6c65 2050 4620 6c69  ".."Enable PF li
-000005a0: 6d69 7422 203d 2066 616c 7365 0d0a 2270  mit" = false.."p
-000005b0: 664d 696e 2209 3d20 302e 3935 0d0a 2246  fMin".= 0.95.."F
-000005c0: 6f6c 6c6f 7720 7374 616e 6461 7264 2220  ollow standard" 
-000005d0: 3d20 2231 3534 372d 3230 3138 220d 0a22  = "1547-2018".."
-000005e0: 5269 6465 2d74 6872 6f75 6768 2043 6174  Ride-through Cat
-000005f0: 6567 6f72 7922 093d 2022 4361 7465 676f  egory".= "Catego
-00000600: 7279 2049 4922 0d0a 224f 5632 202d 2070  ry II".."OV2 - p
-00000610: 2e75 2e22 203d 2031 2e32 0d0a 224f 5632  .u." = 1.2.."OV2
-00000620: 2043 5420 2d20 7365 6322 203d 2030 2e31   CT - sec" = 0.1
-00000630: 360d 0a22 4f56 3120 2d20 702e 752e 2220  6.."OV1 - p.u." 
-00000640: 3d20 312e 310d 0a22 4f56 3120 4354 202d  = 1.1.."OV1 CT -
-00000650: 2073 6563 2220 3d20 322e 300d 0a22 5556   sec" = 2.0.."UV
-00000660: 3120 2d20 702e 752e 2220 3d20 302e 370d  1 - p.u." = 0.7.
-00000670: 0a22 5556 3120 4354 202d 2073 6563 2220  ."UV1 CT - sec" 
-00000680: 3d20 3130 2e30 0d0a 2255 5632 202d 2070  = 10.0.."UV2 - p
-00000690: 2e75 2e22 203d 2030 2e34 350d 0a22 5556  .u." = 0.45.."UV
-000006a0: 3220 4354 202d 2073 6563 2220 3d20 302e  2 CT - sec" = 0.
-000006b0: 3136 0d0a 2252 6563 6f6e 6e65 6374 2064  16.."Reconnect d
-000006c0: 6561 6474 696d 6520 2d20 7365 6322 203d  eadtime - sec" =
-000006d0: 2033 3030 302e 300d 0a22 5265 636f 6e6e   3000.0.."Reconn
-000006e0: 6563 7420 506d 6178 2074 696d 6520 2d20  ect Pmax time - 
-000006f0: 7365 6322 203d 2033 3030 2e30 0d0a 2250  sec" = 300.0.."P
-00000700: 6572 6d69 7373 6976 6520 6f70 6572 6174  ermissive operat
-00000710: 696f 6e22 203d 2022 4375 7272 656e 7420  ion" = "Current 
-00000720: 6c69 6d69 7465 6422 0d0a 224d 6179 2074  limited".."May t
-00000730: 7269 7020 6f70 6572 6174 696f 6e22 203d  rip operation" =
-00000740: 2022 5472 6970 220d 0a22 4d75 6c74 6970   "Trip".."Multip
-00000750: 6c65 2064 6973 7475 7262 616e 6365 7322  le disturbances"
-00000760: 203d 2022 5472 6970 220d 0a0d 0a0d 0a5b   = "Trip"......[
-00000770: 3135 3437 5f43 4154 5f49 4949 5d0d 0a22  1547_CAT_III].."
-00000780: 4361 7465 676f 7279 2220 3d20 2274 6573  Category" = "tes
-00000790: 7422 0d0a 226b 5641 2220 3d20 342e 300d  t".."kVA" = 4.0.
-000007a0: 0a22 6d61 784b 5722 203d 2034 2e30 0d0a  ."maxKW" = 4.0..
-000007b0: 224b 7661 724c 696d 6974 2209 3d20 312e  "KvarLimit".= 1.
-000007c0: 3736 0d0a 2225 5043 7574 696e 2220 3d20  76.."%PCutin" = 
-000007d0: 300d 0a22 2550 4375 746f 7574 2220 3d20  0.."%PCutout" = 
-000007e0: 300d 0a22 5563 616c 634d 6f64 6522 203d  0.."UcalcMode" =
-000007f0: 2022 4d61 7822 0d0a 2250 7269 6f72 6974   "Max".."Priorit
-00000800: 7922 203d 2022 4571 7561 6c22 0d0a 2245  y" = "Equal".."E
-00000810: 6e61 626c 6520 5046 206c 696d 6974 2220  nable PF limit" 
-00000820: 3d20 6661 6c73 650d 0a22 7066 4d69 6e22  = false.."pfMin"
-00000830: 093d 2030 2e39 350d 0a22 466f 6c6c 6f77  .= 0.95.."Follow
-00000840: 2073 7461 6e64 6172 6422 203d 2022 3135   standard" = "15
-00000850: 3437 2d32 3031 3822 0d0a 2252 6964 652d  47-2018".."Ride-
-00000860: 7468 726f 7567 6820 4361 7465 676f 7279  through Category
-00000870: 2209 3d20 2243 6174 6567 6f72 7920 4949  ".= "Category II
-00000880: 4922 0d0a 224f 5632 202d 2070 2e75 2e22  I".."OV2 - p.u."
-00000890: 203d 2031 2e32 0d0a 224f 5632 2043 5420   = 1.2.."OV2 CT 
-000008a0: 2d20 7365 6322 203d 2030 2e31 360d 0a22  - sec" = 0.16.."
-000008b0: 4f56 3120 2d20 702e 752e 2220 3d20 312e  OV1 - p.u." = 1.
-000008c0: 310d 0a22 4f56 3120 4354 202d 2073 6563  1.."OV1 CT - sec
-000008d0: 2220 3d20 3133 2e30 0d0a 2255 5631 202d  " = 13.0.."UV1 -
-000008e0: 2070 2e75 2e22 203d 2030 2e38 380d 0a22   p.u." = 0.88.."
-000008f0: 5556 3120 4354 202d 2073 6563 2220 3d20  UV1 CT - sec" = 
-00000900: 3231 2e30 0d0a 2255 5632 202d 2070 2e75  21.0.."UV2 - p.u
-00000910: 2e22 203d 2030 2e35 0d0a 2255 5632 2043  ." = 0.5.."UV2 C
-00000920: 5420 2d20 7365 6322 203d 2032 2e30 0d0a  T - sec" = 2.0..
-00000930: 2252 6563 6f6e 6e65 6374 2064 6561 6474  "Reconnect deadt
-00000940: 696d 6520 2d20 7365 6322 203d 2033 3030  ime - sec" = 300
-00000950: 302e 300d 0a22 5265 636f 6e6e 6563 7420  0.0.."Reconnect 
-00000960: 506d 6178 2074 696d 6520 2d20 7365 6322  Pmax time - sec"
-00000970: 203d 2033 3030 2e30 0d0a 2250 6572 6d69   = 300.0.."Permi
-00000980: 7373 6976 6520 6f70 6572 6174 696f 6e22  ssive operation"
-00000990: 203d 2022 4375 7272 656e 7420 6c69 6d69   = "Current limi
-000009a0: 7465 6422 0d0a 224d 6179 2074 7269 7020  ted".."May trip 
-000009b0: 6f70 6572 6174 696f 6e22 203d 2022 5472  operation" = "Tr
-000009c0: 6970 220d 0a22 4d75 6c74 6970 6c65 2064  ip".."Multiple d
-000009d0: 6973 7475 7262 616e 6365 7322 203d 2022  isturbances" = "
-000009e0: 5472 6970 220d 0a0d 0a5b 6370 665d 0d0a  Trip"....[cpf]..
-000009f0: 4361 7465 676f 7279 203d 2022 4c65 6761  Category = "Lega
-00000a00: 6379 220d 0a43 6f6e 7472 6f6c 3120 3d20  cy"..Control1 = 
-00000a10: 224e 6f6e 6522 0d0a 436f 6e74 726f 6c32  "None"..Control2
-00000a20: 203d 2022 4e6f 6e65 220d 0a43 6f6e 7472   = "None"..Contr
-00000a30: 6f6c 3320 3d20 224e 6f6e 6522 0d0a 7066  ol3 = "None"..pf
-00000a40: 203d 2022 4e6f 6e65 220d 0a70 664d 696e   = "None"..pfMin
-00000a50: 203d 2030 2e38 0d0a 7066 4d61 7820 3d20   = 0.8..pfMax = 
-00000a60: 310d 0a50 6d69 6e20 3d20 300d 0a50 6d61  1..Pmin = 0..Pma
-00000a70: 7820 3d20 310d 0a75 4d69 6e20 3d20 302e  x = 1..uMin = 0.
-00000a80: 3932 0d0a 7544 624d 696e 203d 2030 2e39  92..uDbMin = 0.9
-00000a90: 3637 0d0a 7544 624d 6178 203d 2031 2e30  67..uDbMax = 1.0
-00000aa0: 3333 0d0a 754d 6178 203d 2031 2e30 3620  33..uMax = 1.06 
-00000ab0: 2023 206d 7573 7420 6265 206c 6573 7320   # must be less 
-00000ac0: 7468 616e 206f 7220 6571 7561 6c20 746f  than or equal to
-00000ad0: 2075 4d69 6e43 0d0a 516c 696d 5055 203d   uMinC..QlimPU =
-00000ae0: 2030 2e34 340d 0a50 466c 696d 203d 2031   0.44..PFlim = 1
-00000af0: 0d0a 2245 6e61 626c 6520 5046 206c 696d  .."Enable PF lim
-00000b00: 6974 2220 3d20 6661 6c73 650d 0a75 4d69  it" = false..uMi
-00000b10: 6e43 203d 2031 2e30 360d 0a75 4d61 7843  nC = 1.06..uMaxC
-00000b20: 203d 2031 2e31 0d0a 506d 696e 5657 203d   = 1.1..PminVW =
-00000b30: 2031 300d 0a56 5774 7970 6520 3d20 2241   10..VWtype = "A
-00000b40: 7661 696c 6162 6c65 2050 6f77 6572 220d  vailable Power".
-00000b50: 0a22 2555 4375 746f 6666 2220 3d20 312e  ."%UCutoff" = 1.
-00000b60: 310d 0a22 2550 4375 7469 6e22 203d 2031  1.."%PCutin" = 1
-00000b70: 300d 0a22 2550 4375 746f 7574 2220 3d20  0.."%PCutout" = 
-00000b80: 3130 0d0a 4566 6669 6369 656e 6379 203d  10..Efficiency =
-00000b90: 2031 3030 0d0a 5072 696f 7269 7479 203d   100..Priority =
-00000ba0: 2022 5661 7222 0d0a 4461 6d70 436f 6566   "Var"..DampCoef
-00000bb0: 203d 2030 2e38 0d0a 0d0a 5b76 6f6c 742d   = 0.8....[volt-
-00000bc0: 7661 725d 0d0a 436f 6e74 726f 6c31 203d  var]..Control1 =
-00000bd0: 2022 5656 6172 220d 0a43 6f6e 7472 6f6c   "VVar"..Control
-00000be0: 3220 3d20 224e 6f6e 6522 0d0a 436f 6e74  2 = "None"..Cont
-00000bf0: 726f 6c33 203d 2022 4e6f 6e65 220d 0a70  rol3 = "None"..p
-00000c00: 6620 3d20 310d 0a70 664d 696e 203d 2030  f = 1..pfMin = 0
-00000c10: 2e38 0d0a 7066 4d61 7820 3d20 310d 0a50  .8..pfMax = 1..P
-00000c20: 6d69 6e20 3d20 300d 0a50 6d61 7820 3d20  min = 0..Pmax = 
-00000c30: 310d 0a75 4d69 6e20 3d20 302e 3933 3939  1..uMin = 0.9399
-00000c40: 3939 3939 3939 3939 3939 3939 0d0a 7544  999999999999..uD
-00000c50: 624d 696e 203d 2030 2e39 370d 0a75 4462  bMin = 0.97..uDb
-00000c60: 4d61 7820 3d20 312e 3033 0d0a 754d 6178  Max = 1.03..uMax
-00000c70: 203d 2031 2e30 360d 0a51 6c69 6d50 5520   = 1.06..QlimPU 
-00000c80: 3d20 302e 3434 0d0a 5046 6c69 6d20 3d20  = 0.44..PFlim = 
-00000c90: 302e 390d 0a22 456e 6162 6c65 2050 4620  0.9.."Enable PF 
-00000ca0: 6c69 6d69 7422 203d 2066 616c 7365 0d0a  limit" = false..
-00000cb0: 754d 696e 4320 3d20 312e 3036 0d0a 754d  uMinC = 1.06..uM
-00000cc0: 6178 4320 3d20 312e 310d 0a50 6d69 6e56  axC = 1.1..PminV
-00000cd0: 5720 3d20 3130 0d0a 5657 7479 7065 203d  W = 10..VWtype =
-00000ce0: 2022 5261 7465 6420 506f 7765 7222 0d0a   "Rated Power"..
-00000cf0: 2225 5043 7574 696e 2220 3d20 3130 0d0a  "%PCutin" = 10..
-00000d00: 2225 5043 7574 6f75 7422 203d 2031 300d  "%PCutout" = 10.
-00000d10: 0a45 6666 6963 6965 6e63 7920 3d20 3130  .Efficiency = 10
-00000d20: 300d 0a50 7269 6f72 6974 7920 3d20 2256  0..Priority = "V
-00000d30: 6172 220d 0a44 616d 7043 6f65 6620 3d20  ar"..DampCoef = 
-00000d40: 302e 380d 0a                             0.8..
+00000000: 5b4e 4f5f 5652 545d 0a22 4361 7465 676f  [NO_VRT]."Catego
+00000010: 7279 2220 3d20 2274 6573 7422 0a22 6b56  ry" = "test"."kV
+00000020: 4122 203d 2034 2e30 0a22 6d61 784b 5722  A" = 4.0."maxKW"
+00000030: 203d 2034 2e30 0a22 4b76 6172 4c69 6d69   = 4.0."KvarLimi
+00000040: 7422 093d 2031 2e37 360a 2225 5043 7574  t".= 1.76."%PCut
+00000050: 696e 2220 3d20 3130 2e30 0a22 2550 4375  in" = 10.0."%PCu
+00000060: 746f 7574 2220 3d20 3130 2e30 0a22 5563  tout" = 10.0."Uc
+00000070: 616c 634d 6f64 6522 203d 2022 4d61 7822  alcMode" = "Max"
+00000080: 0a22 5072 696f 7269 7479 2220 3d20 2245  ."Priority" = "E
+00000090: 7175 616c 220a 2245 6e61 626c 6520 5046  qual"."Enable PF
+000000a0: 206c 696d 6974 2220 3d20 6661 6c73 650a   limit" = false.
+000000b0: 2270 664d 696e 2209 3d20 302e 3935 0a22  "pfMin".= 0.95."
+000000c0: 466f 6c6c 6f77 2073 7461 6e64 6172 6422  Follow standard"
+000000d0: 203d 2022 3135 3437 2d32 3030 3322 0a22   = "1547-2003"."
+000000e0: 5269 6465 2d74 6872 6f75 6768 2043 6174  Ride-through Cat
+000000f0: 6567 6f72 7922 093d 2022 4361 7465 676f  egory".= "Catego
+00000100: 7279 2049 220a 224f 5632 202d 2070 2e75  ry I"."OV2 - p.u
+00000110: 2e22 203d 2031 2e32 0a22 4f56 3220 4354  ." = 1.2."OV2 CT
+00000120: 202d 2073 6563 2220 3d20 302e 3136 0a22   - sec" = 0.16."
+00000130: 4f56 3120 2d20 702e 752e 2220 3d20 312e  OV1 - p.u." = 1.
+00000140: 320a 224f 5631 2043 5420 2d20 7365 6322  2."OV1 CT - sec"
+00000150: 203d 2032 2e30 0a22 5556 3120 2d20 702e   = 2.0."UV1 - p.
+00000160: 752e 2220 3d20 302e 380a 2255 5631 2043  u." = 0.8."UV1 C
+00000170: 5420 2d20 7365 6322 203d 2032 2e30 0a22  T - sec" = 2.0."
+00000180: 5556 3220 2d20 702e 752e 2220 3d20 302e  UV2 - p.u." = 0.
+00000190: 350a 2255 5632 2043 5420 2d20 7365 6322  5."UV2 CT - sec"
+000001a0: 203d 2030 2e31 360a 2252 6563 6f6e 6e65   = 0.16."Reconne
+000001b0: 6374 2064 6561 6474 696d 6520 2d20 7365  ct deadtime - se
+000001c0: 6322 203d 2033 3030 302e 300a 2252 6563  c" = 3000.0."Rec
+000001d0: 6f6e 6e65 6374 2050 6d61 7820 7469 6d65  onnect Pmax time
+000001e0: 202d 2073 6563 2220 3d20 3330 302e 300a   - sec" = 300.0.
+000001f0: 2250 6572 6d69 7373 6976 6520 6f70 6572  "Permissive oper
+00000200: 6174 696f 6e22 203d 2022 4375 7272 656e  ation" = "Curren
+00000210: 7420 6c69 6d69 7465 6422 0a22 4d61 7920  t limited"."May 
+00000220: 7472 6970 206f 7065 7261 7469 6f6e 2220  trip operation" 
+00000230: 3d20 2254 7269 7022 0a22 4d75 6c74 6970  = "Trip"."Multip
+00000240: 6c65 2064 6973 7475 7262 616e 6365 7322  le disturbances"
+00000250: 203d 2022 5472 6970 220a 0a5b 3135 3437   = "Trip"..[1547
+00000260: 5f43 4154 5f49 5d0a 2243 6174 6567 6f72  _CAT_I]."Categor
+00000270: 7922 203d 2022 7465 7374 220a 226b 5641  y" = "test"."kVA
+00000280: 2220 3d20 342e 300a 226d 6178 4b57 2220  " = 4.0."maxKW" 
+00000290: 3d20 342e 300a 224b 7661 724c 696d 6974  = 4.0."KvarLimit
+000002a0: 2209 3d20 312e 3736 0a22 2550 4375 7469  ".= 1.76."%PCuti
+000002b0: 6e22 203d 2030 2e30 0a22 2550 4375 746f  n" = 0.0."%PCuto
+000002c0: 7574 2220 3d20 302e 300a 2255 6361 6c63  ut" = 0.0."Ucalc
+000002d0: 4d6f 6465 2220 3d20 224d 6178 220a 2250  Mode" = "Max"."P
+000002e0: 7269 6f72 6974 7922 203d 2022 4571 7561  riority" = "Equa
+000002f0: 6c22 0a22 456e 6162 6c65 2050 4620 6c69  l"."Enable PF li
+00000300: 6d69 7422 203d 2066 616c 7365 0a22 7066  mit" = false."pf
+00000310: 4d69 6e22 093d 2030 2e39 350a 2246 6f6c  Min".= 0.95."Fol
+00000320: 6c6f 7720 7374 616e 6461 7264 2220 3d20  low standard" = 
+00000330: 2231 3534 372d 3230 3138 220a 2252 6964  "1547-2018"."Rid
+00000340: 652d 7468 726f 7567 6820 4361 7465 676f  e-through Catego
+00000350: 7279 2209 3d20 2243 6174 6567 6f72 7920  ry".= "Category 
+00000360: 4922 0a22 4f56 3220 2d20 702e 752e 2220  I"."OV2 - p.u." 
+00000370: 3d20 312e 320a 224f 5632 2043 5420 2d20  = 1.2."OV2 CT - 
+00000380: 7365 6322 203d 2030 2e31 360a 224f 5631  sec" = 0.16."OV1
+00000390: 202d 2070 2e75 2e22 203d 2031 2e31 0a22   - p.u." = 1.1."
+000003a0: 4f56 3120 4354 202d 2073 6563 2220 3d20  OV1 CT - sec" = 
+000003b0: 322e 300a 2255 5631 202d 2070 2e75 2e22  2.0."UV1 - p.u."
+000003c0: 203d 2030 2e37 0a22 5556 3120 4354 202d   = 0.7."UV1 CT -
+000003d0: 2073 6563 2220 3d20 322e 300a 2255 5632   sec" = 2.0."UV2
+000003e0: 202d 2070 2e75 2e22 203d 2030 2e34 350a   - p.u." = 0.45.
+000003f0: 2255 5632 2043 5420 2d20 7365 6322 203d  "UV2 CT - sec" =
+00000400: 2030 2e31 360a 2252 6563 6f6e 6e65 6374   0.16."Reconnect
+00000410: 2064 6561 6474 696d 6520 2d20 7365 6322   deadtime - sec"
+00000420: 203d 2033 3030 302e 300a 2252 6563 6f6e   = 3000.0."Recon
+00000430: 6e65 6374 2050 6d61 7820 7469 6d65 202d  nect Pmax time -
+00000440: 2073 6563 2220 3d20 3330 302e 300a 2250   sec" = 300.0."P
+00000450: 6572 6d69 7373 6976 6520 6f70 6572 6174  ermissive operat
+00000460: 696f 6e22 203d 2022 4375 7272 656e 7420  ion" = "Current 
+00000470: 6c69 6d69 7465 6422 0a22 4d61 7920 7472  limited"."May tr
+00000480: 6970 206f 7065 7261 7469 6f6e 2220 3d20  ip operation" = 
+00000490: 2254 7269 7022 0a22 4d75 6c74 6970 6c65  "Trip"."Multiple
+000004a0: 2064 6973 7475 7262 616e 6365 7322 203d   disturbances" =
+000004b0: 2022 5472 6970 220a 0a0a 5b31 3534 375f   "Trip"...[1547_
+000004c0: 4341 545f 4949 5d0a 2243 6174 6567 6f72  CAT_II]."Categor
+000004d0: 7922 203d 2022 7465 7374 220a 226b 5641  y" = "test"."kVA
+000004e0: 2220 3d20 342e 300a 226d 6178 4b57 2220  " = 4.0."maxKW" 
+000004f0: 3d20 342e 300a 224b 7661 724c 696d 6974  = 4.0."KvarLimit
+00000500: 2209 3d20 312e 3736 0a22 2550 4375 7469  ".= 1.76."%PCuti
+00000510: 6e22 203d 2030 2e30 0a22 2550 4375 746f  n" = 0.0."%PCuto
+00000520: 7574 2220 3d20 302e 300a 2255 6361 6c63  ut" = 0.0."Ucalc
+00000530: 4d6f 6465 2220 3d20 224d 6178 220a 2250  Mode" = "Max"."P
+00000540: 7269 6f72 6974 7922 203d 2022 4571 7561  riority" = "Equa
+00000550: 6c22 0a22 456e 6162 6c65 2050 4620 6c69  l"."Enable PF li
+00000560: 6d69 7422 203d 2066 616c 7365 0a22 7066  mit" = false."pf
+00000570: 4d69 6e22 093d 2030 2e39 350a 2246 6f6c  Min".= 0.95."Fol
+00000580: 6c6f 7720 7374 616e 6461 7264 2220 3d20  low standard" = 
+00000590: 2231 3534 372d 3230 3138 220a 2252 6964  "1547-2018"."Rid
+000005a0: 652d 7468 726f 7567 6820 4361 7465 676f  e-through Catego
+000005b0: 7279 2209 3d20 2243 6174 6567 6f72 7920  ry".= "Category 
+000005c0: 4949 220a 224f 5632 202d 2070 2e75 2e22  II"."OV2 - p.u."
+000005d0: 203d 2031 2e32 0a22 4f56 3220 4354 202d   = 1.2."OV2 CT -
+000005e0: 2073 6563 2220 3d20 302e 3136 0a22 4f56   sec" = 0.16."OV
+000005f0: 3120 2d20 702e 752e 2220 3d20 312e 310a  1 - p.u." = 1.1.
+00000600: 224f 5631 2043 5420 2d20 7365 6322 203d  "OV1 CT - sec" =
+00000610: 2032 2e30 0a22 5556 3120 2d20 702e 752e   2.0."UV1 - p.u.
+00000620: 2220 3d20 302e 370a 2255 5631 2043 5420  " = 0.7."UV1 CT 
+00000630: 2d20 7365 6322 203d 2031 302e 300a 2255  - sec" = 10.0."U
+00000640: 5632 202d 2070 2e75 2e22 203d 2030 2e34  V2 - p.u." = 0.4
+00000650: 350a 2255 5632 2043 5420 2d20 7365 6322  5."UV2 CT - sec"
+00000660: 203d 2030 2e31 360a 2252 6563 6f6e 6e65   = 0.16."Reconne
+00000670: 6374 2064 6561 6474 696d 6520 2d20 7365  ct deadtime - se
+00000680: 6322 203d 2033 3030 302e 300a 2252 6563  c" = 3000.0."Rec
+00000690: 6f6e 6e65 6374 2050 6d61 7820 7469 6d65  onnect Pmax time
+000006a0: 202d 2073 6563 2220 3d20 3330 302e 300a   - sec" = 300.0.
+000006b0: 2250 6572 6d69 7373 6976 6520 6f70 6572  "Permissive oper
+000006c0: 6174 696f 6e22 203d 2022 4375 7272 656e  ation" = "Curren
+000006d0: 7420 6c69 6d69 7465 6422 0a22 4d61 7920  t limited"."May 
+000006e0: 7472 6970 206f 7065 7261 7469 6f6e 2220  trip operation" 
+000006f0: 3d20 2254 7269 7022 0a22 4d75 6c74 6970  = "Trip"."Multip
+00000700: 6c65 2064 6973 7475 7262 616e 6365 7322  le disturbances"
+00000710: 203d 2022 5472 6970 220a 0a0a 5b31 3534   = "Trip"...[154
+00000720: 375f 4341 545f 4949 495d 0a22 4361 7465  7_CAT_III]."Cate
+00000730: 676f 7279 2220 3d20 2274 6573 7422 0a22  gory" = "test"."
+00000740: 6b56 4122 203d 2034 2e30 0a22 6d61 784b  kVA" = 4.0."maxK
+00000750: 5722 203d 2034 2e30 0a22 4b76 6172 4c69  W" = 4.0."KvarLi
+00000760: 6d69 7422 093d 2031 2e37 360a 2225 5043  mit".= 1.76."%PC
+00000770: 7574 696e 2220 3d20 300a 2225 5043 7574  utin" = 0."%PCut
+00000780: 6f75 7422 203d 2030 0a22 5563 616c 634d  out" = 0."UcalcM
+00000790: 6f64 6522 203d 2022 4d61 7822 0a22 5072  ode" = "Max"."Pr
+000007a0: 696f 7269 7479 2220 3d20 2245 7175 616c  iority" = "Equal
+000007b0: 220a 2245 6e61 626c 6520 5046 206c 696d  "."Enable PF lim
+000007c0: 6974 2220 3d20 6661 6c73 650a 2270 664d  it" = false."pfM
+000007d0: 696e 2209 3d20 302e 3935 0a22 466f 6c6c  in".= 0.95."Foll
+000007e0: 6f77 2073 7461 6e64 6172 6422 203d 2022  ow standard" = "
+000007f0: 3135 3437 2d32 3031 3822 0a22 5269 6465  1547-2018"."Ride
+00000800: 2d74 6872 6f75 6768 2043 6174 6567 6f72  -through Categor
+00000810: 7922 093d 2022 4361 7465 676f 7279 2049  y".= "Category I
+00000820: 4949 220a 224f 5632 202d 2070 2e75 2e22  II"."OV2 - p.u."
+00000830: 203d 2031 2e32 0a22 4f56 3220 4354 202d   = 1.2."OV2 CT -
+00000840: 2073 6563 2220 3d20 302e 3136 0a22 4f56   sec" = 0.16."OV
+00000850: 3120 2d20 702e 752e 2220 3d20 312e 310a  1 - p.u." = 1.1.
+00000860: 224f 5631 2043 5420 2d20 7365 6322 203d  "OV1 CT - sec" =
+00000870: 2031 332e 300a 2255 5631 202d 2070 2e75   13.0."UV1 - p.u
+00000880: 2e22 203d 2030 2e38 380a 2255 5631 2043  ." = 0.88."UV1 C
+00000890: 5420 2d20 7365 6322 203d 2032 312e 300a  T - sec" = 21.0.
+000008a0: 2255 5632 202d 2070 2e75 2e22 203d 2030  "UV2 - p.u." = 0
+000008b0: 2e35 0a22 5556 3220 4354 202d 2073 6563  .5."UV2 CT - sec
+000008c0: 2220 3d20 322e 300a 2252 6563 6f6e 6e65  " = 2.0."Reconne
+000008d0: 6374 2064 6561 6474 696d 6520 2d20 7365  ct deadtime - se
+000008e0: 6322 203d 2033 3030 302e 300a 2252 6563  c" = 3000.0."Rec
+000008f0: 6f6e 6e65 6374 2050 6d61 7820 7469 6d65  onnect Pmax time
+00000900: 202d 2073 6563 2220 3d20 3330 302e 300a   - sec" = 300.0.
+00000910: 2250 6572 6d69 7373 6976 6520 6f70 6572  "Permissive oper
+00000920: 6174 696f 6e22 203d 2022 4375 7272 656e  ation" = "Curren
+00000930: 7420 6c69 6d69 7465 6422 0a22 4d61 7920  t limited"."May 
+00000940: 7472 6970 206f 7065 7261 7469 6f6e 2220  trip operation" 
+00000950: 3d20 2254 7269 7022 0a22 4d75 6c74 6970  = "Trip"."Multip
+00000960: 6c65 2064 6973 7475 7262 616e 6365 7322  le disturbances"
+00000970: 203d 2022 5472 6970 220a 0a5b 6370 665d   = "Trip"..[cpf]
+00000980: 0a43 6174 6567 6f72 7920 3d20 224c 6567  .Category = "Leg
+00000990: 6163 7922 0a43 6f6e 7472 6f6c 3120 3d20  acy".Control1 = 
+000009a0: 224e 6f6e 6522 0a43 6f6e 7472 6f6c 3220  "None".Control2 
+000009b0: 3d20 224e 6f6e 6522 0a43 6f6e 7472 6f6c  = "None".Control
+000009c0: 3320 3d20 224e 6f6e 6522 0a70 6620 3d20  3 = "None".pf = 
+000009d0: 224e 6f6e 6522 0a70 664d 696e 203d 2030  "None".pfMin = 0
+000009e0: 2e38 0a70 664d 6178 203d 2031 0a50 6d69  .8.pfMax = 1.Pmi
+000009f0: 6e20 3d20 300a 506d 6178 203d 2031 0a75  n = 0.Pmax = 1.u
+00000a00: 4d69 6e20 3d20 302e 3932 0a75 4462 4d69  Min = 0.92.uDbMi
+00000a10: 6e20 3d20 302e 3936 370a 7544 624d 6178  n = 0.967.uDbMax
+00000a20: 203d 2031 2e30 3333 0a75 4d61 7820 3d20   = 1.033.uMax = 
+00000a30: 312e 3036 2020 2320 6d75 7374 2062 6520  1.06  # must be 
+00000a40: 6c65 7373 2074 6861 6e20 6f72 2065 7175  less than or equ
+00000a50: 616c 2074 6f20 754d 696e 430a 516c 696d  al to uMinC.Qlim
+00000a60: 5055 203d 2030 2e34 340a 5046 6c69 6d20  PU = 0.44.PFlim 
+00000a70: 3d20 310a 2245 6e61 626c 6520 5046 206c  = 1."Enable PF l
+00000a80: 696d 6974 2220 3d20 6661 6c73 650a 754d  imit" = false.uM
+00000a90: 696e 4320 3d20 312e 3036 0a75 4d61 7843  inC = 1.06.uMaxC
+00000aa0: 203d 2031 2e31 0a50 6d69 6e56 5720 3d20   = 1.1.PminVW = 
+00000ab0: 3130 0a56 5774 7970 6520 3d20 2241 7661  10.VWtype = "Ava
+00000ac0: 696c 6162 6c65 2050 6f77 6572 220a 2225  ilable Power"."%
+00000ad0: 5543 7574 6f66 6622 203d 2031 2e31 0a22  UCutoff" = 1.1."
+00000ae0: 2550 4375 7469 6e22 203d 2031 300a 2225  %PCutin" = 10."%
+00000af0: 5043 7574 6f75 7422 203d 2031 300a 4566  PCutout" = 10.Ef
+00000b00: 6669 6369 656e 6379 203d 2031 3030 0a50  ficiency = 100.P
+00000b10: 7269 6f72 6974 7920 3d20 2256 6172 220a  riority = "Var".
+00000b20: 4461 6d70 436f 6566 203d 2030 2e38 0a0a  DampCoef = 0.8..
+00000b30: 5b76 6f6c 742d 7661 725d 0a43 6f6e 7472  [volt-var].Contr
+00000b40: 6f6c 3120 3d20 2256 5661 7222 0a43 6f6e  ol1 = "VVar".Con
+00000b50: 7472 6f6c 3220 3d20 224e 6f6e 6522 0a43  trol2 = "None".C
+00000b60: 6f6e 7472 6f6c 3320 3d20 224e 6f6e 6522  ontrol3 = "None"
+00000b70: 0a70 6620 3d20 310a 7066 4d69 6e20 3d20  .pf = 1.pfMin = 
+00000b80: 302e 380a 7066 4d61 7820 3d20 310a 506d  0.8.pfMax = 1.Pm
+00000b90: 696e 203d 2030 0a50 6d61 7820 3d20 310a  in = 0.Pmax = 1.
+00000ba0: 754d 696e 203d 2030 2e39 3339 3939 3939  uMin = 0.9399999
+00000bb0: 3939 3939 3939 3939 390a 7544 624d 696e  999999999.uDbMin
+00000bc0: 203d 2030 2e39 370a 7544 624d 6178 203d   = 0.97.uDbMax =
+00000bd0: 2031 2e30 330a 754d 6178 203d 2031 2e30   1.03.uMax = 1.0
+00000be0: 360a 516c 696d 5055 203d 2030 2e34 340a  6.QlimPU = 0.44.
+00000bf0: 5046 6c69 6d20 3d20 302e 390a 2245 6e61  PFlim = 0.9."Ena
+00000c00: 626c 6520 5046 206c 696d 6974 2220 3d20  ble PF limit" = 
+00000c10: 6661 6c73 650a 754d 696e 4320 3d20 312e  false.uMinC = 1.
+00000c20: 3036 0a75 4d61 7843 203d 2031 2e31 0a50  06.uMaxC = 1.1.P
+00000c30: 6d69 6e56 5720 3d20 3130 0a56 5774 7970  minVW = 10.VWtyp
+00000c40: 6520 3d20 2252 6174 6564 2050 6f77 6572  e = "Rated Power
+00000c50: 220a 2225 5043 7574 696e 2220 3d20 3130  "."%PCutin" = 10
+00000c60: 0a22 2550 4375 746f 7574 2220 3d20 3130  ."%PCutout" = 10
+00000c70: 0a45 6666 6963 6965 6e63 7920 3d20 3130  .Efficiency = 10
+00000c80: 300a 5072 696f 7269 7479 203d 2022 5661  0.Priority = "Va
+00000c90: 7222 0a44 616d 7043 6f65 6620 3d20 302e  r".DampCoef = 0.
+00000ca0: 380a                                     8.
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyControllers/Settings/PvControllers.toml` & `nrel_pydss-3.1.4/src/pydss/pyControllers/Settings/PvControllers.toml`

 * *Files 20% similar despite different names*

```diff
@@ -1,159 +1,152 @@
-00000000: 5b4e 4f5f 5652 545d 0d0a 2243 6174 6567  [NO_VRT].."Categ
-00000010: 6f72 7922 203d 2022 7465 7374 220d 0a22  ory" = "test".."
-00000020: 6b56 4122 203d 2034 2e30 0d0a 226d 6178  kVA" = 4.0.."max
-00000030: 4b57 2220 3d20 342e 300d 0a22 4b76 6172  KW" = 4.0.."Kvar
-00000040: 4c69 6d69 7422 093d 2031 2e37 360d 0a22  Limit".= 1.76.."
-00000050: 2550 4375 7469 6e22 203d 2031 302e 300d  %PCutin" = 10.0.
-00000060: 0a22 2550 4375 746f 7574 2220 3d20 3130  ."%PCutout" = 10
-00000070: 2e30 0d0a 2255 6361 6c63 4d6f 6465 2220  .0.."UcalcMode" 
-00000080: 3d20 224d 6178 220d 0a22 5072 696f 7269  = "Max".."Priori
-00000090: 7479 2220 3d20 2245 7175 616c 220d 0a22  ty" = "Equal".."
-000000a0: 456e 6162 6c65 2050 4620 6c69 6d69 7422  Enable PF limit"
-000000b0: 203d 2066 616c 7365 0d0a 2270 664d 696e   = false.."pfMin
-000000c0: 2209 3d20 302e 3935 0d0a 2246 6f6c 6c6f  ".= 0.95.."Follo
-000000d0: 7720 7374 616e 6461 7264 2220 3d20 2231  w standard" = "1
-000000e0: 3534 372d 3230 3033 220d 0a22 5269 6465  547-2003".."Ride
-000000f0: 2d74 6872 6f75 6768 2043 6174 6567 6f72  -through Categor
-00000100: 7922 093d 2022 4361 7465 676f 7279 2049  y".= "Category I
-00000110: 220d 0a22 4f56 3220 2d20 702e 752e 2220  ".."OV2 - p.u." 
-00000120: 3d20 312e 320d 0a22 4f56 3220 4354 202d  = 1.2.."OV2 CT -
-00000130: 2073 6563 2220 3d20 302e 3136 0d0a 224f   sec" = 0.16.."O
-00000140: 5631 202d 2070 2e75 2e22 203d 2031 2e32  V1 - p.u." = 1.2
-00000150: 0d0a 224f 5631 2043 5420 2d20 7365 6322  .."OV1 CT - sec"
-00000160: 203d 2032 2e30 0d0a 2255 5631 202d 2070   = 2.0.."UV1 - p
-00000170: 2e75 2e22 203d 2030 2e38 0d0a 2255 5631  .u." = 0.8.."UV1
-00000180: 2043 5420 2d20 7365 6322 203d 2032 2e30   CT - sec" = 2.0
-00000190: 0d0a 2255 5632 202d 2070 2e75 2e22 203d  .."UV2 - p.u." =
-000001a0: 2030 2e35 0d0a 2255 5632 2043 5420 2d20   0.5.."UV2 CT - 
-000001b0: 7365 6322 203d 2030 2e31 360d 0a22 5265  sec" = 0.16.."Re
-000001c0: 636f 6e6e 6563 7420 6465 6164 7469 6d65  connect deadtime
-000001d0: 202d 2073 6563 2220 3d20 3330 3030 2e30   - sec" = 3000.0
-000001e0: 0d0a 2252 6563 6f6e 6e65 6374 2050 6d61  .."Reconnect Pma
-000001f0: 7820 7469 6d65 202d 2073 6563 2220 3d20  x time - sec" = 
-00000200: 3330 302e 300d 0a22 5065 726d 6973 7369  300.0.."Permissi
-00000210: 7665 206f 7065 7261 7469 6f6e 2220 3d20  ve operation" = 
-00000220: 2243 7572 7265 6e74 206c 696d 6974 6564  "Current limited
-00000230: 220d 0a22 4d61 7920 7472 6970 206f 7065  ".."May trip ope
-00000240: 7261 7469 6f6e 2220 3d20 2254 7269 7022  ration" = "Trip"
-00000250: 0d0a 224d 756c 7469 706c 6520 6469 7374  .."Multiple dist
-00000260: 7572 6261 6e63 6573 2220 3d20 2254 7269  urbances" = "Tri
-00000270: 7022 0d0a 0d0a 5b31 3534 375f 4341 545f  p"....[1547_CAT_
-00000280: 495d 0d0a 2243 6174 6567 6f72 7922 203d  I].."Category" =
-00000290: 2022 7465 7374 220d 0a22 6b56 4122 203d   "test".."kVA" =
-000002a0: 2034 2e30 0d0a 226d 6178 4b57 2220 3d20   4.0.."maxKW" = 
-000002b0: 342e 300d 0a22 4b76 6172 4c69 6d69 7422  4.0.."KvarLimit"
-000002c0: 093d 2031 2e37 360d 0a22 2550 4375 7469  .= 1.76.."%PCuti
-000002d0: 6e22 203d 2030 2e30 0d0a 2225 5043 7574  n" = 0.0.."%PCut
-000002e0: 6f75 7422 203d 2030 2e30 0d0a 2255 6361  out" = 0.0.."Uca
-000002f0: 6c63 4d6f 6465 2220 3d20 224d 6178 220d  lcMode" = "Max".
-00000300: 0a22 5072 696f 7269 7479 2220 3d20 2245  ."Priority" = "E
-00000310: 7175 616c 220d 0a22 456e 6162 6c65 2050  qual".."Enable P
-00000320: 4620 6c69 6d69 7422 203d 2066 616c 7365  F limit" = false
-00000330: 0d0a 2270 664d 696e 2209 3d20 302e 3935  .."pfMin".= 0.95
-00000340: 0d0a 2246 6f6c 6c6f 7720 7374 616e 6461  .."Follow standa
-00000350: 7264 2220 3d20 2231 3534 372d 3230 3138  rd" = "1547-2018
-00000360: 220d 0a22 5269 6465 2d74 6872 6f75 6768  ".."Ride-through
-00000370: 2043 6174 6567 6f72 7922 093d 2022 4361   Category".= "Ca
-00000380: 7465 676f 7279 2049 220d 0a22 4f56 3220  tegory I".."OV2 
-00000390: 2d20 702e 752e 2220 3d20 312e 320d 0a22  - p.u." = 1.2.."
-000003a0: 4f56 3220 4354 202d 2073 6563 2220 3d20  OV2 CT - sec" = 
-000003b0: 302e 3136 0d0a 224f 5631 202d 2070 2e75  0.16.."OV1 - p.u
-000003c0: 2e22 203d 2031 2e31 0d0a 224f 5631 2043  ." = 1.1.."OV1 C
-000003d0: 5420 2d20 7365 6322 203d 2032 2e30 0d0a  T - sec" = 2.0..
-000003e0: 2255 5631 202d 2070 2e75 2e22 203d 2030  "UV1 - p.u." = 0
-000003f0: 2e37 0d0a 2255 5631 2043 5420 2d20 7365  .7.."UV1 CT - se
-00000400: 6322 203d 2032 2e30 0d0a 2255 5632 202d  c" = 2.0.."UV2 -
-00000410: 2070 2e75 2e22 203d 2030 2e34 350d 0a22   p.u." = 0.45.."
-00000420: 5556 3220 4354 202d 2073 6563 2220 3d20  UV2 CT - sec" = 
-00000430: 302e 3136 0d0a 2252 6563 6f6e 6e65 6374  0.16.."Reconnect
-00000440: 2064 6561 6474 696d 6520 2d20 7365 6322   deadtime - sec"
-00000450: 203d 2033 3030 302e 300d 0a22 5265 636f   = 3000.0.."Reco
-00000460: 6e6e 6563 7420 506d 6178 2074 696d 6520  nnect Pmax time 
-00000470: 2d20 7365 6322 203d 2033 3030 2e30 0d0a  - sec" = 300.0..
-00000480: 2250 6572 6d69 7373 6976 6520 6f70 6572  "Permissive oper
-00000490: 6174 696f 6e22 203d 2022 4375 7272 656e  ation" = "Curren
-000004a0: 7420 6c69 6d69 7465 6422 0d0a 224d 6179  t limited".."May
-000004b0: 2074 7269 7020 6f70 6572 6174 696f 6e22   trip operation"
-000004c0: 203d 2022 5472 6970 220d 0a22 4d75 6c74   = "Trip".."Mult
-000004d0: 6970 6c65 2064 6973 7475 7262 616e 6365  iple disturbance
-000004e0: 7322 203d 2022 5472 6970 220d 0a0d 0a0d  s" = "Trip".....
-000004f0: 0a5b 3135 3437 5f43 4154 5f49 495d 0d0a  .[1547_CAT_II]..
-00000500: 2243 6174 6567 6f72 7922 203d 2022 7465  "Category" = "te
-00000510: 7374 220d 0a22 6b56 4122 203d 2034 2e30  st".."kVA" = 4.0
-00000520: 0d0a 226d 6178 4b57 2220 3d20 342e 300d  .."maxKW" = 4.0.
-00000530: 0a22 4b76 6172 4c69 6d69 7422 093d 2031  ."KvarLimit".= 1
-00000540: 2e37 360d 0a22 2550 4375 7469 6e22 203d  .76.."%PCutin" =
-00000550: 2030 2e30 0d0a 2225 5043 7574 6f75 7422   0.0.."%PCutout"
-00000560: 203d 2030 2e30 0d0a 2255 6361 6c63 4d6f   = 0.0.."UcalcMo
-00000570: 6465 2220 3d20 224d 6178 220d 0a22 5072  de" = "Max".."Pr
-00000580: 696f 7269 7479 2220 3d20 2245 7175 616c  iority" = "Equal
-00000590: 220d 0a22 456e 6162 6c65 2050 4620 6c69  ".."Enable PF li
-000005a0: 6d69 7422 203d 2066 616c 7365 0d0a 2270  mit" = false.."p
-000005b0: 664d 696e 2209 3d20 302e 3935 0d0a 2246  fMin".= 0.95.."F
-000005c0: 6f6c 6c6f 7720 7374 616e 6461 7264 2220  ollow standard" 
-000005d0: 3d20 2231 3534 372d 3230 3138 220d 0a22  = "1547-2018".."
-000005e0: 5269 6465 2d74 6872 6f75 6768 2043 6174  Ride-through Cat
-000005f0: 6567 6f72 7922 093d 2022 4361 7465 676f  egory".= "Catego
-00000600: 7279 2049 4922 0d0a 224f 5632 202d 2070  ry II".."OV2 - p
-00000610: 2e75 2e22 203d 2031 2e32 0d0a 224f 5632  .u." = 1.2.."OV2
-00000620: 2043 5420 2d20 7365 6322 203d 2030 2e31   CT - sec" = 0.1
-00000630: 360d 0a22 4f56 3120 2d20 702e 752e 2220  6.."OV1 - p.u." 
-00000640: 3d20 312e 310d 0a22 4f56 3120 4354 202d  = 1.1.."OV1 CT -
-00000650: 2073 6563 2220 3d20 322e 300d 0a22 5556   sec" = 2.0.."UV
-00000660: 3120 2d20 702e 752e 2220 3d20 302e 370d  1 - p.u." = 0.7.
-00000670: 0a22 5556 3120 4354 202d 2073 6563 2220  ."UV1 CT - sec" 
-00000680: 3d20 3130 2e30 0d0a 2255 5632 202d 2070  = 10.0.."UV2 - p
-00000690: 2e75 2e22 203d 2030 2e34 350d 0a22 5556  .u." = 0.45.."UV
-000006a0: 3220 4354 202d 2073 6563 2220 3d20 302e  2 CT - sec" = 0.
-000006b0: 3136 0d0a 2252 6563 6f6e 6e65 6374 2064  16.."Reconnect d
-000006c0: 6561 6474 696d 6520 2d20 7365 6322 203d  eadtime - sec" =
-000006d0: 2033 3030 302e 300d 0a22 5265 636f 6e6e   3000.0.."Reconn
-000006e0: 6563 7420 506d 6178 2074 696d 6520 2d20  ect Pmax time - 
-000006f0: 7365 6322 203d 2033 3030 2e30 0d0a 2250  sec" = 300.0.."P
-00000700: 6572 6d69 7373 6976 6520 6f70 6572 6174  ermissive operat
-00000710: 696f 6e22 203d 2022 4375 7272 656e 7420  ion" = "Current 
-00000720: 6c69 6d69 7465 6422 0d0a 224d 6179 2074  limited".."May t
-00000730: 7269 7020 6f70 6572 6174 696f 6e22 203d  rip operation" =
-00000740: 2022 5472 6970 220d 0a22 4d75 6c74 6970   "Trip".."Multip
-00000750: 6c65 2064 6973 7475 7262 616e 6365 7322  le disturbances"
-00000760: 203d 2022 5472 6970 220d 0a0d 0a0d 0a5b   = "Trip"......[
-00000770: 3135 3437 5f43 4154 5f49 4949 5d0d 0a22  1547_CAT_III].."
-00000780: 4361 7465 676f 7279 2220 3d20 2274 6573  Category" = "tes
-00000790: 7422 0d0a 226b 5641 2220 3d20 342e 300d  t".."kVA" = 4.0.
-000007a0: 0a22 6d61 784b 5722 203d 2034 2e30 0d0a  ."maxKW" = 4.0..
-000007b0: 224b 7661 724c 696d 6974 2209 3d20 312e  "KvarLimit".= 1.
-000007c0: 3736 0d0a 2225 5043 7574 696e 2220 3d20  76.."%PCutin" = 
-000007d0: 300d 0a22 2550 4375 746f 7574 2220 3d20  0.."%PCutout" = 
-000007e0: 300d 0a22 5563 616c 634d 6f64 6522 203d  0.."UcalcMode" =
-000007f0: 2022 4d61 7822 0d0a 2250 7269 6f72 6974   "Max".."Priorit
-00000800: 7922 203d 2022 4571 7561 6c22 0d0a 2245  y" = "Equal".."E
-00000810: 6e61 626c 6520 5046 206c 696d 6974 2220  nable PF limit" 
-00000820: 3d20 6661 6c73 650d 0a22 7066 4d69 6e22  = false.."pfMin"
-00000830: 093d 2030 2e39 350d 0a22 466f 6c6c 6f77  .= 0.95.."Follow
-00000840: 2073 7461 6e64 6172 6422 203d 2022 3135   standard" = "15
-00000850: 3437 2d32 3031 3822 0d0a 2252 6964 652d  47-2018".."Ride-
-00000860: 7468 726f 7567 6820 4361 7465 676f 7279  through Category
-00000870: 2209 3d20 2243 6174 6567 6f72 7920 4949  ".= "Category II
-00000880: 4922 0d0a 224f 5632 202d 2070 2e75 2e22  I".."OV2 - p.u."
-00000890: 203d 2031 2e32 0d0a 224f 5632 2043 5420   = 1.2.."OV2 CT 
-000008a0: 2d20 7365 6322 203d 2030 2e31 360d 0a22  - sec" = 0.16.."
-000008b0: 4f56 3120 2d20 702e 752e 2220 3d20 312e  OV1 - p.u." = 1.
-000008c0: 310d 0a22 4f56 3120 4354 202d 2073 6563  1.."OV1 CT - sec
-000008d0: 2220 3d20 3133 2e30 0d0a 2255 5631 202d  " = 13.0.."UV1 -
-000008e0: 2070 2e75 2e22 203d 2030 2e38 380d 0a22   p.u." = 0.88.."
-000008f0: 5556 3120 4354 202d 2073 6563 2220 3d20  UV1 CT - sec" = 
-00000900: 3231 2e30 0d0a 2255 5632 202d 2070 2e75  21.0.."UV2 - p.u
-00000910: 2e22 203d 2030 2e35 0d0a 2255 5632 2043  ." = 0.5.."UV2 C
-00000920: 5420 2d20 7365 6322 203d 2032 2e30 0d0a  T - sec" = 2.0..
-00000930: 2252 6563 6f6e 6e65 6374 2064 6561 6474  "Reconnect deadt
-00000940: 696d 6520 2d20 7365 6322 203d 2033 3030  ime - sec" = 300
-00000950: 302e 300d 0a22 5265 636f 6e6e 6563 7420  0.0.."Reconnect 
-00000960: 506d 6178 2074 696d 6520 2d20 7365 6322  Pmax time - sec"
-00000970: 203d 2033 3030 2e30 0d0a 2250 6572 6d69   = 300.0.."Permi
-00000980: 7373 6976 6520 6f70 6572 6174 696f 6e22  ssive operation"
-00000990: 203d 2022 4375 7272 656e 7420 6c69 6d69   = "Current limi
-000009a0: 7465 6422 0d0a 224d 6179 2074 7269 7020  ted".."May trip 
-000009b0: 6f70 6572 6174 696f 6e22 203d 2022 5472  operation" = "Tr
-000009c0: 6970 220d 0a22 4d75 6c74 6970 6c65 2064  ip".."Multiple d
-000009d0: 6973 7475 7262 616e 6365 7322 203d 2022  isturbances" = "
-000009e0: 5472 6970 22                             Trip"
+00000000: 5b4e 4f5f 5652 545d 0a22 4361 7465 676f  [NO_VRT]."Catego
+00000010: 7279 2220 3d20 2274 6573 7422 0a22 6b56  ry" = "test"."kV
+00000020: 4122 203d 2034 2e30 0a22 6d61 784b 5722  A" = 4.0."maxKW"
+00000030: 203d 2034 2e30 0a22 4b76 6172 4c69 6d69   = 4.0."KvarLimi
+00000040: 7422 093d 2031 2e37 360a 2225 5043 7574  t".= 1.76."%PCut
+00000050: 696e 2220 3d20 3130 2e30 0a22 2550 4375  in" = 10.0."%PCu
+00000060: 746f 7574 2220 3d20 3130 2e30 0a22 5563  tout" = 10.0."Uc
+00000070: 616c 634d 6f64 6522 203d 2022 4d61 7822  alcMode" = "Max"
+00000080: 0a22 5072 696f 7269 7479 2220 3d20 2245  ."Priority" = "E
+00000090: 7175 616c 220a 2245 6e61 626c 6520 5046  qual"."Enable PF
+000000a0: 206c 696d 6974 2220 3d20 6661 6c73 650a   limit" = false.
+000000b0: 2270 664d 696e 2209 3d20 302e 3935 0a22  "pfMin".= 0.95."
+000000c0: 466f 6c6c 6f77 2073 7461 6e64 6172 6422  Follow standard"
+000000d0: 203d 2022 3135 3437 2d32 3030 3322 0a22   = "1547-2003"."
+000000e0: 5269 6465 2d74 6872 6f75 6768 2043 6174  Ride-through Cat
+000000f0: 6567 6f72 7922 093d 2022 4361 7465 676f  egory".= "Catego
+00000100: 7279 2049 220a 224f 5632 202d 2070 2e75  ry I"."OV2 - p.u
+00000110: 2e22 203d 2031 2e32 0a22 4f56 3220 4354  ." = 1.2."OV2 CT
+00000120: 202d 2073 6563 2220 3d20 302e 3136 0a22   - sec" = 0.16."
+00000130: 4f56 3120 2d20 702e 752e 2220 3d20 312e  OV1 - p.u." = 1.
+00000140: 320a 224f 5631 2043 5420 2d20 7365 6322  2."OV1 CT - sec"
+00000150: 203d 2032 2e30 0a22 5556 3120 2d20 702e   = 2.0."UV1 - p.
+00000160: 752e 2220 3d20 302e 380a 2255 5631 2043  u." = 0.8."UV1 C
+00000170: 5420 2d20 7365 6322 203d 2032 2e30 0a22  T - sec" = 2.0."
+00000180: 5556 3220 2d20 702e 752e 2220 3d20 302e  UV2 - p.u." = 0.
+00000190: 350a 2255 5632 2043 5420 2d20 7365 6322  5."UV2 CT - sec"
+000001a0: 203d 2030 2e31 360a 2252 6563 6f6e 6e65   = 0.16."Reconne
+000001b0: 6374 2064 6561 6474 696d 6520 2d20 7365  ct deadtime - se
+000001c0: 6322 203d 2033 3030 302e 300a 2252 6563  c" = 3000.0."Rec
+000001d0: 6f6e 6e65 6374 2050 6d61 7820 7469 6d65  onnect Pmax time
+000001e0: 202d 2073 6563 2220 3d20 3330 302e 300a   - sec" = 300.0.
+000001f0: 2250 6572 6d69 7373 6976 6520 6f70 6572  "Permissive oper
+00000200: 6174 696f 6e22 203d 2022 4375 7272 656e  ation" = "Curren
+00000210: 7420 6c69 6d69 7465 6422 0a22 4d61 7920  t limited"."May 
+00000220: 7472 6970 206f 7065 7261 7469 6f6e 2220  trip operation" 
+00000230: 3d20 2254 7269 7022 0a22 4d75 6c74 6970  = "Trip"."Multip
+00000240: 6c65 2064 6973 7475 7262 616e 6365 7322  le disturbances"
+00000250: 203d 2022 5472 6970 220a 0a5b 3135 3437   = "Trip"..[1547
+00000260: 5f43 4154 5f49 5d0a 2243 6174 6567 6f72  _CAT_I]."Categor
+00000270: 7922 203d 2022 7465 7374 220a 226b 5641  y" = "test"."kVA
+00000280: 2220 3d20 342e 300a 226d 6178 4b57 2220  " = 4.0."maxKW" 
+00000290: 3d20 342e 300a 224b 7661 724c 696d 6974  = 4.0."KvarLimit
+000002a0: 2209 3d20 312e 3736 0a22 2550 4375 7469  ".= 1.76."%PCuti
+000002b0: 6e22 203d 2030 2e30 0a22 2550 4375 746f  n" = 0.0."%PCuto
+000002c0: 7574 2220 3d20 302e 300a 2255 6361 6c63  ut" = 0.0."Ucalc
+000002d0: 4d6f 6465 2220 3d20 224d 6178 220a 2250  Mode" = "Max"."P
+000002e0: 7269 6f72 6974 7922 203d 2022 4571 7561  riority" = "Equa
+000002f0: 6c22 0a22 456e 6162 6c65 2050 4620 6c69  l"."Enable PF li
+00000300: 6d69 7422 203d 2066 616c 7365 0a22 7066  mit" = false."pf
+00000310: 4d69 6e22 093d 2030 2e39 350a 2246 6f6c  Min".= 0.95."Fol
+00000320: 6c6f 7720 7374 616e 6461 7264 2220 3d20  low standard" = 
+00000330: 2231 3534 372d 3230 3138 220a 2252 6964  "1547-2018"."Rid
+00000340: 652d 7468 726f 7567 6820 4361 7465 676f  e-through Catego
+00000350: 7279 2209 3d20 2243 6174 6567 6f72 7920  ry".= "Category 
+00000360: 4922 0a22 4f56 3220 2d20 702e 752e 2220  I"."OV2 - p.u." 
+00000370: 3d20 312e 320a 224f 5632 2043 5420 2d20  = 1.2."OV2 CT - 
+00000380: 7365 6322 203d 2030 2e31 360a 224f 5631  sec" = 0.16."OV1
+00000390: 202d 2070 2e75 2e22 203d 2031 2e31 0a22   - p.u." = 1.1."
+000003a0: 4f56 3120 4354 202d 2073 6563 2220 3d20  OV1 CT - sec" = 
+000003b0: 322e 300a 2255 5631 202d 2070 2e75 2e22  2.0."UV1 - p.u."
+000003c0: 203d 2030 2e37 0a22 5556 3120 4354 202d   = 0.7."UV1 CT -
+000003d0: 2073 6563 2220 3d20 322e 300a 2255 5632   sec" = 2.0."UV2
+000003e0: 202d 2070 2e75 2e22 203d 2030 2e34 350a   - p.u." = 0.45.
+000003f0: 2255 5632 2043 5420 2d20 7365 6322 203d  "UV2 CT - sec" =
+00000400: 2030 2e31 360a 2252 6563 6f6e 6e65 6374   0.16."Reconnect
+00000410: 2064 6561 6474 696d 6520 2d20 7365 6322   deadtime - sec"
+00000420: 203d 2033 3030 302e 300a 2252 6563 6f6e   = 3000.0."Recon
+00000430: 6e65 6374 2050 6d61 7820 7469 6d65 202d  nect Pmax time -
+00000440: 2073 6563 2220 3d20 3330 302e 300a 2250   sec" = 300.0."P
+00000450: 6572 6d69 7373 6976 6520 6f70 6572 6174  ermissive operat
+00000460: 696f 6e22 203d 2022 4375 7272 656e 7420  ion" = "Current 
+00000470: 6c69 6d69 7465 6422 0a22 4d61 7920 7472  limited"."May tr
+00000480: 6970 206f 7065 7261 7469 6f6e 2220 3d20  ip operation" = 
+00000490: 2254 7269 7022 0a22 4d75 6c74 6970 6c65  "Trip"."Multiple
+000004a0: 2064 6973 7475 7262 616e 6365 7322 203d   disturbances" =
+000004b0: 2022 5472 6970 220a 0a0a 5b31 3534 375f   "Trip"...[1547_
+000004c0: 4341 545f 4949 5d0a 2243 6174 6567 6f72  CAT_II]."Categor
+000004d0: 7922 203d 2022 7465 7374 220a 226b 5641  y" = "test"."kVA
+000004e0: 2220 3d20 342e 300a 226d 6178 4b57 2220  " = 4.0."maxKW" 
+000004f0: 3d20 342e 300a 224b 7661 724c 696d 6974  = 4.0."KvarLimit
+00000500: 2209 3d20 312e 3736 0a22 2550 4375 7469  ".= 1.76."%PCuti
+00000510: 6e22 203d 2030 2e30 0a22 2550 4375 746f  n" = 0.0."%PCuto
+00000520: 7574 2220 3d20 302e 300a 2255 6361 6c63  ut" = 0.0."Ucalc
+00000530: 4d6f 6465 2220 3d20 224d 6178 220a 2250  Mode" = "Max"."P
+00000540: 7269 6f72 6974 7922 203d 2022 4571 7561  riority" = "Equa
+00000550: 6c22 0a22 456e 6162 6c65 2050 4620 6c69  l"."Enable PF li
+00000560: 6d69 7422 203d 2066 616c 7365 0a22 7066  mit" = false."pf
+00000570: 4d69 6e22 093d 2030 2e39 350a 2246 6f6c  Min".= 0.95."Fol
+00000580: 6c6f 7720 7374 616e 6461 7264 2220 3d20  low standard" = 
+00000590: 2231 3534 372d 3230 3138 220a 2252 6964  "1547-2018"."Rid
+000005a0: 652d 7468 726f 7567 6820 4361 7465 676f  e-through Catego
+000005b0: 7279 2209 3d20 2243 6174 6567 6f72 7920  ry".= "Category 
+000005c0: 4949 220a 224f 5632 202d 2070 2e75 2e22  II"."OV2 - p.u."
+000005d0: 203d 2031 2e32 0a22 4f56 3220 4354 202d   = 1.2."OV2 CT -
+000005e0: 2073 6563 2220 3d20 302e 3136 0a22 4f56   sec" = 0.16."OV
+000005f0: 3120 2d20 702e 752e 2220 3d20 312e 310a  1 - p.u." = 1.1.
+00000600: 224f 5631 2043 5420 2d20 7365 6322 203d  "OV1 CT - sec" =
+00000610: 2032 2e30 0a22 5556 3120 2d20 702e 752e   2.0."UV1 - p.u.
+00000620: 2220 3d20 302e 370a 2255 5631 2043 5420  " = 0.7."UV1 CT 
+00000630: 2d20 7365 6322 203d 2031 302e 300a 2255  - sec" = 10.0."U
+00000640: 5632 202d 2070 2e75 2e22 203d 2030 2e34  V2 - p.u." = 0.4
+00000650: 350a 2255 5632 2043 5420 2d20 7365 6322  5."UV2 CT - sec"
+00000660: 203d 2030 2e31 360a 2252 6563 6f6e 6e65   = 0.16."Reconne
+00000670: 6374 2064 6561 6474 696d 6520 2d20 7365  ct deadtime - se
+00000680: 6322 203d 2033 3030 302e 300a 2252 6563  c" = 3000.0."Rec
+00000690: 6f6e 6e65 6374 2050 6d61 7820 7469 6d65  onnect Pmax time
+000006a0: 202d 2073 6563 2220 3d20 3330 302e 300a   - sec" = 300.0.
+000006b0: 2250 6572 6d69 7373 6976 6520 6f70 6572  "Permissive oper
+000006c0: 6174 696f 6e22 203d 2022 4375 7272 656e  ation" = "Curren
+000006d0: 7420 6c69 6d69 7465 6422 0a22 4d61 7920  t limited"."May 
+000006e0: 7472 6970 206f 7065 7261 7469 6f6e 2220  trip operation" 
+000006f0: 3d20 2254 7269 7022 0a22 4d75 6c74 6970  = "Trip"."Multip
+00000700: 6c65 2064 6973 7475 7262 616e 6365 7322  le disturbances"
+00000710: 203d 2022 5472 6970 220a 0a0a 5b31 3534   = "Trip"...[154
+00000720: 375f 4341 545f 4949 495d 0a22 4361 7465  7_CAT_III]."Cate
+00000730: 676f 7279 2220 3d20 2274 6573 7422 0a22  gory" = "test"."
+00000740: 6b56 4122 203d 2034 2e30 0a22 6d61 784b  kVA" = 4.0."maxK
+00000750: 5722 203d 2034 2e30 0a22 4b76 6172 4c69  W" = 4.0."KvarLi
+00000760: 6d69 7422 093d 2031 2e37 360a 2225 5043  mit".= 1.76."%PC
+00000770: 7574 696e 2220 3d20 300a 2225 5043 7574  utin" = 0."%PCut
+00000780: 6f75 7422 203d 2030 0a22 5563 616c 634d  out" = 0."UcalcM
+00000790: 6f64 6522 203d 2022 4d61 7822 0a22 5072  ode" = "Max"."Pr
+000007a0: 696f 7269 7479 2220 3d20 2245 7175 616c  iority" = "Equal
+000007b0: 220a 2245 6e61 626c 6520 5046 206c 696d  "."Enable PF lim
+000007c0: 6974 2220 3d20 6661 6c73 650a 2270 664d  it" = false."pfM
+000007d0: 696e 2209 3d20 302e 3935 0a22 466f 6c6c  in".= 0.95."Foll
+000007e0: 6f77 2073 7461 6e64 6172 6422 203d 2022  ow standard" = "
+000007f0: 3135 3437 2d32 3031 3822 0a22 5269 6465  1547-2018"."Ride
+00000800: 2d74 6872 6f75 6768 2043 6174 6567 6f72  -through Categor
+00000810: 7922 093d 2022 4361 7465 676f 7279 2049  y".= "Category I
+00000820: 4949 220a 224f 5632 202d 2070 2e75 2e22  II"."OV2 - p.u."
+00000830: 203d 2031 2e32 0a22 4f56 3220 4354 202d   = 1.2."OV2 CT -
+00000840: 2073 6563 2220 3d20 302e 3136 0a22 4f56   sec" = 0.16."OV
+00000850: 3120 2d20 702e 752e 2220 3d20 312e 310a  1 - p.u." = 1.1.
+00000860: 224f 5631 2043 5420 2d20 7365 6322 203d  "OV1 CT - sec" =
+00000870: 2031 332e 300a 2255 5631 202d 2070 2e75   13.0."UV1 - p.u
+00000880: 2e22 203d 2030 2e38 380a 2255 5631 2043  ." = 0.88."UV1 C
+00000890: 5420 2d20 7365 6322 203d 2032 312e 300a  T - sec" = 21.0.
+000008a0: 2255 5632 202d 2070 2e75 2e22 203d 2030  "UV2 - p.u." = 0
+000008b0: 2e35 0a22 5556 3220 4354 202d 2073 6563  .5."UV2 CT - sec
+000008c0: 2220 3d20 322e 300a 2252 6563 6f6e 6e65  " = 2.0."Reconne
+000008d0: 6374 2064 6561 6474 696d 6520 2d20 7365  ct deadtime - se
+000008e0: 6322 203d 2033 3030 302e 300a 2252 6563  c" = 3000.0."Rec
+000008f0: 6f6e 6e65 6374 2050 6d61 7820 7469 6d65  onnect Pmax time
+00000900: 202d 2073 6563 2220 3d20 3330 302e 300a   - sec" = 300.0.
+00000910: 2250 6572 6d69 7373 6976 6520 6f70 6572  "Permissive oper
+00000920: 6174 696f 6e22 203d 2022 4375 7272 656e  ation" = "Curren
+00000930: 7420 6c69 6d69 7465 6422 0a22 4d61 7920  t limited"."May 
+00000940: 7472 6970 206f 7065 7261 7469 6f6e 2220  trip operation" 
+00000950: 3d20 2254 7269 7022 0a22 4d75 6c74 6970  = "Trip"."Multip
+00000960: 6c65 2064 6973 7475 7262 616e 6365 7322  le disturbances"
+00000970: 203d 2022 5472 6970 22                    = "Trip"
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyPostprocessor/pyPostprocess.py` & `nrel_pydss-3.1.4/src/pydss/pyPostprocessor/pyPostprocess.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-from os.path import dirname, basename, isfile
-import glob
-
-from pydss.pyPostprocessor import PostprocessScripts
-
-
-
-
-modules = glob.glob(PostprocessScripts.__path__[0]+"/*.py")
-pythonFiles = [basename(f)[:-3] for f in modules if isfile(f) and not f.endswith('__init__.py')]
-
-POST_PROCESSES = {}
-for file in pythonFiles:
-    exec('from pydss.pyPostprocessor.PostprocessScripts import {}'.format(file))
-    exec('POST_PROCESSES["{}"] = {}.{}'.format(file, file, file))
-
-def Create(project, scenario, ppInfo, dssInstance, dssSolver, dssObjects, dssObjectsByClass, simulationSettings, Logger):
-    test = None
-    PostProcessorClass = None
-    ScriptName = ppInfo.script
-    assert (ScriptName in pythonFiles), \
-        f"Definition for '{ScriptName}' post process script not found. \n" \
-        "Please define the controller in PyDSS/pyPostprocessor/PostprocessScripts"
-    PostProcessor = POST_PROCESSES[ScriptName](
-        project,
-        scenario,
-        ppInfo,
-        dssInstance,
-        dssSolver,
-        dssObjects,
-        dssObjectsByClass,
-        simulationSettings,
-        Logger,
-    )
-    return PostProcessor
+from os.path import dirname, basename, isfile
+import glob
+
+from pydss.pyPostprocessor import PostprocessScripts
+
+
+
+
+modules = glob.glob(PostprocessScripts.__path__[0]+"/*.py")
+pythonFiles = [basename(f)[:-3] for f in modules if isfile(f) and not f.endswith('__init__.py')]
+
+POST_PROCESSES = {}
+for file in pythonFiles:
+    exec('from pydss.pyPostprocessor.PostprocessScripts import {}'.format(file))
+    exec('POST_PROCESSES["{}"] = {}.{}'.format(file, file, file))
+
+def Create(project, scenario, ppInfo, dssInstance, dssSolver, dssObjects, dssObjectsByClass, simulationSettings, Logger):
+    test = None
+    PostProcessorClass = None
+    ScriptName = ppInfo.script
+    assert (ScriptName in pythonFiles), \
+        f"Definition for '{ScriptName}' post process script not found. \n" \
+        "Please define the controller in PyDSS/pyPostprocessor/PostprocessScripts"
+    PostProcessor = POST_PROCESSES[ScriptName](
+        project,
+        scenario,
+        ppInfo,
+        dssInstance,
+        dssSolver,
+        dssObjects,
+        dssObjectsByClass,
+        simulationSettings,
+        Logger,
+    )
+    return PostProcessor
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyPostprocessor/pyPostprocessAbstract.py` & `nrel_pydss-3.1.4/src/pydss/pyPostprocessor/pyPostprocessAbstract.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,78 +1,78 @@
-
-import abc
-import os
-
-from pydss.exceptions import InvalidParameter
-from pydss.utils.utils import load_data
-
-
-class AbstractPostprocess(abc.ABC):
-    """An abstract class that serves as template for all pyPlot classes in :mod:`pydss.pyPlots.Plots` module.
-
-    :param project: A :class:`pydss.pydss_project.PyDssProject` object representing a project
-    :type project: PyDssProject
-    :param scenario: A :class:`pydss.pydss_project.PyDssScenario` object representing a scenario
-    :type scenario: PyDssScenario
-    :param inputs: user inputs
-    :type inputs: dict
-    :param dssInstance: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'Fault' element
-    :type dssInstance: dict
-    :param dssBuses: Dictionary of all :class:`pydss.dssBus.dssBus` objects in pydss
-    :type dssBuses: dict of :class:`pydss.dssBus.dssBus` objects
-    :param dssObjects: Dictionary of all :class:`pydss.dssElement.dssElement` objects in pydss
-    :type dssObjects: dict of :class:`pydss.dssElement.dssElement` objects
-    :param dssObjectsByClass:  Dictionary of all :class:`pydss.dssElement.dssElement` objects in pydss sorted by class
-    :type dssObjectsByClass: dict of :class:`pydss.dssElement.dssElement` objects
-    :param dssSolver: An instance of one of the classes defined in :mod:`pydss.SolveMode`.
-    :type dssSolver: :mod:`pydss.SolveMode`
-
-    """
-
-    def __init__(self, project, scenario, inputs, dssInstance, dssSolver, dssObjects, dssObjectsByClass, simulationSettings, logger):
-        """This is the constructor class.
-        """
-        self.project = project
-        self.scenario = scenario
-        if inputs.config_file == "":
-            self.config = {}
-        else:
-            self.config = load_data(inputs.config_file)
-        self.config["Outputs"] = project.get_post_process_directory(scenario.name)
-        os.makedirs(self.config["Outputs"], exist_ok=True)
-        self.Settings = simulationSettings
-
-        self._dssInstance = dssInstance
-        self.logger = logger
-        self._check_input_fields()
-
-    @abc.abstractmethod
-    def run(self, step, stepMax, simulation=None):
-        """Method used to run a post processing script.
-
-        Parameters
-        ----------
-        step : int
-            Current step
-        stepMax : int
-            Last step of the simulation
-        simulation : OpenDSS
-            pydss simulation control class. Provided for access to control algorithms.
-            Subclasses should not hold references to this instance after this method exits.
-
-        """
-
-    @abc.abstractmethod
-    def _get_required_input_fields(self):
-        """Return the required input fields."""
-
-    @abc.abstractmethod
-    def finalize(self):
-        """Method used to combine post processing results from all steps.
-        """
-        
-    def _check_input_fields(self):
-        required_fields = self._get_required_input_fields()
-        fields = set(self.config.keys())
-        for field in required_fields:
-            if field not in fields:
-                raise InvalidParameter(f"{self.__class__.__name__} requires input field {field}")
+
+import abc
+import os
+
+from pydss.exceptions import InvalidParameter
+from pydss.utils.utils import load_data
+
+
+class AbstractPostprocess(abc.ABC):
+    """An abstract class that serves as template for all pyPlot classes in :mod:`pydss.pyPlots.Plots` module.
+
+    :param project: A :class:`pydss.pydss_project.PyDssProject` object representing a project
+    :type project: PyDssProject
+    :param scenario: A :class:`pydss.pydss_project.PyDssScenario` object representing a scenario
+    :type scenario: PyDssScenario
+    :param inputs: user inputs
+    :type inputs: dict
+    :param dssInstance: A :class:`pydss.dssElement.dssElement` object that wraps around an OpenDSS 'Fault' element
+    :type dssInstance: dict
+    :param dssBuses: Dictionary of all :class:`pydss.dssBus.dssBus` objects in pydss
+    :type dssBuses: dict of :class:`pydss.dssBus.dssBus` objects
+    :param dssObjects: Dictionary of all :class:`pydss.dssElement.dssElement` objects in pydss
+    :type dssObjects: dict of :class:`pydss.dssElement.dssElement` objects
+    :param dssObjectsByClass:  Dictionary of all :class:`pydss.dssElement.dssElement` objects in pydss sorted by class
+    :type dssObjectsByClass: dict of :class:`pydss.dssElement.dssElement` objects
+    :param dssSolver: An instance of one of the classes defined in :mod:`pydss.SolveMode`.
+    :type dssSolver: :mod:`pydss.SolveMode`
+
+    """
+
+    def __init__(self, project, scenario, inputs, dssInstance, dssSolver, dssObjects, dssObjectsByClass, simulationSettings, logger):
+        """This is the constructor class.
+        """
+        self.project = project
+        self.scenario = scenario
+        if inputs.config_file == "":
+            self.config = {}
+        else:
+            self.config = load_data(inputs.config_file)
+        self.config["Outputs"] = project.get_post_process_directory(scenario.name)
+        os.makedirs(self.config["Outputs"], exist_ok=True)
+        self.Settings = simulationSettings
+
+        self._dssInstance = dssInstance
+        self.logger = logger
+        self._check_input_fields()
+
+    @abc.abstractmethod
+    def run(self, step, stepMax, simulation=None):
+        """Method used to run a post processing script.
+
+        Parameters
+        ----------
+        step : int
+            Current step
+        stepMax : int
+            Last step of the simulation
+        simulation : OpenDSS
+            pydss simulation control class. Provided for access to control algorithms.
+            Subclasses should not hold references to this instance after this method exits.
+
+        """
+
+    @abc.abstractmethod
+    def _get_required_input_fields(self):
+        """Return the required input fields."""
+
+    @abc.abstractmethod
+    def finalize(self):
+        """Method used to combine post processing results from all steps.
+        """
+        
+    def _check_input_fields(self):
+        required_fields = self._get_required_input_fields()
+        fields = set(self.config.keys())
+        for field in required_fields:
+            if field not in fields:
+                raise InvalidParameter(f"{self.__class__.__name__} requires input field {field}")
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyPostprocessor/PostprocessScripts/DERMSOptimizer.py` & `nrel_pydss-3.1.4/src/pydss/pyPostprocessor/PostprocessScripts/DERMSOptimizer.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,439 +1,439 @@
-from pydss.pyPostprocessor.pyPostprocessAbstract import AbstractPostprocess
-from pydss.pyPostprocessor.PostprocessScripts.DERMSOptimizer_helper_modules.opt_funcs import *
-from scipy.sparse import lil_matrix
-import scipy.sparse.linalg as sp
-import scipy.sparse as sparse
-from scipy import stats
-import pydss.dssInstance as dI
-import pandas as pd
-from math import *
-import numpy as np
-import os
-
-class DERMSOptimizer(AbstractPostprocess):
-    REQUIRED_INPUT_FIELDS_AND_DEFAULTS = {
-        "control_flag": True,
-        "control_all_flag": True,
-        "num_DERMS": 1,
-        "Vlower": 0.955,
-        "Vupper": 1.038,
-        "coeff_p": 0.0005,
-        "coeff_q": 0.000001,
-        "stepsize_xp": 1,
-        "stepsize_xq": 5,
-        "stepsize_mu": 10,
-        "opf_iteration": 60,
-        "max_iterations": 50,
-        'time_trigger_sec': 900,
-        "measurements_noises_flag": True,
-        "implementation_mode": "Voltage triggered",
-        'Distributions' : {
-            "Vmes": ["norm", [0.0, 0.01]],
-            "Imes": ["norm", [0.0, 0.01]],
-        },
-    }
-
-    IMPLEMENTATION_MODES = ["Continuous", "Voltage triggered", "Time triggered", "Off"]
-
-    def __init__(self, project, scenario, inputs, dssInstance, dssSolver, dssObjects, dssObjectsByClass, simulationSettings, Logger):
-        """Constructor method
-        """
-        super(DERMSOptimizer, self).__init__(project, scenario, inputs, dssInstance, dssSolver, dssObjects, dssObjectsByClass, simulationSettings, Logger)
-        self.Options = {**self.REQUIRED_INPUT_FIELDS_AND_DEFAULTS, **inputs}
-        self.Settings = simulationSettings
-        self.Objects = dssObjectsByClass
-
-        self.dssSolver = dssSolver
-        self.dss = dssInstance
-        self.Logger = Logger
-        self.logger.info('Creating DERMS Optimizer module')
-
-        self.rootPath = simulationSettings['Project']['Project Path']
-        self.dssPath = os.path.join(self.rootPath, simulationSettings['Project']['Active Project'], 'DSSfiles')
-
-        self.Buses = []
-        self.BusDistance = []
-        self.Vbase_allnode = []
-
-        self.initialize_optimizer()
-        self.ExportCSVfiles = True
-        self.sim_start = False
-        self.DERMS_trigger_fail_count = 0
-        self.DERMS_trigger_count = 0
-        self.DERMS_trigger_success_count = 0
-
-        PVdata = self.getDERdata()
-
-        assert self.Options["implementation_mode"] in self.IMPLEMENTATION_MODES, 'valid implementation modes are ()'.fomrmat(
-            self.IMPLEMENTATION_MODES
-        )
-        self.dermsController = DERMS(
-            PVdata, self.controlbus, self.controlelem, self.BEcapacity, self.Nodes[self.nSlack:], self.BEname
-        )
-
-    def initialize_optimizer(self):
-        self.dss.utils.run_command('calcV')
-        self.Nodes = self.dss.Circuit.YNodeOrder()
-        self.nNodes = len(self.Nodes)
-
-        self.Slack = self.Objects['Vsources']["Vsource.source"]
-        self.nSlack = int(self.Slack.GetValue("phases"))
-
-        for node in self.Nodes:
-            node = node.lower()
-            bus, phaseInfo = node.split(".", 1)
-            self.Vbase_allnode.append(self.Objects['Buses'][bus].GetVariable("kVBase") * 1000)
-            self.BusDistance.append(self.Objects['Buses'][bus].GetVariable("Distance"))
-
-        for x in self.Nodes:
-            bName = x.split('.')[0]
-            if bName not in self.Buses:
-                self.Buses.append(bName)
-        self.nBus = len(self.Buses)
-
-        Y00, Y01, Y10, Y11, Y11_sparse, Y11_inv, Ybus = self.ExtractImpednceMatrix()
-        self.getBranchInfo()
-
-        self.dssSolver.Solve()
-        V1, V1_pu = self.get_voltage_Yorder()
-
-        capNames = self.dss.Capacitors.AllNames()
-        hCapNames = ",".join(capNames)
-
-        loadData = self.get_elem_data(
-            "Loads",
-            ["Name", "kV", "kW", "pf", "phase", "bus", "phases", "VoltagesMagAng", "Powers", "conn"]
-        )
-        total_load = sum([L["kW"] for L in loadData])
-        #Do we want profile implementation here or a dedicatied profile manager?
-
-        self.PVSystem = self.get_elem_data(
-            "Generators",
-            ["Name", "bus", "busFull", "phase", "kVA", "kW", "phases", "conn"]
-        )
-        self.PVSystem_1phase = self.convert_3phasePV_to_1phasePV(self.PVSystem)
-        self.PVSystem_1phaseDF = pd.DataFrame(self.PVSystem_1phase)
-        if self.Options["control_flag"]:
-            PVlocation = []
-            NPV = len(self.PVSystem_1phase)
-            nodeIndex_withPV = []
-            PV_inverter_size = []
-            for pv in self.PVSystem_1phase:
-                allpvbus = pv["bus"].split('.')
-                PVlocation.append(allpvbus[0])
-                if len(allpvbus) == 1:
-                    allpvbus = allpvbus + ['1', '2', '3']
-                for ii in range(len(allpvbus) - 1):
-                    pvbus = allpvbus[0] + '.' + allpvbus[ii + 1]
-                    nodeIndex_withPV.append(self.Nodes.index(pvbus.upper()))
-                PV_inverter_size.append(float(pv['kVA']) / (len(allpvbus) - 1))
-
-            capacitors = self.get_elem_data("Capacitors", ["Name", "bus", "phase", "phases", "kV", "kvar"])
-            PQ_load, PQ_PV, PQ_node, Qcap = self.calc_node_PQ()
-
-            if self.Options["control_all_flag"]:
-                self.controlbus = self.Nodes[self.nSlack:]
-            else:
-                self.controlbus = self.getContolBuses(["Generators", "Capacitors"])
-
-            self.controlelem = []
-            self.mu0 = [
-                np.zeros(len(self.controlbus)),
-                np.zeros(len(self.controlbus)),
-                np.zeros(len(self.controlelem))
-            ]
-
-            self.power_flow_data = linear_powerflow_model(Y00, Y01, Y10, Y11_inv, [], V1, self.nSlack)
-
-            self.stepsize_control = [self.Options["stepsize_xp"], self.Options["stepsize_xq"],
-                                     self.Options["stepsize_mu"]]
-            self.Vlimit = [self.Options["Vupper"], self.Options["Vlower"]]
-        return
-
-    def calc_node_PQ(self):
-        PQ_load = self.get_PQ_by_class("Loads")
-        PQ_PV = self.get_PQ_by_class("Generators")
-        PQ_CAP = self.get_PQ_by_class("Capacitors")
-        Qcap = PQ_CAP.imag
-        PQ_node = - PQ_load + PQ_PV - 1j * np.array(Qcap)  # power injection
-        return PQ_load, PQ_PV, PQ_node, Qcap
-
-    def get_PQ_by_class(self, className):
-        P = [0] * len(self.Nodes)
-        Q = [0] * len(self.Nodes)
-        for name, obj in self.Objects[className].items():
-            power = obj.GetValue("Powers")
-            bus = obj.GetValue("BusNames")[0]
-            nodes = bus.split(".")[1:] if len(bus.split(".")[1:]) else [1, 2, 3]
-
-            for i, ii in enumerate(nodes):
-                nodeName = "{}.{}".format(bus.split(".")[0], ii)
-                index = self.Nodes.index(nodeName.upper())
-                P[index] = power[2 * i]
-                Q[index] = power[2 * i + 1]
-        PQ = np.array(P) + 1j * np.array(Q)
-        return PQ
-
-    def get_elem_data(self, ElementClass, Properties):
-        data = []
-        for name, obj in self.Objects[ElementClass].items():
-            datum = {}
-            for ppty in Properties:
-                if ppty == "bus":
-                    datum[ppty] = obj.GetValue("BusNames")[0].split(".")[0]
-                elif ppty == "phase":
-                    phases = obj.GetValue("BusNames")[0].split(".")[1:]
-                    datum[ppty] = phases if len(phases) else [1, 2, 3]
-                elif ppty == "busFull":
-                    datum[ppty] = obj.GetValue("BusNames")[0]
-                else:
-                    datum[ppty] = obj.GetValue(ppty)
-            data.append(datum)
-        return data
-
-    def convert_3phasePV_to_1phasePV(self, PVSystems):
-        # convert multi-phase PV into multiple 1-phase PVs for control implementation purpose
-        PVSystem_1phase = []
-        for pv in PVSystems:
-            for i, ii in enumerate(pv["phase"]):
-                pv_perphase = {}
-                pv_perphase["Name"] = pv["Name"]  # +'_node'+bus[ii+1]
-                pv_perphase["bus"] = pv["bus"] + '.' + ii
-                pv_perphase["kW"] = pv["kW"] / pv["phases"]
-                pv_perphase["kVA"] = pv["kVA"] / pv["phases"]
-                PVSystem_1phase.append(pv_perphase)
-        return PVSystem_1phase
-
-    def get_incidence_matrix(self):
-        #self.dss.utils.run_command('solve mode=fault') dont think this is needed. May be wrong
-        Ybranch_prim, branch_node_incidence, nNeutral, record_index = self.construct_Yprime()
-        current_coeff_matrix = np.dot(Ybranch_prim, branch_node_incidence)
-        current_coeff_matrix = current_coeff_matrix[record_index, :-nNeutral]
-        branch_node_incidence = branch_node_incidence[record_index, :-nNeutral]
-        return current_coeff_matrix, branch_node_incidence, current_coeff_matrix
-
-    def ExtractImpednceMatrix(self):
-        Y = self.dss.Circuit.SystemY()
-        Ybus = self.to_complex(Y)
-        Y00 = Ybus[0:self.nSlack, 0:self.nSlack]
-        Y01 = Ybus[0:self.nSlack, self.nSlack:]
-        Y10 = Ybus[self.nSlack:, 0:self.nSlack]
-        Y11 = Ybus[self.nSlack:, self.nSlack:]
-        Y11_sparse = lil_matrix(Y11)
-        Y11_sparse = Y11_sparse.tocsr()
-        a_sps = sparse.csc_matrix(Y11)
-        lu_obj = sp.splu(a_sps)
-        Y11_inv = lu_obj.solve(np.eye(self.nNodes - self.nSlack))
-        return Y00, Y01, Y10, Y11, Y11_sparse, Y11_inv, Ybus
-
-    def get_voltage_Yorder(self):
-        temp_Vbus = self.dss.Circuit.YNodeVArray()
-        voltage = [complex(0, 0)] * self.nNodes
-        for ii in range(self.nNodes):
-            voltage[ii] = complex(temp_Vbus[ii * 2], temp_Vbus[ii * 2 + 1])
-        voltage_pu = list(map(lambda x: abs(x[0]) / x[1], zip(voltage, self.Vbase_allnode)))
-        return voltage, voltage_pu
-
-    def to_complex(self, values):
-        Ydim = int(sqrt(len(values) / 2))
-        Yreal = np.array(values[0::2]).reshape((Ydim, Ydim))
-        Yimag = np.array(values[1::2]).reshape((Ydim, Ydim))
-        Ycmplx = Yreal + 1j * Yimag
-        return Ycmplx
-
-    def construct_Yprime(self):
-        Ybranch_prim = np.array([[complex(0, 0)] * 2 * self.nBarnchElements] * 2 * self.nBarnchElements)
-        branch_node_incidence = np.zeros([2 * self.nBarnchElements, self.nNodes + 422])  #TODO fix hard coded 422 for Green, 104 for Diamond
-
-        record_index = []
-        nNeutral = 0
-        start_no = 0
-        count = 0
-
-        temp_AllNodeNames = self.dss.Circuit.YNodeOrder()
-        for elemName, elmObj in self.branchElements.items():
-            values = elmObj.GetValue('YPrim')
-            Yprim = self.to_complex(values)
-            buses = [ii.split('.')[0] for ii in elmObj.GetValue('BusNames')]
-            nodes = elmObj.GetValue('NodeOrder')
-            nPhases = int(len(nodes) / 2)
-            nNeutral += nodes.count(0)
-            for ii in range(nPhases):
-                from_node = buses[0] + '.' + str(nodes[ii])
-                to_node = buses[1] + '.' + str(nodes[ii + nPhases])
-                if nodes[ii] == 0:
-                    temp_AllNodeNames.append(from_node.upper())
-                if nodes[ii + nPhases] == 0:
-                    temp_AllNodeNames.append(to_node.upper())
-                from_node_index = temp_AllNodeNames.index(from_node.upper())
-                to_node_index = temp_AllNodeNames.index(to_node.upper())
-                branch_node_incidence[2 * count + ii, from_node_index] = 1
-                branch_node_incidence[2 * count + nPhases + ii, to_node_index] = 1
-                record_index.append(2 * count + ii)
-            count = count + nPhases
-            end_no = start_no + 2 * nPhases
-            Ybranch_prim[start_no:end_no, start_no:end_no] = Yprim
-            start_no = end_no
-
-        return Ybranch_prim, branch_node_incidence, nNeutral, record_index
-
-    def getContolBuses(self, classNames):
-        controlbus = []
-        for className in classNames:
-            for elm in self.get_elem_data(className, ["bus", "phases"]):
-                for phase in elm["phases"]:
-                    controlbus.append("{}.{}".format(elm["bus"], phase))
-        return controlbus
-
-    @staticmethod
-    def _get_required_input_fields():
-        return {}
-
-    def getBranchInfo(self):
-        self.branchElements = {**self.Objects["Lines"], **self.Objects["Transformers"]}
-        self.nBarnchElements = len(self.branchElements)
-        self.BEindex = []
-        self.BEcapacity = []
-        self.BEname = []
-
-        for elmName, elmObj in self.branchElements.items():
-            nTerms = elmObj.GetValue("NumTerminals")
-            Nodes = elmObj.GetValue("NodeOrder")
-            nPhases = int(len(Nodes) / nTerms)
-            self.BEindex.append(list(range(nPhases)))
-            for ii in range(nPhases):
-                self.BEcapacity.append(elmObj.GetValue("NormalAmps"))
-                self.BEname.append("{}.{}".format(elmName, Nodes[ii]))
-
-    def getDERdata(self):
-        data = self.get_elem_data("Generators", ["Name", "busFull", "phase", "kVA", "kW", "phases", "conn"])
-        data = pd.DataFrame(data)
-        data.index = data["Name"]
-        PVdata = {
-            "pvName": [],
-            "pvLocation": [],
-            "bus": [],
-            "pvSize": [],
-            "inverterSize": [],
-        }
-        for pv1P in self.PVSystem_1phaseDF["Name"].tolist():
-            pv = data.loc[pv1P]
-            PVdata["pvName"].append(pv['Name'])
-            PVdata["bus"].append(pv['busFull'])
-            PVdata["pvSize"].append(pv['kW'])
-            PVdata["inverterSize"].append(pv['kVA'])
-            for b in self.buses(pv):
-                if b.upper() not in PVdata["pvLocation"]:
-                    PVdata["pvLocation"].append(b.upper())
-                    break
-        return PVdata
-
-    def buses(self, pv):
-        bus = pv['busFull'].split(".")
-        if len(bus) == 1:
-            bus += ["1", "2", "3"]
-        return [f"{bus[0]}.{i}" for i in bus[1:]]
-
-    def run(self, step, stepMax, simulation=None):
-        """Induces and removes a fault as the simulation runs as per user defined settings.
-        """
-        self.logger.info('Running DERMS optimization module')
-        opt_iter = 0
-
-        #Really bad codeing form
-        mu0 = [
-            [0 for x in range(len(self.controlbus))],
-            [0 for x in range(len(self.controlbus))],
-            [0 for x in range(len(self.controlelem))]
-        ]
-
-        PVmax = [pv["kW"] if pv["kW"] < pv["kVA"] else pv["kVA"] for pv in self.PVSystem_1phase]
-
-        while opt_iter < self.Options["max_iterations"]:
-            self.dssSolver.reSolve()
-            PVlocation, PVpower, Vmes, Imes = self.dermsController.monitor(self._dssInstance, self.Objects,
-                                                                           self.PVSystem_1phaseDF)
-            self.Logger.info('Voltage measurements: {}'.format(len(Vmes)))
-            self.Logger.info('Maximum voltage: {}'.format(max(Vmes)))
-
-            if self.Options["measurements_noises_flag"]:
-                distName, distParams = self.Options['Distributions']["Vmes"]
-                dist = getattr(stats, distName)
-                Vmes = Vmes + dist.rvs(*distParams, size=len(Vmes))
-                distName, distParams = self.Options['Distributions']["Imes"]
-                dist = getattr(stats, distName)
-                Imes = Imes + dist.rvs(*distParams, size=len(Imes))
-
-            if self.Options["implementation_mode"] == "Continuous":
-                DERMS_trigger = 1
-            elif self.Options["implementation_mode"] == "Voltage triggered":
-                if max(Vmes) > self.Options["Vupper"] or min(Vmes) < self.Options["Vlower"]:
-                    DERMS_trigger = 1
-                    #TODO: Should DERMS trigger count be in this if statement?
-                elif max(Vmes) <= self.Options["Vupper"] and min(Vmes) >= self.Options["Vlower"] and opt_iter >= 1:
-                    DERMS_trigger = 0
-                    self.DERMS_trigger_count += 1
-                    self.DERMS_trigger_success_count += 1
-                    self.Logger.info('DERMS OPF successfully triggered, maxV: {}'.fotmat(max(Vmes)))
-                    break
-                else:
-                    DERMS_trigger = 0
-            elif self.Options["implementation_mode"] == "Time triggered":
-                #TODO: Is step * stepsize_sim >= time_mode_resolution required? Is the idea to ensure that the controller does not run on the first iteration?
-                if step > 0 and self.dssSolver.GetTotalSeconds() % self.Options["time_trigger_sec"] == 0:
-                    DERMS_trigger = 1
-                else:
-                    DERMS_trigger = 0
-            elif self.Options["implementation_mode"] == "Off":
-                DERMS_trigger = 0
-
-            if DERMS_trigger == 1:
-
-                [x1, mu1] = self.dermsController.control(
-                    self.power_flow_data,
-                    self.Options,
-                    self.stepsize_control,
-                    mu0,
-                    self.Vlimit,
-                    PVpower,
-                    Imes,
-                    Vmes,
-                    PVmax
-                )
-                # ------ apply the setpoint ------
-                nPV_1_phs = len(self.PVSystem_1phase)
-                for pv in self.PVSystem:
-                    idx = [i for i, x in enumerate(self.PVSystem_1phase) if x["Name"] == pv["Name"]]
-                    self.Objects["Generators"][pv["Name"]].SetParameter("kW", sum([x1[ii] for ii in idx]))
-                    self.Objects["Generators"][pv["Name"]].SetParameter("kvar",  sum([x1[ii + nPV_1_phs] for ii in idx]))
-
-                mu0 = mu1
-                # resV_record.append(max(Vmes))
-                opt_iter = opt_iter + 1
-                if max(Vmes) <= self.Options["Vupper"] and min(Vmes) >= self.Options["Vlower"]:
-                    self.DERMS_trigger_count += 1
-                    self.DERMS_trigger_success_count += 1
-                    self.Logger.info('DERMS OPF successfully triggered')
-                    break
-            elif DERMS_trigger == 0:
-                # TODO: Is this piece of code required?
-                # for pv in self.PVSystem:
-                #     PVgen = float(pv['kW']) * PVshape[
-                #         int((present_step - 1) * stepsize_sim / data_resolution + 3600 / data_resolution * startH)]
-                #     if PVgen > float(pv["kVA"]):
-                #         PVgen = float(pv["kVA"])
-                #     dss.run_command('edit ' + str(pv["name"]) + ' kW=' + str(PVgen) + ' pf=' + str(pf_auto))
-                break
-            if opt_iter == self.Options["max_iterations"]:
-                self.DERMS_trigger_count += 1
-                if max(Vmes) > self.Options["Vupper"] or min(Vmes) < self.Options["Vlower"]:
-                    self.DERMS_trigger_fail_count += 1
-                    self.Logger.warning('DERMS OPF triggered and failed')
-
-
-            opt_iter += 1
-        #step-=1 # uncomment the line if the post process needs to rerun for the same point in time
-        return step
-
-    def getPreoptimizationResults(self):
+from pydss.pyPostprocessor.pyPostprocessAbstract import AbstractPostprocess
+from pydss.pyPostprocessor.PostprocessScripts.DERMSOptimizer_helper_modules.opt_funcs import *
+from scipy.sparse import lil_matrix
+import scipy.sparse.linalg as sp
+import scipy.sparse as sparse
+from scipy import stats
+import pydss.dssInstance as dI
+import pandas as pd
+from math import *
+import numpy as np
+import os
+
+class DERMSOptimizer(AbstractPostprocess):
+    REQUIRED_INPUT_FIELDS_AND_DEFAULTS = {
+        "control_flag": True,
+        "control_all_flag": True,
+        "num_DERMS": 1,
+        "Vlower": 0.955,
+        "Vupper": 1.038,
+        "coeff_p": 0.0005,
+        "coeff_q": 0.000001,
+        "stepsize_xp": 1,
+        "stepsize_xq": 5,
+        "stepsize_mu": 10,
+        "opf_iteration": 60,
+        "max_iterations": 50,
+        'time_trigger_sec': 900,
+        "measurements_noises_flag": True,
+        "implementation_mode": "Voltage triggered",
+        'Distributions' : {
+            "Vmes": ["norm", [0.0, 0.01]],
+            "Imes": ["norm", [0.0, 0.01]],
+        },
+    }
+
+    IMPLEMENTATION_MODES = ["Continuous", "Voltage triggered", "Time triggered", "Off"]
+
+    def __init__(self, project, scenario, inputs, dssInstance, dssSolver, dssObjects, dssObjectsByClass, simulationSettings, Logger):
+        """Constructor method
+        """
+        super(DERMSOptimizer, self).__init__(project, scenario, inputs, dssInstance, dssSolver, dssObjects, dssObjectsByClass, simulationSettings, Logger)
+        self.Options = {**self.REQUIRED_INPUT_FIELDS_AND_DEFAULTS, **inputs}
+        self.Settings = simulationSettings
+        self.Objects = dssObjectsByClass
+
+        self.dssSolver = dssSolver
+        self.dss = dssInstance
+        self.Logger = Logger
+        self.logger.info('Creating DERMS Optimizer module')
+
+        self.rootPath = simulationSettings['Project']['Project Path']
+        self.dssPath = os.path.join(self.rootPath, simulationSettings['Project']['Active Project'], 'DSSfiles')
+
+        self.Buses = []
+        self.BusDistance = []
+        self.Vbase_allnode = []
+
+        self.initialize_optimizer()
+        self.ExportCSVfiles = True
+        self.sim_start = False
+        self.DERMS_trigger_fail_count = 0
+        self.DERMS_trigger_count = 0
+        self.DERMS_trigger_success_count = 0
+
+        PVdata = self.getDERdata()
+
+        assert self.Options["implementation_mode"] in self.IMPLEMENTATION_MODES, 'valid implementation modes are ()'.fomrmat(
+            self.IMPLEMENTATION_MODES
+        )
+        self.dermsController = DERMS(
+            PVdata, self.controlbus, self.controlelem, self.BEcapacity, self.Nodes[self.nSlack:], self.BEname
+        )
+
+    def initialize_optimizer(self):
+        self.dss.utils.run_command('calcV')
+        self.Nodes = self.dss.Circuit.YNodeOrder()
+        self.nNodes = len(self.Nodes)
+
+        self.Slack = self.Objects['Vsources']["Vsource.source"]
+        self.nSlack = int(self.Slack.GetValue("phases"))
+
+        for node in self.Nodes:
+            node = node.lower()
+            bus, phaseInfo = node.split(".", 1)
+            self.Vbase_allnode.append(self.Objects['Buses'][bus].GetVariable("kVBase") * 1000)
+            self.BusDistance.append(self.Objects['Buses'][bus].GetVariable("Distance"))
+
+        for x in self.Nodes:
+            bName = x.split('.')[0]
+            if bName not in self.Buses:
+                self.Buses.append(bName)
+        self.nBus = len(self.Buses)
+
+        Y00, Y01, Y10, Y11, Y11_sparse, Y11_inv, Ybus = self.ExtractImpednceMatrix()
+        self.getBranchInfo()
+
+        self.dssSolver.Solve()
+        V1, V1_pu = self.get_voltage_Yorder()
+
+        capNames = self.dss.Capacitors.AllNames()
+        hCapNames = ",".join(capNames)
+
+        loadData = self.get_elem_data(
+            "Loads",
+            ["Name", "kV", "kW", "pf", "phase", "bus", "phases", "VoltagesMagAng", "Powers", "conn"]
+        )
+        total_load = sum([L["kW"] for L in loadData])
+        #Do we want profile implementation here or a dedicatied profile manager?
+
+        self.PVSystem = self.get_elem_data(
+            "Generators",
+            ["Name", "bus", "busFull", "phase", "kVA", "kW", "phases", "conn"]
+        )
+        self.PVSystem_1phase = self.convert_3phasePV_to_1phasePV(self.PVSystem)
+        self.PVSystem_1phaseDF = pd.DataFrame(self.PVSystem_1phase)
+        if self.Options["control_flag"]:
+            PVlocation = []
+            NPV = len(self.PVSystem_1phase)
+            nodeIndex_withPV = []
+            PV_inverter_size = []
+            for pv in self.PVSystem_1phase:
+                allpvbus = pv["bus"].split('.')
+                PVlocation.append(allpvbus[0])
+                if len(allpvbus) == 1:
+                    allpvbus = allpvbus + ['1', '2', '3']
+                for ii in range(len(allpvbus) - 1):
+                    pvbus = allpvbus[0] + '.' + allpvbus[ii + 1]
+                    nodeIndex_withPV.append(self.Nodes.index(pvbus.upper()))
+                PV_inverter_size.append(float(pv['kVA']) / (len(allpvbus) - 1))
+
+            capacitors = self.get_elem_data("Capacitors", ["Name", "bus", "phase", "phases", "kV", "kvar"])
+            PQ_load, PQ_PV, PQ_node, Qcap = self.calc_node_PQ()
+
+            if self.Options["control_all_flag"]:
+                self.controlbus = self.Nodes[self.nSlack:]
+            else:
+                self.controlbus = self.getContolBuses(["Generators", "Capacitors"])
+
+            self.controlelem = []
+            self.mu0 = [
+                np.zeros(len(self.controlbus)),
+                np.zeros(len(self.controlbus)),
+                np.zeros(len(self.controlelem))
+            ]
+
+            self.power_flow_data = linear_powerflow_model(Y00, Y01, Y10, Y11_inv, [], V1, self.nSlack)
+
+            self.stepsize_control = [self.Options["stepsize_xp"], self.Options["stepsize_xq"],
+                                     self.Options["stepsize_mu"]]
+            self.Vlimit = [self.Options["Vupper"], self.Options["Vlower"]]
+        return
+
+    def calc_node_PQ(self):
+        PQ_load = self.get_PQ_by_class("Loads")
+        PQ_PV = self.get_PQ_by_class("Generators")
+        PQ_CAP = self.get_PQ_by_class("Capacitors")
+        Qcap = PQ_CAP.imag
+        PQ_node = - PQ_load + PQ_PV - 1j * np.array(Qcap)  # power injection
+        return PQ_load, PQ_PV, PQ_node, Qcap
+
+    def get_PQ_by_class(self, className):
+        P = [0] * len(self.Nodes)
+        Q = [0] * len(self.Nodes)
+        for name, obj in self.Objects[className].items():
+            power = obj.GetValue("Powers")
+            bus = obj.GetValue("BusNames")[0]
+            nodes = bus.split(".")[1:] if len(bus.split(".")[1:]) else [1, 2, 3]
+
+            for i, ii in enumerate(nodes):
+                nodeName = "{}.{}".format(bus.split(".")[0], ii)
+                index = self.Nodes.index(nodeName.upper())
+                P[index] = power[2 * i]
+                Q[index] = power[2 * i + 1]
+        PQ = np.array(P) + 1j * np.array(Q)
+        return PQ
+
+    def get_elem_data(self, ElementClass, Properties):
+        data = []
+        for name, obj in self.Objects[ElementClass].items():
+            datum = {}
+            for ppty in Properties:
+                if ppty == "bus":
+                    datum[ppty] = obj.GetValue("BusNames")[0].split(".")[0]
+                elif ppty == "phase":
+                    phases = obj.GetValue("BusNames")[0].split(".")[1:]
+                    datum[ppty] = phases if len(phases) else [1, 2, 3]
+                elif ppty == "busFull":
+                    datum[ppty] = obj.GetValue("BusNames")[0]
+                else:
+                    datum[ppty] = obj.GetValue(ppty)
+            data.append(datum)
+        return data
+
+    def convert_3phasePV_to_1phasePV(self, PVSystems):
+        # convert multi-phase PV into multiple 1-phase PVs for control implementation purpose
+        PVSystem_1phase = []
+        for pv in PVSystems:
+            for i, ii in enumerate(pv["phase"]):
+                pv_perphase = {}
+                pv_perphase["Name"] = pv["Name"]  # +'_node'+bus[ii+1]
+                pv_perphase["bus"] = pv["bus"] + '.' + ii
+                pv_perphase["kW"] = pv["kW"] / pv["phases"]
+                pv_perphase["kVA"] = pv["kVA"] / pv["phases"]
+                PVSystem_1phase.append(pv_perphase)
+        return PVSystem_1phase
+
+    def get_incidence_matrix(self):
+        #self.dss.utils.run_command('solve mode=fault') dont think this is needed. May be wrong
+        Ybranch_prim, branch_node_incidence, nNeutral, record_index = self.construct_Yprime()
+        current_coeff_matrix = np.dot(Ybranch_prim, branch_node_incidence)
+        current_coeff_matrix = current_coeff_matrix[record_index, :-nNeutral]
+        branch_node_incidence = branch_node_incidence[record_index, :-nNeutral]
+        return current_coeff_matrix, branch_node_incidence, current_coeff_matrix
+
+    def ExtractImpednceMatrix(self):
+        Y = self.dss.Circuit.SystemY()
+        Ybus = self.to_complex(Y)
+        Y00 = Ybus[0:self.nSlack, 0:self.nSlack]
+        Y01 = Ybus[0:self.nSlack, self.nSlack:]
+        Y10 = Ybus[self.nSlack:, 0:self.nSlack]
+        Y11 = Ybus[self.nSlack:, self.nSlack:]
+        Y11_sparse = lil_matrix(Y11)
+        Y11_sparse = Y11_sparse.tocsr()
+        a_sps = sparse.csc_matrix(Y11)
+        lu_obj = sp.splu(a_sps)
+        Y11_inv = lu_obj.solve(np.eye(self.nNodes - self.nSlack))
+        return Y00, Y01, Y10, Y11, Y11_sparse, Y11_inv, Ybus
+
+    def get_voltage_Yorder(self):
+        temp_Vbus = self.dss.Circuit.YNodeVArray()
+        voltage = [complex(0, 0)] * self.nNodes
+        for ii in range(self.nNodes):
+            voltage[ii] = complex(temp_Vbus[ii * 2], temp_Vbus[ii * 2 + 1])
+        voltage_pu = list(map(lambda x: abs(x[0]) / x[1], zip(voltage, self.Vbase_allnode)))
+        return voltage, voltage_pu
+
+    def to_complex(self, values):
+        Ydim = int(sqrt(len(values) / 2))
+        Yreal = np.array(values[0::2]).reshape((Ydim, Ydim))
+        Yimag = np.array(values[1::2]).reshape((Ydim, Ydim))
+        Ycmplx = Yreal + 1j * Yimag
+        return Ycmplx
+
+    def construct_Yprime(self):
+        Ybranch_prim = np.array([[complex(0, 0)] * 2 * self.nBarnchElements] * 2 * self.nBarnchElements)
+        branch_node_incidence = np.zeros([2 * self.nBarnchElements, self.nNodes + 422])  #TODO fix hard coded 422 for Green, 104 for Diamond
+
+        record_index = []
+        nNeutral = 0
+        start_no = 0
+        count = 0
+
+        temp_AllNodeNames = self.dss.Circuit.YNodeOrder()
+        for elemName, elmObj in self.branchElements.items():
+            values = elmObj.GetValue('YPrim')
+            Yprim = self.to_complex(values)
+            buses = [ii.split('.')[0] for ii in elmObj.GetValue('BusNames')]
+            nodes = elmObj.GetValue('NodeOrder')
+            nPhases = int(len(nodes) / 2)
+            nNeutral += nodes.count(0)
+            for ii in range(nPhases):
+                from_node = buses[0] + '.' + str(nodes[ii])
+                to_node = buses[1] + '.' + str(nodes[ii + nPhases])
+                if nodes[ii] == 0:
+                    temp_AllNodeNames.append(from_node.upper())
+                if nodes[ii + nPhases] == 0:
+                    temp_AllNodeNames.append(to_node.upper())
+                from_node_index = temp_AllNodeNames.index(from_node.upper())
+                to_node_index = temp_AllNodeNames.index(to_node.upper())
+                branch_node_incidence[2 * count + ii, from_node_index] = 1
+                branch_node_incidence[2 * count + nPhases + ii, to_node_index] = 1
+                record_index.append(2 * count + ii)
+            count = count + nPhases
+            end_no = start_no + 2 * nPhases
+            Ybranch_prim[start_no:end_no, start_no:end_no] = Yprim
+            start_no = end_no
+
+        return Ybranch_prim, branch_node_incidence, nNeutral, record_index
+
+    def getContolBuses(self, classNames):
+        controlbus = []
+        for className in classNames:
+            for elm in self.get_elem_data(className, ["bus", "phases"]):
+                for phase in elm["phases"]:
+                    controlbus.append("{}.{}".format(elm["bus"], phase))
+        return controlbus
+
+    @staticmethod
+    def _get_required_input_fields():
+        return {}
+
+    def getBranchInfo(self):
+        self.branchElements = {**self.Objects["Lines"], **self.Objects["Transformers"]}
+        self.nBarnchElements = len(self.branchElements)
+        self.BEindex = []
+        self.BEcapacity = []
+        self.BEname = []
+
+        for elmName, elmObj in self.branchElements.items():
+            nTerms = elmObj.GetValue("NumTerminals")
+            Nodes = elmObj.GetValue("NodeOrder")
+            nPhases = int(len(Nodes) / nTerms)
+            self.BEindex.append(list(range(nPhases)))
+            for ii in range(nPhases):
+                self.BEcapacity.append(elmObj.GetValue("NormalAmps"))
+                self.BEname.append("{}.{}".format(elmName, Nodes[ii]))
+
+    def getDERdata(self):
+        data = self.get_elem_data("Generators", ["Name", "busFull", "phase", "kVA", "kW", "phases", "conn"])
+        data = pd.DataFrame(data)
+        data.index = data["Name"]
+        PVdata = {
+            "pvName": [],
+            "pvLocation": [],
+            "bus": [],
+            "pvSize": [],
+            "inverterSize": [],
+        }
+        for pv1P in self.PVSystem_1phaseDF["Name"].tolist():
+            pv = data.loc[pv1P]
+            PVdata["pvName"].append(pv['Name'])
+            PVdata["bus"].append(pv['busFull'])
+            PVdata["pvSize"].append(pv['kW'])
+            PVdata["inverterSize"].append(pv['kVA'])
+            for b in self.buses(pv):
+                if b.upper() not in PVdata["pvLocation"]:
+                    PVdata["pvLocation"].append(b.upper())
+                    break
+        return PVdata
+
+    def buses(self, pv):
+        bus = pv['busFull'].split(".")
+        if len(bus) == 1:
+            bus += ["1", "2", "3"]
+        return [f"{bus[0]}.{i}" for i in bus[1:]]
+
+    def run(self, step, stepMax, simulation=None):
+        """Induces and removes a fault as the simulation runs as per user defined settings.
+        """
+        self.logger.info('Running DERMS optimization module')
+        opt_iter = 0
+
+        #Really bad codeing form
+        mu0 = [
+            [0 for x in range(len(self.controlbus))],
+            [0 for x in range(len(self.controlbus))],
+            [0 for x in range(len(self.controlelem))]
+        ]
+
+        PVmax = [pv["kW"] if pv["kW"] < pv["kVA"] else pv["kVA"] for pv in self.PVSystem_1phase]
+
+        while opt_iter < self.Options["max_iterations"]:
+            self.dssSolver.reSolve()
+            PVlocation, PVpower, Vmes, Imes = self.dermsController.monitor(self._dssInstance, self.Objects,
+                                                                           self.PVSystem_1phaseDF)
+            self.Logger.info('Voltage measurements: {}'.format(len(Vmes)))
+            self.Logger.info('Maximum voltage: {}'.format(max(Vmes)))
+
+            if self.Options["measurements_noises_flag"]:
+                distName, distParams = self.Options['Distributions']["Vmes"]
+                dist = getattr(stats, distName)
+                Vmes = Vmes + dist.rvs(*distParams, size=len(Vmes))
+                distName, distParams = self.Options['Distributions']["Imes"]
+                dist = getattr(stats, distName)
+                Imes = Imes + dist.rvs(*distParams, size=len(Imes))
+
+            if self.Options["implementation_mode"] == "Continuous":
+                DERMS_trigger = 1
+            elif self.Options["implementation_mode"] == "Voltage triggered":
+                if max(Vmes) > self.Options["Vupper"] or min(Vmes) < self.Options["Vlower"]:
+                    DERMS_trigger = 1
+                    #TODO: Should DERMS trigger count be in this if statement?
+                elif max(Vmes) <= self.Options["Vupper"] and min(Vmes) >= self.Options["Vlower"] and opt_iter >= 1:
+                    DERMS_trigger = 0
+                    self.DERMS_trigger_count += 1
+                    self.DERMS_trigger_success_count += 1
+                    self.Logger.info('DERMS OPF successfully triggered, maxV: {}'.fotmat(max(Vmes)))
+                    break
+                else:
+                    DERMS_trigger = 0
+            elif self.Options["implementation_mode"] == "Time triggered":
+                #TODO: Is step * stepsize_sim >= time_mode_resolution required? Is the idea to ensure that the controller does not run on the first iteration?
+                if step > 0 and self.dssSolver.GetTotalSeconds() % self.Options["time_trigger_sec"] == 0:
+                    DERMS_trigger = 1
+                else:
+                    DERMS_trigger = 0
+            elif self.Options["implementation_mode"] == "Off":
+                DERMS_trigger = 0
+
+            if DERMS_trigger == 1:
+
+                [x1, mu1] = self.dermsController.control(
+                    self.power_flow_data,
+                    self.Options,
+                    self.stepsize_control,
+                    mu0,
+                    self.Vlimit,
+                    PVpower,
+                    Imes,
+                    Vmes,
+                    PVmax
+                )
+                # ------ apply the setpoint ------
+                nPV_1_phs = len(self.PVSystem_1phase)
+                for pv in self.PVSystem:
+                    idx = [i for i, x in enumerate(self.PVSystem_1phase) if x["Name"] == pv["Name"]]
+                    self.Objects["Generators"][pv["Name"]].SetParameter("kW", sum([x1[ii] for ii in idx]))
+                    self.Objects["Generators"][pv["Name"]].SetParameter("kvar",  sum([x1[ii + nPV_1_phs] for ii in idx]))
+
+                mu0 = mu1
+                # resV_record.append(max(Vmes))
+                opt_iter = opt_iter + 1
+                if max(Vmes) <= self.Options["Vupper"] and min(Vmes) >= self.Options["Vlower"]:
+                    self.DERMS_trigger_count += 1
+                    self.DERMS_trigger_success_count += 1
+                    self.Logger.info('DERMS OPF successfully triggered')
+                    break
+            elif DERMS_trigger == 0:
+                # TODO: Is this piece of code required?
+                # for pv in self.PVSystem:
+                #     PVgen = float(pv['kW']) * PVshape[
+                #         int((present_step - 1) * stepsize_sim / data_resolution + 3600 / data_resolution * startH)]
+                #     if PVgen > float(pv["kVA"]):
+                #         PVgen = float(pv["kVA"])
+                #     dss.run_command('edit ' + str(pv["name"]) + ' kW=' + str(PVgen) + ' pf=' + str(pf_auto))
+                break
+            if opt_iter == self.Options["max_iterations"]:
+                self.DERMS_trigger_count += 1
+                if max(Vmes) > self.Options["Vupper"] or min(Vmes) < self.Options["Vlower"]:
+                    self.DERMS_trigger_fail_count += 1
+                    self.Logger.warning('DERMS OPF triggered and failed')
+
+
+            opt_iter += 1
+        #step-=1 # uncomment the line if the post process needs to rerun for the same point in time
+        return step
+
+    def getPreoptimizationResults(self):
         return
```

### Comparing `nrel_pydss-3.1.3/src/pydss/pyPostprocessor/PostprocessScripts/DERMSOptimizer_helper_modules/opt_funcs.py` & `nrel_pydss-3.1.4/src/pydss/pyPostprocessor/PostprocessScripts/DERMSOptimizer_helper_modules/opt_funcs.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,439 +1,439 @@
-import numpy as np
-from scipy.sparse import lil_matrix
-import scipy.sparse.linalg as sp
-import scipy.sparse as sparse
-import math
-import csv
-import matplotlib.pyplot as plt
-
-def linear_powerflow_model(Y00,Y01,Y10,Y11_inv,I_coeff,V1,slack_no):
-    # voltage linearlization
-    V1_conj = np.conj(V1[slack_no:])
-    V1_conj_inv = 1 / V1_conj
-    coeff_V = Y11_inv * V1_conj_inv
-    coeff_V_P = coeff_V
-    coeff_V_Q = -1j*coeff_V
-    coeff_Vm = -np.dot(Y11_inv,np.dot(Y10,V1[:slack_no]))
-
-    # voltage magnitude linearization
-    m = coeff_Vm
-    m_inv = 1 / coeff_Vm
-    coeff_Vmag_k = abs(m)
-    A = (np.multiply(coeff_V.transpose(),m_inv)).transpose()
-    coeff_Vmag_P = (np.multiply(A.real.transpose(),coeff_Vmag_k)).transpose()
-    coeff_Vmag_Q = (np.multiply((-1j*A).real.transpose(),coeff_Vmag_k)).transpose()
-
-    # current linearization
-    if len(I_coeff):
-        coeff_I_P = np.dot(I_coeff[:,slack_no:],coeff_V_P)
-        coeff_I_Q = np.dot(I_coeff[:,slack_no:],coeff_V_Q)
-        coeff_I_const = np.dot(I_coeff[:,slack_no:],coeff_Vm) + np.dot(I_coeff[:,:slack_no],V1[:slack_no])
-    else:
-        coeff_I_P = []
-        coeff_I_Q = []
-        coeff_I_const = []
-
-    #=========================================Yiyun's Notes===========================================#
-    # Output relations: Vmag = coeff_Vmag_P * Pnode + coeff_Vmag_Q * Qnode + coeff_Vm
-    #                      I = coeff_I_P * Pnode + coeff_I_Q * Qnode + coeff_I_const (complex value)
-    # ================================================================================================#
-
-    return coeff_V_P, coeff_V_Q, coeff_Vm, coeff_Vmag_P, coeff_Vmag_Q, coeff_Vmag_k, coeff_I_P, coeff_I_Q, coeff_I_const
-
-def validate_linear_model(coeff_Vp,coeff_Vq,coeff_Vm,PQ_node,slack_number):
-    V_cal = coeff_Vm + np.dot(coeff_Vp,np.array([np.real(ii)*1000 for ii in PQ_node[slack_number:]])) + np.dot(coeff_Vq,np.array([np.imag(ii)*1000 for ii in PQ_node[slack_number:]]))
-    v_cal_1 = coeff_Vm + np.dot(coeff_Vp,np.conj(PQ_node[slack_number:]*1000))
-    #coeff_Vp*Pnode + coeff_Vq*Qnode + coeff_Vm
-
-    # =========================================Yiyun's Notes===========================================#
-    # 1000 should be the S base
-    # =================================================================================================#
-
-    return [V_cal,v_cal_1]
-
-def check_VI_correct(V1,PQ_node,slack_number,coeff_V,coeff_Vm,coeff_Vmag_P,coeff_Vmag_Q,coeff_Vmag_k,Y10,Y11,coeff_I_P, coeff_I_Q, coeff_I_const,I_coeff):
-    V1_linear = np.dot(coeff_V,np.conj(PQ_node[slack_number:]*1000)) + coeff_Vm
-    V1_linear = list(V1_linear)
-    Vdiff = list(map(lambda x: abs(x[0]-x[1])/abs(x[0])*100,zip(V1[slack_number:],V1_linear)))
-    with open('voltage_diff.csv','w') as f:
-        csvwriter = csv.writer(f)
-        csvwriter.writerow(Vdiff)
-    f.close()
-
-    V1_mag_linear = np.dot(coeff_Vmag_P,(PQ_node[slack_number:]*1000).real) + np.dot(coeff_Vmag_Q,(PQ_node[slack_number:]*1000).imag) + coeff_Vmag_k
-    V1_mag_linear = list(V1_mag_linear)
-    Vdiff = list(map(lambda x: abs(abs(x[0])-x[1])/abs(x[0])*100,zip(V1[slack_number:],V1_mag_linear)))
-    with open('voltageMag_diff.csv','w') as f:
-        csvwriter = csv.writer(f)
-        csvwriter.writerow(Vdiff)
-    f.close()
-
-    # get Ibus   
-    Ibus = list(map(lambda x: (x[0]*1000/x[1]).conjugate(),zip(list(PQ_node)[slack_number:],V1[slack_number:])))
-    Ibus_cal_0 = np.dot(Y10,V1[0:slack_number])
-    Ibus_cal_1 = np.dot(Y11,V1[slack_number:])
-    Ibus_cal = list(map(lambda x: x[0]+x[1],zip(Ibus_cal_0,Ibus_cal_1)))
-    Idiff = list(map(lambda x: abs(x[0]-x[1]),zip(Ibus,Ibus_cal)))
-    with open('currentBus_diff.csv','w') as f:
-        csvwriter = csv.writer(f)
-        csvwriter.writerow(Idiff)
-    f.close()
-
-    # get Ibranch
-    Ibranch = np.dot(I_coeff,V1)
-    Ibranch_cal = np.dot(I_coeff[:,slack_number:],V1_linear)+np.dot(I_coeff[:,0:slack_number],V1[:slack_number])
-    Ibranch_diff = list(map(lambda x: abs(x[0]-x[1]),zip(Ibranch,Ibranch_cal)))
-    with open('current_diff.csv','w') as f:
-        csvwriter = csv.writer(f)
-        csvwriter.writerow(Ibranch_diff)
-    f.close()
-
-def costFun(x,dual_upper,dual_lower,v1_pu,Ppv_max,coeff_p,coeff_q,NPV,control_bus_index,Vupper,Vlower,dual_current,ThermalLimit,I1_mag):
-    # cost_function = coeff_p*(Pmax-P)^2+coeff_q*Q^2+dual_upper*(v1-1.05)+dual_lower*(0.95-v1)
-    f1 = 0
-    for ii in range(NPV):
-        f1 = f1 + coeff_p*(Ppv_max[ii]-x[ii])*(Ppv_max[ii]-x[ii])+coeff_q*x[ii+NPV]*x[ii+NPV]
-    #f = f1 + np.dot(dual_upper,(np.array(v1_pu)[control_bus_index]-Vupper)) + np.dot(dual_lower,(Vlower-np.array(v1_pu)[control_bus_index]))
-    v_evaluate = [v1_pu[ii] for ii in control_bus_index]
-    f2 = f1 + np.dot(dual_upper,np.array([max(ii-Vupper,0) for ii in v_evaluate])) + np.dot(dual_lower,np.array([max(Vlower-ii,0) for ii in v_evaluate]))
-    f3 = np.dot(dual_current,np.array([max(ii,0) for ii in list(map(lambda x: x[0]*x[0]-x[1]*x[1],zip(I1_mag,ThermalLimit)))]))
-    f = f2+f3
-
-    # =========================================Yiyun's Notes===========================================#
-    # f1 is the quadratic PV curtailment plus quadratic reactive power injection
-    # f2 is the Lagrangian term for voltage violations and line current violations
-    # ===> Note the "control_bus_index" might be the index for measurement sensitivity analysis
-    # =================================================================================================#
-
-    return [f1,f]
-
-def PV_costFun_gradient(x, coeff_p, coeff_q, Pmax):
-    grad = np.zeros(len(x))
-    for ii in range(int(len(x)/2)):
-        grad[ii] = -2*coeff_p*(Pmax[ii]*1000-x[ii]*1000)
-        grad[ii+int(len(x)/2)] = 2*coeff_q*x[ii+int(len(x)/2)]*1000
-        #grad[ii + int(len(x) / 2)] = 0
-
-    # =========================================Yiyun's Notes===========================================#
-    # x is the decision vector [P,Q]
-    # =================================================================================================#
-
-    return grad
-
-def voltage_constraint_gradient(AllNodeNames,node_withPV, dual_upper, dual_lower, coeff_Vmag_p, coeff_Vmag_q):
-    node_noslackbus = AllNodeNames
-    node_noslackbus[0:3] = []
-
-    # =========================================Yiyun's Notes===========================================#
-    # remove the slack bus
-    # =================================================================================================#
-
-    grad_upper = np.matrix([0] * len(node_noslackbus)*2).transpose()
-    grad_lower = np.matrix([0] * len(node_noslackbus)*2).transpose()
-    count = 0
-    for node in node_noslackbus:
-        if node in node_withPV:
-            grad_upper[count] = dual_upper.transpose()*coeff_Vmag_p[:,count]
-            grad_upper[count+len(node_noslackbus)] = dual_upper.transpose() * coeff_Vmag_q[:,count]
-            grad_lower[count] = -dual_lower.transpose() * coeff_Vmag_p[:, count]
-            grad_lower[count + len(node_noslackbus)] = -dual_lower.transpose() * coeff_Vmag_q[:, count]
-        count = count + 1
-    return [grad_upper,grad_lower]
-
-def current_constraint_gradient(AllNodeNames,node_withPV, dual_upper,coeff_Imag_p, coeff_Imag_q):
-    node_noslackbus = AllNodeNames
-    node_noslackbus[0:3] = []
-    grad_upper = np.matrix([0] * len(node_noslackbus)*2).transpose()
-    count = 0
-    for node in node_noslackbus:
-        if node in node_withPV:
-            grad_upper[count] = dual_upper.transpose()*coeff_Imag_p[:,count]
-            grad_upper[count+len(node_noslackbus)] = dual_upper.transpose() * coeff_Imag_q[:,count]
-        count = count + 1
-    return grad_upper
-
-    # =========================================Yiyun's Notes===========================================#
-    # PV_costFun_gradient,  voltage_constraint_gradient, current_constraint_gradient and project_PV..
-    # ... are set up for updating the PV decision variables in eq(10)
-    # =================================================================================================#
-
-def voltage_constraint(V1_mag):
-    g = V1_mag-1.05
-    g.append(0.95-V1_mag)
-    return g
-
-def current_constraint(I1_mag,Imax):
-    g = []
-    g.append(I1_mag-Imax)
-
-    # =========================================Yiyun's Notes===========================================#
-    # assume single directional power flow
-    # voltage_constraint, current_constraint, and project_dualvariable are set up for updating the dual...
-    # ... variables in eq (11)
-    # =================================================================================================#
-
-    return g
-
-def project_dualvariable(mu):
-    for ii in range(len(mu)):
-        mu[ii] = max(mu[ii],0)
-
-    # =========================================Yiyun's Notes===========================================#
-    # If the corresponding constraints in primal problem is in canonical form, then dual variable is >=0
-    # =================================================================================================#
-
-    return mu
-
-def project_PV(x,Pmax,Sinv):
-    Qavailable = 0
-    Pavailable = 0
-    num = len(Sinv)
-    for ii in range(num):
-        if x[ii] > Pmax[ii]:
-            x[ii] = Pmax[ii]
-        elif x[ii] < 0:
-            x[ii] = 0
-
-        if Sinv[ii] > x[ii]:
-            Qmax = math.sqrt(Sinv[ii]*Sinv[ii]-x[ii]*x[ii])
-        else:
-            Qmax = 0
-        if x[ii+num] > Qmax:
-            x[ii+num] = Qmax
-        # elif x[ii + num] < 0:
-        #     x[ii + num] = 0
-        elif x[ii+num] < -Qmax:
-            x[ii+num] = -Qmax
-
-        Pavailable = Pavailable + Pmax[ii]
-        Qavailable = Qavailable + Qmax
-    return [x,Pavailable,Qavailable]
-
-def dual_update(mu,coeff_mu,constraint):
-    mu_new = mu + coeff_mu*constraint
-    mu_new = project_dualvariable(mu_new)
-
-    # =========================================Yiyun's Notes===========================================#
-    # normal way for update Lagrangian variable is by the sub-gradient of cost function
-    # Here is the equation (11) in the draft paper
-    # =================================================================================================#
-
-    return mu_new
-
-def matrix_cal_for_subPower(V0, Y00, Y01, Y11, V1_noload):
-    diag_V0 = np.matrix([[complex(0, 0)] * 3] * 3)
-    diag_V0[0, 0] = V0[0]
-    diag_V0[1, 1] = V0[1]
-    diag_V0[2, 2] = V0[2]
-    K = diag_V0 * Y01.conj() * np.linalg.inv(Y11.conj())
-    g = diag_V0 * Y00.conj() * np.matrix(V0).transpose().conj() + diag_V0 * Y01.conj() * V1_noload.conj()
-    return[K,g]
-
-def subPower_PQ(V1, PQ_node, K, g):
-    diag_V1 = np.matrix([[complex(0, 0)] * len(V1)] * len(V1))
-    for ii in range(len(V1)):
-        diag_V1[ii, ii] = V1[ii]
-    M = K * np.linalg.inv(diag_V1)
-    MR = M.real
-    MI = M.imag
-    P0 = g.real + (MR.dot(PQ_node.real)*1000 - MI.dot(PQ_node.imag)*1000)
-    Q0 = g.imag + (MR.dot(PQ_node.imag)*1000 + MI.dot(PQ_node.real)*1000)
-
-    P0 = P0/1000
-    Q0 = Q0/1000 # convert to kW/kVar
-
-    # =========================================Yiyun's Notes===========================================#
-    # Power injection at substation/feeder head
-    # =================================================================================================#
-
-    return [P0, Q0, M]
-
-def sub_costFun_gradient(x, sub_ref, coeff_sub, sub_measure, M, node_withPV):
-    grad_a = np.matrix([0] * len(x)).transpose()
-    grad_b = np.matrix([0] * len(x)).transpose()
-    grad_c = np.matrix([0] * len(x)).transpose()
-
-    MR = M.real
-    MI = M.imag
-    count = 0
-    for node in node_withPV:
-        grad_a[count] = -MR[0, int(node)]
-        grad_b[count] = -MR[1, int(node)]
-        grad_c[count] = -MR[2, int(node)]
-
-        grad_a[count + len(node_withPV)] = MI[0, int(node)]
-        grad_b[count + len(node_withPV)] = MI[1, int(node)]
-        grad_c[count + len(node_withPV)] = MI[2, int(node)]
-
-        count = count + 1
-
-    res = coeff_sub * ((sub_measure[0] - sub_ref[0]) *1000* grad_a + (sub_measure[1] - sub_ref[1])*1000 * grad_b
-                       + (sub_measure[2] - sub_ref[2])*1000 * grad_c)
-    res = res/1000
-
-    return res
-
-def projection(x,xmax,xmin):
-    for ii in range(len(x)):
-        if x.item(ii) > xmax[ii]:
-            x[ii] = xmax[ii]
-        if x.item(ii) < xmin[ii]:
-            x[ii] = xmin[ii]
-    return x
-
-class DERMS:
-    def __init__(self, pvData,controlbus,controlelem,controlelem_limit,sub_node_names,sub_elem_names):
-        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-        # PV_name: names of all PVs in the zone
-        # PV_size: sizes of all PVs in the zone
-        # PV_location: busnames of all PVs in the zone
-        # controlbus: names of all controlled nodes
-        # sub_node_names: names of all nodes in the zone
-        # sub_node_names "include" controlbus
-        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-        self.PV_name = pvData["pvName"]
-        self.PV_location = pvData["pvLocation"]
-        self.PV_size = pvData["pvSize"]
-        self.inverter_size = pvData["inverterSize"]
-        self.control_bus = controlbus
-
-        sub_node_names = [ii.upper() for ii in sub_node_names]
-        self.controlbus_index = [sub_node_names.index(ii.upper()) for ii in controlbus] # control bus index in the sub system (number)
-        # here
-        PVbus_index = []
-        for bus in self.PV_location:
-            temp = bus.split('.')
-            if len(temp) == 1:
-                temp = temp + ['1', '2', '3']
-            for ii in range(len(temp) - 1):
-                PVbus_index.append(sub_node_names.index((temp[0] + '.' + temp[ii + 1]).upper()))
-
-        # =========================================Yiyun's Notes===========================================#
-        # adding .1 .2 .3 following the number to recognize the three phases.
-        # =================================================================================================#
-        self.PVbus_index = PVbus_index
-        self.control_elem = controlelem
-        self.controlelem_limit = controlelem_limit
-        self.controlelem_index = [sub_elem_names.index(ii) for ii in controlelem] # control branches index in the sub system (number)
-
-    def monitor(self, dss, dssObjects, PVSystem_1phase):
-        PVpowers = []
-        for pv in PVSystem_1phase["Name"].tolist():
-            nPhases = dssObjects["Generators"][pv].GetValue("phases")
-            power = dssObjects["Generators"][pv].GetValue("Powers")
-            PVpowers.append([sum(power[::2])/nPhases, sum(power[1::2])/nPhases])
-        PVpowers = np.asarray(PVpowers)
-
-        Vmes = []
-        for bus in self.control_bus:
-            busName = bus.split('.')[0].lower()
-            Vmag = dssObjects["Buses"][busName].GetValue("puVmagAngle")[::2]
-            allbusnode = dss.Bus.Nodes()
-            phase = bus.split('.')[1]
-            index = allbusnode.index(int(phase))
-            Vnode = Vmag[index]
-            Vmes.append(Vnode)
-
-        Imes = []
-        for elem in self.control_elem:
-            className = elem.split('.')[0] + "s"
-            I = dssObjects[className][elem].GetValue("CurrentsMagAng")[::2][:3] #TODO: Why is there a hardcoded [:3] ?
-            Imes.append(I)
-
-        return [self.PV_location,PVpowers,Vmes,Imes]
-
-
-
-    def control(self, linear_PF_coeff, Options,stepsize,mu0,Vlimit,PVpower,Imes,Vmes,PV_Pmax_forecast):
-        coeff_p = Options["coeff_p"]
-        coeff_q = Options["coeff_q"]
-
-        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-        # linear_PF_coeff is the linear power flow model coefficients for the zone, and linear power flow model
-        #                coefficients are the result vector from function "linear_powerflow_model"
-        # coeff_p, coeff_q are constant coefficients in PV cost function
-        # stepsize is a vector of stepsize constants
-        # mu0 is the dual variable from last time step: mu_Vmag_upper0, mu_Vmag_lower0, mu_I0
-        # Vlimit is the allowed voltage limit: Vupper and Vlower
-        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-        PVname = self.PV_name
-        NPV = len(PVname)
-        x0 = np.zeros(2 * NPV)
-        for ii in range(NPV):
-            x0[ii] = -PVpower[ii][0] # in kW
-            x0[ii + NPV] = -PVpower[ii][1] # in kVar
-
-        #coeff_V_P = linear_PF_coeff[0]
-        #coeff_V_Q = linear_PF_coeff[1]
-        #coeff_Vm = linear_PF_coeff[2]
-        coeff_Vmag_P = linear_PF_coeff[3]
-        coeff_Vmag_Q = linear_PF_coeff[4]
-        #coeff_Vmag_k = linear_PF_coeff[5]
-        coeff_I_P = linear_PF_coeff[6]
-        coeff_I_Q = linear_PF_coeff[7]
-        #coeff_I_const = linear_PF_coeff[8]
-        stepsize_xp = stepsize[0]
-        stepsize_xq = stepsize[1]
-        stepsize_mu = stepsize[2]
-        Vupper = Vlimit[0]
-        Vlower = Vlimit[1]
-
-        controlbus_index = self.controlbus_index
-        PVbus_index = self.PVbus_index
-        controlelem_index = self.controlelem_index
-        PV_inverter_size = self.inverter_size
-        Imes_limit = self.controlelem_limit
-
-        mu_Vmag_upper0 = mu0[0]
-        mu_Vmag_lower0 = mu0[1]
-        mu_I0 = mu0[2]
-
-        PVcost_fun_gradient = PV_costFun_gradient(x0, coeff_p, coeff_q, PV_Pmax_forecast)
-
-        Vmag_upper_gradient = np.concatenate((np.dot(coeff_Vmag_P[np.ix_([ii for ii in controlbus_index],[ii for ii in PVbus_index])].transpose(), mu_Vmag_upper0),
-                                              np.dot(coeff_Vmag_Q[np.ix_([ii for ii in controlbus_index], [ii for ii in PVbus_index])].transpose(), mu_Vmag_upper0)),axis=0)
-        Vmag_lower_gradient = np.concatenate((np.dot(coeff_Vmag_P[np.ix_([ii for ii in controlbus_index],[ii for ii in PVbus_index])].transpose(), mu_Vmag_lower0),
-                                              np.dot(coeff_Vmag_Q[np.ix_([ii for ii in controlbus_index],[ii for ii in PVbus_index])].transpose(), mu_Vmag_lower0)),axis=0)
-
-        Vmag_gradient = Vmag_upper_gradient - Vmag_lower_gradient
-        if len(mu_I0)>0 :
-            temp_real = mu_I0 * np.array(Imes.real)
-            temp_imag = mu_I0 * np.array(Imes.imag)
-
-            I_gradient_real = np.concatenate((np.dot(
-                coeff_I_P[np.ix_([ii for ii in controlelem_index], [ii for ii in PVbus_index])].real.transpose(),
-                temp_real), np.dot(
-                coeff_I_Q[np.ix_([ii for ii in controlelem_index], [ii for ii in PVbus_index])].real.transpose(),
-                temp_real)), axis=0)
-            I_gradient_imag = np.concatenate((np.dot(
-                coeff_I_P[np.ix_([ii for ii in controlelem_index], [ii for ii in PVbus_index])].imag.transpose(),
-                temp_imag), np.dot(
-                coeff_I_Q[np.ix_([ii for ii in controlelem_index], [ii for ii in PVbus_index])].imag.transpose(),
-                temp_imag)), axis=0)
-            I_gradient = 2 * I_gradient_real + 2 * I_gradient_imag
-        else:
-            I_gradient = 0
-
-        gradient = PVcost_fun_gradient + Vmag_gradient + I_gradient / 1000
-
-        # compute x1, mu1
-        x1 = np.concatenate([x0[:NPV] - stepsize_xp * gradient[:NPV], x0[NPV:] - stepsize_xq * gradient[NPV:]])
-        [x1, Pmax_allPV, Qmax_allPV] = project_PV(x1, PV_Pmax_forecast, PV_inverter_size)
-        x1 = np.array([round(ii, 5) for ii in x1])
-
-        mu_Vmag_lower1 = mu_Vmag_lower0 + stepsize_mu * (Vlower - np.array(Vmes))
-        mu_Vmag_upper1 = mu_Vmag_upper0 + stepsize_mu * (np.array(Vmes) - Vupper)
-        mu_Vmag_lower1 = project_dualvariable(mu_Vmag_lower1)
-        mu_Vmag_upper1 = project_dualvariable(mu_Vmag_upper1)
-        if mu_I0:
-            mu_I1 = mu_I0 + stepsize_mu / 300 * np.array(list(map(lambda x: x[0] * x[0] - x[1] * x[1], zip(Imes, Imes_limit))))
-            mu_I1 = project_dualvariable(mu_I1)
-        else:
-            mu_I1 = mu_I0
-        mu1 = [mu_Vmag_upper1,mu_Vmag_lower1,mu_I1]
-        # =========================================Yiyun's Notes===========================================#
-        # Each time of calling DERMS.control, it is a one step update of PV real and reactive power outputs
-        # =================================================================================================#
-
-        return [x1,mu1]
+import numpy as np
+from scipy.sparse import lil_matrix
+import scipy.sparse.linalg as sp
+import scipy.sparse as sparse
+import math
+import csv
+import matplotlib.pyplot as plt
+
+def linear_powerflow_model(Y00,Y01,Y10,Y11_inv,I_coeff,V1,slack_no):
+    # voltage linearlization
+    V1_conj = np.conj(V1[slack_no:])
+    V1_conj_inv = 1 / V1_conj
+    coeff_V = Y11_inv * V1_conj_inv
+    coeff_V_P = coeff_V
+    coeff_V_Q = -1j*coeff_V
+    coeff_Vm = -np.dot(Y11_inv,np.dot(Y10,V1[:slack_no]))
+
+    # voltage magnitude linearization
+    m = coeff_Vm
+    m_inv = 1 / coeff_Vm
+    coeff_Vmag_k = abs(m)
+    A = (np.multiply(coeff_V.transpose(),m_inv)).transpose()
+    coeff_Vmag_P = (np.multiply(A.real.transpose(),coeff_Vmag_k)).transpose()
+    coeff_Vmag_Q = (np.multiply((-1j*A).real.transpose(),coeff_Vmag_k)).transpose()
+
+    # current linearization
+    if len(I_coeff):
+        coeff_I_P = np.dot(I_coeff[:,slack_no:],coeff_V_P)
+        coeff_I_Q = np.dot(I_coeff[:,slack_no:],coeff_V_Q)
+        coeff_I_const = np.dot(I_coeff[:,slack_no:],coeff_Vm) + np.dot(I_coeff[:,:slack_no],V1[:slack_no])
+    else:
+        coeff_I_P = []
+        coeff_I_Q = []
+        coeff_I_const = []
+
+    #=========================================Yiyun's Notes===========================================#
+    # Output relations: Vmag = coeff_Vmag_P * Pnode + coeff_Vmag_Q * Qnode + coeff_Vm
+    #                      I = coeff_I_P * Pnode + coeff_I_Q * Qnode + coeff_I_const (complex value)
+    # ================================================================================================#
+
+    return coeff_V_P, coeff_V_Q, coeff_Vm, coeff_Vmag_P, coeff_Vmag_Q, coeff_Vmag_k, coeff_I_P, coeff_I_Q, coeff_I_const
+
+def validate_linear_model(coeff_Vp,coeff_Vq,coeff_Vm,PQ_node,slack_number):
+    V_cal = coeff_Vm + np.dot(coeff_Vp,np.array([np.real(ii)*1000 for ii in PQ_node[slack_number:]])) + np.dot(coeff_Vq,np.array([np.imag(ii)*1000 for ii in PQ_node[slack_number:]]))
+    v_cal_1 = coeff_Vm + np.dot(coeff_Vp,np.conj(PQ_node[slack_number:]*1000))
+    #coeff_Vp*Pnode + coeff_Vq*Qnode + coeff_Vm
+
+    # =========================================Yiyun's Notes===========================================#
+    # 1000 should be the S base
+    # =================================================================================================#
+
+    return [V_cal,v_cal_1]
+
+def check_VI_correct(V1,PQ_node,slack_number,coeff_V,coeff_Vm,coeff_Vmag_P,coeff_Vmag_Q,coeff_Vmag_k,Y10,Y11,coeff_I_P, coeff_I_Q, coeff_I_const,I_coeff):
+    V1_linear = np.dot(coeff_V,np.conj(PQ_node[slack_number:]*1000)) + coeff_Vm
+    V1_linear = list(V1_linear)
+    Vdiff = list(map(lambda x: abs(x[0]-x[1])/abs(x[0])*100,zip(V1[slack_number:],V1_linear)))
+    with open('voltage_diff.csv','w') as f:
+        csvwriter = csv.writer(f)
+        csvwriter.writerow(Vdiff)
+    f.close()
+
+    V1_mag_linear = np.dot(coeff_Vmag_P,(PQ_node[slack_number:]*1000).real) + np.dot(coeff_Vmag_Q,(PQ_node[slack_number:]*1000).imag) + coeff_Vmag_k
+    V1_mag_linear = list(V1_mag_linear)
+    Vdiff = list(map(lambda x: abs(abs(x[0])-x[1])/abs(x[0])*100,zip(V1[slack_number:],V1_mag_linear)))
+    with open('voltageMag_diff.csv','w') as f:
+        csvwriter = csv.writer(f)
+        csvwriter.writerow(Vdiff)
+    f.close()
+
+    # get Ibus   
+    Ibus = list(map(lambda x: (x[0]*1000/x[1]).conjugate(),zip(list(PQ_node)[slack_number:],V1[slack_number:])))
+    Ibus_cal_0 = np.dot(Y10,V1[0:slack_number])
+    Ibus_cal_1 = np.dot(Y11,V1[slack_number:])
+    Ibus_cal = list(map(lambda x: x[0]+x[1],zip(Ibus_cal_0,Ibus_cal_1)))
+    Idiff = list(map(lambda x: abs(x[0]-x[1]),zip(Ibus,Ibus_cal)))
+    with open('currentBus_diff.csv','w') as f:
+        csvwriter = csv.writer(f)
+        csvwriter.writerow(Idiff)
+    f.close()
+
+    # get Ibranch
+    Ibranch = np.dot(I_coeff,V1)
+    Ibranch_cal = np.dot(I_coeff[:,slack_number:],V1_linear)+np.dot(I_coeff[:,0:slack_number],V1[:slack_number])
+    Ibranch_diff = list(map(lambda x: abs(x[0]-x[1]),zip(Ibranch,Ibranch_cal)))
+    with open('current_diff.csv','w') as f:
+        csvwriter = csv.writer(f)
+        csvwriter.writerow(Ibranch_diff)
+    f.close()
+
+def costFun(x,dual_upper,dual_lower,v1_pu,Ppv_max,coeff_p,coeff_q,NPV,control_bus_index,Vupper,Vlower,dual_current,ThermalLimit,I1_mag):
+    # cost_function = coeff_p*(Pmax-P)^2+coeff_q*Q^2+dual_upper*(v1-1.05)+dual_lower*(0.95-v1)
+    f1 = 0
+    for ii in range(NPV):
+        f1 = f1 + coeff_p*(Ppv_max[ii]-x[ii])*(Ppv_max[ii]-x[ii])+coeff_q*x[ii+NPV]*x[ii+NPV]
+    #f = f1 + np.dot(dual_upper,(np.array(v1_pu)[control_bus_index]-Vupper)) + np.dot(dual_lower,(Vlower-np.array(v1_pu)[control_bus_index]))
+    v_evaluate = [v1_pu[ii] for ii in control_bus_index]
+    f2 = f1 + np.dot(dual_upper,np.array([max(ii-Vupper,0) for ii in v_evaluate])) + np.dot(dual_lower,np.array([max(Vlower-ii,0) for ii in v_evaluate]))
+    f3 = np.dot(dual_current,np.array([max(ii,0) for ii in list(map(lambda x: x[0]*x[0]-x[1]*x[1],zip(I1_mag,ThermalLimit)))]))
+    f = f2+f3
+
+    # =========================================Yiyun's Notes===========================================#
+    # f1 is the quadratic PV curtailment plus quadratic reactive power injection
+    # f2 is the Lagrangian term for voltage violations and line current violations
+    # ===> Note the "control_bus_index" might be the index for measurement sensitivity analysis
+    # =================================================================================================#
+
+    return [f1,f]
+
+def PV_costFun_gradient(x, coeff_p, coeff_q, Pmax):
+    grad = np.zeros(len(x))
+    for ii in range(int(len(x)/2)):
+        grad[ii] = -2*coeff_p*(Pmax[ii]*1000-x[ii]*1000)
+        grad[ii+int(len(x)/2)] = 2*coeff_q*x[ii+int(len(x)/2)]*1000
+        #grad[ii + int(len(x) / 2)] = 0
+
+    # =========================================Yiyun's Notes===========================================#
+    # x is the decision vector [P,Q]
+    # =================================================================================================#
+
+    return grad
+
+def voltage_constraint_gradient(AllNodeNames,node_withPV, dual_upper, dual_lower, coeff_Vmag_p, coeff_Vmag_q):
+    node_noslackbus = AllNodeNames
+    node_noslackbus[0:3] = []
+
+    # =========================================Yiyun's Notes===========================================#
+    # remove the slack bus
+    # =================================================================================================#
+
+    grad_upper = np.matrix([0] * len(node_noslackbus)*2).transpose()
+    grad_lower = np.matrix([0] * len(node_noslackbus)*2).transpose()
+    count = 0
+    for node in node_noslackbus:
+        if node in node_withPV:
+            grad_upper[count] = dual_upper.transpose()*coeff_Vmag_p[:,count]
+            grad_upper[count+len(node_noslackbus)] = dual_upper.transpose() * coeff_Vmag_q[:,count]
+            grad_lower[count] = -dual_lower.transpose() * coeff_Vmag_p[:, count]
+            grad_lower[count + len(node_noslackbus)] = -dual_lower.transpose() * coeff_Vmag_q[:, count]
+        count = count + 1
+    return [grad_upper,grad_lower]
+
+def current_constraint_gradient(AllNodeNames,node_withPV, dual_upper,coeff_Imag_p, coeff_Imag_q):
+    node_noslackbus = AllNodeNames
+    node_noslackbus[0:3] = []
+    grad_upper = np.matrix([0] * len(node_noslackbus)*2).transpose()
+    count = 0
+    for node in node_noslackbus:
+        if node in node_withPV:
+            grad_upper[count] = dual_upper.transpose()*coeff_Imag_p[:,count]
+            grad_upper[count+len(node_noslackbus)] = dual_upper.transpose() * coeff_Imag_q[:,count]
+        count = count + 1
+    return grad_upper
+
+    # =========================================Yiyun's Notes===========================================#
+    # PV_costFun_gradient,  voltage_constraint_gradient, current_constraint_gradient and project_PV..
+    # ... are set up for updating the PV decision variables in eq(10)
+    # =================================================================================================#
+
+def voltage_constraint(V1_mag):
+    g = V1_mag-1.05
+    g.append(0.95-V1_mag)
+    return g
+
+def current_constraint(I1_mag,Imax):
+    g = []
+    g.append(I1_mag-Imax)
+
+    # =========================================Yiyun's Notes===========================================#
+    # assume single directional power flow
+    # voltage_constraint, current_constraint, and project_dualvariable are set up for updating the dual...
+    # ... variables in eq (11)
+    # =================================================================================================#
+
+    return g
+
+def project_dualvariable(mu):
+    for ii in range(len(mu)):
+        mu[ii] = max(mu[ii],0)
+
+    # =========================================Yiyun's Notes===========================================#
+    # If the corresponding constraints in primal problem is in canonical form, then dual variable is >=0
+    # =================================================================================================#
+
+    return mu
+
+def project_PV(x,Pmax,Sinv):
+    Qavailable = 0
+    Pavailable = 0
+    num = len(Sinv)
+    for ii in range(num):
+        if x[ii] > Pmax[ii]:
+            x[ii] = Pmax[ii]
+        elif x[ii] < 0:
+            x[ii] = 0
+
+        if Sinv[ii] > x[ii]:
+            Qmax = math.sqrt(Sinv[ii]*Sinv[ii]-x[ii]*x[ii])
+        else:
+            Qmax = 0
+        if x[ii+num] > Qmax:
+            x[ii+num] = Qmax
+        # elif x[ii + num] < 0:
+        #     x[ii + num] = 0
+        elif x[ii+num] < -Qmax:
+            x[ii+num] = -Qmax
+
+        Pavailable = Pavailable + Pmax[ii]
+        Qavailable = Qavailable + Qmax
+    return [x,Pavailable,Qavailable]
+
+def dual_update(mu,coeff_mu,constraint):
+    mu_new = mu + coeff_mu*constraint
+    mu_new = project_dualvariable(mu_new)
+
+    # =========================================Yiyun's Notes===========================================#
+    # normal way for update Lagrangian variable is by the sub-gradient of cost function
+    # Here is the equation (11) in the draft paper
+    # =================================================================================================#
+
+    return mu_new
+
+def matrix_cal_for_subPower(V0, Y00, Y01, Y11, V1_noload):
+    diag_V0 = np.matrix([[complex(0, 0)] * 3] * 3)
+    diag_V0[0, 0] = V0[0]
+    diag_V0[1, 1] = V0[1]
+    diag_V0[2, 2] = V0[2]
+    K = diag_V0 * Y01.conj() * np.linalg.inv(Y11.conj())
+    g = diag_V0 * Y00.conj() * np.matrix(V0).transpose().conj() + diag_V0 * Y01.conj() * V1_noload.conj()
+    return[K,g]
+
+def subPower_PQ(V1, PQ_node, K, g):
+    diag_V1 = np.matrix([[complex(0, 0)] * len(V1)] * len(V1))
+    for ii in range(len(V1)):
+        diag_V1[ii, ii] = V1[ii]
+    M = K * np.linalg.inv(diag_V1)
+    MR = M.real
+    MI = M.imag
+    P0 = g.real + (MR.dot(PQ_node.real)*1000 - MI.dot(PQ_node.imag)*1000)
+    Q0 = g.imag + (MR.dot(PQ_node.imag)*1000 + MI.dot(PQ_node.real)*1000)
+
+    P0 = P0/1000
+    Q0 = Q0/1000 # convert to kW/kVar
+
+    # =========================================Yiyun's Notes===========================================#
+    # Power injection at substation/feeder head
+    # =================================================================================================#
+
+    return [P0, Q0, M]
+
+def sub_costFun_gradient(x, sub_ref, coeff_sub, sub_measure, M, node_withPV):
+    grad_a = np.matrix([0] * len(x)).transpose()
+    grad_b = np.matrix([0] * len(x)).transpose()
+    grad_c = np.matrix([0] * len(x)).transpose()
+
+    MR = M.real
+    MI = M.imag
+    count = 0
+    for node in node_withPV:
+        grad_a[count] = -MR[0, int(node)]
+        grad_b[count] = -MR[1, int(node)]
+        grad_c[count] = -MR[2, int(node)]
+
+        grad_a[count + len(node_withPV)] = MI[0, int(node)]
+        grad_b[count + len(node_withPV)] = MI[1, int(node)]
+        grad_c[count + len(node_withPV)] = MI[2, int(node)]
+
+        count = count + 1
+
+    res = coeff_sub * ((sub_measure[0] - sub_ref[0]) *1000* grad_a + (sub_measure[1] - sub_ref[1])*1000 * grad_b
+                       + (sub_measure[2] - sub_ref[2])*1000 * grad_c)
+    res = res/1000
+
+    return res
+
+def projection(x,xmax,xmin):
+    for ii in range(len(x)):
+        if x.item(ii) > xmax[ii]:
+            x[ii] = xmax[ii]
+        if x.item(ii) < xmin[ii]:
+            x[ii] = xmin[ii]
+    return x
+
+class DERMS:
+    def __init__(self, pvData,controlbus,controlelem,controlelem_limit,sub_node_names,sub_elem_names):
+        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+        # PV_name: names of all PVs in the zone
+        # PV_size: sizes of all PVs in the zone
+        # PV_location: busnames of all PVs in the zone
+        # controlbus: names of all controlled nodes
+        # sub_node_names: names of all nodes in the zone
+        # sub_node_names "include" controlbus
+        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+        self.PV_name = pvData["pvName"]
+        self.PV_location = pvData["pvLocation"]
+        self.PV_size = pvData["pvSize"]
+        self.inverter_size = pvData["inverterSize"]
+        self.control_bus = controlbus
+
+        sub_node_names = [ii.upper() for ii in sub_node_names]
+        self.controlbus_index = [sub_node_names.index(ii.upper()) for ii in controlbus] # control bus index in the sub system (number)
+        # here
+        PVbus_index = []
+        for bus in self.PV_location:
+            temp = bus.split('.')
+            if len(temp) == 1:
+                temp = temp + ['1', '2', '3']
+            for ii in range(len(temp) - 1):
+                PVbus_index.append(sub_node_names.index((temp[0] + '.' + temp[ii + 1]).upper()))
+
+        # =========================================Yiyun's Notes===========================================#
+        # adding .1 .2 .3 following the number to recognize the three phases.
+        # =================================================================================================#
+        self.PVbus_index = PVbus_index
+        self.control_elem = controlelem
+        self.controlelem_limit = controlelem_limit
+        self.controlelem_index = [sub_elem_names.index(ii) for ii in controlelem] # control branches index in the sub system (number)
+
+    def monitor(self, dss, dssObjects, PVSystem_1phase):
+        PVpowers = []
+        for pv in PVSystem_1phase["Name"].tolist():
+            nPhases = dssObjects["Generators"][pv].GetValue("phases")
+            power = dssObjects["Generators"][pv].GetValue("Powers")
+            PVpowers.append([sum(power[::2])/nPhases, sum(power[1::2])/nPhases])
+        PVpowers = np.asarray(PVpowers)
+
+        Vmes = []
+        for bus in self.control_bus:
+            busName = bus.split('.')[0].lower()
+            Vmag = dssObjects["Buses"][busName].GetValue("puVmagAngle")[::2]
+            allbusnode = dss.Bus.Nodes()
+            phase = bus.split('.')[1]
+            index = allbusnode.index(int(phase))
+            Vnode = Vmag[index]
+            Vmes.append(Vnode)
+
+        Imes = []
+        for elem in self.control_elem:
+            className = elem.split('.')[0] + "s"
+            I = dssObjects[className][elem].GetValue("CurrentsMagAng")[::2][:3] #TODO: Why is there a hardcoded [:3] ?
+            Imes.append(I)
+
+        return [self.PV_location,PVpowers,Vmes,Imes]
+
+
+
+    def control(self, linear_PF_coeff, Options,stepsize,mu0,Vlimit,PVpower,Imes,Vmes,PV_Pmax_forecast):
+        coeff_p = Options["coeff_p"]
+        coeff_q = Options["coeff_q"]
+
+        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+        # linear_PF_coeff is the linear power flow model coefficients for the zone, and linear power flow model
+        #                coefficients are the result vector from function "linear_powerflow_model"
+        # coeff_p, coeff_q are constant coefficients in PV cost function
+        # stepsize is a vector of stepsize constants
+        # mu0 is the dual variable from last time step: mu_Vmag_upper0, mu_Vmag_lower0, mu_I0
+        # Vlimit is the allowed voltage limit: Vupper and Vlower
+        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+        PVname = self.PV_name
+        NPV = len(PVname)
+        x0 = np.zeros(2 * NPV)
+        for ii in range(NPV):
+            x0[ii] = -PVpower[ii][0] # in kW
+            x0[ii + NPV] = -PVpower[ii][1] # in kVar
+
+        #coeff_V_P = linear_PF_coeff[0]
+        #coeff_V_Q = linear_PF_coeff[1]
+        #coeff_Vm = linear_PF_coeff[2]
+        coeff_Vmag_P = linear_PF_coeff[3]
+        coeff_Vmag_Q = linear_PF_coeff[4]
+        #coeff_Vmag_k = linear_PF_coeff[5]
+        coeff_I_P = linear_PF_coeff[6]
+        coeff_I_Q = linear_PF_coeff[7]
+        #coeff_I_const = linear_PF_coeff[8]
+        stepsize_xp = stepsize[0]
+        stepsize_xq = stepsize[1]
+        stepsize_mu = stepsize[2]
+        Vupper = Vlimit[0]
+        Vlower = Vlimit[1]
+
+        controlbus_index = self.controlbus_index
+        PVbus_index = self.PVbus_index
+        controlelem_index = self.controlelem_index
+        PV_inverter_size = self.inverter_size
+        Imes_limit = self.controlelem_limit
+
+        mu_Vmag_upper0 = mu0[0]
+        mu_Vmag_lower0 = mu0[1]
+        mu_I0 = mu0[2]
+
+        PVcost_fun_gradient = PV_costFun_gradient(x0, coeff_p, coeff_q, PV_Pmax_forecast)
+
+        Vmag_upper_gradient = np.concatenate((np.dot(coeff_Vmag_P[np.ix_([ii for ii in controlbus_index],[ii for ii in PVbus_index])].transpose(), mu_Vmag_upper0),
+                                              np.dot(coeff_Vmag_Q[np.ix_([ii for ii in controlbus_index], [ii for ii in PVbus_index])].transpose(), mu_Vmag_upper0)),axis=0)
+        Vmag_lower_gradient = np.concatenate((np.dot(coeff_Vmag_P[np.ix_([ii for ii in controlbus_index],[ii for ii in PVbus_index])].transpose(), mu_Vmag_lower0),
+                                              np.dot(coeff_Vmag_Q[np.ix_([ii for ii in controlbus_index],[ii for ii in PVbus_index])].transpose(), mu_Vmag_lower0)),axis=0)
+
+        Vmag_gradient = Vmag_upper_gradient - Vmag_lower_gradient
+        if len(mu_I0)>0 :
+            temp_real = mu_I0 * np.array(Imes.real)
+            temp_imag = mu_I0 * np.array(Imes.imag)
+
+            I_gradient_real = np.concatenate((np.dot(
+                coeff_I_P[np.ix_([ii for ii in controlelem_index], [ii for ii in PVbus_index])].real.transpose(),
+                temp_real), np.dot(
+                coeff_I_Q[np.ix_([ii for ii in controlelem_index], [ii for ii in PVbus_index])].real.transpose(),
+                temp_real)), axis=0)
+            I_gradient_imag = np.concatenate((np.dot(
+                coeff_I_P[np.ix_([ii for ii in controlelem_index], [ii for ii in PVbus_index])].imag.transpose(),
+                temp_imag), np.dot(
+                coeff_I_Q[np.ix_([ii for ii in controlelem_index], [ii for ii in PVbus_index])].imag.transpose(),
+                temp_imag)), axis=0)
+            I_gradient = 2 * I_gradient_real + 2 * I_gradient_imag
+        else:
+            I_gradient = 0
+
+        gradient = PVcost_fun_gradient + Vmag_gradient + I_gradient / 1000
+
+        # compute x1, mu1
+        x1 = np.concatenate([x0[:NPV] - stepsize_xp * gradient[:NPV], x0[NPV:] - stepsize_xq * gradient[NPV:]])
+        [x1, Pmax_allPV, Qmax_allPV] = project_PV(x1, PV_Pmax_forecast, PV_inverter_size)
+        x1 = np.array([round(ii, 5) for ii in x1])
+
+        mu_Vmag_lower1 = mu_Vmag_lower0 + stepsize_mu * (Vlower - np.array(Vmes))
+        mu_Vmag_upper1 = mu_Vmag_upper0 + stepsize_mu * (np.array(Vmes) - Vupper)
+        mu_Vmag_lower1 = project_dualvariable(mu_Vmag_lower1)
+        mu_Vmag_upper1 = project_dualvariable(mu_Vmag_upper1)
+        if mu_I0:
+            mu_I1 = mu_I0 + stepsize_mu / 300 * np.array(list(map(lambda x: x[0] * x[0] - x[1] * x[1], zip(Imes, Imes_limit))))
+            mu_I1 = project_dualvariable(mu_I1)
+        else:
+            mu_I1 = mu_I0
+        mu1 = [mu_Vmag_upper1,mu_Vmag_lower1,mu_I1]
+        # =========================================Yiyun's Notes===========================================#
+        # Each time of calling DERMS.control, it is a one step update of PV real and reactive power outputs
+        # =================================================================================================#
+
+        return [x1,mu1]
```

### Comparing `nrel_pydss-3.1.3/src/pydss/reports/capacitor_change_count.py` & `nrel_pydss-3.1.4/src/pydss/reports/capacitor_change_count.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,104 +1,101 @@
-00000000: 0d0a 696d 706f 7274 206f 730d 0a0d 0a66  ..import os....f
-00000010: 726f 6d20 6c6f 6775 7275 2069 6d70 6f72  rom loguru impor
-00000020: 7420 6c6f 6767 6572 0d0a 0d0a 6672 6f6d  t logger....from
-00000030: 2070 7964 7373 2e72 6570 6f72 7473 2e72   pydss.reports.r
-00000040: 6570 6f72 7473 2069 6d70 6f72 7420 5265  eports import Re
-00000050: 706f 7274 4261 7365 0d0a 6672 6f6d 2070  portBase..from p
-00000060: 7964 7373 2e75 7469 6c73 2e75 7469 6c73  ydss.utils.utils
-00000070: 2069 6d70 6f72 7420 6475 6d70 5f64 6174   import dump_dat
-00000080: 610d 0a0d 0a63 6c61 7373 2043 6170 6163  a....class Capac
-00000090: 6974 6f72 5374 6174 6543 6861 6e67 6552  itorStateChangeR
-000000a0: 6570 6f72 7428 5265 706f 7274 4261 7365  eport(ReportBase
-000000b0: 293a 0d0a 2020 2020 2222 2252 6570 6f72  ):..    """Repor
-000000c0: 7473 2074 6865 2073 7461 7465 2063 6861  ts the state cha
-000000d0: 6e67 6573 2070 6572 2043 6170 6163 6974  nges per Capacit
-000000e0: 6f72 2e0d 0a0d 0a20 2020 2054 6865 2072  or.....    The r
-000000f0: 6570 6f72 7420 6765 6e65 7261 7465 7320  eport generates 
-00000100: 6120 6361 7061 6369 746f 725f 7374 6174  a capacitor_stat
-00000110: 655f 6368 616e 6765 732e 6a73 6f6e 206f  e_changes.json o
-00000120: 7574 7075 7420 6669 6c65 2e0d 0a0d 0a20  utput file..... 
-00000130: 2020 2054 4f44 4f3a 2054 6869 7320 6973     TODO: This is
-00000140: 2061 6e20 6578 7065 7269 6d65 6e74 616c   an experimental
-00000150: 2072 6570 6f72 742e 204f 7574 7075 7473   report. Outputs
-00000160: 2068 6176 6520 6e6f 7420 6265 656e 2076   have not been v
-00000170: 616c 6964 6174 6564 2e0d 0a0d 0a20 2020  alidated.....   
-00000180: 2022 2222 0d0a 0d0a 2020 2020 4649 4c45   """....    FILE
-00000190: 4e41 4d45 203d 2022 6361 7061 6369 746f  NAME = "capacito
-000001a0: 725f 7374 6174 655f 6368 616e 6765 732e  r_state_changes.
-000001b0: 6a73 6f6e 220d 0a20 2020 204e 414d 4520  json"..    NAME 
-000001c0: 3d20 2243 6170 6163 6974 6f72 2053 7461  = "Capacitor Sta
-000001d0: 7465 2043 6861 6e67 6520 436f 756e 7473  te Change Counts
-000001e0: 220d 0a0d 0a20 2020 2064 6566 2067 656e  "....    def gen
-000001f0: 6572 6174 6528 7365 6c66 2c20 6f75 7470  erate(self, outp
-00000200: 7574 5f64 6972 293a 0d0a 2020 2020 2020  ut_dir):..      
-00000210: 2020 6461 7461 203d 207b 2273 6365 6e61    data = {"scena
-00000220: 7269 6f73 223a 205b 5d7d 0d0a 2020 2020  rios": []}..    
-00000230: 2020 2020 666f 7220 7363 656e 6172 696f      for scenario
-00000240: 2069 6e20 7365 6c66 2e5f 7265 7375 6c74   in self._result
-00000250: 732e 7363 656e 6172 696f 733a 0d0a 2020  s.scenarios:..  
-00000260: 2020 2020 2020 2020 2020 7363 656e 6172            scenar
-00000270: 696f 5f64 6174 6120 3d20 7b22 6e61 6d65  io_data = {"name
-00000280: 223a 2073 6365 6e61 7269 6f2e 6e61 6d65  ": scenario.name
-00000290: 2c20 2263 6170 6163 6974 6f72 7322 3a20  , "capacitors": 
-000002a0: 5b5d 7d0d 0a20 2020 2020 2020 2020 2020  []}..           
-000002b0: 2066 6f72 2063 6170 6163 6974 6f72 2069   for capacitor i
-000002c0: 6e20 7363 656e 6172 696f 2e6c 6973 745f  n scenario.list_
-000002d0: 656c 656d 656e 745f 6e61 6d65 7328 2243  element_names("C
-000002e0: 6170 6163 6974 6f72 7322 293a 0d0a 2020  apacitors"):..  
-000002f0: 2020 2020 2020 2020 2020 2020 2020 6368                ch
-00000300: 616e 6765 5f63 6f75 6e74 203d 2069 6e74  ange_count = int
-00000310: 2873 6365 6e61 7269 6f2e 6765 745f 656c  (scenario.get_el
-00000320: 656d 656e 745f 7072 6f70 6572 7479 5f76  ement_property_v
-00000330: 616c 7565 280d 0a20 2020 2020 2020 2020  alue(..         
-00000340: 2020 2020 2020 2020 2020 2022 4361 7061             "Capa
-00000350: 6369 746f 7273 222c 2022 5472 6163 6b53  citors", "TrackS
-00000360: 7461 7465 4368 616e 6765 7322 2c20 6361  tateChanges", ca
-00000370: 7061 6369 746f 720d 0a20 2020 2020 2020  pacitor..       
-00000380: 2020 2020 2020 2020 2029 290d 0a20 2020           ))..   
-00000390: 2020 2020 2020 2020 2020 2020 2063 6861               cha
-000003a0: 6e67 6573 203d 207b 226e 616d 6522 3a20  nges = {"name": 
-000003b0: 6361 7061 6369 746f 722c 2022 6368 616e  capacitor, "chan
-000003c0: 6765 5f63 6f75 6e74 223a 2063 6861 6e67  ge_count": chang
-000003d0: 655f 636f 756e 747d 0d0a 2020 2020 2020  e_count}..      
-000003e0: 2020 2020 2020 2020 2020 7363 656e 6172            scenar
-000003f0: 696f 5f64 6174 615b 2263 6170 6163 6974  io_data["capacit
-00000400: 6f72 7322 5d2e 6170 7065 6e64 2863 6861  ors"].append(cha
-00000410: 6e67 6573 290d 0a20 2020 2020 2020 2020  nges)..         
-00000420: 2020 2064 6174 615b 2273 6365 6e61 7269     data["scenari
-00000430: 6f73 225d 2e61 7070 656e 6428 7363 656e  os"].append(scen
-00000440: 6172 696f 5f64 6174 6129 0d0a 0d0a 2020  ario_data)....  
-00000450: 2020 2020 2020 6669 6c65 6e61 6d65 203d        filename =
-00000460: 206f 732e 7061 7468 2e6a 6f69 6e28 6f75   os.path.join(ou
-00000470: 7470 7574 5f64 6972 2c20 7365 6c66 2e46  tput_dir, self.F
-00000480: 494c 454e 414d 4529 0d0a 2020 2020 2020  ILENAME)..      
-00000490: 2020 6475 6d70 5f64 6174 6128 6461 7461    dump_data(data
-000004a0: 2c20 6669 6c65 6e61 6d65 2c20 696e 6465  , filename, inde
-000004b0: 6e74 3d32 290d 0a20 2020 2020 2020 206c  nt=2)..        l
-000004c0: 6f67 6765 722e 696e 666f 2822 4765 6e65  ogger.info("Gene
-000004d0: 7261 7465 6420 2573 222c 2066 696c 656e  rated %s", filen
-000004e0: 616d 6529 0d0a 2020 2020 2020 2020 7265  ame)..        re
-000004f0: 7475 726e 2066 696c 656e 616d 650d 0a0d  turn filename...
-00000500: 0a20 2020 2040 7374 6174 6963 6d65 7468  .    @staticmeth
-00000510: 6f64 0d0a 2020 2020 6465 6620 6765 745f  od..    def get_
-00000520: 7265 7175 6972 6564 5f65 7870 6f72 7473  required_exports
-00000530: 2873 696d 756c 6174 696f 6e5f 636f 6e66  (simulation_conf
-00000540: 6967 293a 0d0a 2020 2020 2020 2020 7265  ig):..        re
-00000550: 7475 726e 207b 0d0a 2020 2020 2020 2020  turn {..        
-00000560: 2020 2020 2243 6170 6163 6974 6f72 7322      "Capacitors"
-00000570: 3a20 5b0d 0a20 2020 2020 2020 2020 2020  : [..           
-00000580: 2020 2020 207b 0d0a 2020 2020 2020 2020       {..        
-00000590: 2020 2020 2020 2020 2020 2020 2270 726f              "pro
-000005a0: 7065 7274 7922 3a20 2254 7261 636b 5374  perty": "TrackSt
-000005b0: 6174 6543 6861 6e67 6573 222c 0d0a 2020  ateChanges",..  
-000005c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000005d0: 2020 2273 746f 7265 5f76 616c 7565 735f    "store_values_
-000005e0: 7479 7065 223a 2022 6368 616e 6765 5f63  type": "change_c
-000005f0: 6f75 6e74 222c 0d0a 2020 2020 2020 2020  ount",..        
-00000600: 2020 2020 2020 2020 7d0d 0a20 2020 2020          }..     
-00000610: 2020 2020 2020 205d 0d0a 2020 2020 2020         ]..      
-00000620: 2020 7d0d 0a0d 0a20 2020 2040 7374 6174    }....    @stat
-00000630: 6963 6d65 7468 6f64 0d0a 2020 2020 6465  icmethod..    de
-00000640: 6620 6765 745f 7265 7175 6972 6564 5f73  f get_required_s
-00000650: 6365 6e61 7269 6f5f 6e61 6d65 7328 293a  cenario_names():
-00000660: 0d0a 2020 2020 2020 2020 7265 7475 726e  ..        return
-00000670: 2073 6574 2829 0d0a                       set()..
+00000000: 0a69 6d70 6f72 7420 6f73 0a0a 6672 6f6d  .import os..from
+00000010: 206c 6f67 7572 7520 696d 706f 7274 206c   loguru import l
+00000020: 6f67 6765 720a 0a66 726f 6d20 7079 6473  ogger..from pyds
+00000030: 732e 7265 706f 7274 732e 7265 706f 7274  s.reports.report
+00000040: 7320 696d 706f 7274 2052 6570 6f72 7442  s import ReportB
+00000050: 6173 650a 6672 6f6d 2070 7964 7373 2e75  ase.from pydss.u
+00000060: 7469 6c73 2e75 7469 6c73 2069 6d70 6f72  tils.utils impor
+00000070: 7420 6475 6d70 5f64 6174 610a 0a63 6c61  t dump_data..cla
+00000080: 7373 2043 6170 6163 6974 6f72 5374 6174  ss CapacitorStat
+00000090: 6543 6861 6e67 6552 6570 6f72 7428 5265  eChangeReport(Re
+000000a0: 706f 7274 4261 7365 293a 0a20 2020 2022  portBase):.    "
+000000b0: 2222 5265 706f 7274 7320 7468 6520 7374  ""Reports the st
+000000c0: 6174 6520 6368 616e 6765 7320 7065 7220  ate changes per 
+000000d0: 4361 7061 6369 746f 722e 0a0a 2020 2020  Capacitor...    
+000000e0: 5468 6520 7265 706f 7274 2067 656e 6572  The report gener
+000000f0: 6174 6573 2061 2063 6170 6163 6974 6f72  ates a capacitor
+00000100: 5f73 7461 7465 5f63 6861 6e67 6573 2e6a  _state_changes.j
+00000110: 736f 6e20 6f75 7470 7574 2066 696c 652e  son output file.
+00000120: 0a0a 2020 2020 544f 444f 3a20 5468 6973  ..    TODO: This
+00000130: 2069 7320 616e 2065 7870 6572 696d 656e   is an experimen
+00000140: 7461 6c20 7265 706f 7274 2e20 4f75 7470  tal report. Outp
+00000150: 7574 7320 6861 7665 206e 6f74 2062 6565  uts have not bee
+00000160: 6e20 7661 6c69 6461 7465 642e 0a0a 2020  n validated...  
+00000170: 2020 2222 220a 0a20 2020 2046 494c 454e    """..    FILEN
+00000180: 414d 4520 3d20 2263 6170 6163 6974 6f72  AME = "capacitor
+00000190: 5f73 7461 7465 5f63 6861 6e67 6573 2e6a  _state_changes.j
+000001a0: 736f 6e22 0a20 2020 204e 414d 4520 3d20  son".    NAME = 
+000001b0: 2243 6170 6163 6974 6f72 2053 7461 7465  "Capacitor State
+000001c0: 2043 6861 6e67 6520 436f 756e 7473 220a   Change Counts".
+000001d0: 0a20 2020 2064 6566 2067 656e 6572 6174  .    def generat
+000001e0: 6528 7365 6c66 2c20 6f75 7470 7574 5f64  e(self, output_d
+000001f0: 6972 293a 0a20 2020 2020 2020 2064 6174  ir):.        dat
+00000200: 6120 3d20 7b22 7363 656e 6172 696f 7322  a = {"scenarios"
+00000210: 3a20 5b5d 7d0a 2020 2020 2020 2020 666f  : []}.        fo
+00000220: 7220 7363 656e 6172 696f 2069 6e20 7365  r scenario in se
+00000230: 6c66 2e5f 7265 7375 6c74 732e 7363 656e  lf._results.scen
+00000240: 6172 696f 733a 0a20 2020 2020 2020 2020  arios:.         
+00000250: 2020 2073 6365 6e61 7269 6f5f 6461 7461     scenario_data
+00000260: 203d 207b 226e 616d 6522 3a20 7363 656e   = {"name": scen
+00000270: 6172 696f 2e6e 616d 652c 2022 6361 7061  ario.name, "capa
+00000280: 6369 746f 7273 223a 205b 5d7d 0a20 2020  citors": []}.   
+00000290: 2020 2020 2020 2020 2066 6f72 2063 6170           for cap
+000002a0: 6163 6974 6f72 2069 6e20 7363 656e 6172  acitor in scenar
+000002b0: 696f 2e6c 6973 745f 656c 656d 656e 745f  io.list_element_
+000002c0: 6e61 6d65 7328 2243 6170 6163 6974 6f72  names("Capacitor
+000002d0: 7322 293a 0a20 2020 2020 2020 2020 2020  s"):.           
+000002e0: 2020 2020 2063 6861 6e67 655f 636f 756e       change_coun
+000002f0: 7420 3d20 696e 7428 7363 656e 6172 696f  t = int(scenario
+00000300: 2e67 6574 5f65 6c65 6d65 6e74 5f70 726f  .get_element_pro
+00000310: 7065 7274 795f 7661 6c75 6528 0a20 2020  perty_value(.   
+00000320: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000330: 2022 4361 7061 6369 746f 7273 222c 2022   "Capacitors", "
+00000340: 5472 6163 6b53 7461 7465 4368 616e 6765  TrackStateChange
+00000350: 7322 2c20 6361 7061 6369 746f 720a 2020  s", capacitor.  
+00000360: 2020 2020 2020 2020 2020 2020 2020 2929                ))
+00000370: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000380: 2063 6861 6e67 6573 203d 207b 226e 616d   changes = {"nam
+00000390: 6522 3a20 6361 7061 6369 746f 722c 2022  e": capacitor, "
+000003a0: 6368 616e 6765 5f63 6f75 6e74 223a 2063  change_count": c
+000003b0: 6861 6e67 655f 636f 756e 747d 0a20 2020  hange_count}.   
+000003c0: 2020 2020 2020 2020 2020 2020 2073 6365               sce
+000003d0: 6e61 7269 6f5f 6461 7461 5b22 6361 7061  nario_data["capa
+000003e0: 6369 746f 7273 225d 2e61 7070 656e 6428  citors"].append(
+000003f0: 6368 616e 6765 7329 0a20 2020 2020 2020  changes).       
+00000400: 2020 2020 2064 6174 615b 2273 6365 6e61       data["scena
+00000410: 7269 6f73 225d 2e61 7070 656e 6428 7363  rios"].append(sc
+00000420: 656e 6172 696f 5f64 6174 6129 0a0a 2020  enario_data)..  
+00000430: 2020 2020 2020 6669 6c65 6e61 6d65 203d        filename =
+00000440: 206f 732e 7061 7468 2e6a 6f69 6e28 6f75   os.path.join(ou
+00000450: 7470 7574 5f64 6972 2c20 7365 6c66 2e46  tput_dir, self.F
+00000460: 494c 454e 414d 4529 0a20 2020 2020 2020  ILENAME).       
+00000470: 2064 756d 705f 6461 7461 2864 6174 612c   dump_data(data,
+00000480: 2066 696c 656e 616d 652c 2069 6e64 656e   filename, inden
+00000490: 743d 3229 0a20 2020 2020 2020 206c 6f67  t=2).        log
+000004a0: 6765 722e 696e 666f 2822 4765 6e65 7261  ger.info("Genera
+000004b0: 7465 6420 2573 222c 2066 696c 656e 616d  ted %s", filenam
+000004c0: 6529 0a20 2020 2020 2020 2072 6574 7572  e).        retur
+000004d0: 6e20 6669 6c65 6e61 6d65 0a0a 2020 2020  n filename..    
+000004e0: 4073 7461 7469 636d 6574 686f 640a 2020  @staticmethod.  
+000004f0: 2020 6465 6620 6765 745f 7265 7175 6972    def get_requir
+00000500: 6564 5f65 7870 6f72 7473 2873 696d 756c  ed_exports(simul
+00000510: 6174 696f 6e5f 636f 6e66 6967 293a 0a20  ation_config):. 
+00000520: 2020 2020 2020 2072 6574 7572 6e20 7b0a         return {.
+00000530: 2020 2020 2020 2020 2020 2020 2243 6170              "Cap
+00000540: 6163 6974 6f72 7322 3a20 5b0a 2020 2020  acitors": [.    
+00000550: 2020 2020 2020 2020 2020 2020 7b0a 2020              {.  
+00000560: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000570: 2020 2270 726f 7065 7274 7922 3a20 2254    "property": "T
+00000580: 7261 636b 5374 6174 6543 6861 6e67 6573  rackStateChanges
+00000590: 222c 0a20 2020 2020 2020 2020 2020 2020  ",.             
+000005a0: 2020 2020 2020 2022 7374 6f72 655f 7661         "store_va
+000005b0: 6c75 6573 5f74 7970 6522 3a20 2263 6861  lues_type": "cha
+000005c0: 6e67 655f 636f 756e 7422 2c0a 2020 2020  nge_count",.    
+000005d0: 2020 2020 2020 2020 2020 2020 7d0a 2020              }.  
+000005e0: 2020 2020 2020 2020 2020 5d0a 2020 2020            ].    
+000005f0: 2020 2020 7d0a 0a20 2020 2040 7374 6174      }..    @stat
+00000600: 6963 6d65 7468 6f64 0a20 2020 2064 6566  icmethod.    def
+00000610: 2067 6574 5f72 6571 7569 7265 645f 7363   get_required_sc
+00000620: 656e 6172 696f 5f6e 616d 6573 2829 3a0a  enario_names():.
+00000630: 2020 2020 2020 2020 7265 7475 726e 2073          return s
+00000640: 6574 2829 0a                             et().
```

### Comparing `nrel_pydss-3.1.3/src/pydss/reports/feeder_losses.py` & `nrel_pydss-3.1.4/src/pydss/reports/feeder_losses.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,447 +1,435 @@
-00000000: 0d0a 6672 6f6d 2074 7970 696e 6720 696d  ..from typing im
-00000010: 706f 7274 2041 6e6e 6f74 6174 6564 2c20  port Annotated, 
-00000020: 4469 6374 0d0a 6672 6f6d 2064 6174 6574  Dict..from datet
-00000030: 696d 6520 696d 706f 7274 2074 696d 6564  ime import timed
-00000040: 656c 7461 0d0a 696d 706f 7274 206d 6174  elta..import mat
-00000050: 680d 0a69 6d70 6f72 7420 6f73 0d0a 0d0a  h..import os....
-00000060: 6672 6f6d 2070 7964 616e 7469 6320 696d  from pydantic im
-00000070: 706f 7274 2042 6173 654d 6f64 656c 2c20  port BaseModel, 
-00000080: 4669 656c 640d 0a66 726f 6d20 7079 6461  Field..from pyda
-00000090: 6e74 6963 2069 6d70 6f72 7420 436f 6e66  ntic import Conf
-000000a0: 6967 4469 6374 0d0a 6672 6f6d 206c 6f67  igDict..from log
-000000b0: 7572 7520 696d 706f 7274 206c 6f67 6765  uru import logge
-000000c0: 720d 0a0d 0a66 726f 6d20 7079 6473 732e  r....from pydss.
-000000d0: 7265 706f 7274 732e 7265 706f 7274 7320  reports.reports 
-000000e0: 696d 706f 7274 2052 6570 6f72 7442 6173  import ReportBas
-000000f0: 650d 0a0d 0a63 6c61 7373 2046 6565 6465  e....class Feede
-00000100: 724c 6f73 7365 734d 6574 7269 6373 4d6f  rLossesMetricsMo
-00000110: 6465 6c28 4261 7365 4d6f 6465 6c29 3a0d  del(BaseModel):.
-00000120: 0a20 2020 2022 2222 4461 7461 206d 6f64  .    """Data mod
-00000130: 656c 2066 6f72 206d 6574 7269 6373 2064  el for metrics d
-00000140: 6573 6372 6962 696e 6720 6665 6564 6572  escribing feeder
-00000150: 206c 6f73 7365 7322 2222 0d0a 2020 2020   losses"""..    
-00000160: 6d6f 6465 6c5f 636f 6e66 6967 203d 2043  model_config = C
-00000170: 6f6e 6669 6744 6963 7428 7469 746c 653d  onfigDict(title=
-00000180: 2246 6565 6465 724c 6f73 7365 734d 6574  "FeederLossesMet
-00000190: 7269 6373 4d6f 6465 6c22 2c20 7374 725f  ricsModel", str_
-000001a0: 7374 7269 705f 7768 6974 6573 7061 6365  strip_whitespace
-000001b0: 3d54 7275 652c 2076 616c 6964 6174 655f  =True, validate_
-000001c0: 6173 7369 676e 6d65 6e74 3d54 7275 652c  assignment=True,
-000001d0: 2076 616c 6964 6174 655f 6465 6661 756c   validate_defaul
-000001e0: 743d 5472 7565 2c20 6578 7472 613d 2266  t=True, extra="f
-000001f0: 6f72 6269 6422 2c20 7573 655f 656e 756d  orbid", use_enum
-00000200: 5f76 616c 7565 733d 4661 6c73 6529 0d0a  _values=False)..
-00000210: 0d0a 2020 2020 746f 7461 6c5f 6c6f 7373  ..    total_loss
-00000220: 6573 5f6b 7768 3a20 416e 6e6f 7461 7465  es_kwh: Annotate
-00000230: 645b 0d0a 2020 2020 2020 2020 666c 6f61  d[..        floa
-00000240: 742c 0d0a 2020 2020 2020 2020 4669 656c  t,..        Fiel
-00000250: 6428 0d0a 2020 2020 2020 2020 2020 2020  d(..            
-00000260: 4e6f 6e65 2c20 0d0a 2020 2020 2020 2020  None, ..        
-00000270: 2020 2020 7469 746c 653d 2274 6f74 616c      title="total
-00000280: 5f6c 6f73 7365 735f 6b77 6822 2c0d 0a20  _losses_kwh",.. 
-00000290: 2020 2020 2020 2020 2020 2064 6573 6372             descr
-000002a0: 6970 7469 6f6e 3d22 546f 7461 6c20 6c6f  iption="Total lo
-000002b0: 7373 6573 2069 6e20 7468 6520 6369 7263  sses in the circ
-000002c0: 7569 7422 2c0d 0a20 2020 2020 2020 2029  uit",..        )
-000002d0: 5d0d 0a20 2020 206c 696e 655f 6c6f 7373  ]..    line_loss
-000002e0: 6573 5f6b 7768 3a20 2041 6e6e 6f74 6174  es_kwh:  Annotat
-000002f0: 6564 5b0d 0a20 2020 2020 2020 2066 6c6f  ed[..        flo
-00000300: 6174 2c0d 0a20 2020 2020 2020 2046 6965  at,..        Fie
-00000310: 6c64 280d 0a20 2020 2020 2020 2020 2020  ld(..           
-00000320: 204e 6f6e 652c 200d 0a20 2020 2020 2020   None, ..       
-00000330: 2020 2020 2074 6974 6c65 3d22 6c69 6e65       title="line
-00000340: 5f6c 6f73 7365 735f 6b77 6822 2c0d 0a20  _losses_kwh",.. 
-00000350: 2020 2020 2020 2020 2020 2064 6573 6372             descr
-00000360: 6970 7469 6f6e 3d22 546f 7461 6c20 6c69  iption="Total li
-00000370: 6e65 206c 6f73 7365 7322 2c0d 0a20 2020  ne losses",..   
-00000380: 2020 2020 2029 5d0d 0a20 2020 2074 7261       )]..    tra
-00000390: 6e73 666f 726d 6572 5f6c 6f73 7365 735f  nsformer_losses_
-000003a0: 6b77 683a 2020 416e 6e6f 7461 7465 645b  kwh:  Annotated[
-000003b0: 0d0a 2020 2020 2020 2020 666c 6f61 742c  ..        float,
-000003c0: 0d0a 2020 2020 2020 2020 4669 656c 6428  ..        Field(
-000003d0: 0d0a 2020 2020 2020 2020 2020 2020 4e6f  ..            No
-000003e0: 6e65 2c20 0d0a 2020 2020 2020 2020 2020  ne, ..          
-000003f0: 2020 7469 746c 653d 2274 7261 6e73 666f    title="transfo
-00000400: 726d 6572 5f6c 6f73 7365 735f 6b77 6822  rmer_losses_kwh"
-00000410: 2c0d 0a20 2020 2020 2020 2020 2020 2064  ,..            d
-00000420: 6573 6372 6970 7469 6f6e 3d22 546f 7461  escription="Tota
-00000430: 6c20 7472 616e 7366 6f72 6d65 7220 6c6f  l transformer lo
-00000440: 7373 6573 222c 0d0a 2020 2020 2020 2020  sses",..        
-00000450: 295d 0d0a 2020 2020 746f 7461 6c5f 6c6f  )]..    total_lo
-00000460: 6164 5f64 656d 616e 645f 6b77 683a 2020  ad_demand_kwh:  
-00000470: 416e 6e6f 7461 7465 645b 0d0a 2020 2020  Annotated[..    
-00000480: 2020 2020 666c 6f61 742c 0d0a 2020 2020      float,..    
-00000490: 2020 2020 4669 656c 6428 0d0a 2020 2020      Field(..    
-000004a0: 2020 2020 2020 2020 4e6f 6e65 2c20 0d0a          None, ..
-000004b0: 2020 2020 2020 2020 2020 2020 7469 746c              titl
-000004c0: 653d 2274 6f74 616c 5f6c 6f61 645f 6465  e="total_load_de
-000004d0: 6d61 6e64 5f6b 7768 222c 0d0a 2020 2020  mand_kwh",..    
-000004e0: 2020 2020 2020 2020 6465 7363 7269 7074          descript
-000004f0: 696f 6e3d 2254 6f74 616c 2070 6f77 6572  ion="Total power
-00000500: 206f 7574 7075 7420 6f66 206c 6f61 6473   output of loads
-00000510: 222c 0d0a 2020 2020 2020 2020 295d 0d0a  ",..        )]..
-00000520: 0d0a 0d0a 636c 6173 7320 5369 6d75 6c61  ....class Simula
-00000530: 7469 6f6e 4665 6564 6572 4c6f 7373 6573  tionFeederLosses
-00000540: 4d65 7472 6963 734d 6f64 656c 2842 6173  MetricsModel(Bas
-00000550: 654d 6f64 656c 293a 0d0a 2020 2020 7363  eModel):..    sc
-00000560: 656e 6172 696f 733a 2044 6963 745b 7374  enarios: Dict[st
-00000570: 722c 2046 6565 6465 724c 6f73 7365 734d  r, FeederLossesM
-00000580: 6574 7269 6373 4d6f 6465 6c5d 203d 2046  etricsModel] = F
-00000590: 6965 6c64 280d 0a20 2020 2020 2020 2074  ield(..        t
-000005a0: 6974 6c65 3d22 7363 656e 6172 696f 7322  itle="scenarios"
-000005b0: 2c0d 0a20 2020 2020 2020 2064 6573 6372  ,..        descr
-000005c0: 6970 7469 6f6e 3d22 4665 6564 6572 206c  iption="Feeder l
-000005d0: 6f73 7365 7320 6279 2070 7964 7373 2073  osses by pydss s
-000005e0: 6365 6e61 7269 6f20 6e61 6d65 222c 0d0a  cenario name",..
-000005f0: 2020 2020 290d 0a0d 0a0d 0a64 6566 2063      )......def c
-00000600: 6f6d 7061 7265 5f66 6565 6465 725f 6c6f  ompare_feeder_lo
-00000610: 7373 6573 280d 0a20 2020 2020 2020 206d  sses(..        m
-00000620: 6574 7269 6373 313a 2053 696d 756c 6174  etrics1: Simulat
+00000000: 0a66 726f 6d20 7479 7069 6e67 2069 6d70  .from typing imp
+00000010: 6f72 7420 416e 6e6f 7461 7465 642c 2044  ort Annotated, D
+00000020: 6963 740a 6672 6f6d 2064 6174 6574 696d  ict.from datetim
+00000030: 6520 696d 706f 7274 2074 696d 6564 656c  e import timedel
+00000040: 7461 0a69 6d70 6f72 7420 6d61 7468 0a69  ta.import math.i
+00000050: 6d70 6f72 7420 6f73 0a0a 6672 6f6d 2070  mport os..from p
+00000060: 7964 616e 7469 6320 696d 706f 7274 2042  ydantic import B
+00000070: 6173 654d 6f64 656c 2c20 4669 656c 640a  aseModel, Field.
+00000080: 6672 6f6d 2070 7964 616e 7469 6320 696d  from pydantic im
+00000090: 706f 7274 2043 6f6e 6669 6744 6963 740a  port ConfigDict.
+000000a0: 6672 6f6d 206c 6f67 7572 7520 696d 706f  from loguru impo
+000000b0: 7274 206c 6f67 6765 720a 0a66 726f 6d20  rt logger..from 
+000000c0: 7079 6473 732e 7265 706f 7274 732e 7265  pydss.reports.re
+000000d0: 706f 7274 7320 696d 706f 7274 2052 6570  ports import Rep
+000000e0: 6f72 7442 6173 650a 0a63 6c61 7373 2046  ortBase..class F
+000000f0: 6565 6465 724c 6f73 7365 734d 6574 7269  eederLossesMetri
+00000100: 6373 4d6f 6465 6c28 4261 7365 4d6f 6465  csModel(BaseMode
+00000110: 6c29 3a0a 2020 2020 2222 2244 6174 6120  l):.    """Data 
+00000120: 6d6f 6465 6c20 666f 7220 6d65 7472 6963  model for metric
+00000130: 7320 6465 7363 7269 6269 6e67 2066 6565  s describing fee
+00000140: 6465 7220 6c6f 7373 6573 2222 220a 2020  der losses""".  
+00000150: 2020 6d6f 6465 6c5f 636f 6e66 6967 203d    model_config =
+00000160: 2043 6f6e 6669 6744 6963 7428 7469 746c   ConfigDict(titl
+00000170: 653d 2246 6565 6465 724c 6f73 7365 734d  e="FeederLossesM
+00000180: 6574 7269 6373 4d6f 6465 6c22 2c20 7374  etricsModel", st
+00000190: 725f 7374 7269 705f 7768 6974 6573 7061  r_strip_whitespa
+000001a0: 6365 3d54 7275 652c 2076 616c 6964 6174  ce=True, validat
+000001b0: 655f 6173 7369 676e 6d65 6e74 3d54 7275  e_assignment=Tru
+000001c0: 652c 2076 616c 6964 6174 655f 6465 6661  e, validate_defa
+000001d0: 756c 743d 5472 7565 2c20 6578 7472 613d  ult=True, extra=
+000001e0: 2266 6f72 6269 6422 2c20 7573 655f 656e  "forbid", use_en
+000001f0: 756d 5f76 616c 7565 733d 4661 6c73 6529  um_values=False)
+00000200: 0a0a 2020 2020 746f 7461 6c5f 6c6f 7373  ..    total_loss
+00000210: 6573 5f6b 7768 3a20 416e 6e6f 7461 7465  es_kwh: Annotate
+00000220: 645b 0a20 2020 2020 2020 2066 6c6f 6174  d[.        float
+00000230: 2c0a 2020 2020 2020 2020 4669 656c 6428  ,.        Field(
+00000240: 0a20 2020 2020 2020 2020 2020 204e 6f6e  .            Non
+00000250: 652c 200a 2020 2020 2020 2020 2020 2020  e, .            
+00000260: 7469 746c 653d 2274 6f74 616c 5f6c 6f73  title="total_los
+00000270: 7365 735f 6b77 6822 2c0a 2020 2020 2020  ses_kwh",.      
+00000280: 2020 2020 2020 6465 7363 7269 7074 696f        descriptio
+00000290: 6e3d 2254 6f74 616c 206c 6f73 7365 7320  n="Total losses 
+000002a0: 696e 2074 6865 2063 6972 6375 6974 222c  in the circuit",
+000002b0: 0a20 2020 2020 2020 2029 5d0a 2020 2020  .        )].    
+000002c0: 6c69 6e65 5f6c 6f73 7365 735f 6b77 683a  line_losses_kwh:
+000002d0: 2020 416e 6e6f 7461 7465 645b 0a20 2020    Annotated[.   
+000002e0: 2020 2020 2066 6c6f 6174 2c0a 2020 2020       float,.    
+000002f0: 2020 2020 4669 656c 6428 0a20 2020 2020      Field(.     
+00000300: 2020 2020 2020 204e 6f6e 652c 200a 2020         None, .  
+00000310: 2020 2020 2020 2020 2020 7469 746c 653d            title=
+00000320: 226c 696e 655f 6c6f 7373 6573 5f6b 7768  "line_losses_kwh
+00000330: 222c 0a20 2020 2020 2020 2020 2020 2064  ",.            d
+00000340: 6573 6372 6970 7469 6f6e 3d22 546f 7461  escription="Tota
+00000350: 6c20 6c69 6e65 206c 6f73 7365 7322 2c0a  l line losses",.
+00000360: 2020 2020 2020 2020 295d 0a20 2020 2074          )].    t
+00000370: 7261 6e73 666f 726d 6572 5f6c 6f73 7365  ransformer_losse
+00000380: 735f 6b77 683a 2020 416e 6e6f 7461 7465  s_kwh:  Annotate
+00000390: 645b 0a20 2020 2020 2020 2066 6c6f 6174  d[.        float
+000003a0: 2c0a 2020 2020 2020 2020 4669 656c 6428  ,.        Field(
+000003b0: 0a20 2020 2020 2020 2020 2020 204e 6f6e  .            Non
+000003c0: 652c 200a 2020 2020 2020 2020 2020 2020  e, .            
+000003d0: 7469 746c 653d 2274 7261 6e73 666f 726d  title="transform
+000003e0: 6572 5f6c 6f73 7365 735f 6b77 6822 2c0a  er_losses_kwh",.
+000003f0: 2020 2020 2020 2020 2020 2020 6465 7363              desc
+00000400: 7269 7074 696f 6e3d 2254 6f74 616c 2074  ription="Total t
+00000410: 7261 6e73 666f 726d 6572 206c 6f73 7365  ransformer losse
+00000420: 7322 2c0a 2020 2020 2020 2020 295d 0a20  s",.        )]. 
+00000430: 2020 2074 6f74 616c 5f6c 6f61 645f 6465     total_load_de
+00000440: 6d61 6e64 5f6b 7768 3a20 2041 6e6e 6f74  mand_kwh:  Annot
+00000450: 6174 6564 5b0a 2020 2020 2020 2020 666c  ated[.        fl
+00000460: 6f61 742c 0a20 2020 2020 2020 2046 6965  oat,.        Fie
+00000470: 6c64 280a 2020 2020 2020 2020 2020 2020  ld(.            
+00000480: 4e6f 6e65 2c20 0a20 2020 2020 2020 2020  None, .         
+00000490: 2020 2074 6974 6c65 3d22 746f 7461 6c5f     title="total_
+000004a0: 6c6f 6164 5f64 656d 616e 645f 6b77 6822  load_demand_kwh"
+000004b0: 2c0a 2020 2020 2020 2020 2020 2020 6465  ,.            de
+000004c0: 7363 7269 7074 696f 6e3d 2254 6f74 616c  scription="Total
+000004d0: 2070 6f77 6572 206f 7574 7075 7420 6f66   power output of
+000004e0: 206c 6f61 6473 222c 0a20 2020 2020 2020   loads",.       
+000004f0: 2029 5d0a 0a0a 636c 6173 7320 5369 6d75   )]...class Simu
+00000500: 6c61 7469 6f6e 4665 6564 6572 4c6f 7373  lationFeederLoss
+00000510: 6573 4d65 7472 6963 734d 6f64 656c 2842  esMetricsModel(B
+00000520: 6173 654d 6f64 656c 293a 0a20 2020 2073  aseModel):.    s
+00000530: 6365 6e61 7269 6f73 3a20 4469 6374 5b73  cenarios: Dict[s
+00000540: 7472 2c20 4665 6564 6572 4c6f 7373 6573  tr, FeederLosses
+00000550: 4d65 7472 6963 734d 6f64 656c 5d20 3d20  MetricsModel] = 
+00000560: 4669 656c 6428 0a20 2020 2020 2020 2074  Field(.        t
+00000570: 6974 6c65 3d22 7363 656e 6172 696f 7322  itle="scenarios"
+00000580: 2c0a 2020 2020 2020 2020 6465 7363 7269  ,.        descri
+00000590: 7074 696f 6e3d 2246 6565 6465 7220 6c6f  ption="Feeder lo
+000005a0: 7373 6573 2062 7920 7079 6473 7320 7363  sses by pydss sc
+000005b0: 656e 6172 696f 206e 616d 6522 2c0a 2020  enario name",.  
+000005c0: 2020 290a 0a0a 6465 6620 636f 6d70 6172    )...def compar
+000005d0: 655f 6665 6564 6572 5f6c 6f73 7365 7328  e_feeder_losses(
+000005e0: 0a20 2020 2020 2020 206d 6574 7269 6373  .        metrics
+000005f0: 313a 2053 696d 756c 6174 696f 6e46 6565  1: SimulationFee
+00000600: 6465 724c 6f73 7365 734d 6574 7269 6373  derLossesMetrics
+00000610: 4d6f 6465 6c2c 0a20 2020 2020 2020 206d  Model,.        m
+00000620: 6574 7269 6373 323a 2053 696d 756c 6174  etrics2: Simulat
 00000630: 696f 6e46 6565 6465 724c 6f73 7365 734d  ionFeederLossesM
-00000640: 6574 7269 6373 4d6f 6465 6c2c 0d0a 2020  etricsModel,..  
-00000650: 2020 2020 2020 6d65 7472 6963 7332 3a20        metrics2: 
-00000660: 5369 6d75 6c61 7469 6f6e 4665 6564 6572  SimulationFeeder
-00000670: 4c6f 7373 6573 4d65 7472 6963 734d 6f64  LossesMetricsMod
-00000680: 656c 2c0d 0a20 2020 2020 2020 2072 656c  el,..        rel
-00000690: 5f74 6f6c 3d30 2e30 3030 3030 312c 0d0a  _tol=0.000001,..
-000006a0: 293a 0d0a 2020 2020 2222 2243 6f6d 7061  ):..    """Compa
-000006b0: 7265 7320 7468 6520 7661 6c75 6573 206f  res the values o
-000006c0: 6620 7477 6f20 696e 7374 616e 6365 7320  f two instances 
-000006d0: 6f66 2046 6565 6465 724c 6f73 7365 734d  of FeederLossesM
-000006e0: 6574 7269 6373 4d6f 6465 6c2e 0d0a 0d0a  etricsModel.....
-000006f0: 2020 2020 5265 7475 726e 730d 0a20 2020      Returns..   
-00000700: 202d 2d2d 2d2d 2d2d 0d0a 2020 2020 626f   -------..    bo
-00000710: 6f6c 0d0a 2020 2020 2020 2020 5265 7475  ol..        Retu
-00000720: 726e 2054 7275 6520 6966 2074 6865 7920  rn True if they 
-00000730: 6d61 7463 682e 0d0a 0d0a 2020 2020 2222  match.....    ""
-00000740: 220d 0a20 2020 206d 6174 6368 203d 2054  "..    match = T
-00000750: 7275 650d 0a20 2020 2066 6f72 2073 6365  rue..    for sce
-00000760: 6e61 7269 6f20 696e 206d 6574 7269 6373  nario in metrics
-00000770: 312e 7363 656e 6172 696f 733a 0d0a 2020  1.scenarios:..  
-00000780: 2020 2020 2020 666f 7220 6669 656c 6420        for field 
-00000790: 696e 2046 6565 6465 724c 6f73 7365 734d  in FeederLossesM
-000007a0: 6574 7269 6373 4d6f 6465 6c2e 6d6f 6465  etricsModel.mode
-000007b0: 6c5f 6669 656c 6473 3a0d 0a20 2020 2020  l_fields:..     
-000007c0: 2020 2020 2020 2076 616c 3120 3d20 6765         val1 = ge
-000007d0: 7461 7474 7228 6d65 7472 6963 7331 2e73  tattr(metrics1.s
-000007e0: 6365 6e61 7269 6f73 5b73 6365 6e61 7269  cenarios[scenari
-000007f0: 6f5d 2c20 6669 656c 6429 0d0a 2020 2020  o], field)..    
-00000800: 2020 2020 2020 2020 7661 6c32 203d 2067          val2 = g
-00000810: 6574 6174 7472 286d 6574 7269 6373 322e  etattr(metrics2.
-00000820: 7363 656e 6172 696f 735b 7363 656e 6172  scenarios[scenar
-00000830: 696f 5d2c 2066 6965 6c64 290d 0a20 2020  io], field)..   
-00000840: 2020 2020 2020 2020 2069 6620 6e6f 7420           if not 
-00000850: 6d61 7468 2e69 7363 6c6f 7365 2876 616c  math.isclose(val
-00000860: 312c 2076 616c 322c 2072 656c 5f74 6f6c  1, val2, rel_tol
-00000870: 3d72 656c 5f74 6f6c 293a 0d0a 2020 2020  =rel_tol):..    
-00000880: 2020 2020 2020 2020 2020 2020 6c6f 6767              logg
-00000890: 6572 2e65 7272 6f72 2822 6669 656c 643d  er.error("field=
-000008a0: 2573 206d 6973 6d61 7463 6820 2573 203a  %s mismatch %s :
-000008b0: 2025 7322 2c20 6669 656c 642c 2076 616c   %s", field, val
-000008c0: 312c 2076 616c 3229 0d0a 2020 2020 2020  1, val2)..      
-000008d0: 2020 2020 2020 2020 2020 6d61 7463 6820            match 
-000008e0: 3d20 4661 6c73 650d 0a0d 0a20 2020 2072  = False....    r
-000008f0: 6574 7572 6e20 6d61 7463 680d 0a0d 0a0d  eturn match.....
-00000900: 0a63 6c61 7373 2046 6565 6465 724c 6f73  .class FeederLos
-00000910: 7365 7352 6570 6f72 7428 5265 706f 7274  sesReport(Report
-00000920: 4261 7365 293a 0d0a 2020 2020 2222 2252  Base):..    """R
-00000930: 6570 6f72 7473 2074 6865 2066 6565 6465  eports the feede
-00000940: 7220 6c6f 7373 6573 2e0d 0a0d 0a20 2020  r losses.....   
-00000950: 2054 6865 2072 6570 6f72 7420 6765 6e65   The report gene
-00000960: 7261 7465 7320 6120 6665 6564 6572 5f6c  rates a feeder_l
-00000970: 6f73 7365 732e 6a73 6f6e 206f 7574 7075  osses.json outpu
-00000980: 7420 6669 6c65 2e0d 0a0d 0a20 2020 2022  t file.....    "
-00000990: 2222 0d0a 0d0a 2020 2020 4649 4c45 4e41  ""....    FILENA
-000009a0: 4d45 203d 2022 6665 6564 6572 5f6c 6f73  ME = "feeder_los
-000009b0: 7365 732e 6a73 6f6e 220d 0a20 2020 204e  ses.json"..    N
-000009c0: 414d 4520 3d20 2246 6565 6465 7220 4c6f  AME = "Feeder Lo
-000009d0: 7373 6573 220d 0a20 2020 2044 4546 4155  sses"..    DEFAU
-000009e0: 4c54 5320 3d20 7b0d 0a20 2020 2020 2020  LTS = {..       
-000009f0: 2022 7374 6f72 655f 616c 6c5f 7469 6d65   "store_all_time
-00000a00: 5f70 6f69 6e74 7322 3a20 4661 6c73 652c  _points": False,
-00000a10: 0d0a 2020 2020 7d0d 0a0d 0a20 2020 2064  ..    }....    d
-00000a20: 6566 2067 656e 6572 6174 6528 7365 6c66  ef generate(self
-00000a30: 2c20 6f75 7470 7574 5f64 6972 293a 0d0a  , output_dir):..
-00000a40: 2020 2020 2020 2020 7265 736f 6c75 7469          resoluti
-00000a50: 6f6e 203d 2073 656c 662e 5f67 6574 5f73  on = self._get_s
-00000a60: 696d 756c 6174 696f 6e5f 7265 736f 6c75  imulation_resolu
-00000a70: 7469 6f6e 2829 0d0a 2020 2020 2020 2020  tion()..        
-00000a80: 746f 5f6b 7768 203d 2072 6573 6f6c 7574  to_kwh = resolut
-00000a90: 696f 6e20 2f20 7469 6d65 6465 6c74 6128  ion / timedelta(
-00000aa0: 686f 7572 733d 3129 0d0a 2020 2020 2020  hours=1)..      
-00000ab0: 2020 6173 7365 7274 206c 656e 2873 656c    assert len(sel
-00000ac0: 662e 5f72 6573 756c 7473 2e73 6365 6e61  f._results.scena
-00000ad0: 7269 6f73 2920 3e3d 2031 0d0a 2020 2020  rios) >= 1..    
-00000ae0: 2020 2020 7363 656e 6172 696f 7320 3d20      scenarios = 
-00000af0: 7b7d 0d0a 2020 2020 2020 2020 666f 7220  {}..        for 
-00000b00: 7363 656e 6172 696f 2069 6e20 7365 6c66  scenario in self
-00000b10: 2e5f 7265 7375 6c74 732e 7363 656e 6172  ._results.scenar
-00000b20: 696f 733a 0d0a 2020 2020 2020 2020 2020  ios:..          
-00000b30: 2020 696e 7075 7473 203d 2046 6565 6465    inputs = Feede
-00000b40: 724c 6f73 7365 7352 6570 6f72 742e 6765  rLossesReport.ge
-00000b50: 745f 696e 7075 7473 5f66 726f 6d5f 6465  t_inputs_from_de
-00000b60: 6661 756c 7473 2873 656c 662e 5f73 6574  faults(self._set
-00000b70: 7469 6e67 732c 2073 656c 662e 4e41 4d45  tings, self.NAME
-00000b80: 290d 0a20 2020 2020 2020 2020 2020 2069  )..            i
-00000b90: 6620 696e 7075 7473 5b22 7374 6f72 655f  f inputs["store_
-00000ba0: 616c 6c5f 7469 6d65 5f70 6f69 6e74 7322  all_time_points"
-00000bb0: 5d3a 0d0a 2020 2020 2020 2020 2020 2020  ]:..            
-00000bc0: 2020 2020 7363 656e 6172 696f 735b 7363      scenarios[sc
-00000bd0: 656e 6172 696f 2e6e 616d 655d 203d 2073  enario.name] = s
-00000be0: 656c 662e 5f67 656e 6572 6174 655f 6672  elf._generate_fr
-00000bf0: 6f6d 5f61 6c6c 5f74 696d 655f 706f 696e  om_all_time_poin
-00000c00: 7473 2873 6365 6e61 7269 6f2c 2074 6f5f  ts(scenario, to_
-00000c10: 6b77 6829 0d0a 2020 2020 2020 2020 2020  kwh)..          
-00000c20: 2020 656c 7365 3a0d 0a20 2020 2020 2020    else:..       
-00000c30: 2020 2020 2020 2020 2073 6365 6e61 7269           scenari
-00000c40: 6f73 5b73 6365 6e61 7269 6f2e 6e61 6d65  os[scenario.name
-00000c50: 5d20 3d20 7365 6c66 2e5f 6765 6e65 7261  ] = self._genera
-00000c60: 7465 5f66 726f 6d5f 696e 5f6d 656d 6f72  te_from_in_memor
-00000c70: 795f 6d65 7472 6963 7328 7363 656e 6172  y_metrics(scenar
-00000c80: 696f 2c20 746f 5f6b 7768 290d 0a0d 0a20  io, to_kwh).... 
-00000c90: 2020 2020 2020 206d 6f64 656c 203d 2053         model = S
-00000ca0: 696d 756c 6174 696f 6e46 6565 6465 724c  imulationFeederL
-00000cb0: 6f73 7365 734d 6574 7269 6373 4d6f 6465  ossesMetricsMode
-00000cc0: 6c28 7363 656e 6172 696f 733d 7363 656e  l(scenarios=scen
-00000cd0: 6172 696f 7329 0d0a 2020 2020 2020 2020  arios)..        
-00000ce0: 6669 6c65 6e61 6d65 203d 206f 732e 7061  filename = os.pa
-00000cf0: 7468 2e6a 6f69 6e28 6f75 7470 7574 5f64  th.join(output_d
-00000d00: 6972 2c20 7365 6c66 2e46 494c 454e 414d  ir, self.FILENAM
-00000d10: 4529 0d0a 2020 2020 2020 2020 7769 7468  E)..        with
-00000d20: 206f 7065 6e28 6669 6c65 6e61 6d65 2c20   open(filename, 
-00000d30: 2277 2229 2061 7320 665f 6f75 743a 0d0a  "w") as f_out:..
-00000d40: 2020 2020 2020 2020 2020 2020 665f 6f75              f_ou
-00000d50: 742e 7772 6974 6528 6d6f 6465 6c2e 6d6f  t.write(model.mo
-00000d60: 6465 6c5f 6475 6d70 5f6a 736f 6e28 696e  del_dump_json(in
-00000d70: 6465 6e74 3d32 2929 0d0a 2020 2020 2020  dent=2))..      
-00000d80: 2020 2020 2020 665f 6f75 742e 7772 6974        f_out.writ
-00000d90: 6528 225c 6e22 290d 0a20 2020 2020 2020  e("\n")..       
-00000da0: 206c 6f67 6765 722e 696e 666f 2822 4765   logger.info("Ge
-00000db0: 6e65 7261 7465 6420 2573 222c 2066 696c  nerated %s", fil
-00000dc0: 656e 616d 6529 0d0a 2020 2020 2020 2020  ename)..        
-00000dd0: 7265 7475 726e 2066 696c 656e 616d 650d  return filename.
-00000de0: 0a0d 0a20 2020 2064 6566 205f 6765 6e65  ...    def _gene
-00000df0: 7261 7465 5f66 726f 6d5f 696e 5f6d 656d  rate_from_in_mem
-00000e00: 6f72 795f 6d65 7472 6963 7328 7365 6c66  ory_metrics(self
-00000e10: 2c20 7363 656e 6172 696f 2c20 746f 5f6b  , scenario, to_k
-00000e20: 7768 293a 0d0a 2020 2020 2020 2020 746f  wh):..        to
-00000e30: 7461 6c5f 6c6f 7373 6573 5f64 6963 7420  tal_losses_dict 
-00000e40: 3d20 7363 656e 6172 696f 2e67 6574 5f73  = scenario.get_s
-00000e50: 756d 6d65 645f 656c 656d 656e 745f 746f  ummed_element_to
-00000e60: 7461 6c28 2243 6972 6375 6974 7322 2c20  tal("Circuits", 
-00000e70: 224c 6f73 7365 7353 756d 2229 0d0a 2020  "LossesSum")..  
-00000e80: 2020 2020 2020 746f 7461 6c5f 6c6f 7373        total_loss
-00000e90: 6573 203d 2061 6273 286e 6578 7428 6974  es = abs(next(it
-00000ea0: 6572 2874 6f74 616c 5f6c 6f73 7365 735f  er(total_losses_
-00000eb0: 6469 6374 2e76 616c 7565 7328 2929 2929  dict.values())))
-00000ec0: 202f 2031 3030 3020 2320 4f70 656e 4453   / 1000 # OpenDS
-00000ed0: 5320 7265 706f 7274 7320 746f 7461 6c20  S reports total 
-00000ee0: 6c6f 7373 6573 2069 6e20 5761 7474 730d  losses in Watts.
-00000ef0: 0a20 2020 2020 2020 206c 696e 655f 6c6f  .        line_lo
-00000f00: 7373 6573 5f64 6963 7420 3d20 7363 656e  sses_dict = scen
-00000f10: 6172 696f 2e67 6574 5f73 756d 6d65 645f  ario.get_summed_
-00000f20: 656c 656d 656e 745f 746f 7461 6c28 2243  element_total("C
-00000f30: 6972 6375 6974 7322 2c20 224c 696e 654c  ircuits", "LineL
-00000f40: 6f73 7365 7353 756d 2229 0d0a 2020 2020  ossesSum")..    
-00000f50: 2020 2020 6c69 6e65 5f6c 6f73 7365 7320      line_losses 
-00000f60: 3d20 6162 7328 6e65 7874 2869 7465 7228  = abs(next(iter(
-00000f70: 6c69 6e65 5f6c 6f73 7365 735f 6469 6374  line_losses_dict
-00000f80: 2e76 616c 7565 7328 2929 2929 0d0a 2020  .values())))..  
-00000f90: 2020 2020 2020 7472 616e 7366 6f72 6d65        transforme
-00000fa0: 725f 6c6f 7373 6573 203d 2028 746f 7461  r_losses = (tota
-00000fb0: 6c5f 6c6f 7373 6573 202d 206c 696e 655f  l_losses - line_
-00000fc0: 6c6f 7373 6573 290d 0a20 2020 2020 2020  losses)..       
-00000fd0: 2074 6f74 616c 5f6c 6f61 645f 706f 7765   total_load_powe
-00000fe0: 725f 6469 6374 203d 2073 6365 6e61 7269  r_dict = scenari
-00000ff0: 6f2e 6765 745f 7375 6d6d 6564 5f65 6c65  o.get_summed_ele
-00001000: 6d65 6e74 5f74 6f74 616c 2822 4c6f 6164  ment_total("Load
-00001010: 7322 2c20 2250 6f77 6572 7353 756d 2229  s", "PowersSum")
-00001020: 0d0a 2020 2020 2020 2020 746f 7461 6c5f  ..        total_
-00001030: 6c6f 6164 5f70 6f77 6572 203d 2030 0d0a  load_power = 0..
-00001040: 2020 2020 2020 2020 666f 7220 7661 6c20          for val 
-00001050: 696e 2074 6f74 616c 5f6c 6f61 645f 706f  in total_load_po
-00001060: 7765 725f 6469 6374 2e76 616c 7565 7328  wer_dict.values(
-00001070: 293a 0d0a 2020 2020 2020 2020 2020 2020  ):..            
-00001080: 746f 7461 6c5f 6c6f 6164 5f70 6f77 6572  total_load_power
-00001090: 202b 3d20 7661 6c2e 7265 616c 0d0a 0d0a   += val.real....
-000010a0: 2020 2020 2020 2020 2320 544f 444f 3a20          # TODO: 
-000010b0: 746f 7461 6c20 6c6f 7373 6573 2061 7320  total losses as 
-000010c0: 6120 7065 7263 656e 7461 6765 206f 6620  a percentage of 
-000010d0: 746f 7461 6c20 6c6f 6164 2064 656d 616e  total load deman
-000010e0: 643f 0d0a 2020 2020 2020 2020 7265 7475  d?..        retu
-000010f0: 726e 2046 6565 6465 724c 6f73 7365 734d  rn FeederLossesM
-00001100: 6574 7269 6373 4d6f 6465 6c28 0d0a 2020  etricsModel(..  
-00001110: 2020 2020 2020 2020 2020 746f 7461 6c5f            total_
-00001120: 6c6f 7373 6573 5f6b 7768 3d74 6f74 616c  losses_kwh=total
-00001130: 5f6c 6f73 7365 7320 2a20 746f 5f6b 7768  _losses * to_kwh
-00001140: 2c0d 0a20 2020 2020 2020 2020 2020 206c  ,..            l
-00001150: 696e 655f 6c6f 7373 6573 5f6b 7768 3d6c  ine_losses_kwh=l
-00001160: 696e 655f 6c6f 7373 6573 202a 2074 6f5f  ine_losses * to_
-00001170: 6b77 682c 0d0a 2020 2020 2020 2020 2020  kwh,..          
-00001180: 2020 7472 616e 7366 6f72 6d65 725f 6c6f    transformer_lo
-00001190: 7373 6573 5f6b 7768 3d74 7261 6e73 666f  sses_kwh=transfo
-000011a0: 726d 6572 5f6c 6f73 7365 7320 2a20 746f  rmer_losses * to
-000011b0: 5f6b 7768 2c0d 0a20 2020 2020 2020 2020  _kwh,..         
-000011c0: 2020 2074 6f74 616c 5f6c 6f61 645f 6465     total_load_de
-000011d0: 6d61 6e64 5f6b 7768 3d74 6f74 616c 5f6c  mand_kwh=total_l
-000011e0: 6f61 645f 706f 7765 7220 2a20 746f 5f6b  oad_power * to_k
-000011f0: 7768 2c0d 0a20 2020 2020 2020 2029 0d0a  wh,..        )..
-00001200: 0d0a 2020 2020 6465 6620 5f67 656e 6572  ..    def _gener
-00001210: 6174 655f 6672 6f6d 5f61 6c6c 5f74 696d  ate_from_all_tim
-00001220: 655f 706f 696e 7473 2873 656c 662c 2073  e_points(self, s
-00001230: 6365 6e61 7269 6f2c 2074 6f5f 6b77 6829  cenario, to_kwh)
-00001240: 3a0d 0a20 2020 2020 2020 2064 665f 6c6f  :..        df_lo
-00001250: 7373 6573 203d 2073 6365 6e61 7269 6f2e  sses = scenario.
-00001260: 6765 745f 6675 6c6c 5f64 6174 6166 7261  get_full_datafra
-00001270: 6d65 2822 4369 7263 7569 7473 222c 2022  me("Circuits", "
-00001280: 4c6f 7373 6573 2229 0d0a 2020 2020 2020  Losses")..      
-00001290: 2020 6173 7365 7274 206c 656e 2864 665f    assert len(df_
-000012a0: 6c6f 7373 6573 2e63 6f6c 756d 6e73 2920  losses.columns) 
-000012b0: 3d3d 2031 0d0a 0d0a 2020 2020 2020 2020  == 1....        
-000012c0: 6466 5f6c 696e 655f 6c6f 7373 6573 203d  df_line_losses =
-000012d0: 2073 6365 6e61 7269 6f2e 6765 745f 6675   scenario.get_fu
-000012e0: 6c6c 5f64 6174 6166 7261 6d65 2822 4369  ll_dataframe("Ci
-000012f0: 7263 7569 7473 222c 2022 4c69 6e65 4c6f  rcuits", "LineLo
-00001300: 7373 6573 2229 0d0a 2020 2020 2020 2020  sses")..        
-00001310: 6173 7365 7274 206c 656e 2864 665f 6c69  assert len(df_li
-00001320: 6e65 5f6c 6f73 7365 732e 636f 6c75 6d6e  ne_losses.column
-00001330: 7329 203d 3d20 310d 0a20 2020 2020 2020  s) == 1..       
-00001340: 2064 665f 6c6f 6164 735f 706f 7765 7273   df_loads_powers
-00001350: 203d 2073 6365 6e61 7269 6f2e 6765 745f   = scenario.get_
-00001360: 6675 6c6c 5f64 6174 6166 7261 6d65 2822  full_dataframe("
-00001370: 4c6f 6164 7322 2c20 2250 6f77 6572 7322  Loads", "Powers"
-00001380: 290d 0a20 2020 2020 2020 2074 6f74 616c  )..        total
-00001390: 5f6c 6f73 7365 7320 3d20 6162 7328 6466  _losses = abs(df
-000013a0: 5f6c 6f73 7365 732e 7375 6d28 292e 7375  _losses.sum().su
-000013b0: 6d28 2929 202f 2031 3030 3020 2320 4f70  m()) / 1000 # Op
-000013c0: 656e 4453 5320 7265 706f 7274 7320 746f  enDSS reports to
-000013d0: 7461 6c20 6c6f 7373 6573 2069 6e20 5761  tal losses in Wa
-000013e0: 7474 730d 0a20 2020 2020 2020 206c 696e  tts..        lin
-000013f0: 655f 6c6f 7373 6573 203d 2061 6273 2864  e_losses = abs(d
-00001400: 665f 6c69 6e65 5f6c 6f73 7365 732e 7375  f_line_losses.su
-00001410: 6d28 292e 7375 6d28 2929 0d0a 2020 2020  m().sum())..    
-00001420: 2020 2020 7472 616e 7366 6f72 6d65 725f      transformer_
-00001430: 6c6f 7373 6573 203d 2074 6f74 616c 5f6c  losses = total_l
-00001440: 6f73 7365 7320 2d20 6c69 6e65 5f6c 6f73  osses - line_los
-00001450: 7365 730d 0a20 2020 2020 2020 2072 6574  ses..        ret
-00001460: 7572 6e20 4665 6564 6572 4c6f 7373 6573  urn FeederLosses
-00001470: 4d65 7472 6963 734d 6f64 656c 280d 0a20  MetricsModel(.. 
-00001480: 2020 2020 2020 2020 2020 2074 6f74 616c             total
-00001490: 5f6c 6f73 7365 735f 6b77 683d 746f 7461  _losses_kwh=tota
-000014a0: 6c5f 6c6f 7373 6573 202a 2074 6f5f 6b77  l_losses * to_kw
-000014b0: 682c 0d0a 2020 2020 2020 2020 2020 2020  h,..            
-000014c0: 6c69 6e65 5f6c 6f73 7365 735f 6b77 683d  line_losses_kwh=
-000014d0: 6c69 6e65 5f6c 6f73 7365 7320 2a20 746f  line_losses * to
-000014e0: 5f6b 7768 2c0d 0a20 2020 2020 2020 2020  _kwh,..         
-000014f0: 2020 2074 7261 6e73 666f 726d 6572 5f6c     transformer_l
-00001500: 6f73 7365 735f 6b77 683d 7472 616e 7366  osses_kwh=transf
-00001510: 6f72 6d65 725f 6c6f 7373 6573 202a 2074  ormer_losses * t
-00001520: 6f5f 6b77 682c 0d0a 2020 2020 2020 2020  o_kwh,..        
-00001530: 2020 2020 746f 7461 6c5f 6c6f 6164 5f64      total_load_d
-00001540: 656d 616e 645f 6b77 683d 6466 5f6c 6f61  emand_kwh=df_loa
-00001550: 6473 5f70 6f77 6572 732e 7375 6d28 292e  ds_powers.sum().
-00001560: 7375 6d28 2920 2a20 746f 5f6b 7768 2c0d  sum() * to_kwh,.
-00001570: 0a20 2020 2020 2020 2029 0d0a 0d0a 2020  .        )....  
-00001580: 2020 4073 7461 7469 636d 6574 686f 640d    @staticmethod.
-00001590: 0a20 2020 2064 6566 2067 6574 5f72 6571  .    def get_req
-000015a0: 7569 7265 645f 6578 706f 7274 7328 7369  uired_exports(si
-000015b0: 6d75 6c61 7469 6f6e 5f63 6f6e 6669 6729  mulation_config)
-000015c0: 3a0d 0a20 2020 2020 2020 2069 6e70 7574  :..        input
-000015d0: 7320 3d20 4665 6564 6572 4c6f 7373 6573  s = FeederLosses
-000015e0: 5265 706f 7274 2e67 6574 5f69 6e70 7574  Report.get_input
-000015f0: 735f 6672 6f6d 5f64 6566 6175 6c74 7328  s_from_defaults(
-00001600: 0d0a 2020 2020 2020 2020 2020 2020 7369  ..            si
-00001610: 6d75 6c61 7469 6f6e 5f63 6f6e 6669 672c  mulation_config,
-00001620: 2046 6565 6465 724c 6f73 7365 7352 6570   FeederLossesRep
-00001630: 6f72 742e 4e41 4d45 0d0a 2020 2020 2020  ort.NAME..      
-00001640: 2020 290d 0a20 2020 2020 2020 2069 6620    )..        if 
-00001650: 696e 7075 7473 5b22 7374 6f72 655f 616c  inputs["store_al
-00001660: 6c5f 7469 6d65 5f70 6f69 6e74 7322 5d3a  l_time_points"]:
-00001670: 0d0a 2020 2020 2020 2020 2020 2020 7265  ..            re
-00001680: 7475 726e 207b 0d0a 2020 2020 2020 2020  turn {..        
-00001690: 2020 2020 2020 2020 2243 6972 6375 6974          "Circuit
-000016a0: 7322 3a20 5b0d 0a20 2020 2020 2020 2020  s": [..         
-000016b0: 2020 2020 2020 2020 2020 207b 0d0a 2020             {..  
-000016c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000016d0: 2020 2020 2020 2270 726f 7065 7274 7922        "property"
-000016e0: 3a20 224c 6f73 7365 7322 2c0d 0a20 2020  : "Losses",..   
-000016f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001700: 2020 2020 2022 7374 6f72 655f 7661 6c75       "store_valu
-00001710: 6573 5f74 7970 6522 3a20 2261 6c6c 222c  es_type": "all",
-00001720: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00001730: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-00001740: 2020 2020 2020 2020 2020 2020 2020 7b0d                {.
-00001750: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00001760: 2020 2020 2020 2020 2022 7072 6f70 6572           "proper
-00001770: 7479 223a 2022 4c69 6e65 4c6f 7373 6573  ty": "LineLosses
-00001780: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00001790: 2020 2020 2020 2020 2020 2020 2273 746f              "sto
-000017a0: 7265 5f76 616c 7565 735f 7479 7065 223a  re_values_type":
-000017b0: 2022 616c 6c22 2c0d 0a20 2020 2020 2020   "all",..       
-000017c0: 2020 2020 2020 2020 2020 2020 207d 0d0a               }..
-000017d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000017e0: 5d2c 0d0a 2020 2020 2020 2020 2020 2020  ],..            
-000017f0: 2020 2020 224c 6f61 6473 223a 205b 0d0a      "Loads": [..
-00001800: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001810: 2020 2020 7b0d 0a20 2020 2020 2020 2020      {..         
-00001820: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00001830: 7072 6f70 6572 7479 223a 2022 506f 7765  property": "Powe
-00001840: 7273 222c 0d0a 2020 2020 2020 2020 2020  rs",..          
-00001850: 2020 2020 2020 2020 2020 2020 2020 2273                "s
-00001860: 746f 7265 5f76 616c 7565 735f 7479 7065  tore_values_type
-00001870: 223a 2022 616c 6c22 2c0d 0a20 2020 2020  ": "all",..     
+00000640: 6574 7269 6373 4d6f 6465 6c2c 0a20 2020  etricsModel,.   
+00000650: 2020 2020 2072 656c 5f74 6f6c 3d30 2e30       rel_tol=0.0
+00000660: 3030 3030 312c 0a29 3a0a 2020 2020 2222  00001,.):.    ""
+00000670: 2243 6f6d 7061 7265 7320 7468 6520 7661  "Compares the va
+00000680: 6c75 6573 206f 6620 7477 6f20 696e 7374  lues of two inst
+00000690: 616e 6365 7320 6f66 2046 6565 6465 724c  ances of FeederL
+000006a0: 6f73 7365 734d 6574 7269 6373 4d6f 6465  ossesMetricsMode
+000006b0: 6c2e 0a0a 2020 2020 5265 7475 726e 730a  l...    Returns.
+000006c0: 2020 2020 2d2d 2d2d 2d2d 2d0a 2020 2020      -------.    
+000006d0: 626f 6f6c 0a20 2020 2020 2020 2052 6574  bool.        Ret
+000006e0: 7572 6e20 5472 7565 2069 6620 7468 6579  urn True if they
+000006f0: 206d 6174 6368 2e0a 0a20 2020 2022 2222   match...    """
+00000700: 0a20 2020 206d 6174 6368 203d 2054 7275  .    match = Tru
+00000710: 650a 2020 2020 666f 7220 7363 656e 6172  e.    for scenar
+00000720: 696f 2069 6e20 6d65 7472 6963 7331 2e73  io in metrics1.s
+00000730: 6365 6e61 7269 6f73 3a0a 2020 2020 2020  cenarios:.      
+00000740: 2020 666f 7220 6669 656c 6420 696e 2046    for field in F
+00000750: 6565 6465 724c 6f73 7365 734d 6574 7269  eederLossesMetri
+00000760: 6373 4d6f 6465 6c2e 6d6f 6465 6c5f 6669  csModel.model_fi
+00000770: 656c 6473 3a0a 2020 2020 2020 2020 2020  elds:.          
+00000780: 2020 7661 6c31 203d 2067 6574 6174 7472    val1 = getattr
+00000790: 286d 6574 7269 6373 312e 7363 656e 6172  (metrics1.scenar
+000007a0: 696f 735b 7363 656e 6172 696f 5d2c 2066  ios[scenario], f
+000007b0: 6965 6c64 290a 2020 2020 2020 2020 2020  ield).          
+000007c0: 2020 7661 6c32 203d 2067 6574 6174 7472    val2 = getattr
+000007d0: 286d 6574 7269 6373 322e 7363 656e 6172  (metrics2.scenar
+000007e0: 696f 735b 7363 656e 6172 696f 5d2c 2066  ios[scenario], f
+000007f0: 6965 6c64 290a 2020 2020 2020 2020 2020  ield).          
+00000800: 2020 6966 206e 6f74 206d 6174 682e 6973    if not math.is
+00000810: 636c 6f73 6528 7661 6c31 2c20 7661 6c32  close(val1, val2
+00000820: 2c20 7265 6c5f 746f 6c3d 7265 6c5f 746f  , rel_tol=rel_to
+00000830: 6c29 3a0a 2020 2020 2020 2020 2020 2020  l):.            
+00000840: 2020 2020 6c6f 6767 6572 2e65 7272 6f72      logger.error
+00000850: 2822 6669 656c 643d 2573 206d 6973 6d61  ("field=%s misma
+00000860: 7463 6820 2573 203a 2025 7322 2c20 6669  tch %s : %s", fi
+00000870: 656c 642c 2076 616c 312c 2076 616c 3229  eld, val1, val2)
+00000880: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000890: 206d 6174 6368 203d 2046 616c 7365 0a0a   match = False..
+000008a0: 2020 2020 7265 7475 726e 206d 6174 6368      return match
+000008b0: 0a0a 0a63 6c61 7373 2046 6565 6465 724c  ...class FeederL
+000008c0: 6f73 7365 7352 6570 6f72 7428 5265 706f  ossesReport(Repo
+000008d0: 7274 4261 7365 293a 0a20 2020 2022 2222  rtBase):.    """
+000008e0: 5265 706f 7274 7320 7468 6520 6665 6564  Reports the feed
+000008f0: 6572 206c 6f73 7365 732e 0a0a 2020 2020  er losses...    
+00000900: 5468 6520 7265 706f 7274 2067 656e 6572  The report gener
+00000910: 6174 6573 2061 2066 6565 6465 725f 6c6f  ates a feeder_lo
+00000920: 7373 6573 2e6a 736f 6e20 6f75 7470 7574  sses.json output
+00000930: 2066 696c 652e 0a0a 2020 2020 2222 220a   file...    """.
+00000940: 0a20 2020 2046 494c 454e 414d 4520 3d20  .    FILENAME = 
+00000950: 2266 6565 6465 725f 6c6f 7373 6573 2e6a  "feeder_losses.j
+00000960: 736f 6e22 0a20 2020 204e 414d 4520 3d20  son".    NAME = 
+00000970: 2246 6565 6465 7220 4c6f 7373 6573 220a  "Feeder Losses".
+00000980: 2020 2020 4445 4641 554c 5453 203d 207b      DEFAULTS = {
+00000990: 0a20 2020 2020 2020 2022 7374 6f72 655f  .        "store_
+000009a0: 616c 6c5f 7469 6d65 5f70 6f69 6e74 7322  all_time_points"
+000009b0: 3a20 4661 6c73 652c 0a20 2020 207d 0a0a  : False,.    }..
+000009c0: 2020 2020 6465 6620 6765 6e65 7261 7465      def generate
+000009d0: 2873 656c 662c 206f 7574 7075 745f 6469  (self, output_di
+000009e0: 7229 3a0a 2020 2020 2020 2020 7265 736f  r):.        reso
+000009f0: 6c75 7469 6f6e 203d 2073 656c 662e 5f67  lution = self._g
+00000a00: 6574 5f73 696d 756c 6174 696f 6e5f 7265  et_simulation_re
+00000a10: 736f 6c75 7469 6f6e 2829 0a20 2020 2020  solution().     
+00000a20: 2020 2074 6f5f 6b77 6820 3d20 7265 736f     to_kwh = reso
+00000a30: 6c75 7469 6f6e 202f 2074 696d 6564 656c  lution / timedel
+00000a40: 7461 2868 6f75 7273 3d31 290a 2020 2020  ta(hours=1).    
+00000a50: 2020 2020 6173 7365 7274 206c 656e 2873      assert len(s
+00000a60: 656c 662e 5f72 6573 756c 7473 2e73 6365  elf._results.sce
+00000a70: 6e61 7269 6f73 2920 3e3d 2031 0a20 2020  narios) >= 1.   
+00000a80: 2020 2020 2073 6365 6e61 7269 6f73 203d       scenarios =
+00000a90: 207b 7d0a 2020 2020 2020 2020 666f 7220   {}.        for 
+00000aa0: 7363 656e 6172 696f 2069 6e20 7365 6c66  scenario in self
+00000ab0: 2e5f 7265 7375 6c74 732e 7363 656e 6172  ._results.scenar
+00000ac0: 696f 733a 0a20 2020 2020 2020 2020 2020  ios:.           
+00000ad0: 2069 6e70 7574 7320 3d20 4665 6564 6572   inputs = Feeder
+00000ae0: 4c6f 7373 6573 5265 706f 7274 2e67 6574  LossesReport.get
+00000af0: 5f69 6e70 7574 735f 6672 6f6d 5f64 6566  _inputs_from_def
+00000b00: 6175 6c74 7328 7365 6c66 2e5f 7365 7474  aults(self._sett
+00000b10: 696e 6773 2c20 7365 6c66 2e4e 414d 4529  ings, self.NAME)
+00000b20: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+00000b30: 696e 7075 7473 5b22 7374 6f72 655f 616c  inputs["store_al
+00000b40: 6c5f 7469 6d65 5f70 6f69 6e74 7322 5d3a  l_time_points"]:
+00000b50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000b60: 2073 6365 6e61 7269 6f73 5b73 6365 6e61   scenarios[scena
+00000b70: 7269 6f2e 6e61 6d65 5d20 3d20 7365 6c66  rio.name] = self
+00000b80: 2e5f 6765 6e65 7261 7465 5f66 726f 6d5f  ._generate_from_
+00000b90: 616c 6c5f 7469 6d65 5f70 6f69 6e74 7328  all_time_points(
+00000ba0: 7363 656e 6172 696f 2c20 746f 5f6b 7768  scenario, to_kwh
+00000bb0: 290a 2020 2020 2020 2020 2020 2020 656c  ).            el
+00000bc0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
+00000bd0: 2020 2020 7363 656e 6172 696f 735b 7363      scenarios[sc
+00000be0: 656e 6172 696f 2e6e 616d 655d 203d 2073  enario.name] = s
+00000bf0: 656c 662e 5f67 656e 6572 6174 655f 6672  elf._generate_fr
+00000c00: 6f6d 5f69 6e5f 6d65 6d6f 7279 5f6d 6574  om_in_memory_met
+00000c10: 7269 6373 2873 6365 6e61 7269 6f2c 2074  rics(scenario, t
+00000c20: 6f5f 6b77 6829 0a0a 2020 2020 2020 2020  o_kwh)..        
+00000c30: 6d6f 6465 6c20 3d20 5369 6d75 6c61 7469  model = Simulati
+00000c40: 6f6e 4665 6564 6572 4c6f 7373 6573 4d65  onFeederLossesMe
+00000c50: 7472 6963 734d 6f64 656c 2873 6365 6e61  tricsModel(scena
+00000c60: 7269 6f73 3d73 6365 6e61 7269 6f73 290a  rios=scenarios).
+00000c70: 2020 2020 2020 2020 6669 6c65 6e61 6d65          filename
+00000c80: 203d 206f 732e 7061 7468 2e6a 6f69 6e28   = os.path.join(
+00000c90: 6f75 7470 7574 5f64 6972 2c20 7365 6c66  output_dir, self
+00000ca0: 2e46 494c 454e 414d 4529 0a20 2020 2020  .FILENAME).     
+00000cb0: 2020 2077 6974 6820 6f70 656e 2866 696c     with open(fil
+00000cc0: 656e 616d 652c 2022 7722 2920 6173 2066  ename, "w") as f
+00000cd0: 5f6f 7574 3a0a 2020 2020 2020 2020 2020  _out:.          
+00000ce0: 2020 665f 6f75 742e 7772 6974 6528 6d6f    f_out.write(mo
+00000cf0: 6465 6c2e 6d6f 6465 6c5f 6475 6d70 5f6a  del.model_dump_j
+00000d00: 736f 6e28 696e 6465 6e74 3d32 2929 0a20  son(indent=2)). 
+00000d10: 2020 2020 2020 2020 2020 2066 5f6f 7574             f_out
+00000d20: 2e77 7269 7465 2822 5c6e 2229 0a20 2020  .write("\n").   
+00000d30: 2020 2020 206c 6f67 6765 722e 696e 666f       logger.info
+00000d40: 2822 4765 6e65 7261 7465 6420 2573 222c  ("Generated %s",
+00000d50: 2066 696c 656e 616d 6529 0a20 2020 2020   filename).     
+00000d60: 2020 2072 6574 7572 6e20 6669 6c65 6e61     return filena
+00000d70: 6d65 0a0a 2020 2020 6465 6620 5f67 656e  me..    def _gen
+00000d80: 6572 6174 655f 6672 6f6d 5f69 6e5f 6d65  erate_from_in_me
+00000d90: 6d6f 7279 5f6d 6574 7269 6373 2873 656c  mory_metrics(sel
+00000da0: 662c 2073 6365 6e61 7269 6f2c 2074 6f5f  f, scenario, to_
+00000db0: 6b77 6829 3a0a 2020 2020 2020 2020 746f  kwh):.        to
+00000dc0: 7461 6c5f 6c6f 7373 6573 5f64 6963 7420  tal_losses_dict 
+00000dd0: 3d20 7363 656e 6172 696f 2e67 6574 5f73  = scenario.get_s
+00000de0: 756d 6d65 645f 656c 656d 656e 745f 746f  ummed_element_to
+00000df0: 7461 6c28 2243 6972 6375 6974 7322 2c20  tal("Circuits", 
+00000e00: 224c 6f73 7365 7353 756d 2229 0a20 2020  "LossesSum").   
+00000e10: 2020 2020 2074 6f74 616c 5f6c 6f73 7365       total_losse
+00000e20: 7320 3d20 6162 7328 6e65 7874 2869 7465  s = abs(next(ite
+00000e30: 7228 746f 7461 6c5f 6c6f 7373 6573 5f64  r(total_losses_d
+00000e40: 6963 742e 7661 6c75 6573 2829 2929 2920  ict.values()))) 
+00000e50: 2f20 3130 3030 2023 204f 7065 6e44 5353  / 1000 # OpenDSS
+00000e60: 2072 6570 6f72 7473 2074 6f74 616c 206c   reports total l
+00000e70: 6f73 7365 7320 696e 2057 6174 7473 0a20  osses in Watts. 
+00000e80: 2020 2020 2020 206c 696e 655f 6c6f 7373         line_loss
+00000e90: 6573 5f64 6963 7420 3d20 7363 656e 6172  es_dict = scenar
+00000ea0: 696f 2e67 6574 5f73 756d 6d65 645f 656c  io.get_summed_el
+00000eb0: 656d 656e 745f 746f 7461 6c28 2243 6972  ement_total("Cir
+00000ec0: 6375 6974 7322 2c20 224c 696e 654c 6f73  cuits", "LineLos
+00000ed0: 7365 7353 756d 2229 0a20 2020 2020 2020  sesSum").       
+00000ee0: 206c 696e 655f 6c6f 7373 6573 203d 2061   line_losses = a
+00000ef0: 6273 286e 6578 7428 6974 6572 286c 696e  bs(next(iter(lin
+00000f00: 655f 6c6f 7373 6573 5f64 6963 742e 7661  e_losses_dict.va
+00000f10: 6c75 6573 2829 2929 290a 2020 2020 2020  lues()))).      
+00000f20: 2020 7472 616e 7366 6f72 6d65 725f 6c6f    transformer_lo
+00000f30: 7373 6573 203d 2028 746f 7461 6c5f 6c6f  sses = (total_lo
+00000f40: 7373 6573 202d 206c 696e 655f 6c6f 7373  sses - line_loss
+00000f50: 6573 290a 2020 2020 2020 2020 746f 7461  es).        tota
+00000f60: 6c5f 6c6f 6164 5f70 6f77 6572 5f64 6963  l_load_power_dic
+00000f70: 7420 3d20 7363 656e 6172 696f 2e67 6574  t = scenario.get
+00000f80: 5f73 756d 6d65 645f 656c 656d 656e 745f  _summed_element_
+00000f90: 746f 7461 6c28 224c 6f61 6473 222c 2022  total("Loads", "
+00000fa0: 506f 7765 7273 5375 6d22 290a 2020 2020  PowersSum").    
+00000fb0: 2020 2020 746f 7461 6c5f 6c6f 6164 5f70      total_load_p
+00000fc0: 6f77 6572 203d 2030 0a20 2020 2020 2020  ower = 0.       
+00000fd0: 2066 6f72 2076 616c 2069 6e20 746f 7461   for val in tota
+00000fe0: 6c5f 6c6f 6164 5f70 6f77 6572 5f64 6963  l_load_power_dic
+00000ff0: 742e 7661 6c75 6573 2829 3a0a 2020 2020  t.values():.    
+00001000: 2020 2020 2020 2020 746f 7461 6c5f 6c6f          total_lo
+00001010: 6164 5f70 6f77 6572 202b 3d20 7661 6c2e  ad_power += val.
+00001020: 7265 616c 0a0a 2020 2020 2020 2020 2320  real..        # 
+00001030: 544f 444f 3a20 746f 7461 6c20 6c6f 7373  TODO: total loss
+00001040: 6573 2061 7320 6120 7065 7263 656e 7461  es as a percenta
+00001050: 6765 206f 6620 746f 7461 6c20 6c6f 6164  ge of total load
+00001060: 2064 656d 616e 643f 0a20 2020 2020 2020   demand?.       
+00001070: 2072 6574 7572 6e20 4665 6564 6572 4c6f   return FeederLo
+00001080: 7373 6573 4d65 7472 6963 734d 6f64 656c  ssesMetricsModel
+00001090: 280a 2020 2020 2020 2020 2020 2020 746f  (.            to
+000010a0: 7461 6c5f 6c6f 7373 6573 5f6b 7768 3d74  tal_losses_kwh=t
+000010b0: 6f74 616c 5f6c 6f73 7365 7320 2a20 746f  otal_losses * to
+000010c0: 5f6b 7768 2c0a 2020 2020 2020 2020 2020  _kwh,.          
+000010d0: 2020 6c69 6e65 5f6c 6f73 7365 735f 6b77    line_losses_kw
+000010e0: 683d 6c69 6e65 5f6c 6f73 7365 7320 2a20  h=line_losses * 
+000010f0: 746f 5f6b 7768 2c0a 2020 2020 2020 2020  to_kwh,.        
+00001100: 2020 2020 7472 616e 7366 6f72 6d65 725f      transformer_
+00001110: 6c6f 7373 6573 5f6b 7768 3d74 7261 6e73  losses_kwh=trans
+00001120: 666f 726d 6572 5f6c 6f73 7365 7320 2a20  former_losses * 
+00001130: 746f 5f6b 7768 2c0a 2020 2020 2020 2020  to_kwh,.        
+00001140: 2020 2020 746f 7461 6c5f 6c6f 6164 5f64      total_load_d
+00001150: 656d 616e 645f 6b77 683d 746f 7461 6c5f  emand_kwh=total_
+00001160: 6c6f 6164 5f70 6f77 6572 202a 2074 6f5f  load_power * to_
+00001170: 6b77 682c 0a20 2020 2020 2020 2029 0a0a  kwh,.        )..
+00001180: 2020 2020 6465 6620 5f67 656e 6572 6174      def _generat
+00001190: 655f 6672 6f6d 5f61 6c6c 5f74 696d 655f  e_from_all_time_
+000011a0: 706f 696e 7473 2873 656c 662c 2073 6365  points(self, sce
+000011b0: 6e61 7269 6f2c 2074 6f5f 6b77 6829 3a0a  nario, to_kwh):.
+000011c0: 2020 2020 2020 2020 6466 5f6c 6f73 7365          df_losse
+000011d0: 7320 3d20 7363 656e 6172 696f 2e67 6574  s = scenario.get
+000011e0: 5f66 756c 6c5f 6461 7461 6672 616d 6528  _full_dataframe(
+000011f0: 2243 6972 6375 6974 7322 2c20 224c 6f73  "Circuits", "Los
+00001200: 7365 7322 290a 2020 2020 2020 2020 6173  ses").        as
+00001210: 7365 7274 206c 656e 2864 665f 6c6f 7373  sert len(df_loss
+00001220: 6573 2e63 6f6c 756d 6e73 2920 3d3d 2031  es.columns) == 1
+00001230: 0a0a 2020 2020 2020 2020 6466 5f6c 696e  ..        df_lin
+00001240: 655f 6c6f 7373 6573 203d 2073 6365 6e61  e_losses = scena
+00001250: 7269 6f2e 6765 745f 6675 6c6c 5f64 6174  rio.get_full_dat
+00001260: 6166 7261 6d65 2822 4369 7263 7569 7473  aframe("Circuits
+00001270: 222c 2022 4c69 6e65 4c6f 7373 6573 2229  ", "LineLosses")
+00001280: 0a20 2020 2020 2020 2061 7373 6572 7420  .        assert 
+00001290: 6c65 6e28 6466 5f6c 696e 655f 6c6f 7373  len(df_line_loss
+000012a0: 6573 2e63 6f6c 756d 6e73 2920 3d3d 2031  es.columns) == 1
+000012b0: 0a20 2020 2020 2020 2064 665f 6c6f 6164  .        df_load
+000012c0: 735f 706f 7765 7273 203d 2073 6365 6e61  s_powers = scena
+000012d0: 7269 6f2e 6765 745f 6675 6c6c 5f64 6174  rio.get_full_dat
+000012e0: 6166 7261 6d65 2822 4c6f 6164 7322 2c20  aframe("Loads", 
+000012f0: 2250 6f77 6572 7322 290a 2020 2020 2020  "Powers").      
+00001300: 2020 746f 7461 6c5f 6c6f 7373 6573 203d    total_losses =
+00001310: 2061 6273 2864 665f 6c6f 7373 6573 2e73   abs(df_losses.s
+00001320: 756d 2829 2e73 756d 2829 2920 2f20 3130  um().sum()) / 10
+00001330: 3030 2023 204f 7065 6e44 5353 2072 6570  00 # OpenDSS rep
+00001340: 6f72 7473 2074 6f74 616c 206c 6f73 7365  orts total losse
+00001350: 7320 696e 2057 6174 7473 0a20 2020 2020  s in Watts.     
+00001360: 2020 206c 696e 655f 6c6f 7373 6573 203d     line_losses =
+00001370: 2061 6273 2864 665f 6c69 6e65 5f6c 6f73   abs(df_line_los
+00001380: 7365 732e 7375 6d28 292e 7375 6d28 2929  ses.sum().sum())
+00001390: 0a20 2020 2020 2020 2074 7261 6e73 666f  .        transfo
+000013a0: 726d 6572 5f6c 6f73 7365 7320 3d20 746f  rmer_losses = to
+000013b0: 7461 6c5f 6c6f 7373 6573 202d 206c 696e  tal_losses - lin
+000013c0: 655f 6c6f 7373 6573 0a20 2020 2020 2020  e_losses.       
+000013d0: 2072 6574 7572 6e20 4665 6564 6572 4c6f   return FeederLo
+000013e0: 7373 6573 4d65 7472 6963 734d 6f64 656c  ssesMetricsModel
+000013f0: 280a 2020 2020 2020 2020 2020 2020 746f  (.            to
+00001400: 7461 6c5f 6c6f 7373 6573 5f6b 7768 3d74  tal_losses_kwh=t
+00001410: 6f74 616c 5f6c 6f73 7365 7320 2a20 746f  otal_losses * to
+00001420: 5f6b 7768 2c0a 2020 2020 2020 2020 2020  _kwh,.          
+00001430: 2020 6c69 6e65 5f6c 6f73 7365 735f 6b77    line_losses_kw
+00001440: 683d 6c69 6e65 5f6c 6f73 7365 7320 2a20  h=line_losses * 
+00001450: 746f 5f6b 7768 2c0a 2020 2020 2020 2020  to_kwh,.        
+00001460: 2020 2020 7472 616e 7366 6f72 6d65 725f      transformer_
+00001470: 6c6f 7373 6573 5f6b 7768 3d74 7261 6e73  losses_kwh=trans
+00001480: 666f 726d 6572 5f6c 6f73 7365 7320 2a20  former_losses * 
+00001490: 746f 5f6b 7768 2c0a 2020 2020 2020 2020  to_kwh,.        
+000014a0: 2020 2020 746f 7461 6c5f 6c6f 6164 5f64      total_load_d
+000014b0: 656d 616e 645f 6b77 683d 6466 5f6c 6f61  emand_kwh=df_loa
+000014c0: 6473 5f70 6f77 6572 732e 7375 6d28 292e  ds_powers.sum().
+000014d0: 7375 6d28 2920 2a20 746f 5f6b 7768 2c0a  sum() * to_kwh,.
+000014e0: 2020 2020 2020 2020 290a 0a20 2020 2040          )..    @
+000014f0: 7374 6174 6963 6d65 7468 6f64 0a20 2020  staticmethod.   
+00001500: 2064 6566 2067 6574 5f72 6571 7569 7265   def get_require
+00001510: 645f 6578 706f 7274 7328 7369 6d75 6c61  d_exports(simula
+00001520: 7469 6f6e 5f63 6f6e 6669 6729 3a0a 2020  tion_config):.  
+00001530: 2020 2020 2020 696e 7075 7473 203d 2046        inputs = F
+00001540: 6565 6465 724c 6f73 7365 7352 6570 6f72  eederLossesRepor
+00001550: 742e 6765 745f 696e 7075 7473 5f66 726f  t.get_inputs_fro
+00001560: 6d5f 6465 6661 756c 7473 280a 2020 2020  m_defaults(.    
+00001570: 2020 2020 2020 2020 7369 6d75 6c61 7469          simulati
+00001580: 6f6e 5f63 6f6e 6669 672c 2046 6565 6465  on_config, Feede
+00001590: 724c 6f73 7365 7352 6570 6f72 742e 4e41  rLossesReport.NA
+000015a0: 4d45 0a20 2020 2020 2020 2029 0a20 2020  ME.        ).   
+000015b0: 2020 2020 2069 6620 696e 7075 7473 5b22       if inputs["
+000015c0: 7374 6f72 655f 616c 6c5f 7469 6d65 5f70  store_all_time_p
+000015d0: 6f69 6e74 7322 5d3a 0a20 2020 2020 2020  oints"]:.       
+000015e0: 2020 2020 2072 6574 7572 6e20 7b0a 2020       return {.  
+000015f0: 2020 2020 2020 2020 2020 2020 2020 2243                "C
+00001600: 6972 6375 6974 7322 3a20 5b0a 2020 2020  ircuits": [.    
+00001610: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001620: 7b0a 2020 2020 2020 2020 2020 2020 2020  {.              
+00001630: 2020 2020 2020 2020 2020 2270 726f 7065            "prope
+00001640: 7274 7922 3a20 224c 6f73 7365 7322 2c0a  rty": "Losses",.
+00001650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001660: 2020 2020 2020 2020 2273 746f 7265 5f76          "store_v
+00001670: 616c 7565 735f 7479 7065 223a 2022 616c  alues_type": "al
+00001680: 6c22 2c0a 2020 2020 2020 2020 2020 2020  l",.            
+00001690: 2020 2020 2020 2020 7d2c 0a20 2020 2020          },.     
+000016a0: 2020 2020 2020 2020 2020 2020 2020 207b                 {
+000016b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000016c0: 2020 2020 2020 2020 2022 7072 6f70 6572           "proper
+000016d0: 7479 223a 2022 4c69 6e65 4c6f 7373 6573  ty": "LineLosses
+000016e0: 222c 0a20 2020 2020 2020 2020 2020 2020  ",.             
+000016f0: 2020 2020 2020 2020 2020 2022 7374 6f72             "stor
+00001700: 655f 7661 6c75 6573 5f74 7970 6522 3a20  e_values_type": 
+00001710: 2261 6c6c 222c 0a20 2020 2020 2020 2020  "all",.         
+00001720: 2020 2020 2020 2020 2020 207d 0a20 2020             }.   
+00001730: 2020 2020 2020 2020 2020 2020 205d 2c0a               ],.
+00001740: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001750: 224c 6f61 6473 223a 205b 0a20 2020 2020  "Loads": [.     
+00001760: 2020 2020 2020 2020 2020 2020 2020 207b                 {
+00001770: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00001780: 2020 2020 2020 2020 2022 7072 6f70 6572           "proper
+00001790: 7479 223a 2022 506f 7765 7273 222c 0a20  ty": "Powers",. 
+000017a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000017b0: 2020 2020 2020 2022 7374 6f72 655f 7661         "store_va
+000017c0: 6c75 6573 5f74 7970 6522 3a20 2261 6c6c  lues_type": "all
+000017d0: 222c 0a20 2020 2020 2020 2020 2020 2020  ",.             
+000017e0: 2020 2020 2020 2020 2020 2022 6461 7461             "data
+000017f0: 5f63 6f6e 7665 7273 696f 6e22 3a20 2261  _conversion": "a
+00001800: 6273 5f73 756d 222c 0a20 2020 2020 2020  bs_sum",.       
+00001810: 2020 2020 2020 2020 2020 2020 207d 2c0a               },.
+00001820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001830: 5d0a 2020 2020 2020 2020 2020 2020 7d0a  ].            }.
+00001840: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
+00001850: 7b0a 2020 2020 2020 2020 2020 2020 2243  {.            "C
+00001860: 6972 6375 6974 7322 3a20 5b0a 2020 2020  ircuits": [.    
+00001870: 2020 2020 2020 2020 2020 2020 7b0a 2020              {.  
 00001880: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001890: 2020 2022 6461 7461 5f63 6f6e 7665 7273     "data_convers
-000018a0: 696f 6e22 3a20 2261 6273 5f73 756d 222c  ion": "abs_sum",
-000018b0: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-000018c0: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-000018d0: 2020 2020 2020 2020 2020 5d0d 0a20 2020            ]..   
-000018e0: 2020 2020 2020 2020 207d 0d0a 0d0a 2020           }....  
-000018f0: 2020 2020 2020 7265 7475 726e 207b 0d0a        return {..
-00001900: 2020 2020 2020 2020 2020 2020 2243 6972              "Cir
-00001910: 6375 6974 7322 3a20 5b0d 0a20 2020 2020  cuits": [..     
-00001920: 2020 2020 2020 2020 2020 207b 0d0a 2020             {..  
-00001930: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001940: 2020 2270 726f 7065 7274 7922 3a20 224c    "property": "L
-00001950: 6f73 7365 7322 2c0d 0a20 2020 2020 2020  osses",..       
-00001960: 2020 2020 2020 2020 2020 2020 2022 7374               "st
-00001970: 6f72 655f 7661 6c75 6573 5f74 7970 6522  ore_values_type"
-00001980: 3a20 2273 756d 222c 0d0a 2020 2020 2020  : "sum",..      
-00001990: 2020 2020 2020 2020 2020 2020 2020 2273                "s
-000019a0: 756d 5f65 6c65 6d65 6e74 7322 3a20 5472  um_elements": Tr
-000019b0: 7565 2c0d 0a20 2020 2020 2020 2020 2020  ue,..           
-000019c0: 2020 2020 207d 2c0d 0a20 2020 2020 2020       },..       
-000019d0: 2020 2020 2020 2020 207b 0d0a 2020 2020           {..    
-000019e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000019f0: 2270 726f 7065 7274 7922 3a20 224c 696e  "property": "Lin
-00001a00: 654c 6f73 7365 7322 2c0d 0a20 2020 2020  eLosses",..     
-00001a10: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00001a20: 7374 6f72 655f 7661 6c75 6573 5f74 7970  store_values_typ
-00001a30: 6522 3a20 2273 756d 222c 0d0a 2020 2020  e": "sum",..    
-00001a40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001a50: 2273 756d 5f65 6c65 6d65 6e74 7322 3a20  "sum_elements": 
-00001a60: 5472 7565 2c0d 0a20 2020 2020 2020 2020  True,..         
-00001a70: 2020 2020 2020 207d 0d0a 2020 2020 2020         }..      
-00001a80: 2020 2020 2020 5d2c 0d0a 2020 2020 2020        ],..      
-00001a90: 2020 2020 2020 224c 6f61 6473 223a 205b        "Loads": [
-00001aa0: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00001ab0: 2020 7b0d 0a20 2020 2020 2020 2020 2020    {..           
-00001ac0: 2020 2020 2020 2020 2022 7072 6f70 6572           "proper
-00001ad0: 7479 223a 2022 506f 7765 7273 222c 0d0a  ty": "Powers",..
-00001ae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001af0: 2020 2020 2273 746f 7265 5f76 616c 7565      "store_value
-00001b00: 735f 7479 7065 223a 2022 7375 6d22 2c0d  s_type": "sum",.
-00001b10: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00001b20: 2020 2020 2022 7375 6d5f 656c 656d 656e       "sum_elemen
-00001b30: 7473 223a 2054 7275 652c 0d0a 2020 2020  ts": True,..    
-00001b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001b50: 2264 6174 615f 636f 6e76 6572 7369 6f6e  "data_conversion
-00001b60: 223a 2022 6162 735f 7375 6d22 2c0d 0a20  ": "abs_sum",.. 
-00001b70: 2020 2020 2020 2020 2020 2020 2020 207d                 }
-00001b80: 2c0d 0a20 2020 2020 2020 2020 2020 205d  ,..            ]
-00001b90: 0d0a 2020 2020 2020 2020 7d0d 0a0d 0a20  ..        }.... 
-00001ba0: 2020 2040 7374 6174 6963 6d65 7468 6f64     @staticmethod
-00001bb0: 0d0a 2020 2020 6465 6620 6765 745f 7265  ..    def get_re
-00001bc0: 7175 6972 6564 5f73 6365 6e61 7269 6f5f  quired_scenario_
-00001bd0: 6e61 6d65 7328 293a 0d0a 2020 2020 2020  names():..      
-00001be0: 2020 7265 7475 726e 2073 6574 2829 0d0a    return set()..
+00001890: 2020 2270 726f 7065 7274 7922 3a20 224c    "property": "L
+000018a0: 6f73 7365 7322 2c0a 2020 2020 2020 2020  osses",.        
+000018b0: 2020 2020 2020 2020 2020 2020 2273 746f              "sto
+000018c0: 7265 5f76 616c 7565 735f 7479 7065 223a  re_values_type":
+000018d0: 2022 7375 6d22 2c0a 2020 2020 2020 2020   "sum",.        
+000018e0: 2020 2020 2020 2020 2020 2020 2273 756d              "sum
+000018f0: 5f65 6c65 6d65 6e74 7322 3a20 5472 7565  _elements": True
+00001900: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00001910: 2020 7d2c 0a20 2020 2020 2020 2020 2020    },.           
+00001920: 2020 2020 207b 0a20 2020 2020 2020 2020       {.         
+00001930: 2020 2020 2020 2020 2020 2022 7072 6f70             "prop
+00001940: 6572 7479 223a 2022 4c69 6e65 4c6f 7373  erty": "LineLoss
+00001950: 6573 222c 0a20 2020 2020 2020 2020 2020  es",.           
+00001960: 2020 2020 2020 2020 2022 7374 6f72 655f           "store_
+00001970: 7661 6c75 6573 5f74 7970 6522 3a20 2273  values_type": "s
+00001980: 756d 222c 0a20 2020 2020 2020 2020 2020  um",.           
+00001990: 2020 2020 2020 2020 2022 7375 6d5f 656c           "sum_el
+000019a0: 656d 656e 7473 223a 2054 7275 652c 0a20  ements": True,. 
+000019b0: 2020 2020 2020 2020 2020 2020 2020 207d                 }
+000019c0: 0a20 2020 2020 2020 2020 2020 205d 2c0a  .            ],.
+000019d0: 2020 2020 2020 2020 2020 2020 224c 6f61              "Loa
+000019e0: 6473 223a 205b 0a20 2020 2020 2020 2020  ds": [.         
+000019f0: 2020 2020 2020 207b 0a20 2020 2020 2020         {.       
+00001a00: 2020 2020 2020 2020 2020 2020 2022 7072               "pr
+00001a10: 6f70 6572 7479 223a 2022 506f 7765 7273  operty": "Powers
+00001a20: 222c 0a20 2020 2020 2020 2020 2020 2020  ",.             
+00001a30: 2020 2020 2020 2022 7374 6f72 655f 7661         "store_va
+00001a40: 6c75 6573 5f74 7970 6522 3a20 2273 756d  lues_type": "sum
+00001a50: 222c 0a20 2020 2020 2020 2020 2020 2020  ",.             
+00001a60: 2020 2020 2020 2022 7375 6d5f 656c 656d         "sum_elem
+00001a70: 656e 7473 223a 2054 7275 652c 0a20 2020  ents": True,.   
+00001a80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001a90: 2022 6461 7461 5f63 6f6e 7665 7273 696f   "data_conversio
+00001aa0: 6e22 3a20 2261 6273 5f73 756d 222c 0a20  n": "abs_sum",. 
+00001ab0: 2020 2020 2020 2020 2020 2020 2020 207d                 }
+00001ac0: 2c0a 2020 2020 2020 2020 2020 2020 5d0a  ,.            ].
+00001ad0: 2020 2020 2020 2020 7d0a 0a20 2020 2040          }..    @
+00001ae0: 7374 6174 6963 6d65 7468 6f64 0a20 2020  staticmethod.   
+00001af0: 2064 6566 2067 6574 5f72 6571 7569 7265   def get_require
+00001b00: 645f 7363 656e 6172 696f 5f6e 616d 6573  d_scenario_names
+00001b10: 2829 3a0a 2020 2020 2020 2020 7265 7475  ():.        retu
+00001b20: 726e 2073 6574 2829 0a                   rn set().
```

### Comparing `nrel_pydss-3.1.3/src/pydss/reports/pv_reports.py` & `nrel_pydss-3.1.4/src/pydss/reports/pv_reports.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,939 +1,916 @@
-00000000: 0d0a 696d 706f 7274 206d 6174 680d 0a69  ..import math..i
-00000010: 6d70 6f72 7420 6162 630d 0a69 6d70 6f72  mport abc..impor
-00000020: 7420 6f73 0d0a 0d0a 6672 6f6d 206c 6f67  t os....from log
-00000030: 7572 7520 696d 706f 7274 206c 6f67 6765  uru import logge
-00000040: 720d 0a69 6d70 6f72 7420 7061 6e64 6173  r..import pandas
-00000050: 2061 7320 7064 0d0a 696d 706f 7274 206e   as pd..import n
-00000060: 756d 7079 2061 7320 6e70 0d0a 0d0a 6672  umpy as np....fr
-00000070: 6f6d 2070 7964 7373 2e63 6f6d 6d6f 6e20  om pydss.common 
-00000080: 696d 706f 7274 2050 565f 4c4f 4144 5f53  import PV_LOAD_S
-00000090: 4841 5045 5f46 494c 454e 414d 450d 0a66  HAPE_FILENAME..f
-000000a0: 726f 6d20 7079 6473 732e 7265 706f 7274  rom pydss.report
-000000b0: 732e 7265 706f 7274 7320 696d 706f 7274  s.reports import
-000000c0: 2052 6570 6f72 7442 6173 652c 2052 6570   ReportBase, Rep
-000000d0: 6f72 7447 7261 6e75 6c61 7269 7479 0d0a  ortGranularity..
-000000e0: 6672 6f6d 2070 7964 7373 2e75 7469 6c73  from pydss.utils
-000000f0: 2e64 6174 6166 7261 6d65 5f75 7469 6c73  .dataframe_utils
-00000100: 2069 6d70 6f72 7420 7265 6164 5f64 6174   import read_dat
-00000110: 6166 7261 6d65 2c20 7772 6974 655f 6461  aframe, write_da
-00000120: 7461 6672 616d 650d 0a66 726f 6d20 7079  taframe..from py
-00000130: 6473 732e 7574 696c 732e 7574 696c 7320  dss.utils.utils 
-00000140: 696d 706f 7274 2064 756d 705f 6461 7461  import dump_data
-00000150: 0d0a 0d0a 5046 315f 5343 454e 4152 494f  ....PF1_SCENARIO
-00000160: 203d 2022 7066 3122 0d0a 434f 4e54 524f   = "pf1"..CONTRO
-00000170: 4c5f 4d4f 4445 5f53 4345 4e41 5249 4f20  L_MODE_SCENARIO 
-00000180: 3d20 2263 6f6e 7472 6f6c 5f6d 6f64 6522  = "control_mode"
-00000190: 0d0a 0d0a 636c 6173 7320 5076 5265 706f  ....class PvRepo
-000001a0: 7274 4261 7365 2852 6570 6f72 7442 6173  rtBase(ReportBas
-000001b0: 652c 2061 6263 2e41 4243 293a 0d0a 2020  e, abc.ABC):..  
-000001c0: 2020 2222 2242 6173 6520 636c 6173 7320    """Base class 
-000001d0: 666f 7220 5056 2072 6570 6f72 7473 2222  for PV reports""
-000001e0: 220d 0a20 2020 2064 6566 205f 5f69 6e69  "..    def __ini
-000001f0: 745f 5f28 7365 6c66 2c20 6e61 6d65 2c20  t__(self, name, 
-00000200: 7265 7375 6c74 732c 2073 696d 756c 6174  results, simulat
-00000210: 696f 6e5f 636f 6e66 6967 293a 0d0a 2020  ion_config):..  
-00000220: 2020 2020 2020 7375 7065 7228 292e 5f5f        super().__
-00000230: 696e 6974 5f5f 286e 616d 652c 2072 6573  init__(name, res
-00000240: 756c 7473 2c20 7369 6d75 6c61 7469 6f6e  ults, simulation
-00000250: 5f63 6f6e 6669 6729 0d0a 2020 2020 2020  _config)..      
-00000260: 2020 6173 7365 7274 206c 656e 2872 6573    assert len(res
-00000270: 756c 7473 2e73 6365 6e61 7269 6f73 2920  ults.scenarios) 
-00000280: 3d3d 2032 0d0a 2020 2020 2020 2020 7365  == 2..        se
-00000290: 6c66 2e5f 636f 6e74 726f 6c5f 6d6f 6465  lf._control_mode
-000002a0: 5f73 6365 6e61 7269 6f20 3d20 7265 7375  _scenario = resu
-000002b0: 6c74 732e 7363 656e 6172 696f 735b 305d  lts.scenarios[0]
-000002c0: 0d0a 2020 2020 2020 2020 6173 7365 7274  ..        assert
-000002d0: 2073 656c 662e 5f63 6f6e 7472 6f6c 5f6d   self._control_m
-000002e0: 6f64 655f 7363 656e 6172 696f 2e6e 616d  ode_scenario.nam
-000002f0: 6520 3d3d 2022 636f 6e74 726f 6c5f 6d6f  e == "control_mo
-00000300: 6465 220d 0a20 2020 2020 2020 2073 656c  de"..        sel
-00000310: 662e 5f70 6631 5f73 6365 6e61 7269 6f20  f._pf1_scenario 
-00000320: 3d20 7265 7375 6c74 732e 7363 656e 6172  = results.scenar
-00000330: 696f 735b 315d 0d0a 2020 2020 2020 2020  ios[1]..        
-00000340: 636d 5f70 726f 6669 6c65 7320 3d20 7365  cm_profiles = se
-00000350: 6c66 2e5f 636f 6e74 726f 6c5f 6d6f 6465  lf._control_mode
-00000360: 5f73 6365 6e61 7269 6f2e 7265 6164 5f70  _scenario.read_p
-00000370: 765f 7072 6f66 696c 6573 2829 0d0a 2020  v_profiles()..  
-00000380: 2020 2020 2020 6966 206e 6f74 2063 6d5f        if not cm_
-00000390: 7072 6f66 696c 6573 3a0d 0a20 2020 2020  profiles:..     
-000003a0: 2020 2020 2020 2073 656c 662e 5f70 765f         self._pv_
-000003b0: 7379 7374 656d 5f6e 616d 6573 203d 205b  system_names = [
-000003c0: 5d0d 0a20 2020 2020 2020 2020 2020 2072  ]..            r
-000003d0: 6574 7572 6e0d 0a0d 0a20 2020 2020 2020  eturn....       
-000003e0: 2073 656c 662e 5f70 765f 7379 7374 656d   self._pv_system
-000003f0: 5f6e 616d 6573 203d 205b 785b 226e 616d  _names = [x["nam
-00000400: 6522 5d20 666f 7220 7820 696e 2063 6d5f  e"] for x in cm_
-00000410: 7072 6f66 696c 6573 5b22 7076 5f73 7973  profiles["pv_sys
-00000420: 7465 6d73 225d 5d0d 0a0d 0a20 2020 2020  tems"]]....     
-00000430: 2020 2073 656c 662e 5f70 6631 5f70 765f     self._pf1_pv_
-00000440: 7379 7374 656d 7320 3d20 7b0d 0a20 2020  systems = {..   
-00000450: 2020 2020 2020 2020 2078 5b22 6e61 6d65           x["name
-00000460: 225d 3a20 7820 666f 7220 7820 696e 2073  "]: x for x in s
-00000470: 656c 662e 5f70 6631 5f73 6365 6e61 7269  elf._pf1_scenari
-00000480: 6f2e 7265 6164 5f70 765f 7072 6f66 696c  o.read_pv_profil
-00000490: 6573 2829 5b22 7076 5f73 7973 7465 6d73  es()["pv_systems
-000004a0: 225d 0d0a 2020 2020 2020 2020 7d0d 0a20  "]..        }.. 
-000004b0: 2020 2020 2020 2073 656c 662e 5f63 6f6e         self._con
-000004c0: 7472 6f6c 5f6d 6f64 655f 7076 5f73 7973  trol_mode_pv_sys
-000004d0: 7465 6d73 203d 207b 0d0a 2020 2020 2020  tems = {..      
-000004e0: 2020 2020 2020 785b 226e 616d 6522 5d3a        x["name"]:
-000004f0: 2078 2066 6f72 2078 2069 6e20 636d 5f70   x for x in cm_p
-00000500: 726f 6669 6c65 735b 2270 765f 7379 7374  rofiles["pv_syst
-00000510: 656d 7322 5d0d 0a20 2020 2020 2020 207d  ems"]..        }
-00000520: 0d0a 0d0a 2020 2020 6465 6620 5f67 6574  ....    def _get
-00000530: 5f70 765f 7379 7374 656d 5f69 6e66 6f28  _pv_system_info(
-00000540: 7365 6c66 2c20 7076 5f73 7973 7465 6d2c  self, pv_system,
-00000550: 2073 6365 6e61 7269 6f29 3a0d 0a20 2020   scenario):..   
-00000560: 2020 2020 2069 6620 7363 656e 6172 696f       if scenario
-00000570: 203d 3d20 5046 315f 5343 454e 4152 494f   == PF1_SCENARIO
-00000580: 3a0d 0a20 2020 2020 2020 2020 2020 2070  :..            p
-00000590: 765f 7379 7374 656d 7320 3d20 7365 6c66  v_systems = self
-000005a0: 2e5f 7066 315f 7076 5f73 7973 7465 6d73  ._pf1_pv_systems
-000005b0: 0d0a 2020 2020 2020 2020 656c 7365 3a0d  ..        else:.
-000005c0: 0a20 2020 2020 2020 2020 2020 2070 765f  .            pv_
-000005d0: 7379 7374 656d 7320 3d20 7365 6c66 2e5f  systems = self._
-000005e0: 636f 6e74 726f 6c5f 6d6f 6465 5f70 765f  control_mode_pv_
-000005f0: 7379 7374 656d 730d 0a0d 0a20 2020 2020  systems....     
-00000600: 2020 2072 6574 7572 6e20 7076 5f73 7973     return pv_sys
-00000610: 7465 6d73 5b70 765f 7379 7374 656d 5d0d  tems[pv_system].
-00000620: 0a0d 0a20 2020 2064 6566 205f 6861 735f  ...    def _has_
-00000630: 7076 5f73 7973 7465 6d73 2873 656c 6629  pv_systems(self)
-00000640: 3a0d 0a20 2020 2020 2020 2072 6574 7572  :..        retur
-00000650: 6e20 6c65 6e28 7365 6c66 2e5f 7076 5f73  n len(self._pv_s
-00000660: 7973 7465 6d5f 6e61 6d65 7329 203e 2030  ystem_names) > 0
-00000670: 0d0a 0d0a 2020 2020 4073 7461 7469 636d  ....    @staticm
-00000680: 6574 686f 640d 0a20 2020 2064 6566 2067  ethod..    def g
-00000690: 6574 5f72 6571 7569 7265 645f 6578 706f  et_required_expo
-000006a0: 7274 7328 7365 7474 696e 6773 293a 0d0a  rts(settings):..
-000006b0: 2020 2020 2020 2020 6772 616e 756c 6172          granular
-000006c0: 6974 7920 3d20 5265 706f 7274 4772 616e  ity = ReportGran
-000006d0: 756c 6172 6974 7928 7365 7474 696e 6773  ularity(settings
-000006e0: 2e72 6570 6f72 7473 2e67 7261 6e75 6c61  .reports.granula
-000006f0: 7269 7479 290d 0a20 2020 2020 2020 205f  rity)..        _
-00000700: 7479 7065 2c20 7375 6d5f 656c 656d 656e  type, sum_elemen
-00000710: 7473 203d 2052 6570 6f72 7442 6173 652e  ts = ReportBase.
-00000720: 5f70 6172 616d 735f 6672 6f6d 5f67 7261  _params_from_gra
-00000730: 6e75 6c61 7269 7479 2867 7261 6e75 6c61  nularity(granula
-00000740: 7269 7479 290d 0a20 2020 2020 2020 2072  rity)..        r
-00000750: 6574 7572 6e20 7b0d 0a20 2020 2020 2020  eturn {..       
-00000760: 2020 2020 2022 5056 5379 7374 656d 7322       "PVSystems"
-00000770: 3a20 5b0d 0a20 2020 2020 2020 2020 2020  : [..           
-00000780: 2020 2020 207b 0d0a 2020 2020 2020 2020       {..        
-00000790: 2020 2020 2020 2020 2020 2020 2270 726f              "pro
-000007a0: 7065 7274 7922 3a20 2250 6f77 6572 7322  perty": "Powers"
-000007b0: 2c0d 0a20 2020 2020 2020 2020 2020 2020  ,..             
-000007c0: 2020 2020 2020 2022 7374 6f72 655f 7661         "store_va
-000007d0: 6c75 6573 5f74 7970 6522 3a20 5f74 7970  lues_type": _typ
-000007e0: 652c 0d0a 2020 2020 2020 2020 2020 2020  e,..            
-000007f0: 2020 2020 2020 2020 2273 756d 5f65 6c65          "sum_ele
-00000800: 6d65 6e74 7322 3a20 7375 6d5f 656c 656d  ments": sum_elem
-00000810: 656e 7473 2c0d 0a20 2020 2020 2020 2020  ents,..         
-00000820: 2020 2020 2020 2020 2020 2022 6461 7461             "data
-00000830: 5f63 6f6e 7665 7273 696f 6e22 3a20 2273  _conversion": "s
-00000840: 756d 5f61 6273 5f72 6561 6c22 2c0d 0a20  um_abs_real",.. 
-00000850: 2020 2020 2020 2020 2020 2020 2020 207d                 }
-00000860: 2c0d 0a20 2020 2020 2020 2020 2020 205d  ,..            ]
-00000870: 2c0d 0a20 2020 2020 2020 207d 0d0a 0d0a  ,..        }....
-00000880: 2020 2020 4073 7461 7469 636d 6574 686f      @staticmetho
-00000890: 640d 0a20 2020 2064 6566 2067 6574 5f72  d..    def get_r
-000008a0: 6571 7569 7265 645f 7363 656e 6172 696f  equired_scenario
-000008b0: 5f6e 616d 6573 2829 3a0d 0a20 2020 2020  _names():..     
-000008c0: 2020 2072 6574 7572 6e20 7365 7428 5b22     return set(["
-000008d0: 7066 3122 2c20 2263 6f6e 7472 6f6c 5f6d  pf1", "control_m
-000008e0: 6f64 6522 5d29 0d0a 0d0a 2020 2020 4073  ode"])....    @s
-000008f0: 7461 7469 636d 6574 686f 640d 0a20 2020  taticmethod..   
-00000900: 2064 6566 2073 6574 5f72 6571 7569 7265   def set_require
-00000910: 645f 7072 6f6a 6563 745f 7365 7474 696e  d_project_settin
-00000920: 6773 2873 6574 7469 6e67 7329 3a0d 0a20  gs(settings):.. 
-00000930: 2020 2020 2020 2069 6620 6e6f 7420 7365         if not se
-00000940: 7474 696e 6773 2e65 7870 6f72 7473 2e65  ttings.exports.e
-00000950: 7870 6f72 745f 7076 5f70 726f 6669 6c65  xport_pv_profile
-00000960: 733a 0d0a 2020 2020 2020 2020 2020 2020  s:..            
-00000970: 7365 7474 696e 6773 2e65 7870 6f72 7473  settings.exports
-00000980: 2e65 7870 6f72 745f 7076 5f70 726f 6669  .export_pv_profi
-00000990: 6c65 7320 3d20 5472 7565 0d0a 2020 2020  les = True..    
-000009a0: 2020 2020 2020 2020 6c6f 6767 6572 2e69          logger.i
-000009b0: 6e66 6f28 2245 6e61 626c 6564 2045 7870  nfo("Enabled Exp
-000009c0: 6f72 7420 5056 2050 726f 6669 6c65 7322  ort PV Profiles"
-000009d0: 290d 0a0d 0a0d 0a63 6c61 7373 2050 7643  )......class PvC
-000009e0: 6c69 7070 696e 6752 6570 6f72 7428 5076  lippingReport(Pv
-000009f0: 5265 706f 7274 4261 7365 293a 0d0a 2020  ReportBase):..  
-00000a00: 2020 2222 2252 6570 6f72 7473 2050 5620    """Reports PV 
-00000a10: 436c 6970 7069 6e67 2066 6f72 2074 6865  Clipping for the
-00000a20: 2073 696d 756c 6174 696f 6e2e 0d0a 0d0a   simulation.....
-00000a30: 2020 2020 5468 6520 7265 706f 7274 2067      The report g
-00000a40: 656e 6572 6174 6573 2061 2070 765f 636c  enerates a pv_cl
-00000a50: 6970 7069 6e67 206f 7574 7075 7420 6669  ipping output fi
-00000a60: 6c65 2e20 5468 6520 6669 6c65 2065 7874  le. The file ext
-00000a70: 656e 7369 6f6e 2064 6570 656e 6473 0d0a  ension depends..
-00000a80: 2020 2020 6f6e 2074 6865 2069 6e70 7574      on the input
-00000a90: 2070 6172 616d 6574 6572 732e 2049 6620   parameters. If 
-00000aa0: 7468 6520 6461 7461 2077 6173 2063 6f6c  the data was col
-00000ab0: 6c65 6374 6564 2061 7420 6576 6572 7920  lected at every 
-00000ac0: 7469 6d65 2070 6f69 6e74 2074 6865 6e0d  time point then.
-00000ad0: 0a20 2020 2074 6865 206f 7574 7075 7420  .    the output 
-00000ae0: 6669 6c65 2077 696c 6c20 6265 202e 6373  file will be .cs
-00000af0: 7620 6f72 202e 6835 2c20 6465 7065 6e64  v or .h5, depend
-00000b00: 696e 6720 6f6e 2027 4578 706f 7274 2046  ing on 'Export F
-00000b10: 6f72 6d61 742e 270d 0a20 2020 204f 7468  ormat.'..    Oth
-00000b20: 6572 7769 7365 2c20 7468 6520 6f75 7470  erwise, the outp
-00000b30: 7574 2066 696c 6520 7769 6c6c 2062 6520  ut file will be 
-00000b40: 2e6a 736f 6e2e 0d0a 0d0a 2020 2020 544f  .json.....    TO
-00000b50: 444f 3a20 5468 6973 2069 7320 616e 2065  DO: This is an e
-00000b60: 7870 6572 696d 656e 7461 6c20 7265 706f  xperimental repo
-00000b70: 7274 2e20 4f75 7470 7574 7320 6861 7665  rt. Outputs have
-00000b80: 206e 6f74 2062 6565 6e20 7661 6c69 6461   not been valida
-00000b90: 7465 642e 0d0a 0d0a 2020 2020 2222 220d  ted.....    """.
-00000ba0: 0a0d 0a20 2020 2050 4552 5f54 494d 455f  ...    PER_TIME_
-00000bb0: 504f 494e 545f 4649 4c45 4e41 4d45 203d  POINT_FILENAME =
-00000bc0: 2022 7076 5f63 6c69 7070 696e 672e 6835   "pv_clipping.h5
-00000bd0: 220d 0a20 2020 2054 4f54 414c 5f46 494c  "..    TOTAL_FIL
-00000be0: 454e 414d 4520 3d20 2270 765f 636c 6970  ENAME = "pv_clip
-00000bf0: 7069 6e67 2e6a 736f 6e22 0d0a 2020 2020  ping.json"..    
-00000c00: 4e41 4d45 203d 2022 5056 2043 6c69 7070  NAME = "PV Clipp
-00000c10: 696e 6722 0d0a 0d0a 2020 2020 6465 6620  ing"....    def 
-00000c20: 5f5f 696e 6974 5f5f 2873 656c 662c 206e  __init__(self, n
-00000c30: 616d 652c 2072 6573 756c 7473 2c20 7369  ame, results, si
-00000c40: 6d75 6c61 7469 6f6e 5f63 6f6e 6669 6729  mulation_config)
-00000c50: 3a0d 0a20 2020 2020 2020 2073 7570 6572  :..        super
-00000c60: 2829 2e5f 5f69 6e69 745f 5f28 6e61 6d65  ().__init__(name
-00000c70: 2c20 7265 7375 6c74 732c 2073 696d 756c  , results, simul
-00000c80: 6174 696f 6e5f 636f 6e66 6967 290d 0a20  ation_config).. 
-00000c90: 2020 2020 2020 2069 6620 6e6f 7420 7365         if not se
-00000ca0: 6c66 2e5f 6861 735f 7076 5f73 7973 7465  lf._has_pv_syste
-00000cb0: 6d73 2829 3a0d 0a20 2020 2020 2020 2020  ms():..         
-00000cc0: 2020 2072 6574 7572 6e0d 0a0d 0a20 2020     return....   
-00000cd0: 2020 2020 2064 6966 665f 746f 6c65 7261       diff_tolera
-00000ce0: 6e63 6520 3d20 7365 6c66 2e5f 7265 706f  nce = self._repo
-00000cf0: 7274 5f73 6574 7469 6e67 732e 6469 6666  rt_settings.diff
-00000d00: 5f74 6f6c 6572 616e 6365 5f70 6572 6365  _tolerance_perce
-00000d10: 6e74 5f70 6d70 7020 2a20 2e30 310d 0a20  nt_pmpp * .01.. 
-00000d20: 2020 2020 2020 2064 656e 6f6d 696e 6174         denominat
-00000d30: 6f72 5f74 6f6c 6572 616e 6365 203d 2073  or_tolerance = s
-00000d40: 656c 662e 5f72 6570 6f72 745f 7365 7474  elf._report_sett
-00000d50: 696e 6773 2e64 656e 6f6d 696e 6174 6f72  ings.denominator
-00000d60: 5f74 6f6c 6572 616e 6365 5f70 6572 6365  _tolerance_perce
-00000d70: 6e74 5f70 6d70 7020 2a20 2e30 310d 0a20  nt_pmpp * .01.. 
-00000d80: 2020 2020 2020 206c 6f67 6765 722e 6465         logger.de
-00000d90: 6275 6728 2274 6f6c 6572 616e 6365 733a  bug("tolerances:
-00000da0: 2064 6966 663d 2573 2064 656e 6f6d 696e   diff=%s denomin
-00000db0: 6174 6f72 3d25 7322 2c20 6469 6666 5f74  ator=%s", diff_t
-00000dc0: 6f6c 6572 616e 6365 2c20 6465 6e6f 6d69  olerance, denomi
-00000dd0: 6e61 746f 725f 746f 6c65 7261 6e63 6529  nator_tolerance)
-00000de0: 0d0a 2020 2020 2020 2020 7365 6c66 2e5f  ..        self._
-00000df0: 6469 6666 5f74 6f6c 6572 616e 6365 7320  diff_tolerances 
-00000e00: 3d20 7b7d 0d0a 2020 2020 2020 2020 7365  = {}..        se
-00000e10: 6c66 2e5f 6465 6e6f 6d69 6e61 746f 725f  lf._denominator_
-00000e20: 746f 6c65 7261 6e63 6573 203d 207b 7d0d  tolerances = {}.
-00000e30: 0a20 2020 2020 2020 2066 6f72 2070 765f  .        for pv_
-00000e40: 7379 7374 656d 2069 6e20 7365 6c66 2e5f  system in self._
-00000e50: 7066 315f 7363 656e 6172 696f 2e72 6561  pf1_scenario.rea
-00000e60: 645f 7076 5f70 726f 6669 6c65 7328 295b  d_pv_profiles()[
-00000e70: 2270 765f 7379 7374 656d 7322 5d3a 0d0a  "pv_systems"]:..
-00000e80: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00000e90: 2e5f 6469 6666 5f74 6f6c 6572 616e 6365  ._diff_tolerance
-00000ea0: 735b 7076 5f73 7973 7465 6d5b 226e 616d  s[pv_system["nam
-00000eb0: 6522 5d5d 203d 2070 765f 7379 7374 656d  e"]] = pv_system
-00000ec0: 5b22 706d 7070 225d 202a 2064 6966 665f  ["pmpp"] * diff_
-00000ed0: 746f 6c65 7261 6e63 650d 0a20 2020 2020  tolerance..     
-00000ee0: 2020 2020 2020 2073 656c 662e 5f64 656e         self._den
-00000ef0: 6f6d 696e 6174 6f72 5f74 6f6c 6572 616e  ominator_toleran
-00000f00: 6365 735b 7076 5f73 7973 7465 6d5b 226e  ces[pv_system["n
-00000f10: 616d 6522 5d5d 203d 2070 765f 7379 7374  ame"]] = pv_syst
-00000f20: 656d 5b22 706d 7070 225d 202a 2064 656e  em["pmpp"] * den
-00000f30: 6f6d 696e 6174 6f72 5f74 6f6c 6572 616e  ominator_toleran
-00000f40: 6365 0d0a 0d0a 2020 2020 4073 7461 7469  ce....    @stati
-00000f50: 636d 6574 686f 640d 0a20 2020 2064 6566  cmethod..    def
-00000f60: 205f 6361 6c63 756c 6174 655f 636c 6970   _calculate_clip
-00000f70: 7069 6e67 2874 6f74 616c 5f64 635f 706f  ping(total_dc_po
-00000f80: 7765 722c 2070 6631 5f72 6561 6c5f 706f  wer, pf1_real_po
-00000f90: 7765 7229 3a0d 0a20 2020 2020 2020 2072  wer):..        r
-00000fa0: 6574 7572 6e20 2874 6f74 616c 5f64 635f  eturn (total_dc_
-00000fb0: 706f 7765 7220 2d20 7066 315f 7265 616c  power - pf1_real
-00000fc0: 5f70 6f77 6572 2920 2a20 3130 3020 2f20  _power) * 100 / 
-00000fd0: 7066 315f 7265 616c 5f70 6f77 6572 0d0a  pf1_real_power..
-00000fe0: 0d0a 2020 2020 4073 7461 7469 636d 6574  ..    @staticmet
-00000ff0: 686f 640d 0a20 2020 2064 6566 205f 6361  hod..    def _ca
-00001000: 6c63 756c 6174 655f 636c 6970 7069 6e67  lculate_clipping
-00001010: 5f61 7272 6179 2864 635f 706f 7765 722c  _array(dc_power,
-00001020: 2070 6631 5f72 6561 6c5f 706f 7765 7229   pf1_real_power)
-00001030: 3a0d 0a20 2020 2020 2020 2064 6370 203d  :..        dcp =
-00001040: 2064 635f 706f 7765 722e 7661 6c75 6573   dc_power.values
-00001050: 0d0a 2020 2020 2020 2020 7270 203d 2070  ..        rp = p
-00001060: 6631 5f72 6561 6c5f 706f 7765 722e 7661  f1_real_power.va
-00001070: 6c75 6573 0d0a 2020 2020 2020 2020 7270  lues..        rp
-00001080: 203d 206e 702e 7768 6572 6528 7270 3d3d   = np.where(rp==
-00001090: 302c 206e 702e 6e61 6e2c 2072 7029 0d0a  0, np.nan, rp)..
-000010a0: 2020 2020 2020 2020 7265 7475 726e 2028          return (
-000010b0: 6463 7020 2d20 7270 2920 2f20 7270 202a  dcp - rp) / rp *
-000010c0: 2031 3030 0d0a 0d0a 2020 2020 6465 6620   100....    def 
-000010d0: 5f67 6574 5f74 6f74 616c 5f64 635f 706f  _get_total_dc_po
-000010e0: 7765 725f 6163 726f 7373 5f70 765f 7379  wer_across_pv_sy
-000010f0: 7374 656d 7328 7365 6c66 293a 0d0a 2020  stems(self):..  
-00001100: 2020 2020 2020 746f 7461 6c5f 6463 5f70        total_dc_p
-00001110: 6f77 6572 203d 2030 2e30 0d0a 2020 2020  ower = 0.0..    
-00001120: 2020 2020 666f 7220 6e61 6d65 2069 6e20      for name in 
-00001130: 7365 6c66 2e5f 7076 5f73 7973 7465 6d5f  self._pv_system_
-00001140: 6e61 6d65 733a 0d0a 2020 2020 2020 2020  names:..        
-00001150: 2020 2020 746f 7461 6c5f 6463 5f70 6f77      total_dc_pow
-00001160: 6572 202b 3d20 7365 6c66 2e5f 6765 745f  er += self._get_
-00001170: 746f 7461 6c5f 6463 5f70 6f77 6572 286e  total_dc_power(n
-00001180: 616d 6529 0d0a 2020 2020 2020 2020 7265  ame)..        re
-00001190: 7475 726e 2074 6f74 616c 5f64 635f 706f  turn total_dc_po
-000011a0: 7765 720d 0a0d 0a20 2020 2064 6566 205f  wer....    def _
-000011b0: 6765 745f 746f 7461 6c5f 6463 5f70 6f77  get_total_dc_pow
-000011c0: 6572 2873 656c 662c 2070 765f 7379 7374  er(self, pv_syst
-000011d0: 656d 293a 0d0a 2020 2020 2020 2020 636d  em):..        cm
-000011e0: 5f69 6e66 6f20 3d20 7365 6c66 2e5f 6765  _info = self._ge
-000011f0: 745f 7076 5f73 7973 7465 6d5f 696e 666f  t_pv_system_info
-00001200: 2870 765f 7379 7374 656d 2c20 434f 4e54  (pv_system, CONT
-00001210: 524f 4c5f 4d4f 4445 5f53 4345 4e41 5249  ROL_MODE_SCENARI
-00001220: 4f29 0d0a 2020 2020 2020 2020 706d 7070  O)..        pmpp
-00001230: 203d 2063 6d5f 696e 666f 5b22 706d 7070   = cm_info["pmpp
-00001240: 225d 0d0a 2020 2020 2020 2020 6972 7261  "]..        irra
-00001250: 6469 616e 6365 203d 2063 6d5f 696e 666f  diance = cm_info
-00001260: 5b22 6972 7261 6469 616e 6365 225d 0d0a  ["irradiance"]..
-00001270: 2020 2020 2020 2020 746f 7461 6c5f 6972          total_ir
-00001280: 7261 6469 616e 6365 203d 2063 6d5f 696e  radiance = cm_in
-00001290: 666f 5b22 6c6f 6164 5f73 6861 7065 5f70  fo["load_shape_p
-000012a0: 6d75 6c74 5f73 756d 225d 0d0a 2020 2020  mult_sum"]..    
-000012b0: 2020 2020 7265 7475 726e 2070 6d70 7020      return pmpp 
-000012c0: 2a20 6972 7261 6469 616e 6365 202a 2074  * irradiance * t
-000012d0: 6f74 616c 5f69 7272 6164 6961 6e63 650d  otal_irradiance.
-000012e0: 0a0d 0a20 2020 2064 6566 205f 6765 6e65  ...    def _gene
-000012f0: 7261 7465 5f70 6572 5f70 765f 7379 7374  rate_per_pv_syst
-00001300: 656d 5f70 6572 5f74 696d 655f 706f 696e  em_per_time_poin
-00001310: 7428 7365 6c66 2c20 6f75 7470 7574 5f64  t(self, output_d
-00001320: 6972 293a 0d0a 2020 2020 2020 2020 7076  ir):..        pv
-00001330: 5f6c 6f61 645f 7368 6170 6573 203d 2073  _load_shapes = s
-00001340: 656c 662e 5f72 6561 645f 7076 5f6c 6f61  elf._read_pv_loa
-00001350: 645f 7368 6170 6573 2829 0d0a 2020 2020  d_shapes()..    
-00001360: 2020 2020 7066 315f 7265 616c 5f70 6f77      pf1_real_pow
-00001370: 6572 5f66 756c 6c20 3d20 7365 6c66 2e5f  er_full = self._
-00001380: 7066 315f 7363 656e 6172 696f 2e67 6574  pf1_scenario.get
-00001390: 5f66 756c 6c5f 6461 7461 6672 616d 6528  _full_dataframe(
-000013a0: 0d0a 2020 2020 2020 2020 2020 2020 2250  ..            "P
-000013b0: 5653 7973 7465 6d73 222c 2022 506f 7765  VSystems", "Powe
-000013c0: 7273 220d 0a20 2020 2020 2020 2029 0d0a  rs"..        )..
-000013d0: 2020 2020 2020 2020 6e61 6d65 203d 204e          name = N
-000013e0: 6f6e 650d 0a0d 0a20 2020 2020 2020 2023  one....        #
-000013f0: 2054 4f44 4f3a 2041 7070 6c79 2074 6f6c   TODO: Apply tol
-00001400: 6572 616e 6365 7320 746f 206f 7468 6572  erances to other
-00001410: 2067 7261 6e75 6c61 7269 7479 206f 7074   granularity opt
-00001420: 696f 6e73 2e0d 0a20 2020 2020 2020 2064  ions...        d
-00001430: 6566 2063 616c 635f 636c 6970 7069 6e67  ef calc_clipping
-00001440: 2864 6370 2c20 7270 293a 0d0a 2020 2020  (dcp, rp):..    
-00001450: 2020 2020 2020 2020 6966 2064 6370 203c          if dcp <
-00001460: 2073 656c 662e 5f64 656e 6f6d 696e 6174   self._denominat
-00001470: 6f72 5f74 6f6c 6572 616e 6365 735b 6e61  or_tolerances[na
-00001480: 6d65 5d3a 0d0a 2020 2020 2020 2020 2020  me]:..          
-00001490: 2020 2020 2020 7265 7475 726e 2030 0d0a        return 0..
-000014a0: 2020 2020 2020 2020 2020 2020 6469 6666              diff
-000014b0: 203d 2064 6370 202d 2072 700d 0a20 2020   = dcp - rp..   
-000014c0: 2020 2020 2020 2020 2069 6620 6469 6666           if diff
-000014d0: 203c 2030 2061 6e64 2061 6273 2864 6966   < 0 and abs(dif
-000014e0: 6629 203c 2073 656c 662e 5f64 6966 665f  f) < self._diff_
-000014f0: 746f 6c65 7261 6e63 6573 5b6e 616d 655d  tolerances[name]
-00001500: 3a0d 0a20 2020 2020 2020 2020 2020 2020  :..             
-00001510: 2020 2072 6574 7572 6e20 300d 0a20 2020     return 0..   
-00001520: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
-00001530: 2864 6370 202d 2072 7029 202f 2072 7020  (dcp - rp) / rp 
-00001540: 2a20 3130 300d 0a0d 0a20 2020 2020 2020  * 100....       
-00001550: 2064 6174 6120 3d20 7b7d 0d0a 2020 2020   data = {}..    
-00001560: 2020 2020 666f 7220 5f6e 616d 6520 696e      for _name in
-00001570: 2073 656c 662e 5f70 765f 7379 7374 656d   self._pv_system
-00001580: 5f6e 616d 6573 3a0d 0a20 2020 2020 2020  _names:..       
-00001590: 2020 2020 206e 616d 6520 3d20 5f6e 616d       name = _nam
-000015a0: 650d 0a20 2020 2020 2020 2020 2020 2063  e..            c
-000015b0: 6d5f 696e 666f 203d 2073 656c 662e 5f67  m_info = self._g
-000015c0: 6574 5f70 765f 7379 7374 656d 5f69 6e66  et_pv_system_inf
-000015d0: 6f28 6e61 6d65 2c20 434f 4e54 524f 4c5f  o(name, CONTROL_
-000015e0: 4d4f 4445 5f53 4345 4e41 5249 4f29 0d0a  MODE_SCENARIO)..
-000015f0: 2020 2020 2020 2020 2020 2020 7066 315f              pf1_
-00001600: 7265 616c 5f70 6f77 6572 203d 2070 6631  real_power = pf1
-00001610: 5f72 6561 6c5f 706f 7765 725f 6675 6c6c  _real_power_full
-00001620: 5b6e 616d 6520 2b20 225f 5f50 6f77 6572  [name + "__Power
-00001630: 7322 5d0d 0a20 2020 2020 2020 2020 2020  s"]..           
-00001640: 2064 635f 706f 7765 7220 3d20 7076 5f6c   dc_power = pv_l
-00001650: 6f61 645f 7368 6170 6573 5b63 6d5f 696e  oad_shapes[cm_in
-00001660: 666f 5b22 6c6f 6164 5f73 6861 7065 5f70  fo["load_shape_p
-00001670: 726f 6669 6c65 225d 5d20 2a20 5c0d 0a20  rofile"]] * \.. 
-00001680: 2020 2020 2020 2020 2020 2020 2020 2063                 c
-00001690: 6d5f 696e 666f 5b22 706d 7070 225d 202a  m_info["pmpp"] *
-000016a0: 205c 0d0a 2020 2020 2020 2020 2020 2020   \..            
-000016b0: 2020 2020 636d 5f69 6e66 6f5b 2269 7272      cm_info["irr
-000016c0: 6164 6961 6e63 6522 5d0d 0a20 2020 2020  adiance"]..     
-000016d0: 2020 2020 2020 2061 7373 6572 7420 6c65         assert le
-000016e0: 6e28 6463 5f70 6f77 6572 2920 3d3d 206c  n(dc_power) == l
-000016f0: 656e 2870 6631 5f72 6561 6c5f 706f 7765  en(pf1_real_powe
-00001700: 7229 2c20 5c0d 0a20 2020 2020 2020 2020  r), \..         
-00001710: 2020 2020 2020 2066 227b 6c65 6e28 6463         f"{len(dc
-00001720: 5f70 6f77 6572 297d 207b 6c65 6e28 7066  _power)} {len(pf
-00001730: 315f 7265 616c 5f70 6f77 6572 297d 220d  1_real_power)}".
-00001740: 0a20 2020 2020 2020 2020 2020 2063 6f6c  .            col
-00001750: 203d 206e 616d 6520 2b20 225f 5f43 6c69   = name + "__Cli
-00001760: 7070 696e 6722 0d0a 2020 2020 2020 2020  pping"..        
-00001770: 2020 2020 6461 7461 5b63 6f6c 5d20 3d20      data[col] = 
-00001780: 6463 5f70 6f77 6572 2e63 6f6d 6269 6e65  dc_power.combine
-00001790: 2870 6631 5f72 6561 6c5f 706f 7765 722c  (pf1_real_power,
-000017a0: 2063 616c 635f 636c 6970 7069 6e67 292e   calc_clipping).
-000017b0: 7661 6c75 6573 0d0a 0d0a 2020 2020 2020  values....      
-000017c0: 2020 6466 203d 2070 642e 4461 7461 4672    df = pd.DataFr
-000017d0: 616d 6528 6461 7461 2c20 696e 6465 783d  ame(data, index=
-000017e0: 7066 315f 7265 616c 5f70 6f77 6572 5f66  pf1_real_power_f
-000017f0: 756c 6c2e 696e 6465 7829 0d0a 2020 2020  ull.index)..    
-00001800: 2020 2020 7365 6c66 2e5f 6578 706f 7274      self._export
-00001810: 5f64 6174 6166 7261 6d65 5f72 6570 6f72  _dataframe_repor
-00001820: 7428 6466 2c20 6f75 7470 7574 5f64 6972  t(df, output_dir
-00001830: 2c20 2270 765f 636c 6970 7069 6e67 2229  , "pv_clipping")
-00001840: 0d0a 0d0a 2020 2020 6465 6620 5f67 656e  ....    def _gen
-00001850: 6572 6174 655f 7065 725f 7076 5f73 7973  erate_per_pv_sys
-00001860: 7465 6d5f 746f 7461 6c28 7365 6c66 2c20  tem_total(self, 
-00001870: 6f75 7470 7574 5f64 6972 293a 0d0a 2020  output_dir):..  
-00001880: 2020 2020 2020 6461 7461 203d 207b 2270        data = {"p
-00001890: 765f 7379 7374 656d 7322 3a20 5b5d 7d0d  v_systems": []}.
-000018a0: 0a20 2020 2020 2020 2066 6f72 206e 616d  .        for nam
-000018b0: 6520 696e 2073 656c 662e 5f70 765f 7379  e in self._pv_sy
-000018c0: 7374 656d 5f6e 616d 6573 3a0d 0a20 2020  stem_names:..   
-000018d0: 2020 2020 2020 2020 2070 6631 5f72 6561           pf1_rea
-000018e0: 6c5f 706f 7765 7220 3d20 7365 6c66 2e5f  l_power = self._
-000018f0: 7066 315f 7363 656e 6172 696f 2e67 6574  pf1_scenario.get
-00001900: 5f65 6c65 6d65 6e74 5f70 726f 7065 7274  _element_propert
-00001910: 795f 7661 6c75 6528 0d0a 2020 2020 2020  y_value(..      
-00001920: 2020 2020 2020 2020 2020 2250 5653 7973            "PVSys
-00001930: 7465 6d73 222c 2022 506f 7765 7273 5375  tems", "PowersSu
-00001940: 6d22 2c20 6e61 6d65 0d0a 2020 2020 2020  m", name..      
-00001950: 2020 2020 2020 290d 0a20 2020 2020 2020        )..       
-00001960: 2020 2020 2064 635f 706f 7765 7220 3d20       dc_power = 
-00001970: 7365 6c66 2e5f 6765 745f 746f 7461 6c5f  self._get_total_
-00001980: 6463 5f70 6f77 6572 286e 616d 6529 0d0a  dc_power(name)..
-00001990: 2020 2020 2020 2020 2020 2020 636c 6970              clip
-000019a0: 7069 6e67 203d 2073 656c 662e 5f63 616c  ping = self._cal
-000019b0: 6375 6c61 7465 5f63 6c69 7070 696e 6728  culate_clipping(
-000019c0: 6463 5f70 6f77 6572 2c20 7066 315f 7265  dc_power, pf1_re
-000019d0: 616c 5f70 6f77 6572 290d 0a20 2020 2020  al_power)..     
-000019e0: 2020 2020 2020 2064 6174 615b 2270 765f         data["pv_
-000019f0: 7379 7374 656d 7322 5d2e 6170 7065 6e64  systems"].append
-00001a00: 280d 0a20 2020 2020 2020 2020 2020 2020  (..             
-00001a10: 2020 207b 0d0a 2020 2020 2020 2020 2020     {..          
-00001a20: 2020 2020 2020 2020 2020 226e 616d 6522            "name"
-00001a30: 3a20 6e61 6d65 2c0d 0a20 2020 2020 2020  : name,..       
-00001a40: 2020 2020 2020 2020 2020 2020 2022 636c               "cl
-00001a50: 6970 7069 6e67 223a 2063 6c69 7070 696e  ipping": clippin
-00001a60: 672c 0d0a 2020 2020 2020 2020 2020 2020  g,..            
-00001a70: 2020 2020 7d0d 0a20 2020 2020 2020 2020      }..         
-00001a80: 2020 2029 0d0a 2020 2020 2020 2020 7365     )..        se
-00001a90: 6c66 2e5f 6578 706f 7274 5f6a 736f 6e5f  lf._export_json_
-00001aa0: 7265 706f 7274 2864 6174 612c 206f 7574  report(data, out
-00001ab0: 7075 745f 6469 722c 2073 656c 662e 544f  put_dir, self.TO
-00001ac0: 5441 4c5f 4649 4c45 4e41 4d45 290d 0a0d  TAL_FILENAME)...
-00001ad0: 0a20 2020 2064 6566 205f 6765 6e65 7261  .    def _genera
-00001ae0: 7465 5f61 6c6c 5f70 765f 7379 7374 656d  te_all_pv_system
-00001af0: 735f 7065 725f 7469 6d65 5f70 6f69 6e74  s_per_time_point
-00001b00: 2873 656c 662c 206f 7574 7075 745f 6469  (self, output_di
-00001b10: 7229 3a0d 0a20 2020 2020 2020 2070 6631  r):..        pf1
-00001b20: 5f72 6561 6c5f 706f 7765 7220 3d20 7365  _real_power = se
-00001b30: 6c66 2e5f 7066 315f 7363 656e 6172 696f  lf._pf1_scenario
-00001b40: 2e67 6574 5f73 756d 6d65 645f 656c 656d  .get_summed_elem
-00001b50: 656e 745f 6461 7461 6672 616d 6528 0d0a  ent_dataframe(..
-00001b60: 2020 2020 2020 2020 2020 2020 2250 5653              "PVS
-00001b70: 7973 7465 6d73 222c 2022 506f 7765 7273  ystems", "Powers
-00001b80: 220d 0a20 2020 2020 2020 2029 0d0a 2020  "..        )..  
-00001b90: 2020 2020 2020 7076 5f6c 6f61 645f 7368        pv_load_sh
-00001ba0: 6170 6573 203d 2073 656c 662e 5f72 6561  apes = self._rea
-00001bb0: 645f 7076 5f6c 6f61 645f 7368 6170 6573  d_pv_load_shapes
-00001bc0: 2829 0d0a 2020 2020 2020 2020 6463 5f70  ()..        dc_p
-00001bd0: 6f77 6572 7320 3d20 7b7d 0d0a 2020 2020  owers = {}..    
-00001be0: 2020 2020 666f 7220 6e61 6d65 2069 6e20      for name in 
-00001bf0: 7365 6c66 2e5f 7076 5f73 7973 7465 6d5f  self._pv_system_
-00001c00: 6e61 6d65 733a 0d0a 2020 2020 2020 2020  names:..        
-00001c10: 2020 2020 636d 5f69 6e66 6f20 3d20 7365      cm_info = se
-00001c20: 6c66 2e5f 6765 745f 7076 5f73 7973 7465  lf._get_pv_syste
-00001c30: 6d5f 696e 666f 286e 616d 652c 2043 4f4e  m_info(name, CON
-00001c40: 5452 4f4c 5f4d 4f44 455f 5343 454e 4152  TROL_MODE_SCENAR
-00001c50: 494f 290d 0a20 2020 2020 2020 2020 2020  IO)..           
-00001c60: 2073 6572 6965 7320 3d20 7076 5f6c 6f61   series = pv_loa
-00001c70: 645f 7368 6170 6573 5b63 6d5f 696e 666f  d_shapes[cm_info
-00001c80: 5b22 6c6f 6164 5f73 6861 7065 5f70 726f  ["load_shape_pro
-00001c90: 6669 6c65 225d 5d0d 0a20 2020 2020 2020  file"]]..       
-00001ca0: 2020 2020 2064 635f 706f 7765 7220 3d20       dc_power = 
-00001cb0: 7365 7269 6573 202a 2063 6d5f 696e 666f  series * cm_info
-00001cc0: 5b22 706d 7070 225d 202a 2063 6d5f 696e  ["pmpp"] * cm_in
-00001cd0: 666f 5b22 6972 7261 6469 616e 6365 225d  fo["irradiance"]
-00001ce0: 0d0a 2020 2020 2020 2020 2020 2020 6173  ..            as
-00001cf0: 7365 7274 206c 656e 2864 635f 706f 7765  sert len(dc_powe
-00001d00: 7229 203d 3d20 6c65 6e28 7066 315f 7265  r) == len(pf1_re
-00001d10: 616c 5f70 6f77 6572 290d 0a20 2020 2020  al_power)..     
-00001d20: 2020 2020 2020 2064 635f 706f 7765 7273         dc_powers
-00001d30: 5b6e 616d 655d 203d 2064 635f 706f 7765  [name] = dc_powe
-00001d40: 722e 7661 6c75 6573 0d0a 2020 2020 2020  r.values..      
-00001d50: 2020 2020 2020 2320 544f 444f 3a20 6a75        # TODO: ju
-00001d60: 7374 2066 6f72 2076 616c 6964 6174 696f  st for validatio
-00001d70: 6e0d 0a20 2020 2020 2020 2020 2020 2061  n..            a
-00001d80: 7373 6572 7420 6d61 7468 2e69 7363 6c6f  ssert math.isclo
-00001d90: 7365 2873 756d 2864 635f 706f 7765 722e  se(sum(dc_power.
-00001da0: 7661 6c75 6573 292c 2063 6d5f 696e 666f  values), cm_info
-00001db0: 5b22 6c6f 6164 5f73 6861 7065 5f70 6d75  ["load_shape_pmu
-00001dc0: 6c74 5f73 756d 225d 202a 2063 6d5f 696e  lt_sum"] * cm_in
-00001dd0: 666f 5b22 706d 7070 225d 202a 2063 6d5f  fo["pmpp"] * cm_
-00001de0: 696e 666f 5b22 6972 7261 6469 616e 6365  info["irradiance
-00001df0: 225d 290d 0a20 2020 2020 2020 2064 6620  "])..        df 
-00001e00: 3d20 7064 2e44 6174 6146 7261 6d65 2864  = pd.DataFrame(d
-00001e10: 635f 706f 7765 7273 2c20 696e 6465 783d  c_powers, index=
-00001e20: 7066 315f 7265 616c 5f70 6f77 6572 2e69  pf1_real_power.i
-00001e30: 6e64 6578 290d 0a20 2020 2020 2020 2074  ndex)..        t
-00001e40: 6f74 616c 5f64 635f 706f 7765 7220 3d20  otal_dc_power = 
-00001e50: 6466 2e73 756d 2861 7869 733d 3129 0d0a  df.sum(axis=1)..
-00001e60: 0d0a 2020 2020 2020 2020 636c 6970 7069  ..        clippi
-00001e70: 6e67 203d 2070 642e 4461 7461 4672 616d  ng = pd.DataFram
-00001e80: 6528 0d0a 2020 2020 2020 2020 2020 2020  e(..            
-00001e90: 7365 6c66 2e5f 6361 6c63 756c 6174 655f  self._calculate_
-00001ea0: 636c 6970 7069 6e67 5f61 7272 6179 2874  clipping_array(t
-00001eb0: 6f74 616c 5f64 635f 706f 7765 722c 2070  otal_dc_power, p
-00001ec0: 6631 5f72 6561 6c5f 706f 7765 722e 696c  f1_real_power.il
-00001ed0: 6f63 5b3a 2c20 305d 292c 0d0a 2020 2020  oc[:, 0]),..    
-00001ee0: 2020 2020 2020 2020 696e 6465 783d 7066          index=pf
-00001ef0: 315f 7265 616c 5f70 6f77 6572 2e69 6e64  1_real_power.ind
-00001f00: 6578 2c0d 0a20 2020 2020 2020 2020 2020  ex,..           
-00001f10: 2063 6f6c 756d 6e73 3d5b 2254 6f74 616c   columns=["Total
-00001f20: 436c 6970 7069 6e67 225d 2c0d 0a20 2020  Clipping"],..   
-00001f30: 2020 2020 2029 0d0a 2020 2020 2020 2020       )..        
-00001f40: 7365 6c66 2e5f 6578 706f 7274 5f64 6174  self._export_dat
-00001f50: 6166 7261 6d65 5f72 6570 6f72 7428 636c  aframe_report(cl
-00001f60: 6970 7069 6e67 2c20 6f75 7470 7574 5f64  ipping, output_d
-00001f70: 6972 2c20 2270 765f 636c 6970 7069 6e67  ir, "pv_clipping
-00001f80: 2229 0d0a 0d0a 2020 2020 6465 6620 5f67  ")....    def _g
-00001f90: 656e 6572 6174 655f 616c 6c5f 7076 5f73  enerate_all_pv_s
-00001fa0: 7973 7465 6d73 5f74 6f74 616c 2873 656c  ystems_total(sel
-00001fb0: 662c 206f 7574 7075 745f 6469 7229 3a0d  f, output_dir):.
-00001fc0: 0a20 2020 2020 2020 2074 6f74 616c 5f64  .        total_d
-00001fd0: 635f 706f 7765 7220 3d20 7365 6c66 2e5f  c_power = self._
-00001fe0: 6765 745f 746f 7461 6c5f 6463 5f70 6f77  get_total_dc_pow
-00001ff0: 6572 5f61 6372 6f73 735f 7076 5f73 7973  er_across_pv_sys
-00002000: 7465 6d73 2829 0d0a 0d0a 2020 2020 2020  tems()....      
-00002010: 2020 7066 315f 7265 616c 5f70 6f77 6572    pf1_real_power
-00002020: 203d 206e 6578 7428 6974 6572 280d 0a20   = next(iter(.. 
-00002030: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00002040: 5f70 6631 5f73 6365 6e61 7269 6f2e 6765  _pf1_scenario.ge
-00002050: 745f 7375 6d6d 6564 5f65 6c65 6d65 6e74  t_summed_element
-00002060: 5f74 6f74 616c 2822 5056 5379 7374 656d  _total("PVSystem
-00002070: 7322 2c20 2250 6f77 6572 7353 756d 2229  s", "PowersSum")
-00002080: 2e76 616c 7565 7328 290d 0a20 2020 2020  .values()..     
-00002090: 2020 2029 290d 0a20 2020 2020 2020 2063     ))..        c
-000020a0: 6c69 7070 696e 6720 3d20 7365 6c66 2e5f  lipping = self._
-000020b0: 6361 6c63 756c 6174 655f 636c 6970 7069  calculate_clippi
-000020c0: 6e67 2874 6f74 616c 5f64 635f 706f 7765  ng(total_dc_powe
-000020d0: 722c 2070 6631 5f72 6561 6c5f 706f 7765  r, pf1_real_powe
-000020e0: 7229 0d0a 2020 2020 2020 2020 6461 7461  r)..        data
-000020f0: 203d 207b 2263 6c69 7070 696e 6722 3a20   = {"clipping": 
-00002100: 636c 6970 7069 6e67 7d0d 0a20 2020 2020  clipping}..     
-00002110: 2020 2073 656c 662e 5f65 7870 6f72 745f     self._export_
-00002120: 6a73 6f6e 5f72 6570 6f72 7428 6461 7461  json_report(data
-00002130: 2c20 6f75 7470 7574 5f64 6972 2c20 7365  , output_dir, se
-00002140: 6c66 2e54 4f54 414c 5f46 494c 454e 414d  lf.TOTAL_FILENAM
-00002150: 4529 0d0a 0d0a 2020 2020 6465 6620 5f72  E)....    def _r
-00002160: 6561 645f 7076 5f6c 6f61 645f 7368 6170  ead_pv_load_shap
-00002170: 6573 2873 656c 6629 3a0d 0a20 2020 2020  es(self):..     
-00002180: 2020 2070 6174 6820 3d20 6f73 2e70 6174     path = os.pat
-00002190: 682e 6a6f 696e 280d 0a20 2020 2020 2020  h.join(..       
-000021a0: 2020 2020 2073 7472 2873 656c 662e 5f73       str(self._s
-000021b0: 6574 7469 6e67 732e 7072 6f6a 6563 742e  ettings.project.
-000021c0: 6163 7469 7665 5f70 726f 6a65 6374 5f70  active_project_p
-000021d0: 6174 6829 2c0d 0a20 2020 2020 2020 2020  ath),..         
-000021e0: 2020 2022 4578 706f 7274 7322 2c0d 0a20     "Exports",.. 
-000021f0: 2020 2020 2020 2020 2020 2043 4f4e 5452             CONTR
-00002200: 4f4c 5f4d 4f44 455f 5343 454e 4152 494f  OL_MODE_SCENARIO
-00002210: 2c0d 0a20 2020 2020 2020 2020 2020 2050  ,..            P
-00002220: 565f 4c4f 4144 5f53 4841 5045 5f46 494c  V_LOAD_SHAPE_FIL
-00002230: 454e 414d 452c 0d0a 2020 2020 2020 2020  ENAME,..        
-00002240: 290d 0a20 2020 2020 2020 2072 6574 7572  )..        retur
-00002250: 6e20 7265 6164 5f64 6174 6166 7261 6d65  n read_dataframe
-00002260: 2870 6174 6829 0d0a 0d0a 2020 2020 6465  (path)....    de
-00002270: 6620 6765 6e65 7261 7465 2873 656c 662c  f generate(self,
-00002280: 206f 7574 7075 745f 6469 7229 3a0d 0a20   output_dir):.. 
-00002290: 2020 2020 2020 2069 6620 6e6f 7420 7365         if not se
-000022a0: 6c66 2e5f 6861 735f 7076 5f73 7973 7465  lf._has_pv_syste
-000022b0: 6d73 2829 3a0d 0a20 2020 2020 2020 2020  ms():..         
-000022c0: 2020 2072 6574 7572 6e0d 0a0d 0a20 2020     return....   
-000022d0: 2020 2020 2067 7261 6e75 6c61 7269 7479       granularity
-000022e0: 203d 2073 656c 662e 5f73 6574 7469 6e67   = self._setting
-000022f0: 732e 7265 706f 7274 732e 6772 616e 756c  s.reports.granul
-00002300: 6172 6974 790d 0a20 2020 2020 2020 2069  arity..        i
-00002310: 6620 6772 616e 756c 6172 6974 7920 3d3d  f granularity ==
-00002320: 2052 6570 6f72 7447 7261 6e75 6c61 7269   ReportGranulari
-00002330: 7479 2e50 4552 5f45 4c45 4d45 4e54 5f50  ty.PER_ELEMENT_P
-00002340: 4552 5f54 494d 455f 504f 494e 543a 0d0a  ER_TIME_POINT:..
-00002350: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00002360: 2e5f 6765 6e65 7261 7465 5f70 6572 5f70  ._generate_per_p
-00002370: 765f 7379 7374 656d 5f70 6572 5f74 696d  v_system_per_tim
-00002380: 655f 706f 696e 7428 6f75 7470 7574 5f64  e_point(output_d
-00002390: 6972 290d 0a20 2020 2020 2020 2065 6c69  ir)..        eli
-000023a0: 6620 6772 616e 756c 6172 6974 7920 3d3d  f granularity ==
-000023b0: 2052 6570 6f72 7447 7261 6e75 6c61 7269   ReportGranulari
-000023c0: 7479 2e50 4552 5f45 4c45 4d45 4e54 5f54  ty.PER_ELEMENT_T
-000023d0: 4f54 414c 3a0d 0a20 2020 2020 2020 2020  OTAL:..         
-000023e0: 2020 2073 656c 662e 5f67 656e 6572 6174     self._generat
-000023f0: 655f 7065 725f 7076 5f73 7973 7465 6d5f  e_per_pv_system_
-00002400: 746f 7461 6c28 6f75 7470 7574 5f64 6972  total(output_dir
-00002410: 290d 0a20 2020 2020 2020 2065 6c69 6620  )..        elif 
-00002420: 6772 616e 756c 6172 6974 7920 3d3d 2052  granularity == R
-00002430: 6570 6f72 7447 7261 6e75 6c61 7269 7479  eportGranularity
-00002440: 2e41 4c4c 5f45 4c45 4d45 4e54 535f 5045  .ALL_ELEMENTS_PE
-00002450: 525f 5449 4d45 5f50 4f49 4e54 3a0d 0a20  R_TIME_POINT:.. 
-00002460: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00002470: 5f67 656e 6572 6174 655f 616c 6c5f 7076  _generate_all_pv
-00002480: 5f73 7973 7465 6d73 5f70 6572 5f74 696d  _systems_per_tim
-00002490: 655f 706f 696e 7428 6f75 7470 7574 5f64  e_point(output_d
-000024a0: 6972 290d 0a20 2020 2020 2020 2065 6c69  ir)..        eli
-000024b0: 6620 6772 616e 756c 6172 6974 7920 3d3d  f granularity ==
-000024c0: 2052 6570 6f72 7447 7261 6e75 6c61 7269   ReportGranulari
-000024d0: 7479 2e41 4c4c 5f45 4c45 4d45 4e54 535f  ty.ALL_ELEMENTS_
-000024e0: 544f 5441 4c3a 0d0a 2020 2020 2020 2020  TOTAL:..        
-000024f0: 2020 2020 7365 6c66 2e5f 6765 6e65 7261      self._genera
-00002500: 7465 5f61 6c6c 5f70 765f 7379 7374 656d  te_all_pv_system
-00002510: 735f 746f 7461 6c28 6f75 7470 7574 5f64  s_total(output_d
-00002520: 6972 290d 0a20 2020 2020 2020 2065 6c73  ir)..        els
-00002530: 653a 0d0a 2020 2020 2020 2020 2020 2020  e:..            
-00002540: 6173 7365 7274 2046 616c 7365 0d0a 0d0a  assert False....
-00002550: 0d0a 636c 6173 7320 5076 4375 7274 6169  ..class PvCurtai
-00002560: 6c6d 656e 7452 6570 6f72 7428 5076 5265  lmentReport(PvRe
-00002570: 706f 7274 4261 7365 293a 0d0a 2020 2020  portBase):..    
-00002580: 2222 2252 6570 6f72 7473 2050 5620 4375  """Reports PV Cu
-00002590: 7274 6169 6c6d 656e 7420 6174 2065 7665  rtailment at eve
-000025a0: 7279 2074 696d 6520 706f 696e 7420 696e  ry time point in
-000025b0: 2074 6865 2073 696d 756c 6174 696f 6e2e   the simulation.
-000025c0: 0d0a 0d0a 2020 2020 5468 6520 7265 706f  ....    The repo
-000025d0: 7274 2067 656e 6572 6174 6573 2061 2070  rt generates a p
-000025e0: 765f 6375 7274 6169 6c6d 656e 7420 6f75  v_curtailment ou
-000025f0: 7470 7574 2066 696c 652e 2054 6865 2066  tput file. The f
-00002600: 696c 6520 6578 7465 6e73 696f 6e0d 0a20  ile extension.. 
-00002610: 2020 2064 6570 656e 6473 206f 6e20 7468     depends on th
-00002620: 6520 696e 7075 7420 7061 7261 6d65 7465  e input paramete
-00002630: 7273 2e20 4966 2074 6865 2064 6174 6120  rs. If the data 
-00002640: 7761 7320 636f 6c6c 6563 7465 6420 6174  was collected at
-00002650: 2065 7665 7279 2074 696d 650d 0a20 2020   every time..   
-00002660: 2070 6f69 6e74 2074 6865 6e20 7468 6520   point then the 
-00002670: 6f75 7470 7574 2066 696c 6520 7769 6c6c  output file will
-00002680: 2062 6520 2e63 7376 206f 7220 2e68 352c   be .csv or .h5,
-00002690: 2064 6570 656e 6469 6e67 206f 6e20 2745   depending on 'E
-000026a0: 7870 6f72 740d 0a20 2020 2046 6f72 6d61  xport..    Forma
-000026b0: 742e 2720 4f74 6865 7277 6973 652c 2074  t.' Otherwise, t
-000026c0: 6865 206f 7574 7075 7420 6669 6c65 2077  he output file w
-000026d0: 696c 6c20 6265 202e 6a73 6f6e 2e0d 0a0d  ill be .json....
-000026e0: 0a20 2020 2054 4f44 4f3a 2054 6869 7320  .    TODO: This 
-000026f0: 6973 2061 6e20 6578 7065 7269 6d65 6e74  is an experiment
-00002700: 616c 2072 6570 6f72 742e 204f 7574 7075  al report. Outpu
-00002710: 7473 2068 6176 6520 6e6f 7420 6265 656e  ts have not been
-00002720: 2076 616c 6964 6174 6564 2e0d 0a0d 0a20   validated..... 
-00002730: 2020 2022 2222 0d0a 0d0a 2020 2020 5045     """....    PE
-00002740: 525f 5449 4d45 5f50 4f49 4e54 5f46 494c  R_TIME_POINT_FIL
-00002750: 454e 414d 4520 3d20 2270 765f 6375 7274  ENAME = "pv_curt
-00002760: 6169 6c6d 656e 742e 6835 220d 0a20 2020  ailment.h5"..   
-00002770: 2054 4f54 414c 5f46 494c 454e 414d 4520   TOTAL_FILENAME 
-00002780: 3d20 2270 765f 6375 7274 6169 6c6d 656e  = "pv_curtailmen
-00002790: 742e 6a73 6f6e 220d 0a20 2020 204e 414d  t.json"..    NAM
-000027a0: 4520 3d20 2250 5620 4375 7274 6169 6c6d  E = "PV Curtailm
-000027b0: 656e 7422 0d0a 0d0a 2020 2020 6465 6620  ent"....    def 
-000027c0: 5f5f 696e 6974 5f5f 2873 656c 662c 206e  __init__(self, n
-000027d0: 616d 652c 2072 6573 756c 7473 2c20 7369  ame, results, si
-000027e0: 6d75 6c61 7469 6f6e 5f63 6f6e 6669 6729  mulation_config)
-000027f0: 3a0d 0a20 2020 2020 2020 2073 7570 6572  :..        super
-00002800: 2829 2e5f 5f69 6e69 745f 5f28 6e61 6d65  ().__init__(name
-00002810: 2c20 7265 7375 6c74 732c 2073 696d 756c  , results, simul
-00002820: 6174 696f 6e5f 636f 6e66 6967 290d 0a20  ation_config).. 
-00002830: 2020 2020 2020 2069 6620 6e6f 7420 7365         if not se
-00002840: 6c66 2e5f 6861 735f 7076 5f73 7973 7465  lf._has_pv_syste
-00002850: 6d73 2829 3a0d 0a20 2020 2020 2020 2020  ms():..         
-00002860: 2020 2072 6574 7572 6e0d 0a0d 0a20 2020     return....   
-00002870: 2020 2020 2064 6966 665f 746f 6c65 7261       diff_tolera
-00002880: 6e63 6520 3d20 7365 6c66 2e5f 7265 706f  nce = self._repo
-00002890: 7274 5f73 6574 7469 6e67 732e 6469 6666  rt_settings.diff
-000028a0: 5f74 6f6c 6572 616e 6365 5f70 6572 6365  _tolerance_perce
-000028b0: 6e74 5f70 6d70 7020 2a20 2e30 310d 0a20  nt_pmpp * .01.. 
-000028c0: 2020 2020 2020 2064 656e 6f6d 696e 6174         denominat
-000028d0: 6f72 5f74 6f6c 6572 616e 6365 203d 2073  or_tolerance = s
-000028e0: 656c 662e 5f72 6570 6f72 745f 7365 7474  elf._report_sett
-000028f0: 696e 6773 2e64 656e 6f6d 696e 6174 6f72  ings.denominator
-00002900: 5f74 6f6c 6572 616e 6365 5f70 6572 6365  _tolerance_perce
-00002910: 6e74 5f70 6d70 7020 2a20 2e30 310d 0a20  nt_pmpp * .01.. 
-00002920: 2020 2020 2020 206c 6f67 6765 722e 6465         logger.de
-00002930: 6275 6728 2274 6f6c 6572 616e 6365 733a  bug("tolerances:
-00002940: 2064 6966 663d 2573 2064 656e 6f6d 696e   diff=%s denomin
-00002950: 6174 6f72 3d25 7322 2c20 6469 6666 5f74  ator=%s", diff_t
-00002960: 6f6c 6572 616e 6365 2c20 6465 6e6f 6d69  olerance, denomi
-00002970: 6e61 746f 725f 746f 6c65 7261 6e63 6529  nator_tolerance)
-00002980: 0d0a 2020 2020 2020 2020 7365 6c66 2e5f  ..        self._
-00002990: 6469 6666 5f74 6f6c 6572 616e 6365 7320  diff_tolerances 
-000029a0: 3d20 7b7d 0d0a 2020 2020 2020 2020 7365  = {}..        se
-000029b0: 6c66 2e5f 6465 6e6f 6d69 6e61 746f 725f  lf._denominator_
-000029c0: 746f 6c65 7261 6e63 6573 203d 207b 7d0d  tolerances = {}.
-000029d0: 0a20 2020 2020 2020 2066 6f72 2070 765f  .        for pv_
-000029e0: 7379 7374 656d 2069 6e20 7365 6c66 2e5f  system in self._
-000029f0: 7066 315f 7363 656e 6172 696f 2e72 6561  pf1_scenario.rea
-00002a00: 645f 7076 5f70 726f 6669 6c65 7328 295b  d_pv_profiles()[
-00002a10: 2270 765f 7379 7374 656d 7322 5d3a 0d0a  "pv_systems"]:..
-00002a20: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00002a30: 2e5f 6469 6666 5f74 6f6c 6572 616e 6365  ._diff_tolerance
-00002a40: 735b 7076 5f73 7973 7465 6d5b 226e 616d  s[pv_system["nam
-00002a50: 6522 5d5d 203d 2070 765f 7379 7374 656d  e"]] = pv_system
-00002a60: 5b22 706d 7070 225d 202a 2064 6966 665f  ["pmpp"] * diff_
-00002a70: 746f 6c65 7261 6e63 650d 0a20 2020 2020  tolerance..     
-00002a80: 2020 2020 2020 2073 656c 662e 5f64 656e         self._den
-00002a90: 6f6d 696e 6174 6f72 5f74 6f6c 6572 616e  ominator_toleran
-00002aa0: 6365 735b 7076 5f73 7973 7465 6d5b 226e  ces[pv_system["n
-00002ab0: 616d 6522 5d5d 203d 2070 765f 7379 7374  ame"]] = pv_syst
-00002ac0: 656d 5b22 706d 7070 225d 202a 2064 656e  em["pmpp"] * den
-00002ad0: 6f6d 696e 6174 6f72 5f74 6f6c 6572 616e  ominator_toleran
-00002ae0: 6365 0d0a 0d0a 2020 2020 6465 6620 5f67  ce....    def _g
-00002af0: 656e 6572 6174 655f 7065 725f 7076 5f73  enerate_per_pv_s
-00002b00: 7973 7465 6d5f 7065 725f 7469 6d65 5f70  ystem_per_time_p
-00002b10: 6f69 6e74 2873 656c 662c 206f 7574 7075  oint(self, outpu
-00002b20: 745f 6469 7229 3a0d 0a20 2020 2020 2020  t_dir):..       
-00002b30: 2070 6631 5f70 6f77 6572 203d 2073 656c   pf1_power = sel
-00002b40: 662e 5f70 6631 5f73 6365 6e61 7269 6f2e  f._pf1_scenario.
-00002b50: 6765 745f 6675 6c6c 5f64 6174 6166 7261  get_full_datafra
-00002b60: 6d65 2822 5056 5379 7374 656d 7322 2c20  me("PVSystems", 
-00002b70: 2250 6f77 6572 7322 290d 0a20 2020 2020  "Powers")..     
-00002b80: 2020 2063 6f6e 7472 6f6c 5f6d 6f64 655f     control_mode_
-00002b90: 706f 7765 7220 3d20 7365 6c66 2e5f 636f  power = self._co
-00002ba0: 6e74 726f 6c5f 6d6f 6465 5f73 6365 6e61  ntrol_mode_scena
-00002bb0: 7269 6f2e 6765 745f 6675 6c6c 5f64 6174  rio.get_full_dat
-00002bc0: 6166 7261 6d65 280d 0a20 2020 2020 2020  aframe(..       
-00002bd0: 2020 2020 2022 5056 5379 7374 656d 7322       "PVSystems"
-00002be0: 2c20 2250 6f77 6572 7322 0d0a 2020 2020  , "Powers"..    
-00002bf0: 2020 2020 290d 0a20 2020 2020 2020 206e      )..        n
-00002c00: 616d 6520 3d20 4e6f 6e65 0d0a 2020 2020  ame = None..    
-00002c10: 2020 2020 6465 6620 6361 6c63 5f63 7572      def calc_cur
-00002c20: 7461 696c 6d65 6e74 2870 6631 2c20 636d  tailment(pf1, cm
-00002c30: 293a 0d0a 2020 2020 2020 2020 2020 2020  ):..            
-00002c40: 6966 2070 6631 203c 2073 656c 662e 5f64  if pf1 < self._d
-00002c50: 656e 6f6d 696e 6174 6f72 5f74 6f6c 6572  enominator_toler
-00002c60: 616e 6365 735b 6e61 6d65 5d3a 0d0a 2020  ances[name]:..  
-00002c70: 2020 2020 2020 2020 2020 2020 2020 7265                re
-00002c80: 7475 726e 2030 0d0a 2020 2020 2020 2020  turn 0..        
-00002c90: 2020 2020 6469 6666 203d 2070 6631 202d      diff = pf1 -
-00002ca0: 2063 6d0d 0a20 2020 2020 2020 2020 2020   cm..           
-00002cb0: 2069 6620 6469 6666 203c 2030 2061 6e64   if diff < 0 and
-00002cc0: 2061 6273 2864 6966 6629 203c 2073 656c   abs(diff) < sel
-00002cd0: 662e 5f64 6966 665f 746f 6c65 7261 6e63  f._diff_toleranc
-00002ce0: 6573 5b6e 616d 655d 3a0d 0a20 2020 2020  es[name]:..     
-00002cf0: 2020 2020 2020 2020 2020 2072 6574 7572             retur
-00002d00: 6e20 300d 0a20 2020 2020 2020 2020 2020  n 0..           
-00002d10: 2072 6574 7572 6e20 2870 6631 202d 2063   return (pf1 - c
-00002d20: 6d29 202f 2070 6631 202a 2031 3030 0d0a  m) / pf1 * 100..
-00002d30: 0d0a 2020 2020 2020 2020 6461 7461 203d  ..        data =
-00002d40: 207b 7d0d 0a20 2020 2020 2020 2066 6f72   {}..        for
-00002d50: 2063 6f6c 2069 6e20 7066 315f 706f 7765   col in pf1_powe
-00002d60: 722e 636f 6c75 6d6e 733a 0d0a 2020 2020  r.columns:..    
-00002d70: 2020 2020 2020 2020 6e61 6d65 203d 2063          name = c
-00002d80: 6f6c 2e73 706c 6974 2822 5f5f 2229 5b30  ol.split("__")[0
-00002d90: 5d0d 0a20 2020 2020 2020 2020 2020 2073  ]..            s
-00002da0: 5f70 6631 203d 2070 6631 5f70 6f77 6572  _pf1 = pf1_power
-00002db0: 5b63 6f6c 5d0d 0a20 2020 2020 2020 2020  [col]..         
-00002dc0: 2020 2073 5f63 6d20 3d20 636f 6e74 726f     s_cm = contro
-00002dd0: 6c5f 6d6f 6465 5f70 6f77 6572 5b63 6f6c  l_mode_power[col
-00002de0: 5d0d 0a20 2020 2020 2020 2020 2020 206e  ]..            n
-00002df0: 6577 5f6e 616d 6520 3d20 636f 6c2e 7265  ew_name = col.re
-00002e00: 706c 6163 6528 2250 6f77 6572 7322 2c20  place("Powers", 
-00002e10: 2243 7572 7461 696c 6d65 6e74 2229 0d0a  "Curtailment")..
-00002e20: 2020 2020 2020 2020 2020 2020 6461 7461              data
-00002e30: 5b6e 6577 5f6e 616d 655d 203d 2073 5f70  [new_name] = s_p
-00002e40: 6631 2e63 6f6d 6269 6e65 2873 5f63 6d2c  f1.combine(s_cm,
-00002e50: 2063 616c 635f 6375 7274 6169 6c6d 656e   calc_curtailmen
-00002e60: 7429 2e76 616c 7565 730d 0a0d 0a20 2020  t).values....   
-00002e70: 2020 2020 2064 6620 3d20 7064 2e44 6174       df = pd.Dat
-00002e80: 6146 7261 6d65 2864 6174 612c 2069 6e64  aFrame(data, ind
-00002e90: 6578 3d70 6631 5f70 6f77 6572 2e69 6e64  ex=pf1_power.ind
-00002ea0: 6578 290d 0a20 2020 2020 2020 2073 656c  ex)..        sel
-00002eb0: 662e 5f65 7870 6f72 745f 6461 7461 6672  f._export_datafr
-00002ec0: 616d 655f 7265 706f 7274 2864 662c 206f  ame_report(df, o
-00002ed0: 7574 7075 745f 6469 722c 2022 7076 5f63  utput_dir, "pv_c
-00002ee0: 7572 7461 696c 6d65 6e74 2229 0d0a 0d0a  urtailment")....
-00002ef0: 2020 2020 6465 6620 5f67 656e 6572 6174      def _generat
-00002f00: 655f 7065 725f 7076 5f73 7973 7465 6d5f  e_per_pv_system_
-00002f10: 746f 7461 6c28 7365 6c66 2c20 6f75 7470  total(self, outp
-00002f20: 7574 5f64 6972 293a 0d0a 2020 2020 2020  ut_dir):..      
-00002f30: 2020 6461 7461 203d 207b 2270 765f 7379    data = {"pv_sy
-00002f40: 7374 656d 7322 3a20 5b5d 7d0d 0a20 2020  stems": []}..   
-00002f50: 2020 2020 2066 6f72 206e 616d 6520 696e       for name in
-00002f60: 2073 656c 662e 5f70 765f 7379 7374 656d   self._pv_system
-00002f70: 5f6e 616d 6573 3a0d 0a20 2020 2020 2020  _names:..       
-00002f80: 2020 2020 2070 6631 5f70 6f77 6572 203d       pf1_power =
-00002f90: 2073 656c 662e 5f70 6631 5f73 6365 6e61   self._pf1_scena
-00002fa0: 7269 6f2e 6765 745f 656c 656d 656e 745f  rio.get_element_
-00002fb0: 7072 6f70 6572 7479 5f76 616c 7565 280d  property_value(.
-00002fc0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00002fd0: 2022 5056 5379 7374 656d 7322 2c20 2250   "PVSystems", "P
-00002fe0: 6f77 6572 7353 756d 222c 206e 616d 650d  owersSum", name.
-00002ff0: 0a20 2020 2020 2020 2020 2020 2029 0d0a  .            )..
-00003000: 2020 2020 2020 2020 2020 2020 636f 6e74              cont
-00003010: 726f 6c5f 6d6f 6465 5f70 6f77 6572 203d  rol_mode_power =
-00003020: 2073 656c 662e 5f63 6f6e 7472 6f6c 5f6d   self._control_m
-00003030: 6f64 655f 7363 656e 6172 696f 2e67 6574  ode_scenario.get
-00003040: 5f65 6c65 6d65 6e74 5f70 726f 7065 7274  _element_propert
-00003050: 795f 7661 6c75 6528 0d0a 2020 2020 2020  y_value(..      
-00003060: 2020 2020 2020 2020 2020 2250 5653 7973            "PVSys
-00003070: 7465 6d73 222c 2022 506f 7765 7273 5375  tems", "PowersSu
-00003080: 6d22 2c20 6e61 6d65 0d0a 2020 2020 2020  m", name..      
-00003090: 2020 2020 2020 290d 0a20 2020 2020 2020        )..       
-000030a0: 2020 2020 2063 7572 7461 696c 6d65 6e74       curtailment
-000030b0: 203d 2028 7066 315f 706f 7765 7220 2d20   = (pf1_power - 
-000030c0: 636f 6e74 726f 6c5f 6d6f 6465 5f70 6f77  control_mode_pow
-000030d0: 6572 2920 2f20 7066 315f 706f 7765 7220  er) / pf1_power 
-000030e0: 2a20 3130 300d 0a20 2020 2020 2020 2020  * 100..         
-000030f0: 2020 2064 6174 615b 2270 765f 7379 7374     data["pv_syst
-00003100: 656d 7322 5d2e 6170 7065 6e64 280d 0a20  ems"].append(.. 
-00003110: 2020 2020 2020 2020 2020 2020 2020 207b                 {
-00003120: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00003130: 2020 2020 2020 226e 616d 6522 3a20 6e61        "name": na
-00003140: 6d65 2c0d 0a20 2020 2020 2020 2020 2020  me,..           
-00003150: 2020 2020 2020 2020 2022 6375 7274 6169           "curtai
-00003160: 6c6d 656e 7422 3a20 6375 7274 6169 6c6d  lment": curtailm
-00003170: 656e 742c 0d0a 2020 2020 2020 2020 2020  ent,..          
-00003180: 2020 2020 2020 7d0d 0a20 2020 2020 2020        }..       
-00003190: 2020 2020 2029 0d0a 2020 2020 2020 2020       )..        
-000031a0: 7365 6c66 2e5f 6578 706f 7274 5f6a 736f  self._export_jso
-000031b0: 6e5f 7265 706f 7274 2864 6174 612c 206f  n_report(data, o
-000031c0: 7574 7075 745f 6469 722c 2073 656c 662e  utput_dir, self.
-000031d0: 544f 5441 4c5f 4649 4c45 4e41 4d45 290d  TOTAL_FILENAME).
-000031e0: 0a0d 0a20 2020 2064 6566 205f 6765 6e65  ...    def _gene
-000031f0: 7261 7465 5f61 6c6c 5f70 765f 7379 7374  rate_all_pv_syst
-00003200: 656d 735f 7065 725f 7469 6d65 5f70 6f69  ems_per_time_poi
-00003210: 6e74 2873 656c 662c 206f 7574 7075 745f  nt(self, output_
-00003220: 6469 7229 3a0d 0a20 2020 2020 2020 2070  dir):..        p
-00003230: 6631 5f70 6f77 6572 203d 2073 656c 662e  f1_power = self.
-00003240: 5f70 6631 5f73 6365 6e61 7269 6f2e 6765  _pf1_scenario.ge
-00003250: 745f 7375 6d6d 6564 5f65 6c65 6d65 6e74  t_summed_element
-00003260: 5f64 6174 6166 7261 6d65 2822 5056 5379  _dataframe("PVSy
-00003270: 7374 656d 7322 2c20 2250 6f77 6572 7322  stems", "Powers"
-00003280: 290d 0a20 2020 2020 2020 2063 6f6e 7472  )..        contr
-00003290: 6f6c 5f6d 6f64 655f 706f 7765 7220 3d20  ol_mode_power = 
-000032a0: 7365 6c66 2e5f 636f 6e74 726f 6c5f 6d6f  self._control_mo
-000032b0: 6465 5f73 6365 6e61 7269 6f2e 6765 745f  de_scenario.get_
-000032c0: 7375 6d6d 6564 5f65 6c65 6d65 6e74 5f64  summed_element_d
-000032d0: 6174 6166 7261 6d65 280d 0a20 2020 2020  ataframe(..     
-000032e0: 2020 2020 2020 2022 5056 5379 7374 656d         "PVSystem
-000032f0: 7322 2c20 2250 6f77 6572 7322 0d0a 2020  s", "Powers"..  
-00003300: 2020 2020 2020 290d 0a20 2020 2020 2020        )..       
-00003310: 2064 6620 3d20 2870 6631 5f70 6f77 6572   df = (pf1_power
-00003320: 202d 2063 6f6e 7472 6f6c 5f6d 6f64 655f   - control_mode_
-00003330: 706f 7765 7229 202f 2070 6631 5f70 6f77  power) / pf1_pow
-00003340: 6572 202a 2031 3030 0d0a 2020 2020 2020  er * 100..      
-00003350: 2020 7365 6c66 2e5f 6578 706f 7274 5f64    self._export_d
-00003360: 6174 6166 7261 6d65 5f72 6570 6f72 7428  ataframe_report(
-00003370: 6466 2c20 6f75 7470 7574 5f64 6972 2c20  df, output_dir, 
-00003380: 2270 765f 6375 7274 6169 6c6d 656e 7422  "pv_curtailment"
-00003390: 290d 0a0d 0a20 2020 2064 6566 205f 6765  )....    def _ge
-000033a0: 6e65 7261 7465 5f61 6c6c 5f70 765f 7379  nerate_all_pv_sy
-000033b0: 7374 656d 735f 746f 7461 6c28 7365 6c66  stems_total(self
-000033c0: 2c20 6f75 7470 7574 5f64 6972 293a 0d0a  , output_dir):..
-000033d0: 2020 2020 2020 2020 7066 315f 706f 7765          pf1_powe
-000033e0: 7220 3d20 6e65 7874 2869 7465 7228 0d0a  r = next(iter(..
-000033f0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00003400: 2e5f 7066 315f 7363 656e 6172 696f 2e67  ._pf1_scenario.g
-00003410: 6574 5f73 756d 6d65 645f 656c 656d 656e  et_summed_elemen
-00003420: 745f 746f 7461 6c28 2250 5653 7973 7465  t_total("PVSyste
-00003430: 6d73 222c 2022 506f 7765 7273 5375 6d22  ms", "PowersSum"
-00003440: 292e 7661 6c75 6573 2829 0d0a 2020 2020  ).values()..    
-00003450: 2020 2020 2929 0d0a 2020 2020 2020 2020      ))..        
-00003460: 636f 6e74 726f 6c5f 6d6f 6465 5f70 6f77  control_mode_pow
-00003470: 6572 203d 206e 6578 7428 6974 6572 280d  er = next(iter(.
-00003480: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00003490: 662e 5f63 6f6e 7472 6f6c 5f6d 6f64 655f  f._control_mode_
-000034a0: 7363 656e 6172 696f 2e67 6574 5f73 756d  scenario.get_sum
-000034b0: 6d65 645f 656c 656d 656e 745f 746f 7461  med_element_tota
-000034c0: 6c28 2250 5653 7973 7465 6d73 222c 2022  l("PVSystems", "
-000034d0: 506f 7765 7273 5375 6d22 292e 7661 6c75  PowersSum").valu
-000034e0: 6573 2829 0d0a 2020 2020 2020 2020 2929  es()..        ))
-000034f0: 0d0a 0d0a 2020 2020 2020 2020 6375 7274  ....        curt
-00003500: 6169 6c6d 656e 7420 3d20 2870 6631 5f70  ailment = (pf1_p
-00003510: 6f77 6572 202d 2063 6f6e 7472 6f6c 5f6d  ower - control_m
-00003520: 6f64 655f 706f 7765 7229 202f 2070 6631  ode_power) / pf1
-00003530: 5f70 6f77 6572 202a 2031 3030 0d0a 2020  _power * 100..  
-00003540: 2020 2020 2020 6461 7461 203d 207b 2263        data = {"c
-00003550: 7572 7461 696c 6d65 6e74 223a 2063 7572  urtailment": cur
-00003560: 7461 696c 6d65 6e74 7d0d 0a20 2020 2020  tailment}..     
-00003570: 2020 2073 656c 662e 5f65 7870 6f72 745f     self._export_
-00003580: 6a73 6f6e 5f72 6570 6f72 7428 6461 7461  json_report(data
-00003590: 2c20 6f75 7470 7574 5f64 6972 2c20 7365  , output_dir, se
-000035a0: 6c66 2e54 4f54 414c 5f46 494c 454e 414d  lf.TOTAL_FILENAM
-000035b0: 4529 0d0a 0d0a 2020 2020 6465 6620 6765  E)....    def ge
-000035c0: 6e65 7261 7465 2873 656c 662c 206f 7574  nerate(self, out
-000035d0: 7075 745f 6469 7229 3a0d 0a20 2020 2020  put_dir):..     
-000035e0: 2020 2069 6620 6e6f 7420 7365 6c66 2e5f     if not self._
-000035f0: 6861 735f 7076 5f73 7973 7465 6d73 2829  has_pv_systems()
-00003600: 3a0d 0a20 2020 2020 2020 2020 2020 2072  :..            r
-00003610: 6574 7572 6e0d 0a0d 0a20 2020 2020 2020  eturn....       
-00003620: 2067 7261 6e75 6c61 7269 7479 203d 2052   granularity = R
-00003630: 6570 6f72 7447 7261 6e75 6c61 7269 7479  eportGranularity
-00003640: 2873 656c 662e 5f73 6574 7469 6e67 732e  (self._settings.
-00003650: 7265 706f 7274 732e 6772 616e 756c 6172  reports.granular
-00003660: 6974 7929 0d0a 2020 2020 2020 2020 6966  ity)..        if
-00003670: 2067 7261 6e75 6c61 7269 7479 203d 3d20   granularity == 
-00003680: 5265 706f 7274 4772 616e 756c 6172 6974  ReportGranularit
-00003690: 792e 5045 525f 454c 454d 454e 545f 5045  y.PER_ELEMENT_PE
-000036a0: 525f 5449 4d45 5f50 4f49 4e54 3a0d 0a20  R_TIME_POINT:.. 
-000036b0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-000036c0: 5f67 656e 6572 6174 655f 7065 725f 7076  _generate_per_pv
-000036d0: 5f73 7973 7465 6d5f 7065 725f 7469 6d65  _system_per_time
-000036e0: 5f70 6f69 6e74 286f 7574 7075 745f 6469  _point(output_di
-000036f0: 7229 0d0a 2020 2020 2020 2020 656c 6966  r)..        elif
-00003700: 2067 7261 6e75 6c61 7269 7479 203d 3d20   granularity == 
-00003710: 5265 706f 7274 4772 616e 756c 6172 6974  ReportGranularit
-00003720: 792e 5045 525f 454c 454d 454e 545f 544f  y.PER_ELEMENT_TO
-00003730: 5441 4c3a 0d0a 2020 2020 2020 2020 2020  TAL:..          
-00003740: 2020 7365 6c66 2e5f 6765 6e65 7261 7465    self._generate
-00003750: 5f70 6572 5f70 765f 7379 7374 656d 5f74  _per_pv_system_t
-00003760: 6f74 616c 286f 7574 7075 745f 6469 7229  otal(output_dir)
-00003770: 0d0a 2020 2020 2020 2020 656c 6966 2067  ..        elif g
-00003780: 7261 6e75 6c61 7269 7479 203d 3d20 5265  ranularity == Re
-00003790: 706f 7274 4772 616e 756c 6172 6974 792e  portGranularity.
-000037a0: 414c 4c5f 454c 454d 454e 5453 5f50 4552  ALL_ELEMENTS_PER
-000037b0: 5f54 494d 455f 504f 494e 543a 0d0a 2020  _TIME_POINT:..  
-000037c0: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
-000037d0: 6765 6e65 7261 7465 5f61 6c6c 5f70 765f  generate_all_pv_
-000037e0: 7379 7374 656d 735f 7065 725f 7469 6d65  systems_per_time
-000037f0: 5f70 6f69 6e74 286f 7574 7075 745f 6469  _point(output_di
-00003800: 7229 0d0a 2020 2020 2020 2020 656c 6966  r)..        elif
-00003810: 2067 7261 6e75 6c61 7269 7479 203d 3d20   granularity == 
-00003820: 5265 706f 7274 4772 616e 756c 6172 6974  ReportGranularit
-00003830: 792e 414c 4c5f 454c 454d 454e 5453 5f54  y.ALL_ELEMENTS_T
-00003840: 4f54 414c 3a0d 0a20 2020 2020 2020 2020  OTAL:..         
-00003850: 2020 2073 656c 662e 5f67 656e 6572 6174     self._generat
-00003860: 655f 616c 6c5f 7076 5f73 7973 7465 6d73  e_all_pv_systems
-00003870: 5f74 6f74 616c 286f 7574 7075 745f 6469  _total(output_di
-00003880: 7229 0d0a 2020 2020 2020 2020 656c 7365  r)..        else
-00003890: 3a0d 0a20 2020 2020 2020 2020 2020 2061  :..            a
-000038a0: 7373 6572 7420 4661 6c73 650d 0a0d 0a20  ssert False.... 
-000038b0: 2020 2064 6566 2063 616c 6375 6c61 7465     def calculate
-000038c0: 5f70 765f 6375 7274 6169 6c6d 656e 7428  _pv_curtailment(
-000038d0: 7365 6c66 293a 0d0a 2020 2020 2020 2020  self):..        
-000038e0: 2222 2243 616c 6375 6c61 7465 2050 5620  """Calculate PV 
-000038f0: 6375 7274 6169 6c6d 656e 7420 666f 7220  curtailment for 
-00003900: 616c 6c20 5056 2073 7973 7465 6d73 2e0d  all PV systems..
-00003910: 0a0d 0a20 2020 2020 2020 2052 6574 7572  ...        Retur
-00003920: 6e73 0d0a 2020 2020 2020 2020 2d2d 2d2d  ns..        ----
-00003930: 2d2d 2d0d 0a20 2020 2020 2020 2070 642e  ---..        pd.
-00003940: 4461 7461 4672 616d 650d 0a0d 0a20 2020  DataFrame....   
-00003950: 2020 2020 2022 2222 0d0a 2020 2020 2020       """..      
-00003960: 2020 7066 315f 706f 7765 7220 3d20 7365    pf1_power = se
-00003970: 6c66 2e5f 7066 315f 7363 656e 6172 696f  lf._pf1_scenario
-00003980: 2e67 6574 5f66 756c 6c5f 6461 7461 6672  .get_full_datafr
-00003990: 616d 6528 0d0a 2020 2020 2020 2020 2020  ame(..          
-000039a0: 2020 2250 5653 7973 7465 6d73 222c 2022    "PVSystems", "
-000039b0: 506f 7765 7273 222c 2072 6561 6c5f 6f6e  Powers", real_on
-000039c0: 6c79 3d54 7275 650d 0a20 2020 2020 2020  ly=True..       
-000039d0: 2029 0d0a 2020 2020 2020 2020 636f 6e74   )..        cont
-000039e0: 726f 6c5f 6d6f 6465 5f70 6f77 6572 203d  rol_mode_power =
-000039f0: 2073 656c 662e 5f63 6f6e 7472 6f6c 5f6d   self._control_m
-00003a00: 6f64 655f 7363 656e 6172 696f 2e67 6574  ode_scenario.get
-00003a10: 5f66 756c 6c5f 6461 7461 6672 616d 6528  _full_dataframe(
-00003a20: 0d0a 2020 2020 2020 2020 2020 2020 2250  ..            "P
-00003a30: 5653 7973 7465 6d73 222c 2022 506f 7765  VSystems", "Powe
-00003a40: 7273 222c 2072 6561 6c5f 6f6e 6c79 3d54  rs", real_only=T
-00003a50: 7275 650d 0a20 2020 2020 2020 2029 0d0a  rue..        )..
-00003a60: 2020 2020 2020 2020 7265 7475 726e 2028          return (
-00003a70: 7066 315f 706f 7765 7220 2d20 636f 6e74  pf1_power - cont
-00003a80: 726f 6c5f 6d6f 6465 5f70 6f77 6572 2920  rol_mode_power) 
-00003a90: 2f20 7066 315f 706f 7765 7220 2a20 3130  / pf1_power * 10
-00003aa0: 300d 0a                                  0..
+00000000: 0a69 6d70 6f72 7420 6d61 7468 0a69 6d70  .import math.imp
+00000010: 6f72 7420 6162 630a 696d 706f 7274 206f  ort abc.import o
+00000020: 730a 0a66 726f 6d20 6c6f 6775 7275 2069  s..from loguru i
+00000030: 6d70 6f72 7420 6c6f 6767 6572 0a69 6d70  mport logger.imp
+00000040: 6f72 7420 7061 6e64 6173 2061 7320 7064  ort pandas as pd
+00000050: 0a69 6d70 6f72 7420 6e75 6d70 7920 6173  .import numpy as
+00000060: 206e 700a 0a66 726f 6d20 7079 6473 732e   np..from pydss.
+00000070: 636f 6d6d 6f6e 2069 6d70 6f72 7420 5056  common import PV
+00000080: 5f4c 4f41 445f 5348 4150 455f 4649 4c45  _LOAD_SHAPE_FILE
+00000090: 4e41 4d45 0a66 726f 6d20 7079 6473 732e  NAME.from pydss.
+000000a0: 7265 706f 7274 732e 7265 706f 7274 7320  reports.reports 
+000000b0: 696d 706f 7274 2052 6570 6f72 7442 6173  import ReportBas
+000000c0: 652c 2052 6570 6f72 7447 7261 6e75 6c61  e, ReportGranula
+000000d0: 7269 7479 0a66 726f 6d20 7079 6473 732e  rity.from pydss.
+000000e0: 7574 696c 732e 6461 7461 6672 616d 655f  utils.dataframe_
+000000f0: 7574 696c 7320 696d 706f 7274 2072 6561  utils import rea
+00000100: 645f 6461 7461 6672 616d 652c 2077 7269  d_dataframe, wri
+00000110: 7465 5f64 6174 6166 7261 6d65 0a66 726f  te_dataframe.fro
+00000120: 6d20 7079 6473 732e 7574 696c 732e 7574  m pydss.utils.ut
+00000130: 696c 7320 696d 706f 7274 2064 756d 705f  ils import dump_
+00000140: 6461 7461 0a0a 5046 315f 5343 454e 4152  data..PF1_SCENAR
+00000150: 494f 203d 2022 7066 3122 0a43 4f4e 5452  IO = "pf1".CONTR
+00000160: 4f4c 5f4d 4f44 455f 5343 454e 4152 494f  OL_MODE_SCENARIO
+00000170: 203d 2022 636f 6e74 726f 6c5f 6d6f 6465   = "control_mode
+00000180: 220a 0a63 6c61 7373 2050 7652 6570 6f72  "..class PvRepor
+00000190: 7442 6173 6528 5265 706f 7274 4261 7365  tBase(ReportBase
+000001a0: 2c20 6162 632e 4142 4329 3a0a 2020 2020  , abc.ABC):.    
+000001b0: 2222 2242 6173 6520 636c 6173 7320 666f  """Base class fo
+000001c0: 7220 5056 2072 6570 6f72 7473 2222 220a  r PV reports""".
+000001d0: 2020 2020 6465 6620 5f5f 696e 6974 5f5f      def __init__
+000001e0: 2873 656c 662c 206e 616d 652c 2072 6573  (self, name, res
+000001f0: 756c 7473 2c20 7369 6d75 6c61 7469 6f6e  ults, simulation
+00000200: 5f63 6f6e 6669 6729 3a0a 2020 2020 2020  _config):.      
+00000210: 2020 7375 7065 7228 292e 5f5f 696e 6974    super().__init
+00000220: 5f5f 286e 616d 652c 2072 6573 756c 7473  __(name, results
+00000230: 2c20 7369 6d75 6c61 7469 6f6e 5f63 6f6e  , simulation_con
+00000240: 6669 6729 0a20 2020 2020 2020 2061 7373  fig).        ass
+00000250: 6572 7420 6c65 6e28 7265 7375 6c74 732e  ert len(results.
+00000260: 7363 656e 6172 696f 7329 203d 3d20 320a  scenarios) == 2.
+00000270: 2020 2020 2020 2020 7365 6c66 2e5f 636f          self._co
+00000280: 6e74 726f 6c5f 6d6f 6465 5f73 6365 6e61  ntrol_mode_scena
+00000290: 7269 6f20 3d20 7265 7375 6c74 732e 7363  rio = results.sc
+000002a0: 656e 6172 696f 735b 305d 0a20 2020 2020  enarios[0].     
+000002b0: 2020 2061 7373 6572 7420 7365 6c66 2e5f     assert self._
+000002c0: 636f 6e74 726f 6c5f 6d6f 6465 5f73 6365  control_mode_sce
+000002d0: 6e61 7269 6f2e 6e61 6d65 203d 3d20 2263  nario.name == "c
+000002e0: 6f6e 7472 6f6c 5f6d 6f64 6522 0a20 2020  ontrol_mode".   
+000002f0: 2020 2020 2073 656c 662e 5f70 6631 5f73       self._pf1_s
+00000300: 6365 6e61 7269 6f20 3d20 7265 7375 6c74  cenario = result
+00000310: 732e 7363 656e 6172 696f 735b 315d 0a20  s.scenarios[1]. 
+00000320: 2020 2020 2020 2063 6d5f 7072 6f66 696c         cm_profil
+00000330: 6573 203d 2073 656c 662e 5f63 6f6e 7472  es = self._contr
+00000340: 6f6c 5f6d 6f64 655f 7363 656e 6172 696f  ol_mode_scenario
+00000350: 2e72 6561 645f 7076 5f70 726f 6669 6c65  .read_pv_profile
+00000360: 7328 290a 2020 2020 2020 2020 6966 206e  s().        if n
+00000370: 6f74 2063 6d5f 7072 6f66 696c 6573 3a0a  ot cm_profiles:.
+00000380: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00000390: 2e5f 7076 5f73 7973 7465 6d5f 6e61 6d65  ._pv_system_name
+000003a0: 7320 3d20 5b5d 0a20 2020 2020 2020 2020  s = [].         
+000003b0: 2020 2072 6574 7572 6e0a 0a20 2020 2020     return..     
+000003c0: 2020 2073 656c 662e 5f70 765f 7379 7374     self._pv_syst
+000003d0: 656d 5f6e 616d 6573 203d 205b 785b 226e  em_names = [x["n
+000003e0: 616d 6522 5d20 666f 7220 7820 696e 2063  ame"] for x in c
+000003f0: 6d5f 7072 6f66 696c 6573 5b22 7076 5f73  m_profiles["pv_s
+00000400: 7973 7465 6d73 225d 5d0a 0a20 2020 2020  ystems"]]..     
+00000410: 2020 2073 656c 662e 5f70 6631 5f70 765f     self._pf1_pv_
+00000420: 7379 7374 656d 7320 3d20 7b0a 2020 2020  systems = {.    
+00000430: 2020 2020 2020 2020 785b 226e 616d 6522          x["name"
+00000440: 5d3a 2078 2066 6f72 2078 2069 6e20 7365  ]: x for x in se
+00000450: 6c66 2e5f 7066 315f 7363 656e 6172 696f  lf._pf1_scenario
+00000460: 2e72 6561 645f 7076 5f70 726f 6669 6c65  .read_pv_profile
+00000470: 7328 295b 2270 765f 7379 7374 656d 7322  s()["pv_systems"
+00000480: 5d0a 2020 2020 2020 2020 7d0a 2020 2020  ].        }.    
+00000490: 2020 2020 7365 6c66 2e5f 636f 6e74 726f      self._contro
+000004a0: 6c5f 6d6f 6465 5f70 765f 7379 7374 656d  l_mode_pv_system
+000004b0: 7320 3d20 7b0a 2020 2020 2020 2020 2020  s = {.          
+000004c0: 2020 785b 226e 616d 6522 5d3a 2078 2066    x["name"]: x f
+000004d0: 6f72 2078 2069 6e20 636d 5f70 726f 6669  or x in cm_profi
+000004e0: 6c65 735b 2270 765f 7379 7374 656d 7322  les["pv_systems"
+000004f0: 5d0a 2020 2020 2020 2020 7d0a 0a20 2020  ].        }..   
+00000500: 2064 6566 205f 6765 745f 7076 5f73 7973   def _get_pv_sys
+00000510: 7465 6d5f 696e 666f 2873 656c 662c 2070  tem_info(self, p
+00000520: 765f 7379 7374 656d 2c20 7363 656e 6172  v_system, scenar
+00000530: 696f 293a 0a20 2020 2020 2020 2069 6620  io):.        if 
+00000540: 7363 656e 6172 696f 203d 3d20 5046 315f  scenario == PF1_
+00000550: 5343 454e 4152 494f 3a0a 2020 2020 2020  SCENARIO:.      
+00000560: 2020 2020 2020 7076 5f73 7973 7465 6d73        pv_systems
+00000570: 203d 2073 656c 662e 5f70 6631 5f70 765f   = self._pf1_pv_
+00000580: 7379 7374 656d 730a 2020 2020 2020 2020  systems.        
+00000590: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+000005a0: 2020 7076 5f73 7973 7465 6d73 203d 2073    pv_systems = s
+000005b0: 656c 662e 5f63 6f6e 7472 6f6c 5f6d 6f64  elf._control_mod
+000005c0: 655f 7076 5f73 7973 7465 6d73 0a0a 2020  e_pv_systems..  
+000005d0: 2020 2020 2020 7265 7475 726e 2070 765f        return pv_
+000005e0: 7379 7374 656d 735b 7076 5f73 7973 7465  systems[pv_syste
+000005f0: 6d5d 0a0a 2020 2020 6465 6620 5f68 6173  m]..    def _has
+00000600: 5f70 765f 7379 7374 656d 7328 7365 6c66  _pv_systems(self
+00000610: 293a 0a20 2020 2020 2020 2072 6574 7572  ):.        retur
+00000620: 6e20 6c65 6e28 7365 6c66 2e5f 7076 5f73  n len(self._pv_s
+00000630: 7973 7465 6d5f 6e61 6d65 7329 203e 2030  ystem_names) > 0
+00000640: 0a0a 2020 2020 4073 7461 7469 636d 6574  ..    @staticmet
+00000650: 686f 640a 2020 2020 6465 6620 6765 745f  hod.    def get_
+00000660: 7265 7175 6972 6564 5f65 7870 6f72 7473  required_exports
+00000670: 2873 6574 7469 6e67 7329 3a0a 2020 2020  (settings):.    
+00000680: 2020 2020 6772 616e 756c 6172 6974 7920      granularity 
+00000690: 3d20 5265 706f 7274 4772 616e 756c 6172  = ReportGranular
+000006a0: 6974 7928 7365 7474 696e 6773 2e72 6570  ity(settings.rep
+000006b0: 6f72 7473 2e67 7261 6e75 6c61 7269 7479  orts.granularity
+000006c0: 290a 2020 2020 2020 2020 5f74 7970 652c  ).        _type,
+000006d0: 2073 756d 5f65 6c65 6d65 6e74 7320 3d20   sum_elements = 
+000006e0: 5265 706f 7274 4261 7365 2e5f 7061 7261  ReportBase._para
+000006f0: 6d73 5f66 726f 6d5f 6772 616e 756c 6172  ms_from_granular
+00000700: 6974 7928 6772 616e 756c 6172 6974 7929  ity(granularity)
+00000710: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
+00000720: 7b0a 2020 2020 2020 2020 2020 2020 2250  {.            "P
+00000730: 5653 7973 7465 6d73 223a 205b 0a20 2020  VSystems": [.   
+00000740: 2020 2020 2020 2020 2020 2020 207b 0a20               {. 
+00000750: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000760: 2020 2022 7072 6f70 6572 7479 223a 2022     "property": "
+00000770: 506f 7765 7273 222c 0a20 2020 2020 2020  Powers",.       
+00000780: 2020 2020 2020 2020 2020 2020 2022 7374               "st
+00000790: 6f72 655f 7661 6c75 6573 5f74 7970 6522  ore_values_type"
+000007a0: 3a20 5f74 7970 652c 0a20 2020 2020 2020  : _type,.       
+000007b0: 2020 2020 2020 2020 2020 2020 2022 7375               "su
+000007c0: 6d5f 656c 656d 656e 7473 223a 2073 756d  m_elements": sum
+000007d0: 5f65 6c65 6d65 6e74 732c 0a20 2020 2020  _elements,.     
+000007e0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+000007f0: 6461 7461 5f63 6f6e 7665 7273 696f 6e22  data_conversion"
+00000800: 3a20 2273 756d 5f61 6273 5f72 6561 6c22  : "sum_abs_real"
+00000810: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00000820: 2020 7d2c 0a20 2020 2020 2020 2020 2020    },.           
+00000830: 205d 2c0a 2020 2020 2020 2020 7d0a 0a20   ],.        }.. 
+00000840: 2020 2040 7374 6174 6963 6d65 7468 6f64     @staticmethod
+00000850: 0a20 2020 2064 6566 2067 6574 5f72 6571  .    def get_req
+00000860: 7569 7265 645f 7363 656e 6172 696f 5f6e  uired_scenario_n
+00000870: 616d 6573 2829 3a0a 2020 2020 2020 2020  ames():.        
+00000880: 7265 7475 726e 2073 6574 285b 2270 6631  return set(["pf1
+00000890: 222c 2022 636f 6e74 726f 6c5f 6d6f 6465  ", "control_mode
+000008a0: 225d 290a 0a20 2020 2040 7374 6174 6963  "])..    @static
+000008b0: 6d65 7468 6f64 0a20 2020 2064 6566 2073  method.    def s
+000008c0: 6574 5f72 6571 7569 7265 645f 7072 6f6a  et_required_proj
+000008d0: 6563 745f 7365 7474 696e 6773 2873 6574  ect_settings(set
+000008e0: 7469 6e67 7329 3a0a 2020 2020 2020 2020  tings):.        
+000008f0: 6966 206e 6f74 2073 6574 7469 6e67 732e  if not settings.
+00000900: 6578 706f 7274 732e 6578 706f 7274 5f70  exports.export_p
+00000910: 765f 7072 6f66 696c 6573 3a0a 2020 2020  v_profiles:.    
+00000920: 2020 2020 2020 2020 7365 7474 696e 6773          settings
+00000930: 2e65 7870 6f72 7473 2e65 7870 6f72 745f  .exports.export_
+00000940: 7076 5f70 726f 6669 6c65 7320 3d20 5472  pv_profiles = Tr
+00000950: 7565 0a20 2020 2020 2020 2020 2020 206c  ue.            l
+00000960: 6f67 6765 722e 696e 666f 2822 456e 6162  ogger.info("Enab
+00000970: 6c65 6420 4578 706f 7274 2050 5620 5072  led Export PV Pr
+00000980: 6f66 696c 6573 2229 0a0a 0a63 6c61 7373  ofiles")...class
+00000990: 2050 7643 6c69 7070 696e 6752 6570 6f72   PvClippingRepor
+000009a0: 7428 5076 5265 706f 7274 4261 7365 293a  t(PvReportBase):
+000009b0: 0a20 2020 2022 2222 5265 706f 7274 7320  .    """Reports 
+000009c0: 5056 2043 6c69 7070 696e 6720 666f 7220  PV Clipping for 
+000009d0: 7468 6520 7369 6d75 6c61 7469 6f6e 2e0a  the simulation..
+000009e0: 0a20 2020 2054 6865 2072 6570 6f72 7420  .    The report 
+000009f0: 6765 6e65 7261 7465 7320 6120 7076 5f63  generates a pv_c
+00000a00: 6c69 7070 696e 6720 6f75 7470 7574 2066  lipping output f
+00000a10: 696c 652e 2054 6865 2066 696c 6520 6578  ile. The file ex
+00000a20: 7465 6e73 696f 6e20 6465 7065 6e64 730a  tension depends.
+00000a30: 2020 2020 6f6e 2074 6865 2069 6e70 7574      on the input
+00000a40: 2070 6172 616d 6574 6572 732e 2049 6620   parameters. If 
+00000a50: 7468 6520 6461 7461 2077 6173 2063 6f6c  the data was col
+00000a60: 6c65 6374 6564 2061 7420 6576 6572 7920  lected at every 
+00000a70: 7469 6d65 2070 6f69 6e74 2074 6865 6e0a  time point then.
+00000a80: 2020 2020 7468 6520 6f75 7470 7574 2066      the output f
+00000a90: 696c 6520 7769 6c6c 2062 6520 2e63 7376  ile will be .csv
+00000aa0: 206f 7220 2e68 352c 2064 6570 656e 6469   or .h5, dependi
+00000ab0: 6e67 206f 6e20 2745 7870 6f72 7420 466f  ng on 'Export Fo
+00000ac0: 726d 6174 2e27 0a20 2020 204f 7468 6572  rmat.'.    Other
+00000ad0: 7769 7365 2c20 7468 6520 6f75 7470 7574  wise, the output
+00000ae0: 2066 696c 6520 7769 6c6c 2062 6520 2e6a   file will be .j
+00000af0: 736f 6e2e 0a0a 2020 2020 544f 444f 3a20  son...    TODO: 
+00000b00: 5468 6973 2069 7320 616e 2065 7870 6572  This is an exper
+00000b10: 696d 656e 7461 6c20 7265 706f 7274 2e20  imental report. 
+00000b20: 4f75 7470 7574 7320 6861 7665 206e 6f74  Outputs have not
+00000b30: 2062 6565 6e20 7661 6c69 6461 7465 642e   been validated.
+00000b40: 0a0a 2020 2020 2222 220a 0a20 2020 2050  ..    """..    P
+00000b50: 4552 5f54 494d 455f 504f 494e 545f 4649  ER_TIME_POINT_FI
+00000b60: 4c45 4e41 4d45 203d 2022 7076 5f63 6c69  LENAME = "pv_cli
+00000b70: 7070 696e 672e 6835 220a 2020 2020 544f  pping.h5".    TO
+00000b80: 5441 4c5f 4649 4c45 4e41 4d45 203d 2022  TAL_FILENAME = "
+00000b90: 7076 5f63 6c69 7070 696e 672e 6a73 6f6e  pv_clipping.json
+00000ba0: 220a 2020 2020 4e41 4d45 203d 2022 5056  ".    NAME = "PV
+00000bb0: 2043 6c69 7070 696e 6722 0a0a 2020 2020   Clipping"..    
+00000bc0: 6465 6620 5f5f 696e 6974 5f5f 2873 656c  def __init__(sel
+00000bd0: 662c 206e 616d 652c 2072 6573 756c 7473  f, name, results
+00000be0: 2c20 7369 6d75 6c61 7469 6f6e 5f63 6f6e  , simulation_con
+00000bf0: 6669 6729 3a0a 2020 2020 2020 2020 7375  fig):.        su
+00000c00: 7065 7228 292e 5f5f 696e 6974 5f5f 286e  per().__init__(n
+00000c10: 616d 652c 2072 6573 756c 7473 2c20 7369  ame, results, si
+00000c20: 6d75 6c61 7469 6f6e 5f63 6f6e 6669 6729  mulation_config)
+00000c30: 0a20 2020 2020 2020 2069 6620 6e6f 7420  .        if not 
+00000c40: 7365 6c66 2e5f 6861 735f 7076 5f73 7973  self._has_pv_sys
+00000c50: 7465 6d73 2829 3a0a 2020 2020 2020 2020  tems():.        
+00000c60: 2020 2020 7265 7475 726e 0a0a 2020 2020      return..    
+00000c70: 2020 2020 6469 6666 5f74 6f6c 6572 616e      diff_toleran
+00000c80: 6365 203d 2073 656c 662e 5f72 6570 6f72  ce = self._repor
+00000c90: 745f 7365 7474 696e 6773 2e64 6966 665f  t_settings.diff_
+00000ca0: 746f 6c65 7261 6e63 655f 7065 7263 656e  tolerance_percen
+00000cb0: 745f 706d 7070 202a 202e 3031 0a20 2020  t_pmpp * .01.   
+00000cc0: 2020 2020 2064 656e 6f6d 696e 6174 6f72       denominator
+00000cd0: 5f74 6f6c 6572 616e 6365 203d 2073 656c  _tolerance = sel
+00000ce0: 662e 5f72 6570 6f72 745f 7365 7474 696e  f._report_settin
+00000cf0: 6773 2e64 656e 6f6d 696e 6174 6f72 5f74  gs.denominator_t
+00000d00: 6f6c 6572 616e 6365 5f70 6572 6365 6e74  olerance_percent
+00000d10: 5f70 6d70 7020 2a20 2e30 310a 2020 2020  _pmpp * .01.    
+00000d20: 2020 2020 6c6f 6767 6572 2e64 6562 7567      logger.debug
+00000d30: 2822 746f 6c65 7261 6e63 6573 3a20 6469  ("tolerances: di
+00000d40: 6666 3d25 7320 6465 6e6f 6d69 6e61 746f  ff=%s denominato
+00000d50: 723d 2573 222c 2064 6966 665f 746f 6c65  r=%s", diff_tole
+00000d60: 7261 6e63 652c 2064 656e 6f6d 696e 6174  rance, denominat
+00000d70: 6f72 5f74 6f6c 6572 616e 6365 290a 2020  or_tolerance).  
+00000d80: 2020 2020 2020 7365 6c66 2e5f 6469 6666        self._diff
+00000d90: 5f74 6f6c 6572 616e 6365 7320 3d20 7b7d  _tolerances = {}
+00000da0: 0a20 2020 2020 2020 2073 656c 662e 5f64  .        self._d
+00000db0: 656e 6f6d 696e 6174 6f72 5f74 6f6c 6572  enominator_toler
+00000dc0: 616e 6365 7320 3d20 7b7d 0a20 2020 2020  ances = {}.     
+00000dd0: 2020 2066 6f72 2070 765f 7379 7374 656d     for pv_system
+00000de0: 2069 6e20 7365 6c66 2e5f 7066 315f 7363   in self._pf1_sc
+00000df0: 656e 6172 696f 2e72 6561 645f 7076 5f70  enario.read_pv_p
+00000e00: 726f 6669 6c65 7328 295b 2270 765f 7379  rofiles()["pv_sy
+00000e10: 7374 656d 7322 5d3a 0a20 2020 2020 2020  stems"]:.       
+00000e20: 2020 2020 2073 656c 662e 5f64 6966 665f       self._diff_
+00000e30: 746f 6c65 7261 6e63 6573 5b70 765f 7379  tolerances[pv_sy
+00000e40: 7374 656d 5b22 6e61 6d65 225d 5d20 3d20  stem["name"]] = 
+00000e50: 7076 5f73 7973 7465 6d5b 2270 6d70 7022  pv_system["pmpp"
+00000e60: 5d20 2a20 6469 6666 5f74 6f6c 6572 616e  ] * diff_toleran
+00000e70: 6365 0a20 2020 2020 2020 2020 2020 2073  ce.            s
+00000e80: 656c 662e 5f64 656e 6f6d 696e 6174 6f72  elf._denominator
+00000e90: 5f74 6f6c 6572 616e 6365 735b 7076 5f73  _tolerances[pv_s
+00000ea0: 7973 7465 6d5b 226e 616d 6522 5d5d 203d  ystem["name"]] =
+00000eb0: 2070 765f 7379 7374 656d 5b22 706d 7070   pv_system["pmpp
+00000ec0: 225d 202a 2064 656e 6f6d 696e 6174 6f72  "] * denominator
+00000ed0: 5f74 6f6c 6572 616e 6365 0a0a 2020 2020  _tolerance..    
+00000ee0: 4073 7461 7469 636d 6574 686f 640a 2020  @staticmethod.  
+00000ef0: 2020 6465 6620 5f63 616c 6375 6c61 7465    def _calculate
+00000f00: 5f63 6c69 7070 696e 6728 746f 7461 6c5f  _clipping(total_
+00000f10: 6463 5f70 6f77 6572 2c20 7066 315f 7265  dc_power, pf1_re
+00000f20: 616c 5f70 6f77 6572 293a 0a20 2020 2020  al_power):.     
+00000f30: 2020 2072 6574 7572 6e20 2874 6f74 616c     return (total
+00000f40: 5f64 635f 706f 7765 7220 2d20 7066 315f  _dc_power - pf1_
+00000f50: 7265 616c 5f70 6f77 6572 2920 2a20 3130  real_power) * 10
+00000f60: 3020 2f20 7066 315f 7265 616c 5f70 6f77  0 / pf1_real_pow
+00000f70: 6572 0a0a 2020 2020 4073 7461 7469 636d  er..    @staticm
+00000f80: 6574 686f 640a 2020 2020 6465 6620 5f63  ethod.    def _c
+00000f90: 616c 6375 6c61 7465 5f63 6c69 7070 696e  alculate_clippin
+00000fa0: 675f 6172 7261 7928 6463 5f70 6f77 6572  g_array(dc_power
+00000fb0: 2c20 7066 315f 7265 616c 5f70 6f77 6572  , pf1_real_power
+00000fc0: 293a 0a20 2020 2020 2020 2064 6370 203d  ):.        dcp =
+00000fd0: 2064 635f 706f 7765 722e 7661 6c75 6573   dc_power.values
+00000fe0: 0a20 2020 2020 2020 2072 7020 3d20 7066  .        rp = pf
+00000ff0: 315f 7265 616c 5f70 6f77 6572 2e76 616c  1_real_power.val
+00001000: 7565 730a 2020 2020 2020 2020 7270 203d  ues.        rp =
+00001010: 206e 702e 7768 6572 6528 7270 3d3d 302c   np.where(rp==0,
+00001020: 206e 702e 6e61 6e2c 2072 7029 0a20 2020   np.nan, rp).   
+00001030: 2020 2020 2072 6574 7572 6e20 2864 6370       return (dcp
+00001040: 202d 2072 7029 202f 2072 7020 2a20 3130   - rp) / rp * 10
+00001050: 300a 0a20 2020 2064 6566 205f 6765 745f  0..    def _get_
+00001060: 746f 7461 6c5f 6463 5f70 6f77 6572 5f61  total_dc_power_a
+00001070: 6372 6f73 735f 7076 5f73 7973 7465 6d73  cross_pv_systems
+00001080: 2873 656c 6629 3a0a 2020 2020 2020 2020  (self):.        
+00001090: 746f 7461 6c5f 6463 5f70 6f77 6572 203d  total_dc_power =
+000010a0: 2030 2e30 0a20 2020 2020 2020 2066 6f72   0.0.        for
+000010b0: 206e 616d 6520 696e 2073 656c 662e 5f70   name in self._p
+000010c0: 765f 7379 7374 656d 5f6e 616d 6573 3a0a  v_system_names:.
+000010d0: 2020 2020 2020 2020 2020 2020 746f 7461              tota
+000010e0: 6c5f 6463 5f70 6f77 6572 202b 3d20 7365  l_dc_power += se
+000010f0: 6c66 2e5f 6765 745f 746f 7461 6c5f 6463  lf._get_total_dc
+00001100: 5f70 6f77 6572 286e 616d 6529 0a20 2020  _power(name).   
+00001110: 2020 2020 2072 6574 7572 6e20 746f 7461       return tota
+00001120: 6c5f 6463 5f70 6f77 6572 0a0a 2020 2020  l_dc_power..    
+00001130: 6465 6620 5f67 6574 5f74 6f74 616c 5f64  def _get_total_d
+00001140: 635f 706f 7765 7228 7365 6c66 2c20 7076  c_power(self, pv
+00001150: 5f73 7973 7465 6d29 3a0a 2020 2020 2020  _system):.      
+00001160: 2020 636d 5f69 6e66 6f20 3d20 7365 6c66    cm_info = self
+00001170: 2e5f 6765 745f 7076 5f73 7973 7465 6d5f  ._get_pv_system_
+00001180: 696e 666f 2870 765f 7379 7374 656d 2c20  info(pv_system, 
+00001190: 434f 4e54 524f 4c5f 4d4f 4445 5f53 4345  CONTROL_MODE_SCE
+000011a0: 4e41 5249 4f29 0a20 2020 2020 2020 2070  NARIO).        p
+000011b0: 6d70 7020 3d20 636d 5f69 6e66 6f5b 2270  mpp = cm_info["p
+000011c0: 6d70 7022 5d0a 2020 2020 2020 2020 6972  mpp"].        ir
+000011d0: 7261 6469 616e 6365 203d 2063 6d5f 696e  radiance = cm_in
+000011e0: 666f 5b22 6972 7261 6469 616e 6365 225d  fo["irradiance"]
+000011f0: 0a20 2020 2020 2020 2074 6f74 616c 5f69  .        total_i
+00001200: 7272 6164 6961 6e63 6520 3d20 636d 5f69  rradiance = cm_i
+00001210: 6e66 6f5b 226c 6f61 645f 7368 6170 655f  nfo["load_shape_
+00001220: 706d 756c 745f 7375 6d22 5d0a 2020 2020  pmult_sum"].    
+00001230: 2020 2020 7265 7475 726e 2070 6d70 7020      return pmpp 
+00001240: 2a20 6972 7261 6469 616e 6365 202a 2074  * irradiance * t
+00001250: 6f74 616c 5f69 7272 6164 6961 6e63 650a  otal_irradiance.
+00001260: 0a20 2020 2064 6566 205f 6765 6e65 7261  .    def _genera
+00001270: 7465 5f70 6572 5f70 765f 7379 7374 656d  te_per_pv_system
+00001280: 5f70 6572 5f74 696d 655f 706f 696e 7428  _per_time_point(
+00001290: 7365 6c66 2c20 6f75 7470 7574 5f64 6972  self, output_dir
+000012a0: 293a 0a20 2020 2020 2020 2070 765f 6c6f  ):.        pv_lo
+000012b0: 6164 5f73 6861 7065 7320 3d20 7365 6c66  ad_shapes = self
+000012c0: 2e5f 7265 6164 5f70 765f 6c6f 6164 5f73  ._read_pv_load_s
+000012d0: 6861 7065 7328 290a 2020 2020 2020 2020  hapes().        
+000012e0: 7066 315f 7265 616c 5f70 6f77 6572 5f66  pf1_real_power_f
+000012f0: 756c 6c20 3d20 7365 6c66 2e5f 7066 315f  ull = self._pf1_
+00001300: 7363 656e 6172 696f 2e67 6574 5f66 756c  scenario.get_ful
+00001310: 6c5f 6461 7461 6672 616d 6528 0a20 2020  l_dataframe(.   
+00001320: 2020 2020 2020 2020 2022 5056 5379 7374           "PVSyst
+00001330: 656d 7322 2c20 2250 6f77 6572 7322 0a20  ems", "Powers". 
+00001340: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
+00001350: 206e 616d 6520 3d20 4e6f 6e65 0a0a 2020   name = None..  
+00001360: 2020 2020 2020 2320 544f 444f 3a20 4170        # TODO: Ap
+00001370: 706c 7920 746f 6c65 7261 6e63 6573 2074  ply tolerances t
+00001380: 6f20 6f74 6865 7220 6772 616e 756c 6172  o other granular
+00001390: 6974 7920 6f70 7469 6f6e 732e 0a20 2020  ity options..   
+000013a0: 2020 2020 2064 6566 2063 616c 635f 636c       def calc_cl
+000013b0: 6970 7069 6e67 2864 6370 2c20 7270 293a  ipping(dcp, rp):
+000013c0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+000013d0: 6463 7020 3c20 7365 6c66 2e5f 6465 6e6f  dcp < self._deno
+000013e0: 6d69 6e61 746f 725f 746f 6c65 7261 6e63  minator_toleranc
+000013f0: 6573 5b6e 616d 655d 3a0a 2020 2020 2020  es[name]:.      
+00001400: 2020 2020 2020 2020 2020 7265 7475 726e            return
+00001410: 2030 0a20 2020 2020 2020 2020 2020 2064   0.            d
+00001420: 6966 6620 3d20 6463 7020 2d20 7270 0a20  iff = dcp - rp. 
+00001430: 2020 2020 2020 2020 2020 2069 6620 6469             if di
+00001440: 6666 203c 2030 2061 6e64 2061 6273 2864  ff < 0 and abs(d
+00001450: 6966 6629 203c 2073 656c 662e 5f64 6966  iff) < self._dif
+00001460: 665f 746f 6c65 7261 6e63 6573 5b6e 616d  f_tolerances[nam
+00001470: 655d 3a0a 2020 2020 2020 2020 2020 2020  e]:.            
+00001480: 2020 2020 7265 7475 726e 2030 0a20 2020      return 0.   
+00001490: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
+000014a0: 2864 6370 202d 2072 7029 202f 2072 7020  (dcp - rp) / rp 
+000014b0: 2a20 3130 300a 0a20 2020 2020 2020 2064  * 100..        d
+000014c0: 6174 6120 3d20 7b7d 0a20 2020 2020 2020  ata = {}.       
+000014d0: 2066 6f72 205f 6e61 6d65 2069 6e20 7365   for _name in se
+000014e0: 6c66 2e5f 7076 5f73 7973 7465 6d5f 6e61  lf._pv_system_na
+000014f0: 6d65 733a 0a20 2020 2020 2020 2020 2020  mes:.           
+00001500: 206e 616d 6520 3d20 5f6e 616d 650a 2020   name = _name.  
+00001510: 2020 2020 2020 2020 2020 636d 5f69 6e66            cm_inf
+00001520: 6f20 3d20 7365 6c66 2e5f 6765 745f 7076  o = self._get_pv
+00001530: 5f73 7973 7465 6d5f 696e 666f 286e 616d  _system_info(nam
+00001540: 652c 2043 4f4e 5452 4f4c 5f4d 4f44 455f  e, CONTROL_MODE_
+00001550: 5343 454e 4152 494f 290a 2020 2020 2020  SCENARIO).      
+00001560: 2020 2020 2020 7066 315f 7265 616c 5f70        pf1_real_p
+00001570: 6f77 6572 203d 2070 6631 5f72 6561 6c5f  ower = pf1_real_
+00001580: 706f 7765 725f 6675 6c6c 5b6e 616d 6520  power_full[name 
+00001590: 2b20 225f 5f50 6f77 6572 7322 5d0a 2020  + "__Powers"].  
+000015a0: 2020 2020 2020 2020 2020 6463 5f70 6f77            dc_pow
+000015b0: 6572 203d 2070 765f 6c6f 6164 5f73 6861  er = pv_load_sha
+000015c0: 7065 735b 636d 5f69 6e66 6f5b 226c 6f61  pes[cm_info["loa
+000015d0: 645f 7368 6170 655f 7072 6f66 696c 6522  d_shape_profile"
+000015e0: 5d5d 202a 205c 0a20 2020 2020 2020 2020  ]] * \.         
+000015f0: 2020 2020 2020 2063 6d5f 696e 666f 5b22         cm_info["
+00001600: 706d 7070 225d 202a 205c 0a20 2020 2020  pmpp"] * \.     
+00001610: 2020 2020 2020 2020 2020 2063 6d5f 696e             cm_in
+00001620: 666f 5b22 6972 7261 6469 616e 6365 225d  fo["irradiance"]
+00001630: 0a20 2020 2020 2020 2020 2020 2061 7373  .            ass
+00001640: 6572 7420 6c65 6e28 6463 5f70 6f77 6572  ert len(dc_power
+00001650: 2920 3d3d 206c 656e 2870 6631 5f72 6561  ) == len(pf1_rea
+00001660: 6c5f 706f 7765 7229 2c20 5c0a 2020 2020  l_power), \.    
+00001670: 2020 2020 2020 2020 2020 2020 6622 7b6c              f"{l
+00001680: 656e 2864 635f 706f 7765 7229 7d20 7b6c  en(dc_power)} {l
+00001690: 656e 2870 6631 5f72 6561 6c5f 706f 7765  en(pf1_real_powe
+000016a0: 7229 7d22 0a20 2020 2020 2020 2020 2020  r)}".           
+000016b0: 2063 6f6c 203d 206e 616d 6520 2b20 225f   col = name + "_
+000016c0: 5f43 6c69 7070 696e 6722 0a20 2020 2020  _Clipping".     
+000016d0: 2020 2020 2020 2064 6174 615b 636f 6c5d         data[col]
+000016e0: 203d 2064 635f 706f 7765 722e 636f 6d62   = dc_power.comb
+000016f0: 696e 6528 7066 315f 7265 616c 5f70 6f77  ine(pf1_real_pow
+00001700: 6572 2c20 6361 6c63 5f63 6c69 7070 696e  er, calc_clippin
+00001710: 6729 2e76 616c 7565 730a 0a20 2020 2020  g).values..     
+00001720: 2020 2064 6620 3d20 7064 2e44 6174 6146     df = pd.DataF
+00001730: 7261 6d65 2864 6174 612c 2069 6e64 6578  rame(data, index
+00001740: 3d70 6631 5f72 6561 6c5f 706f 7765 725f  =pf1_real_power_
+00001750: 6675 6c6c 2e69 6e64 6578 290a 2020 2020  full.index).    
+00001760: 2020 2020 7365 6c66 2e5f 6578 706f 7274      self._export
+00001770: 5f64 6174 6166 7261 6d65 5f72 6570 6f72  _dataframe_repor
+00001780: 7428 6466 2c20 6f75 7470 7574 5f64 6972  t(df, output_dir
+00001790: 2c20 2270 765f 636c 6970 7069 6e67 2229  , "pv_clipping")
+000017a0: 0a0a 2020 2020 6465 6620 5f67 656e 6572  ..    def _gener
+000017b0: 6174 655f 7065 725f 7076 5f73 7973 7465  ate_per_pv_syste
+000017c0: 6d5f 746f 7461 6c28 7365 6c66 2c20 6f75  m_total(self, ou
+000017d0: 7470 7574 5f64 6972 293a 0a20 2020 2020  tput_dir):.     
+000017e0: 2020 2064 6174 6120 3d20 7b22 7076 5f73     data = {"pv_s
+000017f0: 7973 7465 6d73 223a 205b 5d7d 0a20 2020  ystems": []}.   
+00001800: 2020 2020 2066 6f72 206e 616d 6520 696e       for name in
+00001810: 2073 656c 662e 5f70 765f 7379 7374 656d   self._pv_system
+00001820: 5f6e 616d 6573 3a0a 2020 2020 2020 2020  _names:.        
+00001830: 2020 2020 7066 315f 7265 616c 5f70 6f77      pf1_real_pow
+00001840: 6572 203d 2073 656c 662e 5f70 6631 5f73  er = self._pf1_s
+00001850: 6365 6e61 7269 6f2e 6765 745f 656c 656d  cenario.get_elem
+00001860: 656e 745f 7072 6f70 6572 7479 5f76 616c  ent_property_val
+00001870: 7565 280a 2020 2020 2020 2020 2020 2020  ue(.            
+00001880: 2020 2020 2250 5653 7973 7465 6d73 222c      "PVSystems",
+00001890: 2022 506f 7765 7273 5375 6d22 2c20 6e61   "PowersSum", na
+000018a0: 6d65 0a20 2020 2020 2020 2020 2020 2029  me.            )
+000018b0: 0a20 2020 2020 2020 2020 2020 2064 635f  .            dc_
+000018c0: 706f 7765 7220 3d20 7365 6c66 2e5f 6765  power = self._ge
+000018d0: 745f 746f 7461 6c5f 6463 5f70 6f77 6572  t_total_dc_power
+000018e0: 286e 616d 6529 0a20 2020 2020 2020 2020  (name).         
+000018f0: 2020 2063 6c69 7070 696e 6720 3d20 7365     clipping = se
+00001900: 6c66 2e5f 6361 6c63 756c 6174 655f 636c  lf._calculate_cl
+00001910: 6970 7069 6e67 2864 635f 706f 7765 722c  ipping(dc_power,
+00001920: 2070 6631 5f72 6561 6c5f 706f 7765 7229   pf1_real_power)
+00001930: 0a20 2020 2020 2020 2020 2020 2064 6174  .            dat
+00001940: 615b 2270 765f 7379 7374 656d 7322 5d2e  a["pv_systems"].
+00001950: 6170 7065 6e64 280a 2020 2020 2020 2020  append(.        
+00001960: 2020 2020 2020 2020 7b0a 2020 2020 2020          {.      
+00001970: 2020 2020 2020 2020 2020 2020 2020 226e                "n
+00001980: 616d 6522 3a20 6e61 6d65 2c0a 2020 2020  ame": name,.    
+00001990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000019a0: 2263 6c69 7070 696e 6722 3a20 636c 6970  "clipping": clip
+000019b0: 7069 6e67 2c0a 2020 2020 2020 2020 2020  ping,.          
+000019c0: 2020 2020 2020 7d0a 2020 2020 2020 2020        }.        
+000019d0: 2020 2020 290a 2020 2020 2020 2020 7365      ).        se
+000019e0: 6c66 2e5f 6578 706f 7274 5f6a 736f 6e5f  lf._export_json_
+000019f0: 7265 706f 7274 2864 6174 612c 206f 7574  report(data, out
+00001a00: 7075 745f 6469 722c 2073 656c 662e 544f  put_dir, self.TO
+00001a10: 5441 4c5f 4649 4c45 4e41 4d45 290a 0a20  TAL_FILENAME).. 
+00001a20: 2020 2064 6566 205f 6765 6e65 7261 7465     def _generate
+00001a30: 5f61 6c6c 5f70 765f 7379 7374 656d 735f  _all_pv_systems_
+00001a40: 7065 725f 7469 6d65 5f70 6f69 6e74 2873  per_time_point(s
+00001a50: 656c 662c 206f 7574 7075 745f 6469 7229  elf, output_dir)
+00001a60: 3a0a 2020 2020 2020 2020 7066 315f 7265  :.        pf1_re
+00001a70: 616c 5f70 6f77 6572 203d 2073 656c 662e  al_power = self.
+00001a80: 5f70 6631 5f73 6365 6e61 7269 6f2e 6765  _pf1_scenario.ge
+00001a90: 745f 7375 6d6d 6564 5f65 6c65 6d65 6e74  t_summed_element
+00001aa0: 5f64 6174 6166 7261 6d65 280a 2020 2020  _dataframe(.    
+00001ab0: 2020 2020 2020 2020 2250 5653 7973 7465          "PVSyste
+00001ac0: 6d73 222c 2022 506f 7765 7273 220a 2020  ms", "Powers".  
+00001ad0: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
+00001ae0: 7076 5f6c 6f61 645f 7368 6170 6573 203d  pv_load_shapes =
+00001af0: 2073 656c 662e 5f72 6561 645f 7076 5f6c   self._read_pv_l
+00001b00: 6f61 645f 7368 6170 6573 2829 0a20 2020  oad_shapes().   
+00001b10: 2020 2020 2064 635f 706f 7765 7273 203d       dc_powers =
+00001b20: 207b 7d0a 2020 2020 2020 2020 666f 7220   {}.        for 
+00001b30: 6e61 6d65 2069 6e20 7365 6c66 2e5f 7076  name in self._pv
+00001b40: 5f73 7973 7465 6d5f 6e61 6d65 733a 0a20  _system_names:. 
+00001b50: 2020 2020 2020 2020 2020 2063 6d5f 696e             cm_in
+00001b60: 666f 203d 2073 656c 662e 5f67 6574 5f70  fo = self._get_p
+00001b70: 765f 7379 7374 656d 5f69 6e66 6f28 6e61  v_system_info(na
+00001b80: 6d65 2c20 434f 4e54 524f 4c5f 4d4f 4445  me, CONTROL_MODE
+00001b90: 5f53 4345 4e41 5249 4f29 0a20 2020 2020  _SCENARIO).     
+00001ba0: 2020 2020 2020 2073 6572 6965 7320 3d20         series = 
+00001bb0: 7076 5f6c 6f61 645f 7368 6170 6573 5b63  pv_load_shapes[c
+00001bc0: 6d5f 696e 666f 5b22 6c6f 6164 5f73 6861  m_info["load_sha
+00001bd0: 7065 5f70 726f 6669 6c65 225d 5d0a 2020  pe_profile"]].  
+00001be0: 2020 2020 2020 2020 2020 6463 5f70 6f77            dc_pow
+00001bf0: 6572 203d 2073 6572 6965 7320 2a20 636d  er = series * cm
+00001c00: 5f69 6e66 6f5b 2270 6d70 7022 5d20 2a20  _info["pmpp"] * 
+00001c10: 636d 5f69 6e66 6f5b 2269 7272 6164 6961  cm_info["irradia
+00001c20: 6e63 6522 5d0a 2020 2020 2020 2020 2020  nce"].          
+00001c30: 2020 6173 7365 7274 206c 656e 2864 635f    assert len(dc_
+00001c40: 706f 7765 7229 203d 3d20 6c65 6e28 7066  power) == len(pf
+00001c50: 315f 7265 616c 5f70 6f77 6572 290a 2020  1_real_power).  
+00001c60: 2020 2020 2020 2020 2020 6463 5f70 6f77            dc_pow
+00001c70: 6572 735b 6e61 6d65 5d20 3d20 6463 5f70  ers[name] = dc_p
+00001c80: 6f77 6572 2e76 616c 7565 730a 2020 2020  ower.values.    
+00001c90: 2020 2020 2020 2020 2320 544f 444f 3a20          # TODO: 
+00001ca0: 6a75 7374 2066 6f72 2076 616c 6964 6174  just for validat
+00001cb0: 696f 6e0a 2020 2020 2020 2020 2020 2020  ion.            
+00001cc0: 6173 7365 7274 206d 6174 682e 6973 636c  assert math.iscl
+00001cd0: 6f73 6528 7375 6d28 6463 5f70 6f77 6572  ose(sum(dc_power
+00001ce0: 2e76 616c 7565 7329 2c20 636d 5f69 6e66  .values), cm_inf
+00001cf0: 6f5b 226c 6f61 645f 7368 6170 655f 706d  o["load_shape_pm
+00001d00: 756c 745f 7375 6d22 5d20 2a20 636d 5f69  ult_sum"] * cm_i
+00001d10: 6e66 6f5b 2270 6d70 7022 5d20 2a20 636d  nfo["pmpp"] * cm
+00001d20: 5f69 6e66 6f5b 2269 7272 6164 6961 6e63  _info["irradianc
+00001d30: 6522 5d29 0a20 2020 2020 2020 2064 6620  e"]).        df 
+00001d40: 3d20 7064 2e44 6174 6146 7261 6d65 2864  = pd.DataFrame(d
+00001d50: 635f 706f 7765 7273 2c20 696e 6465 783d  c_powers, index=
+00001d60: 7066 315f 7265 616c 5f70 6f77 6572 2e69  pf1_real_power.i
+00001d70: 6e64 6578 290a 2020 2020 2020 2020 746f  ndex).        to
+00001d80: 7461 6c5f 6463 5f70 6f77 6572 203d 2064  tal_dc_power = d
+00001d90: 662e 7375 6d28 6178 6973 3d31 290a 0a20  f.sum(axis=1).. 
+00001da0: 2020 2020 2020 2063 6c69 7070 696e 6720         clipping 
+00001db0: 3d20 7064 2e44 6174 6146 7261 6d65 280a  = pd.DataFrame(.
+00001dc0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00001dd0: 2e5f 6361 6c63 756c 6174 655f 636c 6970  ._calculate_clip
+00001de0: 7069 6e67 5f61 7272 6179 2874 6f74 616c  ping_array(total
+00001df0: 5f64 635f 706f 7765 722c 2070 6631 5f72  _dc_power, pf1_r
+00001e00: 6561 6c5f 706f 7765 722e 696c 6f63 5b3a  eal_power.iloc[:
+00001e10: 2c20 305d 292c 0a20 2020 2020 2020 2020  , 0]),.         
+00001e20: 2020 2069 6e64 6578 3d70 6631 5f72 6561     index=pf1_rea
+00001e30: 6c5f 706f 7765 722e 696e 6465 782c 0a20  l_power.index,. 
+00001e40: 2020 2020 2020 2020 2020 2063 6f6c 756d             colum
+00001e50: 6e73 3d5b 2254 6f74 616c 436c 6970 7069  ns=["TotalClippi
+00001e60: 6e67 225d 2c0a 2020 2020 2020 2020 290a  ng"],.        ).
+00001e70: 2020 2020 2020 2020 7365 6c66 2e5f 6578          self._ex
+00001e80: 706f 7274 5f64 6174 6166 7261 6d65 5f72  port_dataframe_r
+00001e90: 6570 6f72 7428 636c 6970 7069 6e67 2c20  eport(clipping, 
+00001ea0: 6f75 7470 7574 5f64 6972 2c20 2270 765f  output_dir, "pv_
+00001eb0: 636c 6970 7069 6e67 2229 0a0a 2020 2020  clipping")..    
+00001ec0: 6465 6620 5f67 656e 6572 6174 655f 616c  def _generate_al
+00001ed0: 6c5f 7076 5f73 7973 7465 6d73 5f74 6f74  l_pv_systems_tot
+00001ee0: 616c 2873 656c 662c 206f 7574 7075 745f  al(self, output_
+00001ef0: 6469 7229 3a0a 2020 2020 2020 2020 746f  dir):.        to
+00001f00: 7461 6c5f 6463 5f70 6f77 6572 203d 2073  tal_dc_power = s
+00001f10: 656c 662e 5f67 6574 5f74 6f74 616c 5f64  elf._get_total_d
+00001f20: 635f 706f 7765 725f 6163 726f 7373 5f70  c_power_across_p
+00001f30: 765f 7379 7374 656d 7328 290a 0a20 2020  v_systems()..   
+00001f40: 2020 2020 2070 6631 5f72 6561 6c5f 706f       pf1_real_po
+00001f50: 7765 7220 3d20 6e65 7874 2869 7465 7228  wer = next(iter(
+00001f60: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00001f70: 662e 5f70 6631 5f73 6365 6e61 7269 6f2e  f._pf1_scenario.
+00001f80: 6765 745f 7375 6d6d 6564 5f65 6c65 6d65  get_summed_eleme
+00001f90: 6e74 5f74 6f74 616c 2822 5056 5379 7374  nt_total("PVSyst
+00001fa0: 656d 7322 2c20 2250 6f77 6572 7353 756d  ems", "PowersSum
+00001fb0: 2229 2e76 616c 7565 7328 290a 2020 2020  ").values().    
+00001fc0: 2020 2020 2929 0a20 2020 2020 2020 2063      )).        c
+00001fd0: 6c69 7070 696e 6720 3d20 7365 6c66 2e5f  lipping = self._
+00001fe0: 6361 6c63 756c 6174 655f 636c 6970 7069  calculate_clippi
+00001ff0: 6e67 2874 6f74 616c 5f64 635f 706f 7765  ng(total_dc_powe
+00002000: 722c 2070 6631 5f72 6561 6c5f 706f 7765  r, pf1_real_powe
+00002010: 7229 0a20 2020 2020 2020 2064 6174 6120  r).        data 
+00002020: 3d20 7b22 636c 6970 7069 6e67 223a 2063  = {"clipping": c
+00002030: 6c69 7070 696e 677d 0a20 2020 2020 2020  lipping}.       
+00002040: 2073 656c 662e 5f65 7870 6f72 745f 6a73   self._export_js
+00002050: 6f6e 5f72 6570 6f72 7428 6461 7461 2c20  on_report(data, 
+00002060: 6f75 7470 7574 5f64 6972 2c20 7365 6c66  output_dir, self
+00002070: 2e54 4f54 414c 5f46 494c 454e 414d 4529  .TOTAL_FILENAME)
+00002080: 0a0a 2020 2020 6465 6620 5f72 6561 645f  ..    def _read_
+00002090: 7076 5f6c 6f61 645f 7368 6170 6573 2873  pv_load_shapes(s
+000020a0: 656c 6629 3a0a 2020 2020 2020 2020 7061  elf):.        pa
+000020b0: 7468 203d 206f 732e 7061 7468 2e6a 6f69  th = os.path.joi
+000020c0: 6e28 0a20 2020 2020 2020 2020 2020 2073  n(.            s
+000020d0: 7472 2873 656c 662e 5f73 6574 7469 6e67  tr(self._setting
+000020e0: 732e 7072 6f6a 6563 742e 6163 7469 7665  s.project.active
+000020f0: 5f70 726f 6a65 6374 5f70 6174 6829 2c0a  _project_path),.
+00002100: 2020 2020 2020 2020 2020 2020 2245 7870              "Exp
+00002110: 6f72 7473 222c 0a20 2020 2020 2020 2020  orts",.         
+00002120: 2020 2043 4f4e 5452 4f4c 5f4d 4f44 455f     CONTROL_MODE_
+00002130: 5343 454e 4152 494f 2c0a 2020 2020 2020  SCENARIO,.      
+00002140: 2020 2020 2020 5056 5f4c 4f41 445f 5348        PV_LOAD_SH
+00002150: 4150 455f 4649 4c45 4e41 4d45 2c0a 2020  APE_FILENAME,.  
+00002160: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
+00002170: 7265 7475 726e 2072 6561 645f 6461 7461  return read_data
+00002180: 6672 616d 6528 7061 7468 290a 0a20 2020  frame(path)..   
+00002190: 2064 6566 2067 656e 6572 6174 6528 7365   def generate(se
+000021a0: 6c66 2c20 6f75 7470 7574 5f64 6972 293a  lf, output_dir):
+000021b0: 0a20 2020 2020 2020 2069 6620 6e6f 7420  .        if not 
+000021c0: 7365 6c66 2e5f 6861 735f 7076 5f73 7973  self._has_pv_sys
+000021d0: 7465 6d73 2829 3a0a 2020 2020 2020 2020  tems():.        
+000021e0: 2020 2020 7265 7475 726e 0a0a 2020 2020      return..    
+000021f0: 2020 2020 6772 616e 756c 6172 6974 7920      granularity 
+00002200: 3d20 7365 6c66 2e5f 7365 7474 696e 6773  = self._settings
+00002210: 2e72 6570 6f72 7473 2e67 7261 6e75 6c61  .reports.granula
+00002220: 7269 7479 0a20 2020 2020 2020 2069 6620  rity.        if 
+00002230: 6772 616e 756c 6172 6974 7920 3d3d 2052  granularity == R
+00002240: 6570 6f72 7447 7261 6e75 6c61 7269 7479  eportGranularity
+00002250: 2e50 4552 5f45 4c45 4d45 4e54 5f50 4552  .PER_ELEMENT_PER
+00002260: 5f54 494d 455f 504f 494e 543a 0a20 2020  _TIME_POINT:.   
+00002270: 2020 2020 2020 2020 2073 656c 662e 5f67           self._g
+00002280: 656e 6572 6174 655f 7065 725f 7076 5f73  enerate_per_pv_s
+00002290: 7973 7465 6d5f 7065 725f 7469 6d65 5f70  ystem_per_time_p
+000022a0: 6f69 6e74 286f 7574 7075 745f 6469 7229  oint(output_dir)
+000022b0: 0a20 2020 2020 2020 2065 6c69 6620 6772  .        elif gr
+000022c0: 616e 756c 6172 6974 7920 3d3d 2052 6570  anularity == Rep
+000022d0: 6f72 7447 7261 6e75 6c61 7269 7479 2e50  ortGranularity.P
+000022e0: 4552 5f45 4c45 4d45 4e54 5f54 4f54 414c  ER_ELEMENT_TOTAL
+000022f0: 3a0a 2020 2020 2020 2020 2020 2020 7365  :.            se
+00002300: 6c66 2e5f 6765 6e65 7261 7465 5f70 6572  lf._generate_per
+00002310: 5f70 765f 7379 7374 656d 5f74 6f74 616c  _pv_system_total
+00002320: 286f 7574 7075 745f 6469 7229 0a20 2020  (output_dir).   
+00002330: 2020 2020 2065 6c69 6620 6772 616e 756c       elif granul
+00002340: 6172 6974 7920 3d3d 2052 6570 6f72 7447  arity == ReportG
+00002350: 7261 6e75 6c61 7269 7479 2e41 4c4c 5f45  ranularity.ALL_E
+00002360: 4c45 4d45 4e54 535f 5045 525f 5449 4d45  LEMENTS_PER_TIME
+00002370: 5f50 4f49 4e54 3a0a 2020 2020 2020 2020  _POINT:.        
+00002380: 2020 2020 7365 6c66 2e5f 6765 6e65 7261      self._genera
+00002390: 7465 5f61 6c6c 5f70 765f 7379 7374 656d  te_all_pv_system
+000023a0: 735f 7065 725f 7469 6d65 5f70 6f69 6e74  s_per_time_point
+000023b0: 286f 7574 7075 745f 6469 7229 0a20 2020  (output_dir).   
+000023c0: 2020 2020 2065 6c69 6620 6772 616e 756c       elif granul
+000023d0: 6172 6974 7920 3d3d 2052 6570 6f72 7447  arity == ReportG
+000023e0: 7261 6e75 6c61 7269 7479 2e41 4c4c 5f45  ranularity.ALL_E
+000023f0: 4c45 4d45 4e54 535f 544f 5441 4c3a 0a20  LEMENTS_TOTAL:. 
+00002400: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00002410: 5f67 656e 6572 6174 655f 616c 6c5f 7076  _generate_all_pv
+00002420: 5f73 7973 7465 6d73 5f74 6f74 616c 286f  _systems_total(o
+00002430: 7574 7075 745f 6469 7229 0a20 2020 2020  utput_dir).     
+00002440: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
+00002450: 2020 2020 2061 7373 6572 7420 4661 6c73       assert Fals
+00002460: 650a 0a0a 636c 6173 7320 5076 4375 7274  e...class PvCurt
+00002470: 6169 6c6d 656e 7452 6570 6f72 7428 5076  ailmentReport(Pv
+00002480: 5265 706f 7274 4261 7365 293a 0a20 2020  ReportBase):.   
+00002490: 2022 2222 5265 706f 7274 7320 5056 2043   """Reports PV C
+000024a0: 7572 7461 696c 6d65 6e74 2061 7420 6576  urtailment at ev
+000024b0: 6572 7920 7469 6d65 2070 6f69 6e74 2069  ery time point i
+000024c0: 6e20 7468 6520 7369 6d75 6c61 7469 6f6e  n the simulation
+000024d0: 2e0a 0a20 2020 2054 6865 2072 6570 6f72  ...    The repor
+000024e0: 7420 6765 6e65 7261 7465 7320 6120 7076  t generates a pv
+000024f0: 5f63 7572 7461 696c 6d65 6e74 206f 7574  _curtailment out
+00002500: 7075 7420 6669 6c65 2e20 5468 6520 6669  put file. The fi
+00002510: 6c65 2065 7874 656e 7369 6f6e 0a20 2020  le extension.   
+00002520: 2064 6570 656e 6473 206f 6e20 7468 6520   depends on the 
+00002530: 696e 7075 7420 7061 7261 6d65 7465 7273  input parameters
+00002540: 2e20 4966 2074 6865 2064 6174 6120 7761  . If the data wa
+00002550: 7320 636f 6c6c 6563 7465 6420 6174 2065  s collected at e
+00002560: 7665 7279 2074 696d 650a 2020 2020 706f  very time.    po
+00002570: 696e 7420 7468 656e 2074 6865 206f 7574  int then the out
+00002580: 7075 7420 6669 6c65 2077 696c 6c20 6265  put file will be
+00002590: 202e 6373 7620 6f72 202e 6835 2c20 6465   .csv or .h5, de
+000025a0: 7065 6e64 696e 6720 6f6e 2027 4578 706f  pending on 'Expo
+000025b0: 7274 0a20 2020 2046 6f72 6d61 742e 2720  rt.    Format.' 
+000025c0: 4f74 6865 7277 6973 652c 2074 6865 206f  Otherwise, the o
+000025d0: 7574 7075 7420 6669 6c65 2077 696c 6c20  utput file will 
+000025e0: 6265 202e 6a73 6f6e 2e0a 0a20 2020 2054  be .json...    T
+000025f0: 4f44 4f3a 2054 6869 7320 6973 2061 6e20  ODO: This is an 
+00002600: 6578 7065 7269 6d65 6e74 616c 2072 6570  experimental rep
+00002610: 6f72 742e 204f 7574 7075 7473 2068 6176  ort. Outputs hav
+00002620: 6520 6e6f 7420 6265 656e 2076 616c 6964  e not been valid
+00002630: 6174 6564 2e0a 0a20 2020 2022 2222 0a0a  ated...    """..
+00002640: 2020 2020 5045 525f 5449 4d45 5f50 4f49      PER_TIME_POI
+00002650: 4e54 5f46 494c 454e 414d 4520 3d20 2270  NT_FILENAME = "p
+00002660: 765f 6375 7274 6169 6c6d 656e 742e 6835  v_curtailment.h5
+00002670: 220a 2020 2020 544f 5441 4c5f 4649 4c45  ".    TOTAL_FILE
+00002680: 4e41 4d45 203d 2022 7076 5f63 7572 7461  NAME = "pv_curta
+00002690: 696c 6d65 6e74 2e6a 736f 6e22 0a20 2020  ilment.json".   
+000026a0: 204e 414d 4520 3d20 2250 5620 4375 7274   NAME = "PV Curt
+000026b0: 6169 6c6d 656e 7422 0a0a 2020 2020 6465  ailment"..    de
+000026c0: 6620 5f5f 696e 6974 5f5f 2873 656c 662c  f __init__(self,
+000026d0: 206e 616d 652c 2072 6573 756c 7473 2c20   name, results, 
+000026e0: 7369 6d75 6c61 7469 6f6e 5f63 6f6e 6669  simulation_confi
+000026f0: 6729 3a0a 2020 2020 2020 2020 7375 7065  g):.        supe
+00002700: 7228 292e 5f5f 696e 6974 5f5f 286e 616d  r().__init__(nam
+00002710: 652c 2072 6573 756c 7473 2c20 7369 6d75  e, results, simu
+00002720: 6c61 7469 6f6e 5f63 6f6e 6669 6729 0a20  lation_config). 
+00002730: 2020 2020 2020 2069 6620 6e6f 7420 7365         if not se
+00002740: 6c66 2e5f 6861 735f 7076 5f73 7973 7465  lf._has_pv_syste
+00002750: 6d73 2829 3a0a 2020 2020 2020 2020 2020  ms():.          
+00002760: 2020 7265 7475 726e 0a0a 2020 2020 2020    return..      
+00002770: 2020 6469 6666 5f74 6f6c 6572 616e 6365    diff_tolerance
+00002780: 203d 2073 656c 662e 5f72 6570 6f72 745f   = self._report_
+00002790: 7365 7474 696e 6773 2e64 6966 665f 746f  settings.diff_to
+000027a0: 6c65 7261 6e63 655f 7065 7263 656e 745f  lerance_percent_
+000027b0: 706d 7070 202a 202e 3031 0a20 2020 2020  pmpp * .01.     
+000027c0: 2020 2064 656e 6f6d 696e 6174 6f72 5f74     denominator_t
+000027d0: 6f6c 6572 616e 6365 203d 2073 656c 662e  olerance = self.
+000027e0: 5f72 6570 6f72 745f 7365 7474 696e 6773  _report_settings
+000027f0: 2e64 656e 6f6d 696e 6174 6f72 5f74 6f6c  .denominator_tol
+00002800: 6572 616e 6365 5f70 6572 6365 6e74 5f70  erance_percent_p
+00002810: 6d70 7020 2a20 2e30 310a 2020 2020 2020  mpp * .01.      
+00002820: 2020 6c6f 6767 6572 2e64 6562 7567 2822    logger.debug("
+00002830: 746f 6c65 7261 6e63 6573 3a20 6469 6666  tolerances: diff
+00002840: 3d25 7320 6465 6e6f 6d69 6e61 746f 723d  =%s denominator=
+00002850: 2573 222c 2064 6966 665f 746f 6c65 7261  %s", diff_tolera
+00002860: 6e63 652c 2064 656e 6f6d 696e 6174 6f72  nce, denominator
+00002870: 5f74 6f6c 6572 616e 6365 290a 2020 2020  _tolerance).    
+00002880: 2020 2020 7365 6c66 2e5f 6469 6666 5f74      self._diff_t
+00002890: 6f6c 6572 616e 6365 7320 3d20 7b7d 0a20  olerances = {}. 
+000028a0: 2020 2020 2020 2073 656c 662e 5f64 656e         self._den
+000028b0: 6f6d 696e 6174 6f72 5f74 6f6c 6572 616e  ominator_toleran
+000028c0: 6365 7320 3d20 7b7d 0a20 2020 2020 2020  ces = {}.       
+000028d0: 2066 6f72 2070 765f 7379 7374 656d 2069   for pv_system i
+000028e0: 6e20 7365 6c66 2e5f 7066 315f 7363 656e  n self._pf1_scen
+000028f0: 6172 696f 2e72 6561 645f 7076 5f70 726f  ario.read_pv_pro
+00002900: 6669 6c65 7328 295b 2270 765f 7379 7374  files()["pv_syst
+00002910: 656d 7322 5d3a 0a20 2020 2020 2020 2020  ems"]:.         
+00002920: 2020 2073 656c 662e 5f64 6966 665f 746f     self._diff_to
+00002930: 6c65 7261 6e63 6573 5b70 765f 7379 7374  lerances[pv_syst
+00002940: 656d 5b22 6e61 6d65 225d 5d20 3d20 7076  em["name"]] = pv
+00002950: 5f73 7973 7465 6d5b 2270 6d70 7022 5d20  _system["pmpp"] 
+00002960: 2a20 6469 6666 5f74 6f6c 6572 616e 6365  * diff_tolerance
+00002970: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00002980: 662e 5f64 656e 6f6d 696e 6174 6f72 5f74  f._denominator_t
+00002990: 6f6c 6572 616e 6365 735b 7076 5f73 7973  olerances[pv_sys
+000029a0: 7465 6d5b 226e 616d 6522 5d5d 203d 2070  tem["name"]] = p
+000029b0: 765f 7379 7374 656d 5b22 706d 7070 225d  v_system["pmpp"]
+000029c0: 202a 2064 656e 6f6d 696e 6174 6f72 5f74   * denominator_t
+000029d0: 6f6c 6572 616e 6365 0a0a 2020 2020 6465  olerance..    de
+000029e0: 6620 5f67 656e 6572 6174 655f 7065 725f  f _generate_per_
+000029f0: 7076 5f73 7973 7465 6d5f 7065 725f 7469  pv_system_per_ti
+00002a00: 6d65 5f70 6f69 6e74 2873 656c 662c 206f  me_point(self, o
+00002a10: 7574 7075 745f 6469 7229 3a0a 2020 2020  utput_dir):.    
+00002a20: 2020 2020 7066 315f 706f 7765 7220 3d20      pf1_power = 
+00002a30: 7365 6c66 2e5f 7066 315f 7363 656e 6172  self._pf1_scenar
+00002a40: 696f 2e67 6574 5f66 756c 6c5f 6461 7461  io.get_full_data
+00002a50: 6672 616d 6528 2250 5653 7973 7465 6d73  frame("PVSystems
+00002a60: 222c 2022 506f 7765 7273 2229 0a20 2020  ", "Powers").   
+00002a70: 2020 2020 2063 6f6e 7472 6f6c 5f6d 6f64       control_mod
+00002a80: 655f 706f 7765 7220 3d20 7365 6c66 2e5f  e_power = self._
+00002a90: 636f 6e74 726f 6c5f 6d6f 6465 5f73 6365  control_mode_sce
+00002aa0: 6e61 7269 6f2e 6765 745f 6675 6c6c 5f64  nario.get_full_d
+00002ab0: 6174 6166 7261 6d65 280a 2020 2020 2020  ataframe(.      
+00002ac0: 2020 2020 2020 2250 5653 7973 7465 6d73        "PVSystems
+00002ad0: 222c 2022 506f 7765 7273 220a 2020 2020  ", "Powers".    
+00002ae0: 2020 2020 290a 2020 2020 2020 2020 6e61      ).        na
+00002af0: 6d65 203d 204e 6f6e 650a 2020 2020 2020  me = None.      
+00002b00: 2020 6465 6620 6361 6c63 5f63 7572 7461    def calc_curta
+00002b10: 696c 6d65 6e74 2870 6631 2c20 636d 293a  ilment(pf1, cm):
+00002b20: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+00002b30: 7066 3120 3c20 7365 6c66 2e5f 6465 6e6f  pf1 < self._deno
+00002b40: 6d69 6e61 746f 725f 746f 6c65 7261 6e63  minator_toleranc
+00002b50: 6573 5b6e 616d 655d 3a0a 2020 2020 2020  es[name]:.      
+00002b60: 2020 2020 2020 2020 2020 7265 7475 726e            return
+00002b70: 2030 0a20 2020 2020 2020 2020 2020 2064   0.            d
+00002b80: 6966 6620 3d20 7066 3120 2d20 636d 0a20  iff = pf1 - cm. 
+00002b90: 2020 2020 2020 2020 2020 2069 6620 6469             if di
+00002ba0: 6666 203c 2030 2061 6e64 2061 6273 2864  ff < 0 and abs(d
+00002bb0: 6966 6629 203c 2073 656c 662e 5f64 6966  iff) < self._dif
+00002bc0: 665f 746f 6c65 7261 6e63 6573 5b6e 616d  f_tolerances[nam
+00002bd0: 655d 3a0a 2020 2020 2020 2020 2020 2020  e]:.            
+00002be0: 2020 2020 7265 7475 726e 2030 0a20 2020      return 0.   
+00002bf0: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
+00002c00: 2870 6631 202d 2063 6d29 202f 2070 6631  (pf1 - cm) / pf1
+00002c10: 202a 2031 3030 0a0a 2020 2020 2020 2020   * 100..        
+00002c20: 6461 7461 203d 207b 7d0a 2020 2020 2020  data = {}.      
+00002c30: 2020 666f 7220 636f 6c20 696e 2070 6631    for col in pf1
+00002c40: 5f70 6f77 6572 2e63 6f6c 756d 6e73 3a0a  _power.columns:.
+00002c50: 2020 2020 2020 2020 2020 2020 6e61 6d65              name
+00002c60: 203d 2063 6f6c 2e73 706c 6974 2822 5f5f   = col.split("__
+00002c70: 2229 5b30 5d0a 2020 2020 2020 2020 2020  ")[0].          
+00002c80: 2020 735f 7066 3120 3d20 7066 315f 706f    s_pf1 = pf1_po
+00002c90: 7765 725b 636f 6c5d 0a20 2020 2020 2020  wer[col].       
+00002ca0: 2020 2020 2073 5f63 6d20 3d20 636f 6e74       s_cm = cont
+00002cb0: 726f 6c5f 6d6f 6465 5f70 6f77 6572 5b63  rol_mode_power[c
+00002cc0: 6f6c 5d0a 2020 2020 2020 2020 2020 2020  ol].            
+00002cd0: 6e65 775f 6e61 6d65 203d 2063 6f6c 2e72  new_name = col.r
+00002ce0: 6570 6c61 6365 2822 506f 7765 7273 222c  eplace("Powers",
+00002cf0: 2022 4375 7274 6169 6c6d 656e 7422 290a   "Curtailment").
+00002d00: 2020 2020 2020 2020 2020 2020 6461 7461              data
+00002d10: 5b6e 6577 5f6e 616d 655d 203d 2073 5f70  [new_name] = s_p
+00002d20: 6631 2e63 6f6d 6269 6e65 2873 5f63 6d2c  f1.combine(s_cm,
+00002d30: 2063 616c 635f 6375 7274 6169 6c6d 656e   calc_curtailmen
+00002d40: 7429 2e76 616c 7565 730a 0a20 2020 2020  t).values..     
+00002d50: 2020 2064 6620 3d20 7064 2e44 6174 6146     df = pd.DataF
+00002d60: 7261 6d65 2864 6174 612c 2069 6e64 6578  rame(data, index
+00002d70: 3d70 6631 5f70 6f77 6572 2e69 6e64 6578  =pf1_power.index
+00002d80: 290a 2020 2020 2020 2020 7365 6c66 2e5f  ).        self._
+00002d90: 6578 706f 7274 5f64 6174 6166 7261 6d65  export_dataframe
+00002da0: 5f72 6570 6f72 7428 6466 2c20 6f75 7470  _report(df, outp
+00002db0: 7574 5f64 6972 2c20 2270 765f 6375 7274  ut_dir, "pv_curt
+00002dc0: 6169 6c6d 656e 7422 290a 0a20 2020 2064  ailment")..    d
+00002dd0: 6566 205f 6765 6e65 7261 7465 5f70 6572  ef _generate_per
+00002de0: 5f70 765f 7379 7374 656d 5f74 6f74 616c  _pv_system_total
+00002df0: 2873 656c 662c 206f 7574 7075 745f 6469  (self, output_di
+00002e00: 7229 3a0a 2020 2020 2020 2020 6461 7461  r):.        data
+00002e10: 203d 207b 2270 765f 7379 7374 656d 7322   = {"pv_systems"
+00002e20: 3a20 5b5d 7d0a 2020 2020 2020 2020 666f  : []}.        fo
+00002e30: 7220 6e61 6d65 2069 6e20 7365 6c66 2e5f  r name in self._
+00002e40: 7076 5f73 7973 7465 6d5f 6e61 6d65 733a  pv_system_names:
+00002e50: 0a20 2020 2020 2020 2020 2020 2070 6631  .            pf1
+00002e60: 5f70 6f77 6572 203d 2073 656c 662e 5f70  _power = self._p
+00002e70: 6631 5f73 6365 6e61 7269 6f2e 6765 745f  f1_scenario.get_
+00002e80: 656c 656d 656e 745f 7072 6f70 6572 7479  element_property
+00002e90: 5f76 616c 7565 280a 2020 2020 2020 2020  _value(.        
+00002ea0: 2020 2020 2020 2020 2250 5653 7973 7465          "PVSyste
+00002eb0: 6d73 222c 2022 506f 7765 7273 5375 6d22  ms", "PowersSum"
+00002ec0: 2c20 6e61 6d65 0a20 2020 2020 2020 2020  , name.         
+00002ed0: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
+00002ee0: 2063 6f6e 7472 6f6c 5f6d 6f64 655f 706f   control_mode_po
+00002ef0: 7765 7220 3d20 7365 6c66 2e5f 636f 6e74  wer = self._cont
+00002f00: 726f 6c5f 6d6f 6465 5f73 6365 6e61 7269  rol_mode_scenari
+00002f10: 6f2e 6765 745f 656c 656d 656e 745f 7072  o.get_element_pr
+00002f20: 6f70 6572 7479 5f76 616c 7565 280a 2020  operty_value(.  
+00002f30: 2020 2020 2020 2020 2020 2020 2020 2250                "P
+00002f40: 5653 7973 7465 6d73 222c 2022 506f 7765  VSystems", "Powe
+00002f50: 7273 5375 6d22 2c20 6e61 6d65 0a20 2020  rsSum", name.   
+00002f60: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
+00002f70: 2020 2020 2020 2063 7572 7461 696c 6d65         curtailme
+00002f80: 6e74 203d 2028 7066 315f 706f 7765 7220  nt = (pf1_power 
+00002f90: 2d20 636f 6e74 726f 6c5f 6d6f 6465 5f70  - control_mode_p
+00002fa0: 6f77 6572 2920 2f20 7066 315f 706f 7765  ower) / pf1_powe
+00002fb0: 7220 2a20 3130 300a 2020 2020 2020 2020  r * 100.        
+00002fc0: 2020 2020 6461 7461 5b22 7076 5f73 7973      data["pv_sys
+00002fd0: 7465 6d73 225d 2e61 7070 656e 6428 0a20  tems"].append(. 
+00002fe0: 2020 2020 2020 2020 2020 2020 2020 207b                 {
+00002ff0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00003000: 2020 2020 2022 6e61 6d65 223a 206e 616d       "name": nam
+00003010: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00003020: 2020 2020 2020 2022 6375 7274 6169 6c6d         "curtailm
+00003030: 656e 7422 3a20 6375 7274 6169 6c6d 656e  ent": curtailmen
+00003040: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+00003050: 2020 207d 0a20 2020 2020 2020 2020 2020     }.           
+00003060: 2029 0a20 2020 2020 2020 2073 656c 662e   ).        self.
+00003070: 5f65 7870 6f72 745f 6a73 6f6e 5f72 6570  _export_json_rep
+00003080: 6f72 7428 6461 7461 2c20 6f75 7470 7574  ort(data, output
+00003090: 5f64 6972 2c20 7365 6c66 2e54 4f54 414c  _dir, self.TOTAL
+000030a0: 5f46 494c 454e 414d 4529 0a0a 2020 2020  _FILENAME)..    
+000030b0: 6465 6620 5f67 656e 6572 6174 655f 616c  def _generate_al
+000030c0: 6c5f 7076 5f73 7973 7465 6d73 5f70 6572  l_pv_systems_per
+000030d0: 5f74 696d 655f 706f 696e 7428 7365 6c66  _time_point(self
+000030e0: 2c20 6f75 7470 7574 5f64 6972 293a 0a20  , output_dir):. 
+000030f0: 2020 2020 2020 2070 6631 5f70 6f77 6572         pf1_power
+00003100: 203d 2073 656c 662e 5f70 6631 5f73 6365   = self._pf1_sce
+00003110: 6e61 7269 6f2e 6765 745f 7375 6d6d 6564  nario.get_summed
+00003120: 5f65 6c65 6d65 6e74 5f64 6174 6166 7261  _element_datafra
+00003130: 6d65 2822 5056 5379 7374 656d 7322 2c20  me("PVSystems", 
+00003140: 2250 6f77 6572 7322 290a 2020 2020 2020  "Powers").      
+00003150: 2020 636f 6e74 726f 6c5f 6d6f 6465 5f70    control_mode_p
+00003160: 6f77 6572 203d 2073 656c 662e 5f63 6f6e  ower = self._con
+00003170: 7472 6f6c 5f6d 6f64 655f 7363 656e 6172  trol_mode_scenar
+00003180: 696f 2e67 6574 5f73 756d 6d65 645f 656c  io.get_summed_el
+00003190: 656d 656e 745f 6461 7461 6672 616d 6528  ement_dataframe(
+000031a0: 0a20 2020 2020 2020 2020 2020 2022 5056  .            "PV
+000031b0: 5379 7374 656d 7322 2c20 2250 6f77 6572  Systems", "Power
+000031c0: 7322 0a20 2020 2020 2020 2029 0a20 2020  s".        ).   
+000031d0: 2020 2020 2064 6620 3d20 2870 6631 5f70       df = (pf1_p
+000031e0: 6f77 6572 202d 2063 6f6e 7472 6f6c 5f6d  ower - control_m
+000031f0: 6f64 655f 706f 7765 7229 202f 2070 6631  ode_power) / pf1
+00003200: 5f70 6f77 6572 202a 2031 3030 0a20 2020  _power * 100.   
+00003210: 2020 2020 2073 656c 662e 5f65 7870 6f72       self._expor
+00003220: 745f 6461 7461 6672 616d 655f 7265 706f  t_dataframe_repo
+00003230: 7274 2864 662c 206f 7574 7075 745f 6469  rt(df, output_di
+00003240: 722c 2022 7076 5f63 7572 7461 696c 6d65  r, "pv_curtailme
+00003250: 6e74 2229 0a0a 2020 2020 6465 6620 5f67  nt")..    def _g
+00003260: 656e 6572 6174 655f 616c 6c5f 7076 5f73  enerate_all_pv_s
+00003270: 7973 7465 6d73 5f74 6f74 616c 2873 656c  ystems_total(sel
+00003280: 662c 206f 7574 7075 745f 6469 7229 3a0a  f, output_dir):.
+00003290: 2020 2020 2020 2020 7066 315f 706f 7765          pf1_powe
+000032a0: 7220 3d20 6e65 7874 2869 7465 7228 0a20  r = next(iter(. 
+000032b0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+000032c0: 5f70 6631 5f73 6365 6e61 7269 6f2e 6765  _pf1_scenario.ge
+000032d0: 745f 7375 6d6d 6564 5f65 6c65 6d65 6e74  t_summed_element
+000032e0: 5f74 6f74 616c 2822 5056 5379 7374 656d  _total("PVSystem
+000032f0: 7322 2c20 2250 6f77 6572 7353 756d 2229  s", "PowersSum")
+00003300: 2e76 616c 7565 7328 290a 2020 2020 2020  .values().      
+00003310: 2020 2929 0a20 2020 2020 2020 2063 6f6e    )).        con
+00003320: 7472 6f6c 5f6d 6f64 655f 706f 7765 7220  trol_mode_power 
+00003330: 3d20 6e65 7874 2869 7465 7228 0a20 2020  = next(iter(.   
+00003340: 2020 2020 2020 2020 2073 656c 662e 5f63           self._c
+00003350: 6f6e 7472 6f6c 5f6d 6f64 655f 7363 656e  ontrol_mode_scen
+00003360: 6172 696f 2e67 6574 5f73 756d 6d65 645f  ario.get_summed_
+00003370: 656c 656d 656e 745f 746f 7461 6c28 2250  element_total("P
+00003380: 5653 7973 7465 6d73 222c 2022 506f 7765  VSystems", "Powe
+00003390: 7273 5375 6d22 292e 7661 6c75 6573 2829  rsSum").values()
+000033a0: 0a20 2020 2020 2020 2029 290a 0a20 2020  .        ))..   
+000033b0: 2020 2020 2063 7572 7461 696c 6d65 6e74       curtailment
+000033c0: 203d 2028 7066 315f 706f 7765 7220 2d20   = (pf1_power - 
+000033d0: 636f 6e74 726f 6c5f 6d6f 6465 5f70 6f77  control_mode_pow
+000033e0: 6572 2920 2f20 7066 315f 706f 7765 7220  er) / pf1_power 
+000033f0: 2a20 3130 300a 2020 2020 2020 2020 6461  * 100.        da
+00003400: 7461 203d 207b 2263 7572 7461 696c 6d65  ta = {"curtailme
+00003410: 6e74 223a 2063 7572 7461 696c 6d65 6e74  nt": curtailment
+00003420: 7d0a 2020 2020 2020 2020 7365 6c66 2e5f  }.        self._
+00003430: 6578 706f 7274 5f6a 736f 6e5f 7265 706f  export_json_repo
+00003440: 7274 2864 6174 612c 206f 7574 7075 745f  rt(data, output_
+00003450: 6469 722c 2073 656c 662e 544f 5441 4c5f  dir, self.TOTAL_
+00003460: 4649 4c45 4e41 4d45 290a 0a20 2020 2064  FILENAME)..    d
+00003470: 6566 2067 656e 6572 6174 6528 7365 6c66  ef generate(self
+00003480: 2c20 6f75 7470 7574 5f64 6972 293a 0a20  , output_dir):. 
+00003490: 2020 2020 2020 2069 6620 6e6f 7420 7365         if not se
+000034a0: 6c66 2e5f 6861 735f 7076 5f73 7973 7465  lf._has_pv_syste
+000034b0: 6d73 2829 3a0a 2020 2020 2020 2020 2020  ms():.          
+000034c0: 2020 7265 7475 726e 0a0a 2020 2020 2020    return..      
+000034d0: 2020 6772 616e 756c 6172 6974 7920 3d20    granularity = 
+000034e0: 5265 706f 7274 4772 616e 756c 6172 6974  ReportGranularit
+000034f0: 7928 7365 6c66 2e5f 7365 7474 696e 6773  y(self._settings
+00003500: 2e72 6570 6f72 7473 2e67 7261 6e75 6c61  .reports.granula
+00003510: 7269 7479 290a 2020 2020 2020 2020 6966  rity).        if
+00003520: 2067 7261 6e75 6c61 7269 7479 203d 3d20   granularity == 
+00003530: 5265 706f 7274 4772 616e 756c 6172 6974  ReportGranularit
+00003540: 792e 5045 525f 454c 454d 454e 545f 5045  y.PER_ELEMENT_PE
+00003550: 525f 5449 4d45 5f50 4f49 4e54 3a0a 2020  R_TIME_POINT:.  
+00003560: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
+00003570: 6765 6e65 7261 7465 5f70 6572 5f70 765f  generate_per_pv_
+00003580: 7379 7374 656d 5f70 6572 5f74 696d 655f  system_per_time_
+00003590: 706f 696e 7428 6f75 7470 7574 5f64 6972  point(output_dir
+000035a0: 290a 2020 2020 2020 2020 656c 6966 2067  ).        elif g
+000035b0: 7261 6e75 6c61 7269 7479 203d 3d20 5265  ranularity == Re
+000035c0: 706f 7274 4772 616e 756c 6172 6974 792e  portGranularity.
+000035d0: 5045 525f 454c 454d 454e 545f 544f 5441  PER_ELEMENT_TOTA
+000035e0: 4c3a 0a20 2020 2020 2020 2020 2020 2073  L:.            s
+000035f0: 656c 662e 5f67 656e 6572 6174 655f 7065  elf._generate_pe
+00003600: 725f 7076 5f73 7973 7465 6d5f 746f 7461  r_pv_system_tota
+00003610: 6c28 6f75 7470 7574 5f64 6972 290a 2020  l(output_dir).  
+00003620: 2020 2020 2020 656c 6966 2067 7261 6e75        elif granu
+00003630: 6c61 7269 7479 203d 3d20 5265 706f 7274  larity == Report
+00003640: 4772 616e 756c 6172 6974 792e 414c 4c5f  Granularity.ALL_
+00003650: 454c 454d 454e 5453 5f50 4552 5f54 494d  ELEMENTS_PER_TIM
+00003660: 455f 504f 494e 543a 0a20 2020 2020 2020  E_POINT:.       
+00003670: 2020 2020 2073 656c 662e 5f67 656e 6572       self._gener
+00003680: 6174 655f 616c 6c5f 7076 5f73 7973 7465  ate_all_pv_syste
+00003690: 6d73 5f70 6572 5f74 696d 655f 706f 696e  ms_per_time_poin
+000036a0: 7428 6f75 7470 7574 5f64 6972 290a 2020  t(output_dir).  
+000036b0: 2020 2020 2020 656c 6966 2067 7261 6e75        elif granu
+000036c0: 6c61 7269 7479 203d 3d20 5265 706f 7274  larity == Report
+000036d0: 4772 616e 756c 6172 6974 792e 414c 4c5f  Granularity.ALL_
+000036e0: 454c 454d 454e 5453 5f54 4f54 414c 3a0a  ELEMENTS_TOTAL:.
+000036f0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00003700: 2e5f 6765 6e65 7261 7465 5f61 6c6c 5f70  ._generate_all_p
+00003710: 765f 7379 7374 656d 735f 746f 7461 6c28  v_systems_total(
+00003720: 6f75 7470 7574 5f64 6972 290a 2020 2020  output_dir).    
+00003730: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+00003740: 2020 2020 2020 6173 7365 7274 2046 616c        assert Fal
+00003750: 7365 0a0a 2020 2020 6465 6620 6361 6c63  se..    def calc
+00003760: 756c 6174 655f 7076 5f63 7572 7461 696c  ulate_pv_curtail
+00003770: 6d65 6e74 2873 656c 6629 3a0a 2020 2020  ment(self):.    
+00003780: 2020 2020 2222 2243 616c 6375 6c61 7465      """Calculate
+00003790: 2050 5620 6375 7274 6169 6c6d 656e 7420   PV curtailment 
+000037a0: 666f 7220 616c 6c20 5056 2073 7973 7465  for all PV syste
+000037b0: 6d73 2e0a 0a20 2020 2020 2020 2052 6574  ms...        Ret
+000037c0: 7572 6e73 0a20 2020 2020 2020 202d 2d2d  urns.        ---
+000037d0: 2d2d 2d2d 0a20 2020 2020 2020 2070 642e  ----.        pd.
+000037e0: 4461 7461 4672 616d 650a 0a20 2020 2020  DataFrame..     
+000037f0: 2020 2022 2222 0a20 2020 2020 2020 2070     """.        p
+00003800: 6631 5f70 6f77 6572 203d 2073 656c 662e  f1_power = self.
+00003810: 5f70 6631 5f73 6365 6e61 7269 6f2e 6765  _pf1_scenario.ge
+00003820: 745f 6675 6c6c 5f64 6174 6166 7261 6d65  t_full_dataframe
+00003830: 280a 2020 2020 2020 2020 2020 2020 2250  (.            "P
+00003840: 5653 7973 7465 6d73 222c 2022 506f 7765  VSystems", "Powe
+00003850: 7273 222c 2072 6561 6c5f 6f6e 6c79 3d54  rs", real_only=T
+00003860: 7275 650a 2020 2020 2020 2020 290a 2020  rue.        ).  
+00003870: 2020 2020 2020 636f 6e74 726f 6c5f 6d6f        control_mo
+00003880: 6465 5f70 6f77 6572 203d 2073 656c 662e  de_power = self.
+00003890: 5f63 6f6e 7472 6f6c 5f6d 6f64 655f 7363  _control_mode_sc
+000038a0: 656e 6172 696f 2e67 6574 5f66 756c 6c5f  enario.get_full_
+000038b0: 6461 7461 6672 616d 6528 0a20 2020 2020  dataframe(.     
+000038c0: 2020 2020 2020 2022 5056 5379 7374 656d         "PVSystem
+000038d0: 7322 2c20 2250 6f77 6572 7322 2c20 7265  s", "Powers", re
+000038e0: 616c 5f6f 6e6c 793d 5472 7565 0a20 2020  al_only=True.   
+000038f0: 2020 2020 2029 0a20 2020 2020 2020 2072       ).        r
+00003900: 6574 7572 6e20 2870 6631 5f70 6f77 6572  eturn (pf1_power
+00003910: 202d 2063 6f6e 7472 6f6c 5f6d 6f64 655f   - control_mode_
+00003920: 706f 7765 7229 202f 2070 6631 5f70 6f77  power) / pf1_pow
+00003930: 6572 202a 2031 3030 0a                   er * 100.
```

### Comparing `nrel_pydss-3.1.3/src/pydss/reports/reg_control_tap_number_change_count.py` & `nrel_pydss-3.1.4/src/pydss/reports/reg_control_tap_number_change_count.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,49 +1,49 @@
-import os
-
-from loguru import logger
-
-from pydss.reports.reports import ReportBase
-from pydss.utils.utils import dump_data
-
-
-class RegControlTapNumberChangeReport(ReportBase):
-    """Reports the tap number changes per RegControl.
-
-    TODO: This is an experimental report. Outputs have not been validated.
-
-    """
-
-    FILENAME = "reg_control_tap_value_changes.json"
-    NAME = "RegControl Tap Number Change Counts"
-
-    def generate(self, output_dir):
-        data = {"scenarios": []}
-        for scenario in self._results.scenarios:
-            scenario_data = {"name": scenario.name, "reg_controls": []}
-            for reg_control in scenario.list_element_names("RegControls"):
-                change_count = int(scenario.get_element_property_value(
-                    "RegControls", "TrackTapNumberChanges", reg_control
-                ))
-                changes = {"name": reg_control, "change_count": change_count}
-                scenario_data["reg_controls"].append(changes)
-            data["scenarios"].append(scenario_data)
-
-        filename = os.path.join(output_dir, self.FILENAME)
-        dump_data(data, filename, indent=2)
-        logger.info("Generated %s", filename)
-        return filename
-
-    @staticmethod
-    def get_required_exports(simulation_config):
-        return {
-            "RegControls": [
-                {
-                    "property": "TrackTapNumberChanges",
-                    "store_values_type": "change_count",
-                }
-            ]
-        }
-
-    @staticmethod
-    def get_required_scenario_names():
-        return set()
+import os
+
+from loguru import logger
+
+from pydss.reports.reports import ReportBase
+from pydss.utils.utils import dump_data
+
+
+class RegControlTapNumberChangeReport(ReportBase):
+    """Reports the tap number changes per RegControl.
+
+    TODO: This is an experimental report. Outputs have not been validated.
+
+    """
+
+    FILENAME = "reg_control_tap_value_changes.json"
+    NAME = "RegControl Tap Number Change Counts"
+
+    def generate(self, output_dir):
+        data = {"scenarios": []}
+        for scenario in self._results.scenarios:
+            scenario_data = {"name": scenario.name, "reg_controls": []}
+            for reg_control in scenario.list_element_names("RegControls"):
+                change_count = int(scenario.get_element_property_value(
+                    "RegControls", "TrackTapNumberChanges", reg_control
+                ))
+                changes = {"name": reg_control, "change_count": change_count}
+                scenario_data["reg_controls"].append(changes)
+            data["scenarios"].append(scenario_data)
+
+        filename = os.path.join(output_dir, self.FILENAME)
+        dump_data(data, filename, indent=2)
+        logger.info("Generated %s", filename)
+        return filename
+
+    @staticmethod
+    def get_required_exports(simulation_config):
+        return {
+            "RegControls": [
+                {
+                    "property": "TrackTapNumberChanges",
+                    "store_values_type": "change_count",
+                }
+            ]
+        }
+
+    @staticmethod
+    def get_required_scenario_names():
+        return set()
```

### Comparing `nrel_pydss-3.1.3/src/pydss/reports/reports.py` & `nrel_pydss-3.1.4/src/pydss/reports/reports.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,267 +1,267 @@
-"""Creates reports on data exported by PyDSS"""
-
-from datetime import timedelta
-import time
-import copy
-import math
-import abc
-import os
-
-from loguru import logger
-
-from pydss.common import DataConversion, ReportGranularity
-from pydss.exceptions import InvalidConfiguration
-from pydss.simulation_input_models import SimulationSettingsModel
-from pydss.utils.dataframe_utils import write_dataframe
-from pydss.utils.simulation_utils import create_time_range_from_settings
-from pydss.utils.utils import dump_data, make_json_serializable
-
-
-REPORTS_DIR = "Reports"
-
-class Reports:
-    """Generate reports from a pydss project"""
-    def __init__(self, results):
-        self._results = results
-        self._report_names = []
-        self._settings = results.simulation_config
-        for report in results.simulation_config.reports.types:
-            if report.enabled:
-                self._report_names.append(report.name)
-        self._output_dir = os.path.join(results.project_path, REPORTS_DIR)
-        os.makedirs(self._output_dir, exist_ok=True)
-
-    @staticmethod
-    def append_required_exports(exports, settings: SimulationSettingsModel):
-        """Append export properties required by the configured reports.
-
-        Parameters
-        ----------
-        exports : ExportListReader
-        settings : SimulationSettingsModel
-
-        """
-        all_reports = Reports.get_all_reports()
-        report_settings = settings.reports
-        if not report_settings:
-            return
-
-        existing_scenarios = {x.name for x in settings.project.scenarios}
-        for report in report_settings.types:
-            if not report.enabled:
-                continue
-            name = report.name
-            if name not in all_reports:
-                raise InvalidConfiguration(f"{name} is not a valid report")
-
-            required_scenarios = all_reports[name].get_required_scenario_names()
-            missing = required_scenarios.difference(existing_scenarios)
-            if missing:
-                text = " ".join(missing)
-                raise InvalidConfiguration(f"{name} requires these scenarios: {text}")
-
-            scenarios = report.scenarios
-            active_scenario = settings.project.active_scenario
-            if scenarios and active_scenario not in scenarios:
-                logger.debug("report %s is not enabled for scenario %s", name,
-                             active_scenario)
-                continue
-
-            required = all_reports[name].get_required_exports(settings)
-            for elem_class, required_properties in required.items():
-                for req_prop in required_properties:
-                    found = False
-                    store_type = req_prop.get("store_values_type", "all")
-                    for prop in exports.list_element_properties(elem_class):
-                        if prop.name == req_prop["property"] and \
-                                prop.store_values_type.value == store_type:
-                            if prop.opendss_classes or req_prop.get("opendss_classes"):
-                                assert prop.sum_elements == req_prop.get("sum_elements", False)
-                                assert prop.data_conversion == \
-                                    req_prop.get("data_conversion", DataConversion.NONE)
-                                prop.append_opendss_classes(req_prop["opendss_classes"])
-                            found = True
-                    if not found:
-                        exports.append_property(elem_class, req_prop)
-                        logger.debug("Add required property: %s %s", elem_class, req_prop)
-
-            all_reports[name].set_required_project_settings(settings)
-
-    @staticmethod
-    def get_all_reports():
-        reports = {}
-
-        def append_reports(report_cls):
-            subclasses = report_cls.__subclasses__()
-            if subclasses:
-                for cls in subclasses:
-                    # Recurse.
-                    append_reports(cls)
-            else:
-                reports[report_cls.NAME] = report_cls
-
-        append_reports(ReportBase)
-        return reports
-
-    @classmethod
-    def generate_reports(cls, results):
-        """Generate all reports specified in the configuration.
-
-        Parameters
-        ----------
-        results : PyDssResults
-
-        Returns
-        -------
-        list
-            list of report filenames
-
-        """
-        reports = Reports(results)
-        return reports.generate()
-
-    def generate(self):
-        """Generate all reports specified in the configuration.
-
-        Returns
-        -------
-        list
-            list of report filenames
-
-        """
-        filenames = []
-        all_reports = self.get_all_reports()
-        for name in self._report_names:
-            report = all_reports[name](name, self._results, self._settings)
-            start = time.time()
-            filename = report.generate(self._output_dir)
-            duration = round(time.time() - start, 3)
-            logger.info("Time to create %s report: %s seconds", name, duration)
-            filenames.append(filename)
-
-        return filenames
-
-
-# Note to devs:  all subclasses of ReportBase need to reside in PyDSS/reports
-# in order to be automatically imported. Otherwise, add a direct import in
-# PyDSS/reports/__init__.py.
-
-class ReportBase(abc.ABC):
-    """Base class for reports"""
-    def __init__(self, name, results, settings):
-        self._results = results
-        self._scenarios = results.scenarios
-        self._settings = settings
-        self._report_global_settings = settings.reports
-        self._report_settings = _get_report_settings(settings, name)
-
-    @abc.abstractmethod
-    def generate(self, output_dir):
-        """Generate a report in output_dir.
-
-        Returns
-        -------
-        str
-            path to report
-
-        """
-
-    @staticmethod
-    @abc.abstractmethod
-    def get_required_exports(simulation_config):
-        """Return the properties required for the report for export.
-
-        Parameters
-        ----------
-        simulation_config: dict
-            settings from simulation config file
-
-        Returns
-        -------
-        dict
-
-        """
-
-    @staticmethod
-    @abc.abstractmethod
-    def get_required_scenario_names():
-        """Return the scenario names that the report expects to be able to retrieve.
-
-        Returns
-        -------
-        set
-            Set of strings
-
-        """
-
-    @staticmethod
-    def get_inputs_from_defaults(settings: SimulationSettingsModel, name):
-        all_reports = Reports.get_all_reports()
-        report_settings = _get_report_settings(settings, name)
-        inputs = copy.deepcopy(getattr(all_reports[name], "DEFAULTS"))
-        for key in type(report_settings).model_fields:
-            inputs[key] = getattr(report_settings, key)
-
-        return inputs
-
-    @staticmethod
-    def set_required_project_settings(settings: SimulationSettingsModel):
-        """Make report-required changes to the simulation config.
-
-        Parameters
-        ----------
-        simulation_config : SimulationSettingsModel
-            Settings to be modified.
-
-        """
-        # Default behavior is no change.
-
-    def _export_dataframe_report(self, df, output_dir, basename):
-        """Export report to a dataframe."""
-        fmt = self._report_global_settings.format
-        filename = os.path.join(output_dir, basename + "." + fmt.value)
-        compress = True if fmt == "h5" else False
-        write_dataframe(df, filename, compress=compress)
-        logger.info("Generated %s", filename)
-        return filename
-
-    def _export_json_report(self, data, output_dir, filename):
-        """Export report to a JSON file."""
-        filename = os.path.join(output_dir, filename)
-        dump_data(data, filename, indent=2, default=make_json_serializable)
-        logger.info("Generated %s", filename)
-
-    def _get_simulation_resolution(self):
-        res = self._settings.project.step_resolution_sec
-        return timedelta(seconds=res)
-
-    def _get_num_steps(self):
-        start, end, step = create_time_range_from_settings(self._settings)
-        return math.ceil((end - start) / step)
-
-    @staticmethod
-    def _params_from_granularity(granularity):
-        if granularity == ReportGranularity.PER_ELEMENT_PER_TIME_POINT:
-            store_values_type = "all"
-            sum_elements = False
-        elif granularity == ReportGranularity.PER_ELEMENT_TOTAL:
-            store_values_type = "sum"
-            sum_elements = False
-        elif granularity == ReportGranularity.ALL_ELEMENTS_PER_TIME_POINT:
-            store_values_type = "all"
-            sum_elements = True
-        elif granularity == ReportGranularity.ALL_ELEMENTS_TOTAL:
-            store_values_type = "sum"
-            sum_elements = True
-        else:
-            assert False, str(granularity)
-
-        return store_values_type, sum_elements
-
-
-def _get_report_settings(settings: SimulationSettingsModel, name):
-    for report in settings.reports.types:
-        if report.name == name:
-            return report
-
-    assert False, f"{name} is not present"
+"""Creates reports on data exported by PyDSS"""
+
+from datetime import timedelta
+import time
+import copy
+import math
+import abc
+import os
+
+from loguru import logger
+
+from pydss.common import DataConversion, ReportGranularity
+from pydss.exceptions import InvalidConfiguration
+from pydss.simulation_input_models import SimulationSettingsModel
+from pydss.utils.dataframe_utils import write_dataframe
+from pydss.utils.simulation_utils import create_time_range_from_settings
+from pydss.utils.utils import dump_data, make_json_serializable
+
+
+REPORTS_DIR = "Reports"
+
+class Reports:
+    """Generate reports from a pydss project"""
+    def __init__(self, results):
+        self._results = results
+        self._report_names = []
+        self._settings = results.simulation_config
+        for report in results.simulation_config.reports.types:
+            if report.enabled:
+                self._report_names.append(report.name)
+        self._output_dir = os.path.join(results.project_path, REPORTS_DIR)
+        os.makedirs(self._output_dir, exist_ok=True)
+
+    @staticmethod
+    def append_required_exports(exports, settings: SimulationSettingsModel):
+        """Append export properties required by the configured reports.
+
+        Parameters
+        ----------
+        exports : ExportListReader
+        settings : SimulationSettingsModel
+
+        """
+        all_reports = Reports.get_all_reports()
+        report_settings = settings.reports
+        if not report_settings:
+            return
+
+        existing_scenarios = {x.name for x in settings.project.scenarios}
+        for report in report_settings.types:
+            if not report.enabled:
+                continue
+            name = report.name
+            if name not in all_reports:
+                raise InvalidConfiguration(f"{name} is not a valid report")
+
+            required_scenarios = all_reports[name].get_required_scenario_names()
+            missing = required_scenarios.difference(existing_scenarios)
+            if missing:
+                text = " ".join(missing)
+                raise InvalidConfiguration(f"{name} requires these scenarios: {text}")
+
+            scenarios = report.scenarios
+            active_scenario = settings.project.active_scenario
+            if scenarios and active_scenario not in scenarios:
+                logger.debug("report %s is not enabled for scenario %s", name,
+                             active_scenario)
+                continue
+
+            required = all_reports[name].get_required_exports(settings)
+            for elem_class, required_properties in required.items():
+                for req_prop in required_properties:
+                    found = False
+                    store_type = req_prop.get("store_values_type", "all")
+                    for prop in exports.list_element_properties(elem_class):
+                        if prop.name == req_prop["property"] and \
+                                prop.store_values_type.value == store_type:
+                            if prop.opendss_classes or req_prop.get("opendss_classes"):
+                                assert prop.sum_elements == req_prop.get("sum_elements", False)
+                                assert prop.data_conversion == \
+                                    req_prop.get("data_conversion", DataConversion.NONE)
+                                prop.append_opendss_classes(req_prop["opendss_classes"])
+                            found = True
+                    if not found:
+                        exports.append_property(elem_class, req_prop)
+                        logger.debug("Add required property: %s %s", elem_class, req_prop)
+
+            all_reports[name].set_required_project_settings(settings)
+
+    @staticmethod
+    def get_all_reports():
+        reports = {}
+
+        def append_reports(report_cls):
+            subclasses = report_cls.__subclasses__()
+            if subclasses:
+                for cls in subclasses:
+                    # Recurse.
+                    append_reports(cls)
+            else:
+                reports[report_cls.NAME] = report_cls
+
+        append_reports(ReportBase)
+        return reports
+
+    @classmethod
+    def generate_reports(cls, results):
+        """Generate all reports specified in the configuration.
+
+        Parameters
+        ----------
+        results : PyDssResults
+
+        Returns
+        -------
+        list
+            list of report filenames
+
+        """
+        reports = Reports(results)
+        return reports.generate()
+
+    def generate(self):
+        """Generate all reports specified in the configuration.
+
+        Returns
+        -------
+        list
+            list of report filenames
+
+        """
+        filenames = []
+        all_reports = self.get_all_reports()
+        for name in self._report_names:
+            report = all_reports[name](name, self._results, self._settings)
+            start = time.time()
+            filename = report.generate(self._output_dir)
+            duration = round(time.time() - start, 3)
+            logger.info("Time to create %s report: %s seconds", name, duration)
+            filenames.append(filename)
+
+        return filenames
+
+
+# Note to devs:  all subclasses of ReportBase need to reside in PyDSS/reports
+# in order to be automatically imported. Otherwise, add a direct import in
+# PyDSS/reports/__init__.py.
+
+class ReportBase(abc.ABC):
+    """Base class for reports"""
+    def __init__(self, name, results, settings):
+        self._results = results
+        self._scenarios = results.scenarios
+        self._settings = settings
+        self._report_global_settings = settings.reports
+        self._report_settings = _get_report_settings(settings, name)
+
+    @abc.abstractmethod
+    def generate(self, output_dir):
+        """Generate a report in output_dir.
+
+        Returns
+        -------
+        str
+            path to report
+
+        """
+
+    @staticmethod
+    @abc.abstractmethod
+    def get_required_exports(simulation_config):
+        """Return the properties required for the report for export.
+
+        Parameters
+        ----------
+        simulation_config: dict
+            settings from simulation config file
+
+        Returns
+        -------
+        dict
+
+        """
+
+    @staticmethod
+    @abc.abstractmethod
+    def get_required_scenario_names():
+        """Return the scenario names that the report expects to be able to retrieve.
+
+        Returns
+        -------
+        set
+            Set of strings
+
+        """
+
+    @staticmethod
+    def get_inputs_from_defaults(settings: SimulationSettingsModel, name):
+        all_reports = Reports.get_all_reports()
+        report_settings = _get_report_settings(settings, name)
+        inputs = copy.deepcopy(getattr(all_reports[name], "DEFAULTS"))
+        for key in type(report_settings).model_fields:
+            inputs[key] = getattr(report_settings, key)
+
+        return inputs
+
+    @staticmethod
+    def set_required_project_settings(settings: SimulationSettingsModel):
+        """Make report-required changes to the simulation config.
+
+        Parameters
+        ----------
+        simulation_config : SimulationSettingsModel
+            Settings to be modified.
+
+        """
+        # Default behavior is no change.
+
+    def _export_dataframe_report(self, df, output_dir, basename):
+        """Export report to a dataframe."""
+        fmt = self._report_global_settings.format
+        filename = os.path.join(output_dir, basename + "." + fmt.value)
+        compress = True if fmt == "h5" else False
+        write_dataframe(df, filename, compress=compress)
+        logger.info("Generated %s", filename)
+        return filename
+
+    def _export_json_report(self, data, output_dir, filename):
+        """Export report to a JSON file."""
+        filename = os.path.join(output_dir, filename)
+        dump_data(data, filename, indent=2, default=make_json_serializable)
+        logger.info("Generated %s", filename)
+
+    def _get_simulation_resolution(self):
+        res = self._settings.project.step_resolution_sec
+        return timedelta(seconds=res)
+
+    def _get_num_steps(self):
+        start, end, step = create_time_range_from_settings(self._settings)
+        return math.ceil((end - start) / step)
+
+    @staticmethod
+    def _params_from_granularity(granularity):
+        if granularity == ReportGranularity.PER_ELEMENT_PER_TIME_POINT:
+            store_values_type = "all"
+            sum_elements = False
+        elif granularity == ReportGranularity.PER_ELEMENT_TOTAL:
+            store_values_type = "sum"
+            sum_elements = False
+        elif granularity == ReportGranularity.ALL_ELEMENTS_PER_TIME_POINT:
+            store_values_type = "all"
+            sum_elements = True
+        elif granularity == ReportGranularity.ALL_ELEMENTS_TOTAL:
+            store_values_type = "sum"
+            sum_elements = True
+        else:
+            assert False, str(granularity)
+
+        return store_values_type, sum_elements
+
+
+def _get_report_settings(settings: SimulationSettingsModel, name):
+    for report in settings.reports.types:
+        if report.name == name:
+            return report
+
+    assert False, f"{name} is not present"
```

### Comparing `nrel_pydss-3.1.3/src/pydss/reports/thermal_metrics.py` & `nrel_pydss-3.1.4/src/pydss/reports/thermal_metrics.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,185 +1,185 @@
-from datetime import timedelta
-import os
-
-from loguru import logger
-
-from pydss.exceptions import InvalidConfiguration
-from pydss.reports.reports import ReportBase
-from pydss.thermal_metrics import (
-    SimulationThermalMetricsModel,
-    ThermalMetricsSummaryModel,
-    ThermalMetricsModel,
-)
-from pydss.utils.utils import load_data
-
-class ThermalMetrics(ReportBase):
-    """Reports thermal metrics.
-
-    The metrics are defined in this paper:
-    https://www.sciencedirect.com/science/article/pii/S0306261920311351
-
-    The report generates the output file Reports/thermal_metrics.json.
-
-    """
-
-    DEFAULTS = {
-        "line_window_size_hours": 1,
-        "line_loading_percent_threshold": 120,
-        "line_loading_percent_moving_average_threshold": 100,
-        "transformer_loading_percent_threshold": 150,
-        "transformer_window_size_hours": 2,
-        "transformer_loading_percent_moving_average_threshold": 120,
-        "store_all_time_points": False,
-        "store_per_element_data": True,
-    }
-    FILENAME = "thermal_metrics.json"
-    NAME = "Thermal Metrics"
-
-    def __init__(self, name, results, simulation_config):
-        super().__init__(name, results, simulation_config)
-        self._num_lines = 0
-        self._num_transformers = 0
-        self._resolution = self._get_simulation_resolution()
-        self._files_to_delete = []
-
-    def generate(self, output_dir):
-        inputs = ThermalMetrics.get_inputs_from_defaults(self._settings, self.NAME)
-        if inputs["store_all_time_points"]:
-            scenarios = self._generate_from_all_time_points()
-        else:
-            scenarios = self._generate_from_in_memory_metrics()
-
-        model = SimulationThermalMetricsModel(scenarios=scenarios)
-        filename = os.path.join(output_dir, self.FILENAME)
-        with open(filename, "w") as f_out:
-            f_out.write(model.model_dump_json(indent=2))
-            f_out.write("\n")
-
-        logger.info("Generated %s", filename)
-        for filename in self._files_to_delete:
-            os.remove(filename)
-
-    def _generate_from_in_memory_metrics(self):
-        scenarios = {}
-        for scenario in self._results.scenarios:
-            filename = os.path.join(
-                str(self._settings.project.active_project_path),
-                "Exports",
-                scenario.name,
-                self.FILENAME,
-            )
-            scenarios[scenario.name] = ThermalMetricsSummaryModel(**load_data(filename))
-            # We won't need this file after we write the consolidated file.
-            self._files_to_delete.append(filename)
-
-        return scenarios
-
-    def _generate_from_all_time_points(self):
-        inputs = self.get_inputs_from_defaults(self._settings, self.NAME)
-        line_window_size, transformer_window_size = self._get_window_sizes(inputs, self._resolution)
-        scenarios = {}
-        for scenario in self._results.scenarios:
-            df = scenario.get_full_dataframe("CktElement", "ExportLoadingsMetric")
-            # Remove the property label, like "__Loading" from the column.
-            df.columns = [x.split("__")[0] for x in df.columns]
-            line_columns = []
-            transform_columns = []
-            for col in df.columns:
-                if col.startswith("Line"):
-                    line_columns.append(col)
-                elif col.startswith("Transformer"):
-                    transform_columns.append(col)
-            df_lines = df[line_columns]
-            lines_model = self._make_thermal_metrics_model(
-                df_lines,
-                line_window_size,
-                inputs["line_window_size_hours"],
-                inputs["line_loading_percent_threshold"],
-                inputs["line_loading_percent_moving_average_threshold"],
-            )
-            df_transformers = df[transform_columns]
-            transformers_model = self._make_thermal_metrics_model(
-                df_transformers,
-                transformer_window_size,
-                inputs["transformer_window_size_hours"],
-                inputs["transformer_loading_percent_threshold"],
-                inputs["transformer_loading_percent_moving_average_threshold"],
-            )
-            scenarios[scenario.name] = ThermalMetricsSummaryModel(
-                line_loadings=lines_model,
-                transformer_loadings=transformers_model,
-            )
-
-        return scenarios
-
-    def _make_thermal_metrics_model(self, df, window_size, window_size_hours, inst_threshold, mavg_threshold):
-        df_mavg = df.rolling(window=window_size).mean()
-        max_instantaneous = self._get_max_values(df)
-        max_mavg = self._get_max_values(df_mavg)
-        return ThermalMetricsModel(
-            max_instantaneous_loadings_pct=max_instantaneous,
-            max_instantaneous_loading_pct=max(max_instantaneous.values()),
-            max_moving_average_loadings_pct=max_mavg,
-            max_moving_average_loading_pct=max(max_mavg.values()),
-            window_size_hours=window_size_hours,
-            num_time_points_with_instantaneous_violations=self._get_num_time_points_with_violations(df, inst_threshold),
-            num_time_points_with_moving_average_violations=self._get_num_time_points_with_violations(df_mavg, mavg_threshold),
-            instantaneous_threshold=inst_threshold,
-            moving_average_threshold=mavg_threshold,
-        )
-
-    @staticmethod
-    def _get_window_sizes(inputs, resolution):
-        line_window_size = timedelta(hours=inputs["line_window_size_hours"])
-        if line_window_size % resolution != timedelta(0):
-            raise InvalidConfiguration(
-                f"line_window_size={line_window_size} must be a multiple of {resolution}"
-            )
-        transformer_window_size = timedelta(hours=inputs["transformer_window_size_hours"])
-        if transformer_window_size % resolution != timedelta(0):
-            raise InvalidConfiguration(
-                f"transformer_window_size={transformer_window_size} must be a multiple of {resolution}"
-            )
-        return line_window_size // resolution, transformer_window_size // resolution
-
-    @staticmethod
-    def _get_max_values(df):
-        """Return a dict with max values per column."""
-        return {x: df[x].max() for x in df.columns}
-
-    @staticmethod
-    def _get_num_time_points_with_violations(df, threshold):
-        """Return the number of time points where at least one value exceeds threshold."""
-        num_violations = 0
-        for i, row in df.iterrows():
-            if row.max() > threshold:
-                num_violations += 1
-
-        return num_violations
-
-    @staticmethod
-    def get_required_exports(simulation_config):
-        inputs = ThermalMetrics.get_inputs_from_defaults(simulation_config, ThermalMetrics.NAME)
-        if inputs["store_all_time_points"]:
-            return {
-                "CktElement": [
-                    {
-                        "property": "ExportLoadingsMetric",
-                        "store_values_type": "all",
-                        "opendss_classes": ["Lines", "Transformers"],
-                    }
-                ]
-            }
-
-        return {
-            "CktElement": [
-                {
-                    "property": "OverloadsMetricInMemory",
-                    "opendss_classes": ["Lines", "Transformers"],
-                }
-             ]
-        }
-
-    @staticmethod
-    def get_required_scenario_names():
-        return set()
+from datetime import timedelta
+import os
+
+from loguru import logger
+
+from pydss.exceptions import InvalidConfiguration
+from pydss.reports.reports import ReportBase
+from pydss.thermal_metrics import (
+    SimulationThermalMetricsModel,
+    ThermalMetricsSummaryModel,
+    ThermalMetricsModel,
+)
+from pydss.utils.utils import load_data
+
+class ThermalMetrics(ReportBase):
+    """Reports thermal metrics.
+
+    The metrics are defined in this paper:
+    https://www.sciencedirect.com/science/article/pii/S0306261920311351
+
+    The report generates the output file Reports/thermal_metrics.json.
+
+    """
+
+    DEFAULTS = {
+        "line_window_size_hours": 1,
+        "line_loading_percent_threshold": 120,
+        "line_loading_percent_moving_average_threshold": 100,
+        "transformer_loading_percent_threshold": 150,
+        "transformer_window_size_hours": 2,
+        "transformer_loading_percent_moving_average_threshold": 120,
+        "store_all_time_points": False,
+        "store_per_element_data": True,
+    }
+    FILENAME = "thermal_metrics.json"
+    NAME = "Thermal Metrics"
+
+    def __init__(self, name, results, simulation_config):
+        super().__init__(name, results, simulation_config)
+        self._num_lines = 0
+        self._num_transformers = 0
+        self._resolution = self._get_simulation_resolution()
+        self._files_to_delete = []
+
+    def generate(self, output_dir):
+        inputs = ThermalMetrics.get_inputs_from_defaults(self._settings, self.NAME)
+        if inputs["store_all_time_points"]:
+            scenarios = self._generate_from_all_time_points()
+        else:
+            scenarios = self._generate_from_in_memory_metrics()
+
+        model = SimulationThermalMetricsModel(scenarios=scenarios)
+        filename = os.path.join(output_dir, self.FILENAME)
+        with open(filename, "w") as f_out:
+            f_out.write(model.model_dump_json(indent=2))
+            f_out.write("\n")
+
+        logger.info("Generated %s", filename)
+        for filename in self._files_to_delete:
+            os.remove(filename)
+
+    def _generate_from_in_memory_metrics(self):
+        scenarios = {}
+        for scenario in self._results.scenarios:
+            filename = os.path.join(
+                str(self._settings.project.active_project_path),
+                "Exports",
+                scenario.name,
+                self.FILENAME,
+            )
+            scenarios[scenario.name] = ThermalMetricsSummaryModel(**load_data(filename))
+            # We won't need this file after we write the consolidated file.
+            self._files_to_delete.append(filename)
+
+        return scenarios
+
+    def _generate_from_all_time_points(self):
+        inputs = self.get_inputs_from_defaults(self._settings, self.NAME)
+        line_window_size, transformer_window_size = self._get_window_sizes(inputs, self._resolution)
+        scenarios = {}
+        for scenario in self._results.scenarios:
+            df = scenario.get_full_dataframe("CktElement", "ExportLoadingsMetric")
+            # Remove the property label, like "__Loading" from the column.
+            df.columns = [x.split("__")[0] for x in df.columns]
+            line_columns = []
+            transform_columns = []
+            for col in df.columns:
+                if col.startswith("Line"):
+                    line_columns.append(col)
+                elif col.startswith("Transformer"):
+                    transform_columns.append(col)
+            df_lines = df[line_columns]
+            lines_model = self._make_thermal_metrics_model(
+                df_lines,
+                line_window_size,
+                inputs["line_window_size_hours"],
+                inputs["line_loading_percent_threshold"],
+                inputs["line_loading_percent_moving_average_threshold"],
+            )
+            df_transformers = df[transform_columns]
+            transformers_model = self._make_thermal_metrics_model(
+                df_transformers,
+                transformer_window_size,
+                inputs["transformer_window_size_hours"],
+                inputs["transformer_loading_percent_threshold"],
+                inputs["transformer_loading_percent_moving_average_threshold"],
+            )
+            scenarios[scenario.name] = ThermalMetricsSummaryModel(
+                line_loadings=lines_model,
+                transformer_loadings=transformers_model,
+            )
+
+        return scenarios
+
+    def _make_thermal_metrics_model(self, df, window_size, window_size_hours, inst_threshold, mavg_threshold):
+        df_mavg = df.rolling(window=window_size).mean()
+        max_instantaneous = self._get_max_values(df)
+        max_mavg = self._get_max_values(df_mavg)
+        return ThermalMetricsModel(
+            max_instantaneous_loadings_pct=max_instantaneous,
+            max_instantaneous_loading_pct=max(max_instantaneous.values()),
+            max_moving_average_loadings_pct=max_mavg,
+            max_moving_average_loading_pct=max(max_mavg.values()),
+            window_size_hours=window_size_hours,
+            num_time_points_with_instantaneous_violations=self._get_num_time_points_with_violations(df, inst_threshold),
+            num_time_points_with_moving_average_violations=self._get_num_time_points_with_violations(df_mavg, mavg_threshold),
+            instantaneous_threshold=inst_threshold,
+            moving_average_threshold=mavg_threshold,
+        )
+
+    @staticmethod
+    def _get_window_sizes(inputs, resolution):
+        line_window_size = timedelta(hours=inputs["line_window_size_hours"])
+        if line_window_size % resolution != timedelta(0):
+            raise InvalidConfiguration(
+                f"line_window_size={line_window_size} must be a multiple of {resolution}"
+            )
+        transformer_window_size = timedelta(hours=inputs["transformer_window_size_hours"])
+        if transformer_window_size % resolution != timedelta(0):
+            raise InvalidConfiguration(
+                f"transformer_window_size={transformer_window_size} must be a multiple of {resolution}"
+            )
+        return line_window_size // resolution, transformer_window_size // resolution
+
+    @staticmethod
+    def _get_max_values(df):
+        """Return a dict with max values per column."""
+        return {x: df[x].max() for x in df.columns}
+
+    @staticmethod
+    def _get_num_time_points_with_violations(df, threshold):
+        """Return the number of time points where at least one value exceeds threshold."""
+        num_violations = 0
+        for i, row in df.iterrows():
+            if row.max() > threshold:
+                num_violations += 1
+
+        return num_violations
+
+    @staticmethod
+    def get_required_exports(simulation_config):
+        inputs = ThermalMetrics.get_inputs_from_defaults(simulation_config, ThermalMetrics.NAME)
+        if inputs["store_all_time_points"]:
+            return {
+                "CktElement": [
+                    {
+                        "property": "ExportLoadingsMetric",
+                        "store_values_type": "all",
+                        "opendss_classes": ["Lines", "Transformers"],
+                    }
+                ]
+            }
+
+        return {
+            "CktElement": [
+                {
+                    "property": "OverloadsMetricInMemory",
+                    "opendss_classes": ["Lines", "Transformers"],
+                }
+             ]
+        }
+
+    @staticmethod
+    def get_required_scenario_names():
+        return set()
```

### Comparing `nrel_pydss-3.1.3/src/pydss/reports/voltage_metrics.py` & `nrel_pydss-3.1.4/src/pydss/reports/voltage_metrics.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,278 +1,278 @@
-from datetime import timedelta
-import os
-
-from loguru import logger
-import numpy as np
-
-from pydss.common import MinMax, NODE_NAMES_BY_TYPE_FILENAME
-from pydss.reports.reports import ReportBase, ReportGranularity
-from pydss.utils.utils import serialize_timedelta, deserialize_timedelta, load_data
-from pydss.node_voltage_metrics import (
-    NodeVoltageMetricsByType,
-    SimulationVoltageMetricsModel,
-    VoltageMetricsByBusTypeModel,
-    VoltageMetricsModel,
-    VoltageMetric1,
-    VoltageMetric2,
-    VoltageMetric3,
-    VoltageMetric4,
-    VoltageMetric5,
-    VoltageMetric6,
-)
-
-class VoltageMetrics(ReportBase):
-    """Reports voltage metrics.
-
-    The metrics are defined in this paper:
-    https://www.sciencedirect.com/science/article/pii/S0306261920311351
-
-    The report generates the output file Reports/voltage_metrics.json.
-    Metrics 1, 2, 5, and 6 are included within that file.
-
-    Metric 3 must be read from the raw data as in the example below.
-    Metric 4 must be read from the dataframe-as-binary-file.
-
-    This example assumes that data is stored on a per-time-point basis.
-
-    .. code-block:: python
-
-        from pydss.utils.dataframe_utils import read_dataframe
-        from pydss.pydss_results import PyDssResults
-
-        results = PyDssResults("path_to_project")
-        control_mode_scenario = results.scenarios[1]
-
-        # Read metrics 1, 2, 5, and 6 directly from JSON.
-        voltage_metrics = results.read_report("Voltage Metrics")
-        metric_4_filenames = voltage_metrics["metric_4"]
-        dfs = [read_dataframe(x) for x in filenames]
-
-        # Read all metric 3 dataframes from raw data into memory in one call.
-        dfs = control_mode_scenario.get_filtered_dataframes("Nodes", "VoltageMetric")
-
-        # Read metric 3 dataframes into memory one at a time.
-        for node_name in control_mode_scenario.list_element_names("Nodes", "VoltageMetric"):
-            df = control_mode_scenario.get_dataframe("Nodes", "VoltageMetric", node_name)
-            # If necessary, convert to a moving average with pandas.
-
-    """
-
-    DEFAULTS = {
-        "range_a_limits": [0.95, 1.05],
-        "range_b_limits": [0.90, 1.0583],
-        "window_size_minutes": 10,
-        "store_all_time_points": False,
-        "store_per_element_data": True,
-    }
-    FILENAME = "voltage_metrics.json"
-    NAME = "Voltage Metrics"
-
-    def __init__(self, name, results, simulation_config):
-        super().__init__(name, results, simulation_config)
-        self._granularity = ReportGranularity(
-            self._report_global_settings.granularity
-        )
-        self._range_a_limits = MinMax(
-            min=self._report_settings.range_a_limits[0],
-            max=self._report_settings.range_a_limits[1],
-        )
-        self._range_b_limits = MinMax(
-            min=self._report_settings.range_b_limits[0],
-            max=self._report_settings.range_b_limits[1],
-        )
-        self._resolution = self._get_simulation_resolution()
-        inputs = self.get_inputs_from_defaults(self._settings, self.NAME)
-        self._window_size = timedelta(minutes=inputs["window_size_minutes"]) // self._resolution
-        self._moving_window_minutes = inputs["window_size_minutes"]
-        self._files_to_delete = []
-
-    def generate(self, output_dir):
-        inputs = VoltageMetrics.get_inputs_from_defaults(self._settings, self.NAME)
-        if inputs["store_all_time_points"]:
-            scenarios = self._generate_from_all_time_points()
-        else:
-            scenarios = self._generate_from_in_memory_metrics()
-        model = SimulationVoltageMetricsModel(scenarios=scenarios)
-
-        filename = os.path.join(output_dir, self.FILENAME)
-        with open(filename, "w") as f_out:
-            f_out.write(model.model_dump_json(indent=2))
-            f_out.write("\n")
-
-        logger.info("Generated %s", filename)
-        for filename in self._files_to_delete:
-            os.remove(filename)
-
-    def _generate_from_in_memory_metrics(self):
-        scenarios = {}
-        for scenario in self._results.scenarios:
-            filename = os.path.join(
-                str(self._settings.project.active_project_path),
-                "Exports",
-                scenario.name,
-                self.FILENAME,
-            )
-            scenarios[scenario.name] = VoltageMetricsByBusTypeModel(**load_data(filename))
-            # We won't need this file after we write the consolidated file.
-            self._files_to_delete.append(filename)
-
-        return scenarios
-
-    def _generate_from_all_time_points(self):
-        scenarios = {}
-        for scenario in self._results.scenarios:
-            filename = os.path.join(
-                str(self._settings.project.active_project_path),
-                "Exports",
-                scenario.name,
-                NODE_NAMES_BY_TYPE_FILENAME,
-            )
-            node_names_by_type = load_data(filename)
-            assert len(set(node_names_by_type["primaries"])) == len(node_names_by_type["primaries"])
-            assert len(set(node_names_by_type["secondaries"])) == len(node_names_by_type["secondaries"])
-            df = scenario.get_full_dataframe("Buses", "puVmagAngle", mag_ang="mag")
-            columns = []
-            for column in df.columns:
-                # Make the names match the results from NodeVoltageMetrics.
-                column = column.replace("__mag [pu]", "")
-                column = column.replace("__A1", ".1")
-                column = column.replace("__B1", ".2")
-                column = column.replace("__C1", ".3")
-                columns.append(column)
-            df.columns = columns
-
-            by_type = {}
-            for node_type in ("primaries", "secondaries"):
-                df_by_type = df[node_names_by_type[node_type]]
-                by_type[node_type] = self._gen_metrics(df_by_type)
-            scenarios[scenario.name] = VoltageMetricsByBusTypeModel(**by_type)
-
-        return scenarios
-
-    def _gen_metrics(self, df):
-        assert len(df) > 0
-        metric_2 = {x: 0 for x in df.columns}
-        metric_4 = []
-        metric_5_min = {x: None for x in df.columns}
-        metric_5_max = {x: None for x in df.columns}
-        metric_1_violation_time_points = []
-        num_time_points_violate_range_b = 0
-        for timestamp, row in df.iterrows():
-            num_range_a_violations = 0
-            for name, val in row.items():
-                if val > self._range_a_limits.max or val < self._range_a_limits.min:
-                    metric_2[name] += 1
-                    num_range_a_violations += 1
-                cur_min = metric_5_min[name]
-                cur_max = metric_5_max[name]
-                if not np.isnan(val):
-                    if cur_min is None or val < cur_min:
-                        metric_5_min[name] = val
-                    if cur_max is None or val > cur_max:
-                        metric_5_max[name] = val
-            max_val = row.max()
-            min_val = row.min()
-            if (
-                    not (max_val > self._range_b_limits.max)
-                    and not (min_val < self._range_b_limits.min)
-                    and (max_val > self._range_a_limits.max or min_val < self._range_a_limits.min)
-            ):
-                metric_1_violation_time_points.append(timestamp)
-            if max_val > self._range_b_limits.max or min_val < self._range_b_limits.min:
-                num_time_points_violate_range_b += 1
-
-            if num_range_a_violations > 0:
-                metric_4.append([timestamp, num_range_a_violations / len(df.columns) * 100])
-
-        df_mavg = df.rolling(window=self._window_size).mean()
-        metric_3 = []
-        for timestamp, row in df_mavg.iterrows():
-            max_val = row.max()
-            min_val = row.min()
-            if max_val > self._range_a_limits.max or min_val < self._range_a_limits.min:
-                metric_3.append(timestamp)
-
-        vmetric_1 = VoltageMetric1(
-            time_points=metric_1_violation_time_points,
-            duration=len(metric_1_violation_time_points) * self._resolution,
-        )
-        vmetric_2 = {
-            name: VoltageMetric2(
-                duration=val * self._resolution,
-                duration_percentage=val / len(df) * 100,
-            )
-            for name, val in metric_2.items()
-        }
-        vmetric_3 = VoltageMetric3(
-            time_points=metric_3,
-            duration=len(metric_3) * self._resolution,
-        )
-        vmetric_4 = VoltageMetric4(
-            percent_node_ansi_a_violations=metric_4,
-        )
-        vmetric_5 = VoltageMetric5(
-            min_voltages=metric_5_min,
-            max_voltages=metric_5_max,
-        )
-        vmetric_6 = VoltageMetric6(
-            num_time_points=num_time_points_violate_range_b,
-            percent_time_points=num_time_points_violate_range_b / len(df) * 100,
-            duration=num_time_points_violate_range_b * self._resolution,
-        )
-
-        return VoltageMetricsModel(
-            metric_1=vmetric_1,
-            metric_2=vmetric_2,
-            metric_3=vmetric_3,
-            metric_4=vmetric_4,
-            metric_5=vmetric_5,
-            metric_6=vmetric_6,
-            summary=NodeVoltageMetricsByType.create_summary(
-                vmetric_1, vmetric_2, vmetric_3, vmetric_5, vmetric_6, list(df.columns),
-                len(df), self._resolution, self._range_a_limits, self._range_b_limits,
-                self._moving_window_minutes
-            )
-        )
-
-    @staticmethod
-    def get_required_exports(simulation_config):
-        inputs = VoltageMetrics.get_inputs_from_defaults(
-            simulation_config, VoltageMetrics.NAME
-        )
-        if inputs["store_all_time_points"]:
-            return {
-                # TODO: This should use Circuit.AllBusMagPu for performance reasons.
-                # That reads all voltages in one command but tracks them by bus index.
-                # The code would have to map bus index to bus name.
-                "Buses": [
-                    {
-                        "property": "puVmagAngle",
-                        "store_values_type": "all",
-                    }
-                ]
-            }
-
-        return {
-            "Nodes": [
-                {
-                    "property": "VoltageMetric",
-                    "store_values_type": "all",
-                    "limits": inputs["range_a_limits"],
-                    "limits_b": inputs["range_b_limits"],
-                },
-            ]
-        }
-
-    @staticmethod
-    def get_required_scenario_names():
-        return set()
-
-    @staticmethod
-    def set_required_project_settings(settings):
-        inputs = VoltageMetrics.get_inputs_from_defaults(
-            settings, VoltageMetrics.NAME
-        )
-        exports = settings.exports
-        if inputs["store_all_time_points"] and not exports.export_node_names_by_type:
-            exports.export_node_names_by_type = True
-            logger.info("Enabled Export Node Names By Type")
+from datetime import timedelta
+import os
+
+from loguru import logger
+import numpy as np
+
+from pydss.common import MinMax, NODE_NAMES_BY_TYPE_FILENAME
+from pydss.reports.reports import ReportBase, ReportGranularity
+from pydss.utils.utils import serialize_timedelta, deserialize_timedelta, load_data
+from pydss.node_voltage_metrics import (
+    NodeVoltageMetricsByType,
+    SimulationVoltageMetricsModel,
+    VoltageMetricsByBusTypeModel,
+    VoltageMetricsModel,
+    VoltageMetric1,
+    VoltageMetric2,
+    VoltageMetric3,
+    VoltageMetric4,
+    VoltageMetric5,
+    VoltageMetric6,
+)
+
+class VoltageMetrics(ReportBase):
+    """Reports voltage metrics.
+
+    The metrics are defined in this paper:
+    https://www.sciencedirect.com/science/article/pii/S0306261920311351
+
+    The report generates the output file Reports/voltage_metrics.json.
+    Metrics 1, 2, 5, and 6 are included within that file.
+
+    Metric 3 must be read from the raw data as in the example below.
+    Metric 4 must be read from the dataframe-as-binary-file.
+
+    This example assumes that data is stored on a per-time-point basis.
+
+    .. code-block:: python
+
+        from pydss.utils.dataframe_utils import read_dataframe
+        from pydss.pydss_results import PyDssResults
+
+        results = PyDssResults("path_to_project")
+        control_mode_scenario = results.scenarios[1]
+
+        # Read metrics 1, 2, 5, and 6 directly from JSON.
+        voltage_metrics = results.read_report("Voltage Metrics")
+        metric_4_filenames = voltage_metrics["metric_4"]
+        dfs = [read_dataframe(x) for x in filenames]
+
+        # Read all metric 3 dataframes from raw data into memory in one call.
+        dfs = control_mode_scenario.get_filtered_dataframes("Nodes", "VoltageMetric")
+
+        # Read metric 3 dataframes into memory one at a time.
+        for node_name in control_mode_scenario.list_element_names("Nodes", "VoltageMetric"):
+            df = control_mode_scenario.get_dataframe("Nodes", "VoltageMetric", node_name)
+            # If necessary, convert to a moving average with pandas.
+
+    """
+
+    DEFAULTS = {
+        "range_a_limits": [0.95, 1.05],
+        "range_b_limits": [0.90, 1.0583],
+        "window_size_minutes": 10,
+        "store_all_time_points": False,
+        "store_per_element_data": True,
+    }
+    FILENAME = "voltage_metrics.json"
+    NAME = "Voltage Metrics"
+
+    def __init__(self, name, results, simulation_config):
+        super().__init__(name, results, simulation_config)
+        self._granularity = ReportGranularity(
+            self._report_global_settings.granularity
+        )
+        self._range_a_limits = MinMax(
+            min=self._report_settings.range_a_limits[0],
+            max=self._report_settings.range_a_limits[1],
+        )
+        self._range_b_limits = MinMax(
+            min=self._report_settings.range_b_limits[0],
+            max=self._report_settings.range_b_limits[1],
+        )
+        self._resolution = self._get_simulation_resolution()
+        inputs = self.get_inputs_from_defaults(self._settings, self.NAME)
+        self._window_size = timedelta(minutes=inputs["window_size_minutes"]) // self._resolution
+        self._moving_window_minutes = inputs["window_size_minutes"]
+        self._files_to_delete = []
+
+    def generate(self, output_dir):
+        inputs = VoltageMetrics.get_inputs_from_defaults(self._settings, self.NAME)
+        if inputs["store_all_time_points"]:
+            scenarios = self._generate_from_all_time_points()
+        else:
+            scenarios = self._generate_from_in_memory_metrics()
+        model = SimulationVoltageMetricsModel(scenarios=scenarios)
+
+        filename = os.path.join(output_dir, self.FILENAME)
+        with open(filename, "w") as f_out:
+            f_out.write(model.model_dump_json(indent=2))
+            f_out.write("\n")
+
+        logger.info("Generated %s", filename)
+        for filename in self._files_to_delete:
+            os.remove(filename)
+
+    def _generate_from_in_memory_metrics(self):
+        scenarios = {}
+        for scenario in self._results.scenarios:
+            filename = os.path.join(
+                str(self._settings.project.active_project_path),
+                "Exports",
+                scenario.name,
+                self.FILENAME,
+            )
+            scenarios[scenario.name] = VoltageMetricsByBusTypeModel(**load_data(filename))
+            # We won't need this file after we write the consolidated file.
+            self._files_to_delete.append(filename)
+
+        return scenarios
+
+    def _generate_from_all_time_points(self):
+        scenarios = {}
+        for scenario in self._results.scenarios:
+            filename = os.path.join(
+                str(self._settings.project.active_project_path),
+                "Exports",
+                scenario.name,
+                NODE_NAMES_BY_TYPE_FILENAME,
+            )
+            node_names_by_type = load_data(filename)
+            assert len(set(node_names_by_type["primaries"])) == len(node_names_by_type["primaries"])
+            assert len(set(node_names_by_type["secondaries"])) == len(node_names_by_type["secondaries"])
+            df = scenario.get_full_dataframe("Buses", "puVmagAngle", mag_ang="mag")
+            columns = []
+            for column in df.columns:
+                # Make the names match the results from NodeVoltageMetrics.
+                column = column.replace("__mag [pu]", "")
+                column = column.replace("__A1", ".1")
+                column = column.replace("__B1", ".2")
+                column = column.replace("__C1", ".3")
+                columns.append(column)
+            df.columns = columns
+
+            by_type = {}
+            for node_type in ("primaries", "secondaries"):
+                df_by_type = df[node_names_by_type[node_type]]
+                by_type[node_type] = self._gen_metrics(df_by_type)
+            scenarios[scenario.name] = VoltageMetricsByBusTypeModel(**by_type)
+
+        return scenarios
+
+    def _gen_metrics(self, df):
+        assert len(df) > 0
+        metric_2 = {x: 0 for x in df.columns}
+        metric_4 = []
+        metric_5_min = {x: None for x in df.columns}
+        metric_5_max = {x: None for x in df.columns}
+        metric_1_violation_time_points = []
+        num_time_points_violate_range_b = 0
+        for timestamp, row in df.iterrows():
+            num_range_a_violations = 0
+            for name, val in row.items():
+                if val > self._range_a_limits.max or val < self._range_a_limits.min:
+                    metric_2[name] += 1
+                    num_range_a_violations += 1
+                cur_min = metric_5_min[name]
+                cur_max = metric_5_max[name]
+                if not np.isnan(val):
+                    if cur_min is None or val < cur_min:
+                        metric_5_min[name] = val
+                    if cur_max is None or val > cur_max:
+                        metric_5_max[name] = val
+            max_val = row.max()
+            min_val = row.min()
+            if (
+                    not (max_val > self._range_b_limits.max)
+                    and not (min_val < self._range_b_limits.min)
+                    and (max_val > self._range_a_limits.max or min_val < self._range_a_limits.min)
+            ):
+                metric_1_violation_time_points.append(timestamp)
+            if max_val > self._range_b_limits.max or min_val < self._range_b_limits.min:
+                num_time_points_violate_range_b += 1
+
+            if num_range_a_violations > 0:
+                metric_4.append([timestamp, num_range_a_violations / len(df.columns) * 100])
+
+        df_mavg = df.rolling(window=self._window_size).mean()
+        metric_3 = []
+        for timestamp, row in df_mavg.iterrows():
+            max_val = row.max()
+            min_val = row.min()
+            if max_val > self._range_a_limits.max or min_val < self._range_a_limits.min:
+                metric_3.append(timestamp)
+
+        vmetric_1 = VoltageMetric1(
+            time_points=metric_1_violation_time_points,
+            duration=len(metric_1_violation_time_points) * self._resolution,
+        )
+        vmetric_2 = {
+            name: VoltageMetric2(
+                duration=val * self._resolution,
+                duration_percentage=val / len(df) * 100,
+            )
+            for name, val in metric_2.items()
+        }
+        vmetric_3 = VoltageMetric3(
+            time_points=metric_3,
+            duration=len(metric_3) * self._resolution,
+        )
+        vmetric_4 = VoltageMetric4(
+            percent_node_ansi_a_violations=metric_4,
+        )
+        vmetric_5 = VoltageMetric5(
+            min_voltages=metric_5_min,
+            max_voltages=metric_5_max,
+        )
+        vmetric_6 = VoltageMetric6(
+            num_time_points=num_time_points_violate_range_b,
+            percent_time_points=num_time_points_violate_range_b / len(df) * 100,
+            duration=num_time_points_violate_range_b * self._resolution,
+        )
+
+        return VoltageMetricsModel(
+            metric_1=vmetric_1,
+            metric_2=vmetric_2,
+            metric_3=vmetric_3,
+            metric_4=vmetric_4,
+            metric_5=vmetric_5,
+            metric_6=vmetric_6,
+            summary=NodeVoltageMetricsByType.create_summary(
+                vmetric_1, vmetric_2, vmetric_3, vmetric_5, vmetric_6, list(df.columns),
+                len(df), self._resolution, self._range_a_limits, self._range_b_limits,
+                self._moving_window_minutes
+            )
+        )
+
+    @staticmethod
+    def get_required_exports(simulation_config):
+        inputs = VoltageMetrics.get_inputs_from_defaults(
+            simulation_config, VoltageMetrics.NAME
+        )
+        if inputs["store_all_time_points"]:
+            return {
+                # TODO: This should use Circuit.AllBusMagPu for performance reasons.
+                # That reads all voltages in one command but tracks them by bus index.
+                # The code would have to map bus index to bus name.
+                "Buses": [
+                    {
+                        "property": "puVmagAngle",
+                        "store_values_type": "all",
+                    }
+                ]
+            }
+
+        return {
+            "Nodes": [
+                {
+                    "property": "VoltageMetric",
+                    "store_values_type": "all",
+                    "limits": inputs["range_a_limits"],
+                    "limits_b": inputs["range_b_limits"],
+                },
+            ]
+        }
+
+    @staticmethod
+    def get_required_scenario_names():
+        return set()
+
+    @staticmethod
+    def set_required_project_settings(settings):
+        inputs = VoltageMetrics.get_inputs_from_defaults(
+            settings, VoltageMetrics.NAME
+        )
+        exports = settings.exports
+        if inputs["store_all_time_points"] and not exports.export_node_names_by_type:
+            exports.export_node_names_by_type = True
+            logger.info("Enabled Export Node Names By Type")
```

### Comparing `nrel_pydss-3.1.3/src/pydss/utils/dataframe_utils.py` & `nrel_pydss-3.1.4/src/pydss/utils/dataframe_utils.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,153 +1,153 @@
-
-import shutil
-import gzip
-import os
-
-from loguru import logger
-import pandas as pd
-
-from pydss.exceptions import InvalidParameter
-
-
-def read_dataframe(filename, index_col=None, columns=None, parse_dates=False,
-                   remove_unnamed=True, strip_column_units=False, **kwargs):
-    """Convert filename to a dataframe. Supports .csv, .json, .h5.
-    Handles compressed files.
-
-    Parameters
-    ----------
-    filename : str
-    index_col : str | int | None
-        Index column name or index
-    columns : list or None
-        Use these columns if the file is CSV and does not define them.
-    parse_dates : bool
-    remove_unnamed : bool
-        Remove any column that starts with "Unnamed".
-    strip_column_units : bool
-        Remove units from column names.
-    kwargs : kwargs
-        Passed to underlying library for dataframe conversion.
-        Consider setting parse_dates=True if the index is a timestamp.
-
-    Returns
-    -------
-    pd.DataFrame
-
-    Raises
-    ------
-    FileNotFoundError
-        Raised if the file does not exist.
-
-    """
-    if not os.path.exists(filename):
-        raise FileNotFoundError("filename={} does not exist".format(filename))
-
-    needs_new_index = False
-    ext = os.path.splitext(filename)
-    if ext[1] == ".gz":
-        ext = os.path.splitext(ext[0])[1]
-        open_func = gzip.open
-    else:
-        ext = ext[1]
-        open_func = open
-
-    if ext == ".csv":
-        df = pd.read_csv(filename, index_col=index_col, usecols=columns,
-                         parse_dates=parse_dates, **kwargs)
-    elif ext == ".json":
-        df = pd.read_json(filename, **kwargs)
-    elif ext == ".h5":
-        # This assumes that the file has a single dataframe, and so the
-        # key name is not relevant.
-        df = pd.read_hdf(filename, **kwargs)
-        needs_new_index = True
-    else:
-        raise InvalidParameter(f"unsupported file extension {ext}")
-
-    if index_col is not None and needs_new_index:
-        df.set_index(index_col, inplace=True)
-        if parse_dates:
-            df.set_index(pd.to_datetime(df.index), inplace=True)
-
-    if remove_unnamed:
-        cols_to_remove = [x for x in df.columns if x.startswith("Unnamed")]
-        df.drop(columns=cols_to_remove, inplace=True)
-
-    if strip_column_units:
-        columns = []
-        for column in df.columns:
-            index = column.find(" [")
-            if index != -1:
-                column = column[:index]
-            columns.append(column)
-        df.columns = columns
-
-    return df
-
-def write_dataframe(df, file_path, compress=False, keep_original=False,
-                    **kwargs):
-
-    """Write the dataframe to a file with in a format matching the extension.
-
-    Note that the h5 format does not support row indices.
-    Index columns will be lost for those formats. If the dataframe has an index
-    then it should be converted to a column before calling this function.
-
-    This function only supports storing a single dataframe inside an HDF5 file.
-    It always uses the key 'data'.
-
-    Parameters
-    ----------
-    df : pd.DataFrame
-    file_path : str
-    compress : bool
-    keep_original : bool
-    kwargs : pass keyword arguments to underlying library
-
-    Raises
-    ------
-    InvalidParameter if the file extension is not supported.
-    InvalidParameter if the DataFrame index is set.
-
-    """
-    if not isinstance(df.index, pd.RangeIndex) and not \
-            isinstance(df.index, pd.core.indexes.base.Index):
-        raise InvalidParameter("DataFrame index must not be set")
-
-    ext = os.path.splitext(file_path)[1]
-
-    if ext == ".csv":
-        df.to_csv(file_path, **kwargs)
-    elif ext == ".h5":
-        # HDF5 supports built-in compression, levels 1-9
-        if "complevel" in kwargs:
-            complevel = kwargs["complevel"]
-        elif compress:
-            complevel = 9
-        else:
-            complevel = 0
-        kwargs["complevel"] =  complevel
-        kwargs["mode"]="w"
-        kwargs["key"]= "data"
-        df.to_hdf(file_path, **kwargs)
-    elif ext == ".json":
-        df.to_json(file_path, **kwargs)
-    else:
-        raise InvalidParameter(f"unsupported file extension {ext}")
-
-    logger.debug("Created %s", file_path)
-
-    if compress and ext != ".h5":
-        zipped_path = file_path + ".gz"
-        with open(file_path, "rb") as f_in:
-            with gzip.open(zipped_path, "wb") as f_out:
-                shutil.copyfileobj(f_in, f_out)
-
-        if not keep_original:
-            os.remove(file_path)
-
-        file_path = zipped_path
-        logger.debug("Compressed %s", zipped_path)
-
-    return file_path
+
+import shutil
+import gzip
+import os
+
+from loguru import logger
+import pandas as pd
+
+from pydss.exceptions import InvalidParameter
+
+
+def read_dataframe(filename, index_col=None, columns=None, parse_dates=False,
+                   remove_unnamed=True, strip_column_units=False, **kwargs):
+    """Convert filename to a dataframe. Supports .csv, .json, .h5.
+    Handles compressed files.
+
+    Parameters
+    ----------
+    filename : str
+    index_col : str | int | None
+        Index column name or index
+    columns : list or None
+        Use these columns if the file is CSV and does not define them.
+    parse_dates : bool
+    remove_unnamed : bool
+        Remove any column that starts with "Unnamed".
+    strip_column_units : bool
+        Remove units from column names.
+    kwargs : kwargs
+        Passed to underlying library for dataframe conversion.
+        Consider setting parse_dates=True if the index is a timestamp.
+
+    Returns
+    -------
+    pd.DataFrame
+
+    Raises
+    ------
+    FileNotFoundError
+        Raised if the file does not exist.
+
+    """
+    if not os.path.exists(filename):
+        raise FileNotFoundError("filename={} does not exist".format(filename))
+
+    needs_new_index = False
+    ext = os.path.splitext(filename)
+    if ext[1] == ".gz":
+        ext = os.path.splitext(ext[0])[1]
+        open_func = gzip.open
+    else:
+        ext = ext[1]
+        open_func = open
+
+    if ext == ".csv":
+        df = pd.read_csv(filename, index_col=index_col, usecols=columns,
+                         parse_dates=parse_dates, **kwargs)
+    elif ext == ".json":
+        df = pd.read_json(filename, **kwargs)
+    elif ext == ".h5":
+        # This assumes that the file has a single dataframe, and so the
+        # key name is not relevant.
+        df = pd.read_hdf(filename, **kwargs)
+        needs_new_index = True
+    else:
+        raise InvalidParameter(f"unsupported file extension {ext}")
+
+    if index_col is not None and needs_new_index:
+        df.set_index(index_col, inplace=True)
+        if parse_dates:
+            df.set_index(pd.to_datetime(df.index), inplace=True)
+
+    if remove_unnamed:
+        cols_to_remove = [x for x in df.columns if x.startswith("Unnamed")]
+        df.drop(columns=cols_to_remove, inplace=True)
+
+    if strip_column_units:
+        columns = []
+        for column in df.columns:
+            index = column.find(" [")
+            if index != -1:
+                column = column[:index]
+            columns.append(column)
+        df.columns = columns
+
+    return df
+
+def write_dataframe(df, file_path, compress=False, keep_original=False,
+                    **kwargs):
+
+    """Write the dataframe to a file with in a format matching the extension.
+
+    Note that the h5 format does not support row indices.
+    Index columns will be lost for those formats. If the dataframe has an index
+    then it should be converted to a column before calling this function.
+
+    This function only supports storing a single dataframe inside an HDF5 file.
+    It always uses the key 'data'.
+
+    Parameters
+    ----------
+    df : pd.DataFrame
+    file_path : str
+    compress : bool
+    keep_original : bool
+    kwargs : pass keyword arguments to underlying library
+
+    Raises
+    ------
+    InvalidParameter if the file extension is not supported.
+    InvalidParameter if the DataFrame index is set.
+
+    """
+    if not isinstance(df.index, pd.RangeIndex) and not \
+            isinstance(df.index, pd.core.indexes.base.Index):
+        raise InvalidParameter("DataFrame index must not be set")
+
+    ext = os.path.splitext(file_path)[1]
+
+    if ext == ".csv":
+        df.to_csv(file_path, **kwargs)
+    elif ext == ".h5":
+        # HDF5 supports built-in compression, levels 1-9
+        if "complevel" in kwargs:
+            complevel = kwargs["complevel"]
+        elif compress:
+            complevel = 9
+        else:
+            complevel = 0
+        kwargs["complevel"] =  complevel
+        kwargs["mode"]="w"
+        kwargs["key"]= "data"
+        df.to_hdf(file_path, **kwargs)
+    elif ext == ".json":
+        df.to_json(file_path, **kwargs)
+    else:
+        raise InvalidParameter(f"unsupported file extension {ext}")
+
+    logger.debug("Created %s", file_path)
+
+    if compress and ext != ".h5":
+        zipped_path = file_path + ".gz"
+        with open(file_path, "rb") as f_in:
+            with gzip.open(zipped_path, "wb") as f_out:
+                shutil.copyfileobj(f_in, f_out)
+
+        if not keep_original:
+            os.remove(file_path)
+
+        file_path = zipped_path
+        logger.debug("Compressed %s", zipped_path)
+
+    return file_path
```

### Comparing `nrel_pydss-3.1.3/src/pydss/utils/dss_utils.py` & `nrel_pydss-3.1.4/src/pydss/utils/dss_utils.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,188 +1,188 @@
-
-import re
-
-import opendssdirect as dss
-from loguru import logger
-
-
-def check_redirect(file_name):
-    """Runs redirect command for dss file
-    And checks for exception
-
-    Parameters
-    ----------
-    file_name : str
-        dss file to be redirected
-
-    Raises
-    -------
-    Exception
-        Raised if the command fails
-
-    """
-    logger.debug(f"Redirecting DSS file: {file_name}")
-    result = dss.run_command(f"Redirect {file_name}")
-    if result != "":
-        raise Exception(f"Redirect failed for {file_name}, message: {result}")
-
-
-def read_pv_systems_from_dss_file(filename):
-    """Return PVSystem names specified in OpenDSS deployment file.
-
-    Parameters
-    ----------
-    filename : str
-
-    Returns
-    -------
-    list
-
-    """
-    pv_systems = []
-    """
-    New PVSystem.pv_1114018 bus1=133294_xfmr.1.2 phases=2
-    """
-    regex = re.compile(r"New (PVSystem\.)([\S]+)\s", re.I)
-
-    with open(filename) as fp_in:
-        for line in fp_in:
-            match = regex.search(line)
-            if match:
-                pv_systems.append(match.group(1) + match.group(2).lower())
-
-    logger.debug("Found pv_systems=%s in %s", pv_systems, filename)
-    return pv_systems
-
-
-def get_load_shape_resolution_secs():
-    def func():
-        if dss.LoadShape.Name() == "default":
-            return None
-        return dss.LoadShape.SInterval()
-
-    res = [x for x in iter_elements(dss.LoadShape, func) if x is not None]
-    if len(set(res)) != 1:
-        return None
-        # raise InvalidConfiguration(
-        #     f"SInterval for all LoadShapes must be the same: {res}"
-        # )
-        
-    return res[0]
-
-
-def get_node_names_by_type(kv_base_threshold=1.0):
-    """Return a mapping of node type to node names.
-
-    Parameters
-    ----------
-    kv_base_threshold : float
-        Voltage to use as threshold for identifying primary vs secondary
-
-    Returns
-    -------
-    dict
-        keys are "primaries" or "secondaries"
-        values are a list of node names
-
-    """
-    names_by_type = {"primaries": [], "secondaries": []}
-    for i, name in enumerate(dss.Circuit.AllNodeNames()):
-        dss.Circuit.SetActiveBus(name)
-        kv_base = dss.Bus.kVBase()
-        if kv_base > kv_base_threshold:
-            names_by_type["primaries"].append(name)
-        else:
-            names_by_type["secondaries"].append(name)
-
-    return names_by_type
-
-
-def list_element_names_by_class_name(class_name):
-    """Return a list of names of all elements of a given element class.
-
-    Parameters
-    ----------
-    class_name : str
-        Subclass of opendssdirect.CktElement
-
-    Returns
-    -------
-    list
-
-    Examples
-    --------
-    >>> names = list_element_names_by_class("Loads")
-
-    """
-    if class_name == "Buses":
-        return dss.Circuit.AllBusNames()
-    elif class_name == "Nodes":
-        return dss.Circuit.AllNodeNames()
-
-    dss.Basic.SetActiveClass(class_name)
-    return dss.ActiveClass.AllNames()
-
-
-def list_element_names_by_class(element_class):
-    """Return a list of names of all elements of a given element class.
-
-    Parameters
-    ----------
-    element_class : class
-        Subclass of opendssdirect.CktElement
-
-    Returns
-    -------
-    list
-
-    Examples
-    --------
-    >>> import opendssdirect as dss
-
-    >>> names = list_element_names_by_class(dss.PVsystems)
-
-    """
-    if element_class is dss.PVsystems:
-        class_name = "PVSystem"
-    else:
-        class_name = element_class.__name__.split('.')[1]
-        # TODO: confirm that this covers everything.
-        if class_name.endswith("s"):
-            class_name = class_name[:-1]
-
-    return [f"{class_name}.{x}" for x in iter_elements(element_class, element_class.Name)]
-
-
-def iter_elements(element_class, element_func):
-    """Yield the return of element_func for each element of type element_class.
-
-    Parameters
-    ----------
-    element_class : class
-        Subclass of opendssdirect.CktElement
-    element_func : function
-        Function to run on each element
-
-    Yields
-    ------
-    Return of element_func
-
-    Examples
-    --------
-    >>> import opendssdirect as dss
-
-    >>> def get_reg_control_info():
-        return {
-            "name": dss.RegControls.Name(),
-            "enabled": dss.CktElement.Enabled(),
-            "transformer": dss.RegControls.Transformer(),
-        }
-
-    >>> for reg_control in iter_elements(opendssdirect.RegControls, get_reg_control_info):
-        print(reg_control["name"])
-
-    """
-    flag = element_class.First()
-    while flag > 0:
-        yield element_func()
-        flag = element_class.Next()
+
+import re
+
+import opendssdirect as dss
+from loguru import logger
+
+
+def check_redirect(file_name):
+    """Runs redirect command for dss file
+    And checks for exception
+
+    Parameters
+    ----------
+    file_name : str
+        dss file to be redirected
+
+    Raises
+    -------
+    Exception
+        Raised if the command fails
+
+    """
+    logger.debug(f"Redirecting DSS file: {file_name}")
+    result = dss.run_command(f"Redirect {file_name}")
+    if result != "":
+        raise Exception(f"Redirect failed for {file_name}, message: {result}")
+
+
+def read_pv_systems_from_dss_file(filename):
+    """Return PVSystem names specified in OpenDSS deployment file.
+
+    Parameters
+    ----------
+    filename : str
+
+    Returns
+    -------
+    list
+
+    """
+    pv_systems = []
+    """
+    New PVSystem.pv_1114018 bus1=133294_xfmr.1.2 phases=2
+    """
+    regex = re.compile(r"New (PVSystem\.)([\S]+)\s", re.I)
+
+    with open(filename) as fp_in:
+        for line in fp_in:
+            match = regex.search(line)
+            if match:
+                pv_systems.append(match.group(1) + match.group(2).lower())
+
+    logger.debug("Found pv_systems=%s in %s", pv_systems, filename)
+    return pv_systems
+
+
+def get_load_shape_resolution_secs():
+    def func():
+        if dss.LoadShape.Name() == "default":
+            return None
+        return dss.LoadShape.SInterval()
+
+    res = [x for x in iter_elements(dss.LoadShape, func) if x is not None]
+    if len(set(res)) != 1:
+        return None
+        # raise InvalidConfiguration(
+        #     f"SInterval for all LoadShapes must be the same: {res}"
+        # )
+        
+    return res[0]
+
+
+def get_node_names_by_type(kv_base_threshold=1.0):
+    """Return a mapping of node type to node names.
+
+    Parameters
+    ----------
+    kv_base_threshold : float
+        Voltage to use as threshold for identifying primary vs secondary
+
+    Returns
+    -------
+    dict
+        keys are "primaries" or "secondaries"
+        values are a list of node names
+
+    """
+    names_by_type = {"primaries": [], "secondaries": []}
+    for i, name in enumerate(dss.Circuit.AllNodeNames()):
+        dss.Circuit.SetActiveBus(name)
+        kv_base = dss.Bus.kVBase()
+        if kv_base > kv_base_threshold:
+            names_by_type["primaries"].append(name)
+        else:
+            names_by_type["secondaries"].append(name)
+
+    return names_by_type
+
+
+def list_element_names_by_class_name(class_name):
+    """Return a list of names of all elements of a given element class.
+
+    Parameters
+    ----------
+    class_name : str
+        Subclass of opendssdirect.CktElement
+
+    Returns
+    -------
+    list
+
+    Examples
+    --------
+    >>> names = list_element_names_by_class("Loads")
+
+    """
+    if class_name == "Buses":
+        return dss.Circuit.AllBusNames()
+    elif class_name == "Nodes":
+        return dss.Circuit.AllNodeNames()
+
+    dss.Basic.SetActiveClass(class_name)
+    return dss.ActiveClass.AllNames()
+
+
+def list_element_names_by_class(element_class):
+    """Return a list of names of all elements of a given element class.
+
+    Parameters
+    ----------
+    element_class : class
+        Subclass of opendssdirect.CktElement
+
+    Returns
+    -------
+    list
+
+    Examples
+    --------
+    >>> import opendssdirect as dss
+
+    >>> names = list_element_names_by_class(dss.PVsystems)
+
+    """
+    if element_class is dss.PVsystems:
+        class_name = "PVSystem"
+    else:
+        class_name = element_class.__name__.split('.')[1]
+        # TODO: confirm that this covers everything.
+        if class_name.endswith("s"):
+            class_name = class_name[:-1]
+
+    return [f"{class_name}.{x}" for x in iter_elements(element_class, element_class.Name)]
+
+
+def iter_elements(element_class, element_func):
+    """Yield the return of element_func for each element of type element_class.
+
+    Parameters
+    ----------
+    element_class : class
+        Subclass of opendssdirect.CktElement
+    element_func : function
+        Function to run on each element
+
+    Yields
+    ------
+    Return of element_func
+
+    Examples
+    --------
+    >>> import opendssdirect as dss
+
+    >>> def get_reg_control_info():
+        return {
+            "name": dss.RegControls.Name(),
+            "enabled": dss.CktElement.Enabled(),
+            "transformer": dss.RegControls.Transformer(),
+        }
+
+    >>> for reg_control in iter_elements(opendssdirect.RegControls, get_reg_control_info):
+        print(reg_control["name"])
+
+    """
+    flag = element_class.First()
+    while flag > 0:
+        yield element_func()
+        flag = element_class.Next()
```

### Comparing `nrel_pydss-3.1.3/src/pydss/utils/pydss_utils.py` & `nrel_pydss-3.1.4/src/pydss/utils/pydss_utils.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-import numpy as np
-import math
-
-def form_Yprim(values):
-    dimension = int(math.sqrt(len(values) / 2))
-    Yprim = np.array([[complex(0, 0)] * dimension] * dimension)
-    for ii in range(dimension):
-        for jj in range(dimension):
-            Yprim[ii][jj] = complex(values[dimension * ii * 2 + 2 * jj], values[dimension * ii * 2 + 2 * jj + 1])
-    return Yprim
-
-def form_Yprim_2(values):
-    Ydim = int(math.sqrt(len(values) / 2))
-    Yreal = np.array(values[0::2]).reshape((Ydim, Ydim))
-    Yimag = np.array(values[1::2]).reshape((Ydim, Ydim))
-    Yprim = Yreal + 1j * Yimag
-    return Yprim
-
-
-def get_Yprime_Matrix(dssObjects):
-    Elements = dssObjects["Lines"] + dssObjects["Transformers"]
-    nElements = len(Elements)
-    Ybranch_prim = np.array([[complex(0, 0)] * 2 * nElements] * 2 * nElements)
-
+import numpy as np
+import math
+
+def form_Yprim(values):
+    dimension = int(math.sqrt(len(values) / 2))
+    Yprim = np.array([[complex(0, 0)] * dimension] * dimension)
+    for ii in range(dimension):
+        for jj in range(dimension):
+            Yprim[ii][jj] = complex(values[dimension * ii * 2 + 2 * jj], values[dimension * ii * 2 + 2 * jj + 1])
+    return Yprim
+
+def form_Yprim_2(values):
+    Ydim = int(math.sqrt(len(values) / 2))
+    Yreal = np.array(values[0::2]).reshape((Ydim, Ydim))
+    Yimag = np.array(values[1::2]).reshape((Ydim, Ydim))
+    Yprim = Yreal + 1j * Yimag
+    return Yprim
+
+
+def get_Yprime_Matrix(dssObjects):
+    Elements = dssObjects["Lines"] + dssObjects["Transformers"]
+    nElements = len(Elements)
+    Ybranch_prim = np.array([[complex(0, 0)] * 2 * nElements] * 2 * nElements)
+
```

### Comparing `nrel_pydss-3.1.3/.gitignore` & `nrel_pydss-3.1.4/.gitignore`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,179 +1,179 @@
-Export/*
-ProjectFiles/*/pydss Settings/*
-__pycache__/*
-*/__pycache__/*
-__pycache__
-*.egg-info
-*.pyc
-docs/build/*
-tags
-cscope.files
-tests/data/*/Exports/*
-.idea/*
-.vscode/*
-store.h5
-simulation-run.toml
-*.log
-*.bak
-docs/source/pydss/*
-dist/
-
-
-# Created by https://www.toptal.com/developers/gitignore/api/python
-# Edit at https://www.toptal.com/developers/gitignore?templates=python
-
-### Python ###
-# Byte-compiled / optimized / DLL files
-__pycache__/
-*.py[cod]
-*$py.class
-*.svg
-# C extensions
-*.so
-*$hdf5
-
-# Distribution / packaging
-.Python
-project/
-build/
-develop-eggs/
-dist/
-downloads/
-eggs/
-.eggs/
-parts/
-sdist/
-var/
-wheels/
-pip-wheel-metadata/
-share/python-wheels/
-*.egg-info/
-.installed.cfg
-*.egg
-MANIFEST
-
-# PyInstaller
-#  Usually these files are written by a python script from a template
-#  before PyInstaller builds the exe, so as to inject date/other infos into it.
-*.manifest
-*.spec
-
-# Installer logs
-pip-log.txt
-pip-delete-this-directory.txt
-
-# Unit test / coverage reports
-htmlcov/
-.tox/
-.nox/
-.coverage
-.coverage.*
-.cache
-nosetests.xml
-coverage.xml
-*.cover
-*.py,cover
-.hypothesis/
-.pytest_cache/
-pytestdebug.log
-
-# Translations
-*.mo
-*.pot
-
-*.tar.gz
-*.whl
-
-# Django stuff:
-*.log
-local_settings.py
-db.sqlite3
-db.sqlite3-journal
-
-# Flask stuff:
-instance/
-.webassets-cache
-
-# Scrapy stuff:
-.scrapy
-
-# Sphinx documentation
-docs/_build/
-doc/_build/
-
-# PyBuilder
-target/
-
-# Jupyter Notebook
-.ipynb_checkpoints
-
-# IPython
-profile_default/
-ipython_config.py
-
-# pyenv
-.python-version
-
-# pipenv
-#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
-#   However, in case of collaboration, if having platform-specific dependencies or dependencies
-#   having no cross-platform support, pipenv may install dependencies that don't work, or not
-#   install all needed dependencies.
-#Pipfile.lock
-
-# poetry
-#poetry.lock
-
-# PEP 582; used by e.g. github.com/David-OConnor/pyflow
-__pypackages__/
-
-# Celery stuff
-celerybeat-schedule
-celerybeat.pid
-
-# SageMath parsed files
-*.sage.py
-
-# Environments
-# .env
-.env/
-.venv/
-env/
-venv/
-ENV/
-env.bak/
-venv.bak/
-pythonenv*
-
-# Spyder project settings
-.spyderproject
-.spyproject
-
-# Rope project settings
-.ropeproject
-
-# mkdocs documentation
-/site
-
-# mypy
-.mypy_cache/
-.dmypy.json
-dmypy.json
-
-# Pyre type checker
-.pyre/
-
-# pytype static type analyzer
-.pytype/
-
-# operating system-related files
-# file properties cache/storage on macOS
-*.DS_Store
-# thumbnail cache on Windows
-Thumbs.db
-
-# profiling data
-.prof
-
-
+Export/*
+ProjectFiles/*/pydss Settings/*
+__pycache__/*
+*/__pycache__/*
+__pycache__
+*.egg-info
+*.pyc
+docs/build/*
+tags
+cscope.files
+tests/data/*/Exports/*
+.idea/*
+.vscode/*
+store.h5
+simulation-run.toml
+*.log
+*.bak
+docs/source/pydss/*
+dist/
+
+
+# Created by https://www.toptal.com/developers/gitignore/api/python
+# Edit at https://www.toptal.com/developers/gitignore?templates=python
+
+### Python ###
+# Byte-compiled / optimized / DLL files
+__pycache__/
+*.py[cod]
+*$py.class
+*.svg
+# C extensions
+*.so
+*$hdf5
+
+# Distribution / packaging
+.Python
+project/
+build/
+develop-eggs/
+dist/
+downloads/
+eggs/
+.eggs/
+parts/
+sdist/
+var/
+wheels/
+pip-wheel-metadata/
+share/python-wheels/
+*.egg-info/
+.installed.cfg
+*.egg
+MANIFEST
+
+# PyInstaller
+#  Usually these files are written by a python script from a template
+#  before PyInstaller builds the exe, so as to inject date/other infos into it.
+*.manifest
+*.spec
+
+# Installer logs
+pip-log.txt
+pip-delete-this-directory.txt
+
+# Unit test / coverage reports
+htmlcov/
+.tox/
+.nox/
+.coverage
+.coverage.*
+.cache
+nosetests.xml
+coverage.xml
+*.cover
+*.py,cover
+.hypothesis/
+.pytest_cache/
+pytestdebug.log
+
+# Translations
+*.mo
+*.pot
+
+*.tar.gz
+*.whl
+
+# Django stuff:
+*.log
+local_settings.py
+db.sqlite3
+db.sqlite3-journal
+
+# Flask stuff:
+instance/
+.webassets-cache
+
+# Scrapy stuff:
+.scrapy
+
+# Sphinx documentation
+docs/_build/
+doc/_build/
+
+# PyBuilder
+target/
+
+# Jupyter Notebook
+.ipynb_checkpoints
+
+# IPython
+profile_default/
+ipython_config.py
+
+# pyenv
+.python-version
+
+# pipenv
+#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
+#   However, in case of collaboration, if having platform-specific dependencies or dependencies
+#   having no cross-platform support, pipenv may install dependencies that don't work, or not
+#   install all needed dependencies.
+#Pipfile.lock
+
+# poetry
+#poetry.lock
+
+# PEP 582; used by e.g. github.com/David-OConnor/pyflow
+__pypackages__/
+
+# Celery stuff
+celerybeat-schedule
+celerybeat.pid
+
+# SageMath parsed files
+*.sage.py
+
+# Environments
+# .env
+.env/
+.venv/
+env/
+venv/
+ENV/
+env.bak/
+venv.bak/
+pythonenv*
+
+# Spyder project settings
+.spyderproject
+.spyproject
+
+# Rope project settings
+.ropeproject
+
+# mkdocs documentation
+/site
+
+# mypy
+.mypy_cache/
+.dmypy.json
+dmypy.json
+
+# Pyre type checker
+.pyre/
+
+# pytype static type analyzer
+.pytype/
+
+# operating system-related files
+# file properties cache/storage on macOS
+*.DS_Store
+# thumbnail cache on Windows
+Thumbs.db
+
+# profiling data
+.prof
+
+
 # End of https://www.toptal.com/developers/gitignore/api/python
```

### Comparing `nrel_pydss-3.1.3/LICENSE` & `nrel_pydss-3.1.4/LICENSE`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-BSD 3-Clause License
-
-Copyright (c) 2018, Alliance for Sustainable Energy LLC, All rights reserved.
-
-Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
-
-<<<<<<< HEAD
-- Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
-
-- Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
-
-- Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-=======
-* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
-
-* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
-
-* Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
->>>>>>> 98cba91204224c1b5c9e477759bf012e2f70a369
+BSD 3-Clause License
+
+Copyright (c) 2018, Alliance for Sustainable Energy LLC, All rights reserved.
+
+Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
+
+<<<<<<< HEAD
+- Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
+
+- Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
+
+- Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+=======
+* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
+
+* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
+
+* Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+>>>>>>> 98cba91204224c1b5c9e477759bf012e2f70a369
```

### Comparing `nrel_pydss-3.1.3/PKG-INFO` & `nrel_pydss-3.1.4/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.3
 Name: NREL-pydss
-Version: 3.1.3
+Version: 3.1.4
 Summary: A high-level python interface for OpenDSS
 Project-URL: Homepage, http://www.github.com/nrel/pydss
 Author-email: Aadil Latif <Aadil.Latif@nrel.gov>
 License-Expression: BSD-3-Clause
 License-File: LICENSE
 Classifier: Development Status :: 4 - Beta
 Classifier: Environment :: Console
```

