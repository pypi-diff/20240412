# Comparing `tmp/galah_python-0.8.3-py3-none-any.whl.zip` & `tmp/galah_python-0.9.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,34 +1,69 @@
-Zip file size: 68023 bytes, number of entries: 32
+Zip file size: 172682 bytes, number of entries: 67
 -rw-r--r--  2.0 unx      183 b- defN 23-Oct-04 21:36 galah/ALA_keys.json
--rw-r--r--  2.0 unx     1565 b- defN 24-Mar-07 22:52 galah/__init__.py
+-rw-r--r--  2.0 unx     9887 b- defN 23-Oct-04 21:36 galah/Countries_Downloaded_In_21July2023.xlsx
+-rw-r--r--  2.0 unx     1576 b- defN 24-Apr-11 00:51 galah/__init__.py
 -rw-r--r--  2.0 unx     2042 b- defN 23-Oct-04 21:36 galah/apply_data_profile.py
 -rw-r--r--  2.0 unx     2503 b- defN 23-Jun-29 22:08 galah/atlas_citation.py
--rw-r--r--  2.0 unx    11629 b- defN 23-Dec-17 23:23 galah/atlas_counts.py
--rw-r--r--  2.0 unx    14306 b- defN 24-Mar-07 22:52 galah/atlas_media.py
--rw-r--r--  2.0 unx    19748 b- defN 23-Dec-17 23:23 galah/atlas_occurrences.py
--rw-r--r--  2.0 unx    10293 b- defN 23-Dec-17 23:23 galah/atlas_species.py
+-rw-r--r--  2.0 unx    11609 b- defN 24-Apr-11 00:51 galah/atlas_counts.py
+-rw-r--r--  2.0 unx    14240 b- defN 24-Apr-11 00:51 galah/atlas_media.py
+-rw-r--r--  2.0 unx    19681 b- defN 24-Apr-11 00:51 galah/atlas_occurrences.py
+-rw-r--r--  2.0 unx    10312 b- defN 24-Apr-11 00:51 galah/atlas_species.py
 -rw-r--r--  2.0 unx     4408 b- defN 23-Jun-29 22:08 galah/atlas_taxonomy.py
--rw-r--r--  2.0 unx    10446 b- defN 23-Dec-17 23:23 galah/common_dictionaries.py
--rw-r--r--  2.0 unx    12565 b- defN 23-Dec-17 23:23 galah/common_functions.py
--rw-r--r--  2.0 unx      282 b- defN 23-Dec-17 23:23 galah/config.ini
+-rw-r--r--  2.0 unx    11356 b- defN 24-Apr-11 00:51 galah/common_dictionaries.py
+-rw-r--r--  2.0 unx    12979 b- defN 24-Apr-11 00:51 galah/common_functions.py
+-rw-r--r--  2.0 unx      140 b- defN 24-Apr-11 04:52 galah/config.ini
+-rw-r--r--  2.0 unx      140 b- defN 24-Apr-11 04:40 galah/config.ini.default
 -rw-r--r--  2.0 unx     4779 b- defN 23-Oct-04 21:36 galah/galah_config.py
--rw-r--r--  2.0 unx    10430 b- defN 24-Feb-12 23:44 galah/galah_filter.py
+-rw-r--r--  2.0 unx    10460 b- defN 24-Apr-11 00:51 galah/galah_filter.py
 -rw-r--r--  2.0 unx     2787 b- defN 23-Oct-04 21:36 galah/galah_gbif_filters.py
 -rw-r--r--  2.0 unx     3624 b- defN 23-Dec-20 04:25 galah/galah_geolocate.py
--rw-r--r--  2.0 unx    16882 b- defN 23-Dec-17 23:23 galah/galah_group_by.py
--rw-r--r--  2.0 unx     1712 b- defN 24-Mar-07 22:52 galah/galah_select.py
+-rw-r--r--  2.0 unx    17873 b- defN 24-Apr-11 00:51 galah/galah_group_by.py
+-rw-r--r--  2.0 unx     1712 b- defN 24-Mar-25 03:43 galah/galah_select.py
 -rw-r--r--  2.0 unx     6550 b- defN 23-Jun-29 22:09 galah/gbif_assertions.csv
 -rw-r--r--  2.0 unx    11155 b- defN 23-Jun-29 22:09 galah/gbif_fields.csv
 -rw-r--r--  2.0 unx     4332 b- defN 23-Oct-04 21:36 galah/generate_jwt_token.py
 -rw-r--r--  2.0 unx     4797 b- defN 23-Oct-04 21:36 galah/get_api_url.py
--rw-r--r--  2.0 unx    22992 b- defN 24-Mar-07 22:52 galah/node_config.csv
--rw-r--r--  2.0 unx    17888 b- defN 23-Oct-04 21:36 galah/search_all.py
--rw-r--r--  2.0 unx    15393 b- defN 23-Dec-17 23:23 galah/search_taxa.py
+-rw-r--r--  2.0 unx    16636 b- defN 24-Apr-11 01:20 galah/node_config.csv
+-rw-r--r--  2.0 unx     7358 b- defN 23-Jun-29 22:09 galah/node_config_0.1.0.csv
+-rw-r--r--  2.0 unx    12352 b- defN 23-Oct-04 21:36 galah/node_config_dest.csv
+-rw-r--r--  2.0 unx    21494 b- defN 23-Apr-24 05:01 galah/node_config_dev.csv
+-rw-r--r--  2.0 unx    21688 b- defN 23-Oct-04 21:36 galah/node_config_full.csv
+-rw-r--r--  2.0 unx    12345 b- defN 23-Oct-04 21:36 galah/node_config_old.csv
+-rw-r--r--  2.0 unx    17890 b- defN 24-Apr-11 00:51 galah/search_all.py
+-rw-r--r--  2.0 unx    14829 b- defN 24-Apr-11 00:51 galah/search_taxa.py
 -rw-r--r--  2.0 unx     2300 b- defN 23-Oct-04 21:36 galah/search_values.py
--rw-r--r--  2.0 unx    22588 b- defN 24-Feb-12 23:44 galah/show_all.py
+-rw-r--r--  2.0 unx    22562 b- defN 24-Apr-11 04:52 galah/show_all.py
 -rw-r--r--  2.0 unx     6093 b- defN 23-Oct-04 21:36 galah/show_values.py
--rw-r--r--  2.0 unx      678 b- defN 24-Mar-07 22:55 galah_python-0.8.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-07 22:55 galah_python-0.8.3.dist-info/WHEEL
--rw-r--r--  2.0 unx        6 b- defN 24-Mar-07 22:55 galah_python-0.8.3.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2539 b- defN 24-Mar-07 22:55 galah_python-0.8.3.dist-info/RECORD
-32 files, 247587 bytes uncompressed, 64041 bytes compressed:  74.1%
+-rw-r--r--  2.0 unx      145 b- defN 24-Apr-11 04:54 galah/version.py
+-rw-r--r--  2.0 unx       47 b- defN 22-Aug-31 04:40 galah/.idea/.gitignore
+-rw-r--r--  2.0 unx      336 b- defN 22-Aug-31 04:44 galah/.idea/galah.iml
+-rw-r--r--  2.0 unx      204 b- defN 22-Aug-31 04:44 galah/.idea/misc.xml
+-rw-r--r--  2.0 unx      262 b- defN 22-Aug-31 04:41 galah/.idea/modules.xml
+-rw-r--r--  2.0 unx      189 b- defN 22-Aug-31 04:41 galah/.idea/vcs.xml
+-rw-r--r--  2.0 unx      174 b- defN 22-Aug-31 04:41 galah/.idea/inspectionProfiles/profiles_settings.xml
+-rw-r--r--  2.0 unx     1061 b- defN 24-Apr-09 01:31 galah/__pycache__/__init__.cpython-311.pyc
+-rw-r--r--  2.0 unx     1968 b- defN 24-Apr-09 01:31 galah/__pycache__/apply_data_profile.cpython-311.pyc
+-rw-r--r--  2.0 unx    10085 b- defN 24-Apr-09 02:43 galah/__pycache__/atlas_counts.cpython-311.pyc
+-rw-r--r--  2.0 unx    12683 b- defN 24-Apr-09 01:31 galah/__pycache__/atlas_media.cpython-311.pyc
+-rw-r--r--  2.0 unx    19474 b- defN 24-Apr-09 01:32 galah/__pycache__/atlas_occurrences.cpython-311.pyc
+-rw-r--r--  2.0 unx    10976 b- defN 24-Apr-09 01:31 galah/__pycache__/atlas_species.cpython-311.pyc
+-rw-r--r--  2.0 unx     5035 b- defN 24-Apr-09 04:00 galah/__pycache__/common_dictionaries.cpython-311.pyc
+-rw-r--r--  2.0 unx    16939 b- defN 24-Apr-09 01:31 galah/__pycache__/common_functions.cpython-311.pyc
+-rw-r--r--  2.0 unx     4373 b- defN 24-Apr-09 01:31 galah/__pycache__/galah_config.cpython-311.pyc
+-rw-r--r--  2.0 unx    11127 b- defN 24-Apr-09 01:31 galah/__pycache__/galah_filter.cpython-311.pyc
+-rw-r--r--  2.0 unx     4511 b- defN 24-Apr-09 01:31 galah/__pycache__/galah_geolocate.cpython-311.pyc
+-rw-r--r--  2.0 unx    16163 b- defN 24-Apr-09 01:31 galah/__pycache__/galah_group_by.cpython-311.pyc
+-rw-r--r--  2.0 unx     2099 b- defN 24-Apr-09 01:31 galah/__pycache__/galah_select.cpython-311.pyc
+-rw-r--r--  2.0 unx     3192 b- defN 24-Apr-09 01:31 galah/__pycache__/generate_jwt_token.cpython-311.pyc
+-rw-r--r--  2.0 unx     6427 b- defN 24-Apr-09 01:31 galah/__pycache__/get_api_url.cpython-311.pyc
+-rw-r--r--  2.0 unx    16786 b- defN 24-Apr-09 01:31 galah/__pycache__/search_all.cpython-311.pyc
+-rw-r--r--  2.0 unx    14606 b- defN 24-Apr-09 04:03 galah/__pycache__/search_taxa.cpython-311.pyc
+-rw-r--r--  2.0 unx     3020 b- defN 24-Apr-09 01:31 galah/__pycache__/search_values.cpython-311.pyc
+-rw-r--r--  2.0 unx    19356 b- defN 24-Apr-09 01:31 galah/__pycache__/show_all.cpython-311.pyc
+-rw-r--r--  2.0 unx     4703 b- defN 24-Apr-09 01:31 galah/__pycache__/show_values.cpython-311.pyc
+-rw-r--r--  2.0 unx      201 b- defN 24-Apr-09 01:31 galah/__pycache__/version.cpython-311.pyc
+-rw-r--r--  2.0 unx      710 b- defN 24-Apr-11 23:22 galah_python-0.9.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-11 23:22 galah_python-0.9.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        6 b- defN 24-Apr-11 23:22 galah_python-0.9.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     5891 b- defN 24-Apr-11 23:22 galah_python-0.9.0.dist-info/RECORD
+67 files, 517513 bytes uncompressed, 163334 bytes compressed:  68.4%
```

## zipnote {}

```diff
@@ -1,10 +1,13 @@
 Filename: galah/ALA_keys.json
 Comment: 
 
+Filename: galah/Countries_Downloaded_In_21July2023.xlsx
+Comment: 
+
 Filename: galah/__init__.py
 Comment: 
 
 Filename: galah/apply_data_profile.py
 Comment: 
 
 Filename: galah/atlas_citation.py
@@ -30,14 +33,17 @@
 
 Filename: galah/common_functions.py
 Comment: 
 
 Filename: galah/config.ini
 Comment: 
 
+Filename: galah/config.ini.default
+Comment: 
+
 Filename: galah/galah_config.py
 Comment: 
 
 Filename: galah/galah_filter.py
 Comment: 
 
 Filename: galah/galah_gbif_filters.py
@@ -63,14 +69,29 @@
 
 Filename: galah/get_api_url.py
 Comment: 
 
 Filename: galah/node_config.csv
 Comment: 
 
+Filename: galah/node_config_0.1.0.csv
+Comment: 
+
+Filename: galah/node_config_dest.csv
+Comment: 
+
+Filename: galah/node_config_dev.csv
+Comment: 
+
+Filename: galah/node_config_full.csv
+Comment: 
+
+Filename: galah/node_config_old.csv
+Comment: 
+
 Filename: galah/search_all.py
 Comment: 
 
 Filename: galah/search_taxa.py
 Comment: 
 
 Filename: galah/search_values.py
@@ -78,20 +99,104 @@
 
 Filename: galah/show_all.py
 Comment: 
 
 Filename: galah/show_values.py
 Comment: 
 
-Filename: galah_python-0.8.3.dist-info/METADATA
+Filename: galah/version.py
+Comment: 
+
+Filename: galah/.idea/.gitignore
+Comment: 
+
+Filename: galah/.idea/galah.iml
+Comment: 
+
+Filename: galah/.idea/misc.xml
+Comment: 
+
+Filename: galah/.idea/modules.xml
+Comment: 
+
+Filename: galah/.idea/vcs.xml
+Comment: 
+
+Filename: galah/.idea/inspectionProfiles/profiles_settings.xml
+Comment: 
+
+Filename: galah/__pycache__/__init__.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/apply_data_profile.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/atlas_counts.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/atlas_media.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/atlas_occurrences.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/atlas_species.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/common_dictionaries.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/common_functions.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/galah_config.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/galah_filter.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/galah_geolocate.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/galah_group_by.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/galah_select.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/generate_jwt_token.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/get_api_url.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/search_all.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/search_taxa.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/search_values.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/show_all.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/show_values.cpython-311.pyc
+Comment: 
+
+Filename: galah/__pycache__/version.cpython-311.pyc
+Comment: 
+
+Filename: galah_python-0.9.0.dist-info/METADATA
 Comment: 
 
-Filename: galah_python-0.8.3.dist-info/WHEEL
+Filename: galah_python-0.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: galah_python-0.8.3.dist-info/top_level.txt
+Filename: galah_python-0.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: galah_python-0.8.3.dist-info/RECORD
+Filename: galah_python-0.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## galah/__init__.py

```diff
@@ -29,12 +29,12 @@
 # others (temp)
 #-------------------------------------------------------
 from .generate_jwt_token import generate_token_config
 
 #-------------------------------------------------------
 # version
 #-------------------------------------------------------
-__version__ = "0.8.3"
+from .version import __version__
 
 # get all functions to display
 __all__=["atlas_counts","atlas_media","atlas_occurrences","atlas_species","galah_config","search_all","search_taxa",
          "show_all","search_all","show_values","search_values"]
```

## galah/atlas_counts.py

```diff
@@ -3,14 +3,17 @@
 from .galah_group_by import galah_group_by
 from .get_api_url import get_api_url,readConfig
 from .apply_data_profile import apply_data_profile
 from .galah_geolocate import galah_geolocate
 from .show_all import show_all
 from .common_functions import add_filters,add_to_payload_ALA,generate_list_taxonConceptIDs
 from .common_dictionaries import COUNTS_NAMES
+from .version import __version__
+
+import json
 
 def atlas_counts(taxa=None,
                  scientific_name=None,
                  filters=None,
                  group_by=None,
                  total_group_by=False,
                  expand=True,
@@ -81,45 +84,39 @@
     configs = readConfig()
 
     # get atlas
     atlas = configs['galahSettings']['atlas']
 
     # raise error if argument is wrong type and/or the atlas doesn't have a quality profile but the user has specified one
     if use_data_profile and atlas not in ["Australia","ALA"]:
-                raise ValueError("True and False are the only values accepted for data_profile, and the only atlas using a data \n"
-                                "quality profile is Australia.  Your atlas and data profile is \n"
-                                "set in your config file.  To set your default filter, find out what profiles are on offer:\n"
-                                "profiles = galah.show_all(profiles=True)\n\n"
-                                "and then type\n\n"
-                                "profiles['shortName']\n\n"
-                                "to get the names of the data quality profiles you can use.  To set a data profile, type\n" 
-                                "galah.galah_config(data_profile=\"NAME FROM SHORTNAME HERE\")"
-                                "If you don't want to use a data quality profile, set it to None by typing the following:\n\n"
-                                "galah.galah_config(data_profile=\"None\")")
+        raise ValueError("True and False are the only values accepted for data_profile, and the only atlas using a data \n"
+                        "quality profile is Australia.  Your atlas and data profile is \n"
+                        "set in your config file.  To set your default filter, find out what profiles are on offer:\n"
+                        "profiles = galah.show_all(profiles=True)\n\n"
+                        "and then type\n\n"
+                        "profiles['shortName']\n\n"
+                        "to get the names of the data quality profiles you can use.  To set a data profile, type\n" 
+                        "galah.galah_config(data_profile=\"NAME FROM SHORTNAME HERE\")"
+                        "If you don't want to use a data quality profile, set it to None by typing the following:\n\n"
+                        "galah.galah_config(data_profile=\"None\")")
     
     # check for Brazilian atlas
     elif group_by is not None and atlas in ["Brazil"]:
         baseURL,method = get_api_url(column1='called_by',column1value='atlas_counts',column2="api_name",
                                            column2value="records_facets")
     # use this if they don't want a data quality profile or none exists
     else:
         baseURL,method = get_api_url(column1='called_by',column1value='atlas_counts',column2="api_name",
                                            column2value="records_counts")
 
     # add a question mark at the end of the URL to separate between endpoint and queries
     URL = baseURL + "?"
 
-    # create headers
-    headers = {}
-
-    #future code for API keys
-    #if atlas in ["Australia","ALA"]:
-    #    headers = {"x-api-key": configs["galahSettings"]["ALA_API_key"]}
-    #else:
-    #    headers = {}
+    # get headers
+    headers = {"User-Agent": "galah-python/{}".format(__version__)}
 
     # create payload (for ALA)
     payload = {}
 
     # check for Australian atlas
     if atlas in ["Australia","ALA"]:
 
@@ -153,33 +150,35 @@
         if use_data_profile:
             data_profile_list = list(show_all(profiles=True)['shortName'])
             qid_URL = apply_data_profile(baseURL=qid_URL,data_profile_list=data_profile_list)
         else:
             qid_URL += "?disableAllQualityfilters=true&"
 
         # cache the user's query and get a query ID
-        qid = requests.request(method2,qid_URL,data=payload)
+        qid = requests.request(method2,qid_URL,data=payload,headers=headers)
         
         # create the URL to grab your queryID and counts
         URL = baseURL + "fq=%28qid%3A" + qid.text + "%29&flimit=-1&pageSize=0"
 
         if verbose:
             print()
+            print("headers: {}".format(headers))
+            print()
             print("payload for queryID: {}".format(payload))
             print("queryID URL: {}".format(qid_URL))
             print("method: {}".format(method2))
             print()
             print("qid for query: {}".format(qid.text))
             print("URL for result:{}".format(URL))
             print("method: {}".format(method))
             print()
 
         # get data
         response = requests.request(method,URL,headers=headers)
-        
+
         # check for daily maximum
         if response.status_code == 429:
             raise ValueError("You have reached the maximum number of daily queries for the ALA.")
         
         # get data from response
         response_json = response.json()
         
@@ -187,15 +186,15 @@
         return pd.DataFrame({'totalRecords': [response_json[COUNTS_NAMES[atlas]]]})
 
     else:
 
         # if there is no taxa, assume you will get the total number of records in the ALA
         if taxa is not None:
 
-            URL += generate_list_taxonConceptIDs(taxa=taxa,atlas=atlas)
+            URL += generate_list_taxonConceptIDs(taxa=taxa,atlas=atlas,verbose=verbose)
 
         # check if user wants to gropu counts
         if group_by is not None:
 
             # check for GBIF first
             if configs["galahSettings"]['atlas'] not in ["Global","GBIF"]:
 
@@ -238,14 +237,15 @@
         
         # use this to get only the data we need
         URL += "&pageSize=0"
 
         # check to see if the user wants the querying URL
         if verbose:
             print()
+            print("headers: {}".format(headers))
             print("URL for querying: {}".format(URL))
             print("Method: {}".format(method))
             print()
 
         # get data
         response = requests.request(method,URL,headers=headers)
```

## galah/atlas_media.py

```diff
@@ -1,18 +1,18 @@
 import requests,os
 import pandas as pd
 import json
-import urllib.request
 
 from .atlas_occurrences import atlas_occurrences
 from .get_api_url import get_api_url
 from .get_api_url import readConfig
 from .apply_data_profile import apply_data_profile
 from .atlas_occurrences import atlas_occurrences
 from .show_all import show_all
+from .version import __version__
 
 # this function parses everything to atlas_occurrences first, and it adds something to the galah_filter argument to say
 # that the multimedia field is not empty
 # then, gets the multimedia column, which contains unique identifiers for media files
 # then, hits different API and gets metadata of media
 # next step is collect_media hits all URLs and drops it into my machine
 def atlas_media(taxa=None,
@@ -91,34 +91,29 @@
 
     # get configs
     configs = readConfig()
 
     # get atlas
     atlas = configs['galahSettings']['atlas']
 
-    # get headers for some APIs
-    headers = {}
+    # get headers
+    headers = {"User-Agent": "galah-python/{}".format(__version__)}
 
     # check for fields
     if fields is None:
         fields = ["basic","media"]
 
     # get occurrence data from atlas_occurrences
     dataFrame = atlas_occurrences(taxa=taxa,filters=filters,fields=fields,assertions=assertions,
                                   use_data_profile=use_data_profile,polygon=polygon,bbox=bbox,
                                   simplify_polygon=simplify_polygon,verbose=verbose,
                                   scientific_name=scientific_name)
     if dataFrame.empty:
         raise ValueError("There are no occurrences or media associated with your query.  Please try your query on atlas_counts before trying it again on atlas_media.")
 
-    #if atlas in ["Australia","ALA"]:
-    #    headers = {"x-api-key": configs["galahSettings"]["ALA_API_key"]}
-    #else:
-    #    headers = {}
-
     # create the output data frame
     if atlas == "Australia":
         data_columns = {
             'decimalLatitude': [],
             'decimalLongitude': [],
             'eventDate': [],
             'scientificName': [],
@@ -239,15 +234,15 @@
                     for name in ['decimalLatitude', 'decimalLongitude', 'eventDate', 'scientificName', 'recordID',
                            'dataResourceName', 'occurrenceStatus', 'multimedia', 'images','videos','sounds']:
                         if name not in media:
                             duplicate_dict[name].append(row[name])
 
             # insert the duplicate rows into the array (need to ensure that, in the case they aren't sequential, to take that into consideration)
             new_filtered_media_array = pd.concat([filtered_media_array.head(top_index),pd.DataFrame(duplicate_dict)]).reset_index(drop=True)
-            response = requests.request(method,basemediaURL,data=json.dumps({"imageIds": new_filtered_media_array[media].to_list()}))
+            response = requests.request(method,basemediaURL,data=json.dumps({"imageIds": new_filtered_media_array[media].to_list()}),headers=headers)
             
             # get metadata here
             response_json = response.json()
             media_metadata = {
                 "images": [],
                 "creator": [],
                 "license": [],
@@ -291,15 +286,15 @@
                 f.write(data) 
                 f.close() 
                 '''
                 for i,image in media_metadata_df.iterrows():
                     ext = image["mimetype"].split("/")[1]
                     #if ext == "jpeg":
                     #    ext = "jpg"
-                    data = requests.get("{}.{}".format(image["imageUrl"],ext)).content
+                    data = requests.get("{}.{}".format(image["imageUrl"],ext),headers=headers).content
                     f = open("{}/image-{}.{}".format(path,image["images"],ext),'wb')
                     f.write(data) 
                     f.close() 
                 print("Media written to {}".format(path))
                 return media_metadata_df
             else:
                 return media_metadata_df
```

## galah/atlas_occurrences.py

```diff
@@ -14,14 +14,15 @@
 from .get_api_url import get_api_url, readConfig
 from .apply_data_profile import apply_data_profile
 from .galah_geolocate import galah_geolocate
 from .common_dictionaries import ATLAS_KEYWORDS,ATLAS_SELECTIONS, atlases, ATLAS_OCCURRENCES_ERROR_MESSAGES
 from .common_dictionaries import ATLAS_OCCURRENCES_DOWNLOAD_ARGUMENTS
 from .common_functions import add_filters,add_predicates,add_to_payload_ALA
 from .show_all import show_all
+from .version import __version__
 
 def atlas_occurrences(taxa=None,
                       scientific_name=None,
                       filters=None,
                       test=False,
                       verbose=False,
                       fields=None,
@@ -111,26 +112,32 @@
     # get configs
     configs = readConfig()
 
     # get atlas
     atlas = configs['galahSettings']['atlas']
 
     # check for email
-    if configs["galahSettings"]["email"] is None or configs["galahSettings"]["email"] is "email@example.com":
+    if configs["galahSettings"]["email"] is None or configs["galahSettings"]["email"] == "email@example.com":
         raise ValueError("Please provide an email for querying")
 
-    headers = {}
+    # initialise headers
+    if atlas in ["GBIF","Global"]:
+        headers = {
+            "User-Agent": "galah-python/{}".format(__version__),
+            "X-USER-AGENT": "galah-python/{}".format(__version__),
+            "Content-type": "application/json",
+            "Accept": "application/json",
+        }
+    else:
+        # get headers
+        headers = {"User-Agent": "galah-python {}".format(__version__)}
 
-    payload = {}
 
-    # add API key for ALA
-    #if atlas in ["Australia","ALA"]:
-    #    headers = {"x-api-key": configs["galahSettings"]["ALA_API_key"]}
-    #else:
-    #    headers = {}
+    # initialise payload
+    payload = {}
 
     # create authentication key
     if atlas in ["Global","GBIF"]:
         authentication = HTTPBasicAuth(configs['galahSettings']['usernameGBIF'],configs['galahSettings']['passwordGBIF'])
     else:
         authentication = None
 
@@ -160,46 +167,35 @@
                          "to get the names of the data quality profiles you can use.  To set a data profile, type\n" 
                          "galah.galah_config(data_profile=\"NAME FROM SHORTNAME HERE\")"
                          "If you don't want to use a data quality profile, set it to None by typing the following:\n\n"
                          "galah.galah_config(data_profile=\"None\")"
                          )
     else:
         # check for these atlases first
-        if atlas in ["Australia","Austria","Brazil","France","Spain"]:
+        if atlas in ["Australia","Austria","Brazil","France","Guatemala","Spain","Sweden"]: # added Guatemala
             baseURL, method = get_api_url(column1='called_by', column1value='atlas_occurrences',column2='api_name', 
                                           column2value='records_occurrences',add_email=True)
         elif atlas in ["Global","GBIF"]:
             URL,method = get_api_url(column1='called_by',column1value='atlas_occurrences',
                                     column2='api_name',column2value='records',add_email=False)
         else:
             raise ValueError("Atlas {} not taken into account".format(atlas))       
 
-    # Ensure headers are taken care of
-    #if atlas in ["Australia","ALA"]:
-    #    headers = {"x-api-key": configs["galahSettings"]["ALA_API_key"]}
-    if atlas in ["GBIF","Global"]:
-        headers = {
-            "User-Agent": "galah-python v0.5.0", #.format(VERSION HERE)
-            "X-USER-AGENT": "galah-python v0.5.0",
-            "Content-type": "application/json",
-            "Accept": "application/json",
-        }
-    else:
-        headers = {}
-
     # goes to the 'fields' argument in occurrence download (csv list, commas between)
     if atlas in ["Australia","ALA"]:
         pass
     elif fields is not None and atlas not in ["Global","GBIF","Australia","ALA"]:
         if fields != "basic":
-            baseURL += galah_select(select=fields,atlas=atlas)[:-3] + "&"
+            baseURL += galah_select(select=fields,atlas=atlas) + "&"
         else:
-            baseURL += galah_select(select=ATLAS_SELECTIONS[atlas],atlas=atlas)[:-3] + "&"
-    elif atlas in ["Austria","Brazil","France","Spain"]:
-        baseURL += galah_select(select=ATLAS_SELECTIONS[atlas],atlas=atlas)[:-3] + "&"
+            baseURL += galah_select(select=ATLAS_SELECTIONS[atlas],atlas=atlas) + "&"
+    elif fields is None and atlas in ["Sweden"]:
+        baseURL += galah_select(select=ATLAS_SELECTIONS[atlas],atlas=atlas) + "&"
+    elif atlas in ["Austria","Brazil","France","Spain","Guatemala"]:
+        baseURL += galah_select(select=ATLAS_SELECTIONS[atlas],atlas=atlas) + "&"
     elif fields is not None and atlas in ["Global","GBIF"]:
         print("GBIF, unfortunately, does not support choosing your desired data fields before download.  You will have to download them and then get categories you want.")
     elif atlas in ["Global","GBIF"]:
         pass
     else:
         raise ValueError("We currently cannot get occurrences from the {} atlas.".format(atlas))
 
@@ -219,15 +215,15 @@
 
         # create payload
         payload = add_to_payload_ALA(payload=payload,atlas=atlas,taxa=taxa,filters=filters,polygon=polygon,
                                      bbox=bbox,simplify_polygon=simplify_polygon,scientific_name=scientific_name)
         
         # create the query id
         qid_URL, method2 = get_api_url(column1="api_name",column1value="occurrences_qid")
-        qid = requests.request(method2,qid_URL,data=payload)
+        qid = requests.request(method2,qid_URL,data=payload,headers=headers)
         
         # create the URL to grab your queryID and counts
         if use_data_profile:
             data_profile_list = list(show_all(profiles=True)['shortName'])
             baseURL = apply_data_profile(baseURL=baseURL,data_profile_list=data_profile_list)   
 
         # Add qa=None to not get any assertions 
@@ -237,14 +233,15 @@
         elif fields == "all":
             URL = baseURL + "fq=%28qid%3A" + qid.text + "%29&qa=none&flimit=-1"
         else:
             URL = baseURL + "fq=%28qid%3A" + qid.text + "%29&" + galah_select(select=fields,atlas=atlas) + "&qa=none&flimit=-1"
 
         if verbose:
             print()
+            print("headers: {}".format(headers))
             print("payload for queryID: {}".format(payload))
             print("queryID URL: {}".format(qid_URL))
             print("method: {}".format(method2))
             print()
             print("qid for query: {}".format(qid.text))
             print("URL for result:{}".format(URL))
             print("method: {}".format(method))
@@ -349,14 +346,15 @@
                     "predicates": predicates
                 }
             })
 
             # check to see if user wants the query URL
             if verbose:
                 print("URL for querying:\n\n{}\n".format(URL))
+                print("headers: {}\n".format(headers))
                 print("payload: \n\n{}\n".format(payload))
 
             # check counts
             counts = atlas_counts(taxa,filters=filters)
             if not species_list:
                 print("total records for occurrences: {}\n".format(counts['totalRecords'][0]))
                 if int(counts['totalRecords'][0]) > 101000:
@@ -381,19 +379,14 @@
             ATLAS_OCCURRENCES_ERROR_MESSAGES[atlas]
         ))
     
     # raise an error if user has exceeded the limit of number of daily queries
     if response.status_code == 429:
         raise ValueError("You have reached the maximum number of daily queries for the ALA.")
     
-    # # if we get an error, raise one
-    # if atlas not in ["GBIF","Global"]:
-    #     if response.status_code == 403:
-    #         raise ValueError(response.json()["error"])
-
     # this may take a while - occasionally check if status has changed
     if atlas in ["Global","GBIF"]:
 
         # get job number
         job_number = response.text
 
         # make the download url
```

## galah/atlas_species.py

```diff
@@ -3,14 +3,17 @@
 from .search_taxa import search_taxa
 from .get_api_url import get_api_url,readConfig
 from .atlas_occurrences import atlas_occurrences
 from .apply_data_profile import apply_data_profile
 from .common_dictionaries import ATLAS_KEYWORDS,ATLAS_SPECIES_FIELDS,atlases
 from .common_functions import add_filters,add_to_payload_ALA
 from .show_all import show_all
+from .version import __version__
+
+import json
 
 def atlas_species(taxa=None,
                   scientific_name=None,
                   rank="species",
                   filters=None,
                   verbose=False,
                   status_accepted=True,
@@ -60,25 +63,20 @@
 
     # get configs
     configs = readConfig()
 
     # get atlas
     atlas = configs['galahSettings']['atlas'] 
 
-    # specify headers for API call
-    headers = {}
+    # get headers
+    headers = {"User-Agent": "galah-python/{}".format(__version__)}
 
     # create payload variable so it is available for some atlases
     payload = {}
 
-    #if atlas in ["Australia","ALA"]:
-    #    headers = {"x-api-key": configs["galahSettings"]["ALA_API_key"]}
-    #else:
-    #    headers = {}
-
     # first, check if the user has specified a taxa and if it is of the right variable type
     if type(taxa) is not str and type(taxa) is not list and taxa is not None:
         raise ValueError("Only a string or list can be specified for taxa names")
 
     # check to see if rank is in possible ranks for atlas
     if rank.lower() not in ATLAS_SPECIES_FIELDS[atlas]:
         raise ValueError("{} is not a valid rank for the {} atlas.  Possible ranks are:\n\n{}\n".format(rank,atlas,", ".join(ATLAS_SPECIES_FIELDS[atlas])))
@@ -102,15 +100,15 @@
         
         # create payload and add buffer to polygon if user specifies it
         payload = add_to_payload_ALA(payload=payload,atlas=atlas,taxa=taxa,filters=filters,polygon=polygon,
                                      bbox=bbox,simplify_polygon=simplify_polygon,scientific_name=scientific_name)
 
         # create the query id
         qid_URL, method2 = get_api_url(column1="api_name",column1value="occurrences_qid")
-        qid = requests.request(method2,qid_URL,data=payload)
+        qid = requests.request(method2,qid_URL,data=payload,headers=headers)
         
         # create the URL to grab the species ID and lists
         if use_data_profile:
             data_profile_list = list(show_all(profiles=True)['shortName'])
             baseURL = apply_data_profile(baseURL=baseURL,use_data_profile=use_data_profile,data_profile_list=data_profile_list)
             URL = baseURL + "fq=%28qid%3A" + qid.text + "%29&facets={}&lookup=True".format(rankID)
             if counts:
@@ -118,14 +116,16 @@
         else:
             URL = baseURL + "?fq=%28qid%3A" + qid.text + "%29&facets={}&lookup=True".format(rankID)
             if counts:
                 URL += "&count=true"
 
         if verbose:
             print()
+            print("headers: {}".format(headers))
+            print()
             print("payload for queryID: {}".format(payload))
             print("queryID URL: {}".format(qid_URL))
             print("method: {}".format(method2))
             print()
             print("qid for query: {}".format(qid.text))
             print("URL for result:{}".format(URL))
             print("method: {}".format(method))
```

## galah/common_dictionaries.py

```diff
@@ -1,9 +1,9 @@
 # all available atlases
-atlases = ["Australia","Austria","Brazil","France","Global","GBIF","Spain","Sweden"]
+atlases = ["Australia","Austria","Brazil","France","Global","GBIF","Guatemala","Spain","Sweden"] #,"United Kingdom"]
 
 # common names for each atlas
 ATLAS_COMMON_NAMES = {
     "Australia": "vernacularName",
     "Austria": "commonName",
     "Brazil": "commonName",
     "Canada": "",
@@ -27,15 +27,15 @@
     "Estonia": "guid",
     "France": "id",
     "Global": "usageKey",
     "GBIF": "usageKey",
     "Guatemala": "guid", # was guid
     "Portugal": "usageKey",
     "Spain": "taxonConceptID",
-    "Sweden": "id", # was guid
+    "Sweden": "taxonConceptID", # was guid
     "United Kingdom": "guid",
 }
 
 # error messages to tell the user where to register for the living atlases
 ATLAS_OCCURRENCES_ERROR_MESSAGES = {
     "Australia": "go to https://auth.ala.org.au/cas/login to register.",
     "Austria": "go to https://auth.biodiversityatlas.at/cas/login to register.",
@@ -50,15 +50,17 @@
 ATLAS_OCCURRENCES_DOWNLOAD_ARGUMENTS = {
     "Australia": {"finished_status": "finished","zipURL_arg": "downloadUrl","separator": ","},
     "Austria": {"finished_status": "finished","zipURL_arg": "downloadUrl","separator": ","},
     "Brazil": {"finished_status": "finished","zipURL_arg": "downloadUrl","separator": ","},
     "France": {"finished_status": "finished","zipURL_arg": "downloadUrl","separator": ","},
     "GBIF": {"finished_status": "SUCCEEDED","zipURL_arg": "downloadLink","separator": "\t"},
     "Global": {"finished_status": "SUCCEEDED","zipURL_arg": "downloadLink","separator": "\t"},
-    "Spain": {"finished_status": "finished","zipURL_arg": "downloadUrl","separator": ","}
+    "Guatemala": {"finished_status": "finished","zipURL_arg": "downloadUrl","separator": ","},
+    "Spain": {"finished_status": "finished","zipURL_arg": "downloadUrl","separator": ","},
+    "Sweden": {"finished_status": "finished","zipURL_arg": "downloadUrl","separator": ","}
 }
 
 # expanding species fields
 ATLAS_SPECIES_FIELDS = {
     "Australia": {"kingdom": "kingdomID", "phylum": "phylumID", "class": "classID", 
                   "order": "orderID", "family": "familyID","genus": "genusID", 
                   "species": "speciesID", "subspecies": "subspeciesID"},
@@ -69,28 +71,30 @@
                "order": "order_guid", "family": "family_guid","genus": "genus_guid", 
                "species": "species_guid", "subspecies": "subspecies_guid"},
     "Canada": {},
     "Estonia": {},
     "France": {"kingdom": "kingdomID", "phylum": "phylumID", "class": "classID", 
                   "order": "orderID", "family": "familyID","genus": "genusID", 
                   "species": "speciesID", "subspecies": "subspeciesID"},
-    "GBIF": {"kingdom": "KIMGDOM_KEY", "phylum": "PHYLUM_KEY", "class": "CLASS_KEY", 
+    "GBIF": {"kingdom": "KINGDOM_KEY", "phylum": "PHYLUM_KEY", "class": "CLASS_KEY", 
                   "order": "ORDER_KEY", "family": "FAMILY_KEY","genus": "GENUS_KEY", 
                   "species": "SPECIES_KEY", "subspecies": "SUBSPECIES_KEY"},
     "Global": {"kingdom": "KIMGDOM_KEY", "phylum": "PHYLUM_KEY", "class": "CLASS_KEY", 
                   "order": "ORDER_KEY", "family": "FAMILY_KEY","genus": "GENUS_KEY", 
                   "species": "SPECIES_KEY"}, #, "subspecies": "SUBSPECIES_KEY"},
-    "Guatemala": {},
+    "Guatemala": {"kingdom": "kingdom_guid", "phylum": "phylum_guid","class": "class_guid", 
+                  "order": "order_guid", "family": "family_guid","genus": "genus_guid", 
+                  "species": "species_guid", "subspecies": "subspecies_guid"},
     "Portugal": {},
     "Spain": {"kingdom": "kingdomID", "phylum": "phylumID", 
                   "class": "classID", "order": "orderID", "family": "familyID", 
                   "genus": "genusID", "species": "speciesID", "subspecies":"subspeciesID"},
     "Sweden": {"kingdom": "kingdom_id", "phylum": "phylum_id","class": "class_id", 
-               "order": "order_id", "family": "family_id","genus": "genus_id", 
-               "species": "species_id", "subspecies": "subspecies_id"},
+               "order": "order_id", "family": "family_id","genus": "genus", 
+               "species": "species", "subspecies": "subspecies"},
     "United Kingdom": {}
 }
 
 # default selections for occurrence data
 ATLAS_SELECTIONS = {
     "Australia": "basic",
     "Austria": ["latitude","longitude","occurrence_date","taxon_name","common_name",
@@ -99,19 +103,20 @@
                 "taxon_concept_lsid","occurrence_id","data_resource_uid","occurrence_status"],
     "Canada": [],
     "Estonia": [],
     "France": ["latitude","longitude","occurrence_date","taxon_name","common_name",
                 "taxon_concept_lsid","occurrence_id","data_resource_uid","occurrence_status"],
     "Global": ["decimalLatitude", "decimalLongitude", "eventDate", "scientificName", "taxonConceptID", "recordID", "dataResourceName", "occurrenceStatus"],
     "GBIF": ["decimalLatitude", "decimalLongitude", "eventDate", "scientificName", "taxonConceptID", "recordID", "dataResourceName", "occurrenceStatus"],
-    "Guatemala": [],
+    "Guatemala": ["latitude","longitude","occurrence_date","taxon_name","common_name",
+                "taxon_concept_lsid","occurrence_id","data_resource_uid","occurrence_status"],
     "Portugal": [],
     "Spain": ["latitude","longitude","occurrence_date","taxon_name","common_name",
                 "taxon_concept_lsid","occurrence_id","data_resource_uid","occurrence_status"],
-    "Sweden": [],
+    "Sweden": "basic",
     "United Kingdom": [],
 }
 
 # name of number of counts for atlas_counts
 COUNTS_NAMES = {
     "Australia": "totalRecords",
     "Austria": "totalRecords",
@@ -169,15 +174,17 @@
 
 # denotes keys where the results are
 SEARCH_TAXA_ENTRIES = {
     "Austria": ['searchResults','results'],
     "Brazil": ['searchResults','results'],
     "France": ['_embedded','taxa'],
     "Guatemala": ['searchResults','results'],
-    "Sweden": ['searchResults','results']
+    "Sweden": ['searchResults','results'],
+    "United Kingdom": ['searchResults','results'],
+    "UK": ['searchResults','results']
 }
 
 # fields to return for search_taxa
 SEARCH_TAXA_FIELDS = {
     "Australia": ['scientificName', 'scientificNameAuthorship', 'taxonConceptID','rank','match_type','kingdom', 
                   'phylum', 'class', 'order', 'family', 'genus', 'species', 'issues', 'vernacularName'],
     "Austria": ['scientificName', 'scientificNameAuthorship', 'guid','rank','match_type','kingdom', 
@@ -194,17 +201,18 @@
                   'phylum', 'class', 'order', 'family', 'genus', 'species', 'issues', 'vernacularName'],
     # was guid
     "Guatemala": ['scientificName', 'scientificNameAuthorship', 'guid','rank','match_type','kingdom', 
                   'phylum', 'class', 'order', 'family', 'genus', 'species', 'issues', 'commonName'],
     "Portugal": "",
     "Spain": ['scientificName', 'scientificNameAuthorship', 'taxonConceptID','rank','match_type','kingdom', 
                   'phylum', 'class', 'order', 'family', 'genus', 'species', 'issues', 'vernacularName'],
-    "Sweden": ['scientificName', 'scientificNameAuthorship', 'guid','rank','match_type','kingdom', 
+    "Sweden": ['scientificName', 'scientificNameAuthorship', 'taxonConceptID','rank','match_type','kingdom', 
                   'phylum', 'class', 'order', 'family', 'genus', 'species', 'issues', 'commonName'],
-    "United Kingdom": "",
+    "United Kingdom": ['scientificName', 'scientificNameAuthorship', 'guid','rank','match_type','kingdom', 
+                  'phylum', 'class', 'order', 'family', 'genus', 'species', 'issues', 'commonName']
 }
 
 # vernacular names in each atlas
 VERNACULAR_NAMES = {
     "Australia": ["commonNames","nameString"],
     "Austria": ["commonNames","commonName"], # try this
     "Brazil": ["commonNames","nameString"],
```

## galah/common_functions.py

```diff
@@ -4,14 +4,15 @@
 import shapely
 from .galah_filter import galah_filter
 from .get_api_url import get_api_url
 from .search_taxa import search_taxa
 from .galah_geolocate import galah_geolocate
 from .common_dictionaries import atlases, ATLAS_KEYWORDS
 from shapely import Polygon,MultiPolygon
+from .version import __version__
 
 def add_predicates(predicates=None,
                    filters=None):
     '''for adding filters specifically to atlas_occurrences'''
 
     if type(filters) == str:
         filters = [filters]
@@ -38,53 +39,60 @@
     # check if the atlas being used is GBIF
     if atlas in ["Global","GBIF"]:
 
         # check for filters that are not valid with GBIF
         if any("!=" in f for f in filters):
             raise ValueError("!= cannot be used with GBIF atlas.  Run separate queries.")
         else:
+            ### TODO: check that this is correct
             for f in filters:
                 URL += "&{}".format(galah_filter(f,ifgroupBy=ifGroupBy))
 
     # filters for all other atlases
     else:
 
         # check to see if taxa are already in the URL - if not, add fq
         if "fq=" not in URL:
             URL += "fq=%28"
         else:
             URL += "%28"
 
         # loop over filters
         for f in filters:
-            URL += galah_filter(f,ifgroupBy=ifGroupBy) + "AND" 
-                    
+            URL += galah_filter(f,ifgroupBy=ifGroupBy) + "%20AND%20"
+        
         # remove last AND and add a closing parenthesis
-        URL = URL[:-len("AND")] + "%29"
+        URL = URL[:-len("%20AND%20")] + "%29"
         
     return URL
 
 def put_entries_in_grouped_dict(entry=None,
                                 dict_values=None,
                                 expand=None
                                 ):
     '''Creating dictionaries for galah_group_by'''
 
     if expand:
+
         if len(entry['fq'].split(':')) > 2:
             name_and_values = entry['fq'].split(':')
             name = name_and_values[0]
             value = ":".join(name_and_values[1:])
         else:
             name,value=entry['fq'].split(':')
+
         value = value.replace('"', '')
         if value.isdigit():
             value = int(value)
-        dict_values[name].append(value)
-        dict_values['count'].append(int(entry['count']))
+        
+        # check that this is correct
+        if name in dict_values:
+            dict_values[name].append(value)
+            dict_values['count'].append(int(entry['count']))
+
         for key in dict_values:
             if (key != name) and (key != 'count'):
                 while (len(dict_values[key]) < len(dict_values['count'])):
                     dict_values[key].append("-")
 
     else:
         if len(entry['fq'].split(':')) > 2:
@@ -92,16 +100,18 @@
             name = name_and_values[0]
             value = ":".join(name_and_values[1:])
         else:
             name,value=entry['fq'].split(':')
         value=value.replace('"','')
         if value.isdigit():
             value = int(value)
-        dict_values[name].append(value)
-        dict_values['count'].append(int(entry['count']))
+        # check that this is correct
+        if name in dict_values:
+            dict_values[name].append(value)
+            dict_values['count'].append(int(entry['count']))
         for entry in dict_values:
             if (entry != name) and (entry != 'count'):
                 dict_values[entry].append("-")
 
     return dict_values    
 
 def get_response_show_all(column1=None,
@@ -109,14 +119,16 @@
                       column2=None,
                       column2value=None,
                       atlas=None,
                       headers={},
                       max_entries=-1,
                       offset=None):
     '''Function for getting responses for all of the show_all functions'''
+    # get headers
+    headers = {"User-Agent": "galah-python {}".format(__version__)}
 
     # get data and check for 
     URL,method = get_api_url(column1=column1,column1value=column1value,column2=column2,column2value=column2value)
     if max_entries is not None and offset is not None:
         URL += "?max={}&offset={}".format(max_entries,offset)
     response = requests.request(method,URL,headers=headers)
     if response.status_code == 403:
@@ -125,15 +137,16 @@
         raise ValueError("You have reached the maximum number of daily queries for the ALA.")
 
     # return response
     return response
 
 def generate_list_taxonConceptIDs(taxa=None,
                                   scientific_name=None,
-                                  atlas=None):
+                                  atlas=None,
+                                  verbose=None):
     '''Function for getting more than one taxonConceptIDs'''
 
     if taxa is None and scientific_name is None:
         raise ValueError("Please provide either a taxa or scientific_name for this information")
     
     if atlas is None:
         raise ValueError("Please provide an atlas to this function")
@@ -161,22 +174,22 @@
 
         if type(taxa) is str:
             taxa = [taxa]
         elif type(taxa) is list:
             pass
         else:
             raise TypeError("The taxa argument can only be a string or a list."
-                        "\nExample: atlas.counts(\"Vulpes vulpes\")"
-                        "\n         atlas.counts[\"Osphranter rufus\",\"Vulpes vulpes\",\"Macropus giganteus\",\"Phascolarctos cinereus\"])")
+                        "\nExample: galah.atlas_counts(\"Vulpes vulpes\")"
+                        "\n         galah.atlas_counts[\"Osphranter rufus\",\"Vulpes vulpes\",\"Macropus giganteus\",\"Phascolarctos cinereus\"])")
 
         # get the number of records associated with each taxa
         for name in taxa:
 
             # create temporary dataframe for taxon id
-            tempdf = search_taxa(name)
+            tempdf = search_taxa(taxa=name,verbose=verbose)
             
             # check if dataframe is empty - if so, return None; else, continue
             if tempdf.empty:
                 print("No taxon matches were found for {} in the selected atlas ({})".format(name, atlas))
                 if len(taxa) == 1:
                     return None
                 continue
```

## galah/config.ini

```diff
@@ -1,15 +1,10 @@
 [galahSettings]
-email=email@example.com
-email_notify=False
-atlas=Australia
-data_profile=None
-ranks=all
-reason=4
-# remove defaults when releasing
-#usernameGBIF= 
-#passwordGBIF= 
-# ALA data - DO NOT COMMIT ANYWHERE
-# for galah
-# for ala4r@ala.org.au
-#ALA_API_key=
-# for jwt generation
+email = 
+email_notify = False
+atlas = Australia
+data_profile = None
+ranks = all
+reason = 4
+usernamegbif = 
+passwordgbif = 
+
```

## galah/galah_filter.py

```diff
@@ -143,14 +143,15 @@
 
             # less than or equal to
             elif specialChar == '<=' or specialChar == '=<':
                 return "{}:[* TO {}]".format(parts[0], parts[1])
 
             # not equal to
             elif specialChar == '!=' or specialChar == '=!':
+                print("here")
                 return "-{}:{}".format(parts[0], parts[1])
                 
             # else, there is either an error in the filters or a missing case
             else:
                 raise ValueError("The special character {} is not included in the filters function.  Either it is not a logical operator, or it has not been included yet.".format(specialChar[0]))
 
         # all other atlases are like this
```

## galah/galah_group_by.py

```diff
@@ -2,14 +2,15 @@
 import pandas as pd
 import urllib
 import copy
 import itertools
 from .get_api_url import readConfig
 from .common_functions import add_filters
 from .common_functions import get_api_url,put_entries_in_grouped_dict,add_to_payload_ALA
+from .version import __version__
 
 def galah_group_by(URL=None,
                    method=None,
                    group_by=None,
                    total_group_by=False,
                    filters=None,
                    expand=True,
@@ -26,22 +27,16 @@
     # get configs
     configs = readConfig()
 
     # get atlas
     atlas = configs['galahSettings']['atlas']
 
     # get headers
-    headers = {}
-    #if atlas in ["Australia","ALA"]:
-    #    headers = {"x-api-key": configs["galahSettings"]["ALA_API_key"]}
-    #else:
-    #    headers = {}
-
-    # if atlas in ["Australia","ALA"]:
-
+    headers = {"User-Agent": "galah-python {}".format(__version__)}
+    
     # check to see if the expand option is true
     if expand:
 
         # check to see if you can expand upon this
         if (type(group_by) == str) or (type(group_by) == list and len(group_by) == 1):
             raise ValueError("You can only use the expand=False option with one group")
 
@@ -84,24 +79,26 @@
         if atlas in ["Australia","ALA"]:
             # try startingURL2
             #startingURL2,method = get_api_url(column1='called_by',column1value='atlas_counts',column2="api_name",
             #                            column2value="records_counts")
 
             # get response from your query, which will include all available fields
             qid_URL, method2 = get_api_url(column1="api_name",column1value="occurrences_qid")
-            qid = requests.request(method2,qid_URL,data=payload)
+            qid = requests.request(method2,qid_URL,data=payload,headers=headers)
             facets = "".join("&facets={}".format(g) for g in group_by)
             if startingURL[-1] == "&":
                 URL = startingURL + "fq=%28qid%3A" + qid.text + "%29" + facets + "&flimit=-1&pageSize=0"
             else:
                 URL = startingURL + "?fq=%28qid%3A" + qid.text + "%29" + facets + "&flimit=-1&pageSize=0"
 
             # check to see if the user wants the URL for querying
             if verbose:
                 print()
+                print("headers: {}".format(headers))
+                print()
                 print("payload for queryID: {}".format(payload))
                 print("queryID URL: {}".format(qid_URL))
                 print("method: {}".format(method2))
                 print()
                 print("qid for query: {}".format(qid.text))
                 print("URL for result:{}".format(URL))
                 print("method: {}".format(method))
@@ -124,14 +121,16 @@
 
             # round out the URL
             startingURL += "&flimit=-1&pageSize=0"
             
             # check to see if the user wants the URL for querying
             if verbose:
                 print()
+                print("headers: {}".format(headers))
+                print()
                 print("URL for querying: {}".format(startingURL))
                 print("Method: {}".format(method))
                 print()
 
             # get response from your query, which will include all available fields
             response = requests.request(method,startingURL,headers=headers)
             response_json = response.json()
@@ -148,72 +147,92 @@
                 results_array = response_json['facets']
                 facet_name = 'name'
         elif atlas in ["Brazil"]:
             length = len(response_json)
             results_array = response_json 
             field_name = 'fieldResult' 
             facet_name = 'fq'
+        # elif atlas in ["Guatemala"]:
+        #     print(len(response_json))
+        #     print(response_json)
+        #     import sys
+        #     sys.exit()
         else:
             length = len(response_json['facetResults'])
             results_array = response_json['facetResults']
             field_name = 'fieldResult'
             if expand:
                 facet_name = 'fq'
 
         # get all counts for each value
         dict_values = {entry: [] for entry in [*group_by,'count']}
 
         # do this if the expand option is try
         if expand:
 
             # was 1,len(group_by)
-            for i in range(0,len(group_by)-1):
+            ### TRY THIS
+            if atlas not in ["Global","GBIF"]:
+                start = 0
+                end = len(group_by) - 1
+            else:
+                start = 1
+                end = len(group_by)
+            
+            # now do loop
+            for i in range(start,end):
                 temp_array=[]
                 for entry in results_array[i][field_name]:
                     temp_array.append(entry[facet_name])
                 facets_array.append(temp_array)
 
             combined_facets_array = list(itertools.product(*facets_array))
 
             # loop over facets array
             # was combined_facets_array
-            for i,f in enumerate(combined_facets_array):
+            for f in combined_facets_array:
 
                 # check for GBIF atlas
                 if atlas in ["Global","GBIF"]:
 
-                    # loop over all specified facets
+                    inc = 0
+
                     for facet in f:
 
-                        tempURL = URL + "&{}={}".format(group_by[i+1],urllib.parse.quote(facet)) + "&facet=" + group_by[i] + "&flimit=-1&pageSize=0"
+                        # was i+1
+                        tempURL = URL + "&{}={}".format(group_by[start+inc],urllib.parse.quote(facet)) + "&facet=" + group_by[start-1+inc] + "&flimit=-1&pageSize=0"
+                    
                         # check if user is grouping by scientific name
                         # i + 1
-                        if group_by[i+1] == "scientificName":
-                            tempURL = URL + "&{}={}".format(group_by[i+1],"%20".join(facet.split(" ")[0:2])) + "&facet=" + group_by[i] + "&flimit=-1&pageSize=0"
+                        if group_by[start+inc] == "scientificName":
+                            tempURL = URL + "&{}={}".format(group_by[start+inc],"%20".join(facet.split(" ")[0:2])) + "&facet=" + group_by[start-1+inc] + "&flimit=-1&pageSize=0"
                         else:
-                            tempURL = URL + "&{}={}".format(group_by[i+1],"%20".join(facet.split(" "))) + "&facet=" + group_by[i] + "&flimit=-1&pageSize=0"
-            
-                        if verbose:
-                            print()
-                            print("URL for querying: {}".format(tempURL))
-                            print("Method: {}".format(method))
-                            print()
+                            tempURL = URL + "&{}={}".format(group_by[start+inc],"%20".join(facet.split(" "))) + "&facet=" + group_by[start-1+inc] + "&flimit=-1&pageSize=0"
+        
+                        inc += 1
 
-                        # get the data
-                        response=requests.request(method,tempURL,headers=headers)
-                        response_json = response.json()
-                        
-                        # put data in dict
-                        for entry in response_json['facets'][0]['counts']:
-                            dict_values[group_by[i]].append(entry['name'])
-                            dict_values['count'].append(int(entry['count']))
-                            dict_values[group_by[i+1]].append(facet)
-                            for key in dict_values:
-                                if (key != group_by[i+1]) and (key != group_by[i]) and (key != 'count'):
-                                    dict_values[key].append("-")
+                    if verbose:
+                        print()
+                        print("URL for querying: {}".format(tempURL))
+                        print("Method: {}".format(method))
+                        print()
+
+                    # get the data
+                    response=requests.request(method,tempURL,headers=headers)
+                    response_json = response.json()
+                    
+                    # put data in dict
+                    #### TODO: check if this is correct
+                    for entry in response_json['facets'][0]['counts']:
+                        dict_values[group_by[start-1]].append(entry['name'])
+                        dict_values['count'].append(int(entry['count']))
+                        dict_values[group_by[start]].append(facet)
+                        for key in dict_values:
+                            if (key != group_by[start]) and (key != group_by[start-1]) and (key != 'count'):
+                                dict_values[key].append("-")
                 
                 # do this loop for all other atlases 
                 else:
 
                     # loop over each facet
                     #for facet in f:
                     if atlas in ["Australia","ALA"]:
@@ -224,15 +243,15 @@
                         else:
                             payload["fq"].append(f)
                             
                         payload_for_querying = copy.deepcopy(payload)
                         
                         # create payload and get qid
                         qid_URL, method2 = get_api_url(column1="api_name",column1value="occurrences_qid")
-                        qid = requests.request(method2,qid_URL,data=payload)
+                        qid = requests.request(method2,qid_URL,data=payload,headers=headers)
                         if startingURL[-1] == "&":
                             tempURL = startingURL + "fq=%28qid%3A" + qid.text + "%29" 
                         else:
                             tempURL = startingURL + "?fq=%28qid%3A" + qid.text + "%29"
                         if any("lsid" in fq for fq in payload['fq']):
                             index = [idx for idx, s in enumerate(payload['fq']) if 'lsid' in s][0]
                             payload["fq"] = [payload["fq"][index]]
@@ -242,24 +261,27 @@
                             payload = add_to_payload_ALA(payload=payload,
                                                          atlas=atlas,
                                                          filters=filters)
                         tempURL += "&facets={}".format(group_by[-1])
                         
                     else:
 
-                        # split each facet to make it human readable
-                        name,value = facet.split(':')
-                        value = value.replace('"', '')
-                        if name in group_by:
-                            tempURL = URL + "%20AND%20%28{}%3A%22{}%22%29".format(name,value)
-                        else:
-                            continue
-                        for group in group_by:
-                            if (group != name) and ("facets={}".format(group) not in URL):
-                                tempURL += "&facets={}".format(group)
+                        for facet in f:
+                        
+                            # split each facet to make it human readable
+                            name,value = facet.split(':')
+                            value = value.replace('"', '')
+                            if name in group_by:
+                                tempURL = URL + "%20AND%20%28{}%3A%22{}%22%29".format(name,value)
+                            else:
+                                tempURL = URL
+                                # continue
+                            for group in group_by:
+                                if (group != name) and ("facets={}".format(group) not in URL):
+                                    tempURL += "&facets={}".format(group)
 
                     # finalise the URL for querying
                     tempURL += "&flimit=-1&pageSize=0"
 
                     # check to see if the user wants the URL for querying
                     if verbose:
                         if atlas in ["Australia","ALA"]:
@@ -288,37 +310,43 @@
                             continue
                     else:
                         if response_json is None or not response_json[0]['fieldResult']:
                             continue
 
                     # put data in table (and check if user wants Brazil, because that is an exception)
                     if atlas in ["Brazil"]:
-                        results_array = response_json[0]['fieldResult']
+                        if len(group_by) <= 2:
+                            results_array = response_json[0]['fieldResult']
+                        else:
+                            results_array = response_json[1]['fieldResult']
                     else:
                         results_array = response_json['facetResults'][0]['fieldResult']
 
                     # loop over each entry in the results
                     for entry in results_array:
 
                         if entry['fq'].split(":")[0] in group_by:
-                        
+
                             # add facet value to dictionary for expand
                             for facet in f:
+
                                 if len(facet.split(':')) > 2:
                                     name_and_values = facet.split(':')
                                     name = name_and_values[0]
                                     value = ":".join(name_and_values[1:])
                                 else:
                                     name,value=facet.split(':')
                                 value = value.replace('"','')
-                                # trying this
-                                if name in group_by:
+                                # trying this - potentially remove it
+                                if name in group_by and name not in entry['fq'].split(':'):
                                     dict_values[name].append(value)
-                            dict_values = put_entries_in_grouped_dict(entry=entry,dict_values=dict_values,expand=expand)
-            
+
+                        # potentially tab again
+                        dict_values = put_entries_in_grouped_dict(entry=entry,dict_values=dict_values,expand=expand)
+
             # format table
             counts = pd.DataFrame(dict_values).reset_index(drop=True)
             counts.sort_values(by=group_by)
 
             # if user wants total, return total number of rows
             if total_group_by:
                 return pd.DataFrame({'count': [counts.shape[0]]})
```

## galah/node_config.csv

```diff
@@ -58,38 +58,14 @@
 Brazil,records,records_occurrences,https://biocache-service.sibbr.gov.br/biocache-service/occurrences/offline/download,atlas_occurrences,TRUE,GET
 Brazil,records,records_query,https://biocache-service.sibbr.gov.br/biocache-service/webportal/params,atlas_occurrences,TRUE,GET
 Brazil,records,records_species,https://biocache-service.sibbr.gov.br/biocache-service/occurrences/facets/download,atlas_species,TRUE,GET
 Brazil,spatial,spatial_layers,https://portal-espacial.sibbr.gov.br/spatial-hub/layers,show_all-fields,TRUE,GET
 Brazil,species,names_search_single,https://bie-webservice.sibbr.gov.br/bie-index/search?q={name}&pageSize=5,search_taxa,TRUE,GET
 Brazil,species,species_children,https://bie-webservice.sibbr.gov.br/bie-index/childConcepts/,atlas_taxonomy,TRUE,GET
 Brazil,species,species_lookup,https://bie-webservice.sibbr.gov.br/bie-index/species,atlas_taxonomy,TRUE,GET
-Canada,collections,collections_collections,http://collections.canadensys.net/ws/collection,show_all-collections,TRUE,GET
-Canada,collections,collections_datasets,http://collections.canadensys.net/ws/dataResource,show_all-datasets,TRUE,GET
-Canada,collections,collections_providers,http://collections.canadensys.net/ws/dataProvider,show_all-providers,TRUE,GET
-Canada,logger,logger_reasons,http://logger.canadensys.net/service/logger/reasons,show_all-reasons,TRUE,GET
-Canada,records,records_assertions,http://explorer-ws.canadensys.net/assertions/codes,show_all-assertions,TRUE,GET
-Canada,records,records_counts,http://explorer-ws.canadensys.net/occurrences/search,atlas_counts,TRUE,GET
-Canada,records,records_facets,http://explorer-ws.canadensys.net/occurrence/facets,"atlas_counts, show_values-fields",TRUE,GET
-Canada,records,records_fields,http://explorer-ws.canadensys.net/index/fields,show_all-fields,TRUE,GET
-Canada,records,records_occurrences,http://explorer-ws.canadensys.net/occurrences/offline/download,atlas_occurrences,TRUE,GET
-Canada,records,records_query,http://explorer-ws.canadensys.net/webportal/params,atlas_occurrences,TRUE,GET
-Canada,records,records_species,http://explorer-ws.canadensys.net/occurrences/facets/download,atlas_species,TRUE,GET
-Canada,species,names_search_single,https://api.gbif.org/v1/species/match?verbose=FALSE&name={name},search_taxa,TRUE,GET
-Estonia,collections,collections_collections,https://elurikkus.ee/collectory/ws/collection,show_all-collections,TRUE,GET
-Estonia,collections,collections_datasets,https://elurikkus.ee/collectory/ws/dataResource,show_all-datasets,TRUE,GET
-Estonia,collections,collections_providers,https://elurikkus.ee/collectory/ws/dataProvider,show_all-providers,TRUE,GET
-Estonia,records,records_assertions,https://elurikkus.ee/biocache-service/assertions/codes,show_all-assertions,TRUE,GET
-Estonia,records,records_counts,https://elurikkus.ee/biocache-service/occurrences/search,atlas_counts,TRUE,GET
-Estonia,records,records_facets,https://elurikkus.ee/biocache-service/occurrence/facets,"atlas_counts, show_values-fields",TRUE,GET
-Estonia,records,records_fields,https://elurikkus.ee/biocache-service/index/fields,show_all-fields,TRUE,GET
-Estonia,records,records_query,https://elurikkus.ee/biocache-service/webportal/params,atlas_occurrences,TRUE,POST
-Estonia,records,records_species,https://elurikkus.ee/biocache-service/occurrences/facets/download,atlas_species,TRUE,GET
-Estonia,species,names_search_single,https://elurikkus.ee/bie-index/search?q={name}&pageSize=5,search_taxa,TRUE,GET
-Estonia,species,species_children,https://elurikkus.ee/bie-index/childConcepts/,atlas_taxonomy,TRUE,GET
-Estonia,species,species_lookup,https://elurikkus.ee/bie-index/species,atlas_taxonomy,TRUE,GET
 France,collections,collections_collections,https://openobs.mnhn.fr/collectory/ws/collection,show_all-collections,TRUE,GET
 France,collections,collections_datasets,https://openobs.mnhn.fr/collectory/ws/dataResource,show_all-datasets,TRUE,GET
 France,collections,collections_providers,https://openobs.mnhn.fr/collectory/ws/dataProvider,show_all-providers,TRUE,GET
 France,records,records_assertions,https://openobs.mnhn.fr/biocache-service/assertions/codes,show_all-assertions,TRUE,GET
 France,records,records_counts,https://openobs.mnhn.fr/biocache-service/occurrences/search,atlas_counts,TRUE,GET
 France,records,records_facets,https://openobs.mnhn.fr/biocache-service/occurrence/facets,"atlas_counts, show_values-fields",TRUE,GET
 France,records,records_fields,https://openobs.mnhn.fr/biocache-service/index/fields,show_all-fields,TRUE,GET
@@ -119,28 +95,14 @@
 Guatemala,records,records_occurrences,https://snib.conap.gob.gt/registros-ws/occurrences/offline/download,atlas_occurrences,TRUE,GET
 Guatemala,records,records_query,https://snib.conap.gob.gt/registros-ws/webportal/params,atlas_occurrences,TRUE,GET
 Guatemala,records,records_species,https://snib.conap.gob.gt/registros-ws/occurrences/facets/download,atlas_species,TRUE,GET
 Guatemala,spatial,spatial_layers,https://geoespacial.snib.conap.gob.gt/ws/layers,show_all-fields,TRUE,GET
 Guatemala,species,names_search_single,https://snib.conap.gob.gt/especies-ws/search?q={name}&pageSize=5,search_taxa,TRUE,GET
 Guatemala,species,species_children,https://snib.conap.gob.gt/especies-ws/childConcepts/,atlas_taxonomy,TRUE,GET
 Guatemala,species,species_lookup,https://snib.conap.gob.gt/especies-ws/species,atlas_taxonomy,TRUE,GET
-Portugal,collections,collections_collections,https://metadados.gbif.pt/ws/collection,show_all-collections,TRUE,GET
-Portugal,collections,collections_datasets,https://metadados.gbif.pt/ws/dataResource,show_all-datasets,TRUE,GET
-Portugal,collections,collections_providers,https://metadados.gbif.pt/ws/dataProvider,show_all-providers,TRUE,GET
-Portugal,images,image_licences,https://imagens.gbif.pt/ws/licence,show_all-licences,TRUE,GET
-Portugal,images,image_metadata,https://imagens.gbif.pt/ws/image/,media_metadata,FALSE,GET
-Portugal,logger,logger_reasons,https://logger.gbif.pt/service/logger/reasons,show_all-reasons,TRUE,GET
-Portugal,records,records_assertions,https://registos-ws.gbif.pt/assertions/codes,show_all-assertions,TRUE,GET
-Portugal,records,records_counts,https://registos-ws.gbif.pt/occurrences/search,atlas_counts,TRUE,GET
-Portugal,records,records_facets,https://registos-ws.gbif.pt/occurrence/facets,"atlas_counts, show_values-fields",TRUE,GET
-Portugal,records,records_fields,https://registos-ws.gbif.pt/index/fields,show_all-fields,TRUE,GET
-Portugal,records,records_query,https://registos-ws.gbif.pt/webportal/params,atlas_occurrences,TRUE,GET
-Portugal,records,records_species,https://registos-ws.gbif.pt/occurrences/facets/download,atlas_species,TRUE,GET
-Portugal,spatial,spatial_layers,https://espacial.gbif.pt/ws/layers,show_all-fields,TRUE,GET
-Portugal,species,names_search_single,https://api.gbif.org/v1/species/match?verbose=FALSE&name={name},search_taxa,TRUE,GET
 Spain,collections,collections_collections,https://colecciones.gbif.es/ws/collection,show_all-collections,TRUE,GET
 Spain,collections,collections_datasets,https://colecciones.gbif.es/ws/dataResource,show_all-datasets,TRUE,GET
 Spain,collections,collections_providers,https://colecciones.gbif.es/ws/dataProvider,show_all-providers,TRUE,GET
 Spain,doi,doi_download,https://doi.gbif.es/doi/{doi_string}/download,doi_download,TRUE,GET
 Spain,images,image_licences,https://imagenes.gbif.es/ws/licence,show_all-licences,TRUE,GET
 Spain,images,image_metadata,https://imagenes.gbif.es/ws/image/,media_metadata,FALSE,GET
 Spain,lists,lists_all,https://listas.gbif.es/ws/speciesList,show_all-lists,TRUE,GET
@@ -175,26 +137,8 @@
 Sweden,records,records_query,https://records.biodiversitydata.se/ws/webportal/params,atlas_occurrences,TRUE,GET
 Sweden,records,records_species,https://records.biodiversitydata.se/ws/occurrences/facets/download,atlas_species,TRUE,GET
 Sweden,spatial,spatial_layers,https://sds.biodiversitydata.se/ws/layers,show_all-fields,TRUE,GET
 Sweden,species,species_children,https://species.biodiversitydata.se/ws/childConcepts/,atlas_taxonomy,TRUE,GET
 Sweden,species,species_lookup,https://species.biodiversitydata.se/ws/species,atlas_taxonomy,TRUE,GET
 Sweden,name-matching,names_lookup,https://namematching.biodiversitydata.se/api/getByTaxonID,search_identifiers,TRUE,GET
 Sweden,name-matching,names_search_multiple,https://namematching.biodiversitydata.se/api/searchByClassification,search_taxa,TRUE,GET
-Sweden,name-matching,names_search_single,https://namematching.biodiversitydata.se/api/search?q={name},search_taxa,TRUE,GET
-United Kingdom,collections,collections_collections,https://registry.nbnatlas.org/ws/collection,show_all-collections,TRUE,GET
-United Kingdom,collections,collections_datasets,https://registry.nbnatlas.org/ws/dataResource,show_all-datasets,TRUE,GET
-United Kingdom,collections,collections_providers,https://registry.nbnatlas.org/ws/dataProvider,show_all-providers,TRUE,GET
-United Kingdom,images,image_licences,https://images.nbnatlas.org/licence,show_all-licences,TRUE,GET
-United Kingdom,images,image_metadata,https://images.nbnatlas.org/image/,media_metadata,FALSE,GET
-United Kingdom,lists,lists_all,https://lists.nbnatlas.org/ws/speciesList,show_all-lists,TRUE,GET
-United Kingdom,lists,lists_lookup,https://lists.nbnatlas.org/ws/speciesListItems/{list_id},show_values-lists,TRUE,GET
-United Kingdom,logger,logger_reasons,https://logger.nbnatlas.org/service/logger/reasons,show_all-reasons,TRUE,GET
-United Kingdom,records,records_assertions,https://records-ws.nbnatlas.org/assertions/codes,show_all-assertions,TRUE,GET
-United Kingdom,records,records_counts,https://records-ws.nbnatlas.org/occurrences/search,atlas_counts,TRUE,GET
-United Kingdom,records,records_facets,https://records-ws.nbnatlas.org/occurrence/facets,"atlas_counts, show_values-fields",TRUE,GET
-United Kingdom,records,records_fields,https://records-ws.nbnatlas.org/index/fields,show_all-fields,TRUE,GET
-United Kingdom,records,records_occurrences,https://records-ws.nbnatlas.org/occurrences/offline/download,atlas_occurrences,FALSE,GET
-United Kingdom,records,records_query,https://records-ws.nbnatlas.org/webportal/params,atlas_occurrences,TRUE,GET
-United Kingdom,records,records_species,https://records-ws.nbnatlas.org/occurrences/facets/download,atlas_species,TRUE,GET
-United Kingdom,species,names_search_single,https://species-ws.nbnatlas.org/search?q={name}&pageSize=5,search_taxa,TRUE,GET
-United Kingdom,species,species_children,https://species-ws.nbnatlas.org/childConcepts/,atlas_taxonomy,TRUE,GET
-United Kingdom,species,species_lookup,https://species-ws.nbnatlas.org/species,atlas_taxonomy,TRUE,GET
+Sweden,name-matching,names_search_single,https://namematching.biodiversitydata.se/api/search?q={name},search_taxa,TRUE,GET
```

## galah/search_all.py

```diff
@@ -238,15 +238,15 @@
         else:
             raise ValueError(
                 "You can only pass one string to your search parameter = run show_all(fields=True) to get strings to pass")
     
     # search options for licences
     if licences is not None:
 
-        # call show_all to get all the possible values
+        # call show_all to get all the possible values``
         dataFrame = show_all(licences=True)
 
         # check to see if user wants default column name
         if column_name is None:
             column_name = 'name'
 
         # throw ValueError if column_name variable is not a string
```

## galah/search_taxa.py

```diff
@@ -1,13 +1,14 @@
 import requests
 import pandas as pd
 import urllib
 
 from .get_api_url import get_api_url,readConfig
 from .common_dictionaries import SEARCH_TAXA_ENTRIES,SEARCH_TAXA_FIELDS,TAXONCONCEPT_NAMES,VERNACULAR_NAMES,atlases
+from .version import __version__
 
 # testing
 import json
 
 # debugging
 import sys
 
@@ -80,21 +81,15 @@
 
     .. program-output:: python -c "import galah; import pandas as pd;pd.set_option('display.max_columns', None);print(galah.search_taxa(scientific_name={\\\"family\\\": [\\\"pardalotidae\\\",\\\"maluridae\\\"],\\\"scientificName\\\": [\\\"pardolatus striatus\\\",\\\"malurus cyaneus\\\"]}))"
     """
 
     # get configuration
     configs = readConfig()
 
-    headers = {}
-
-    # get headers for ALA
-    #if configs['galahSettings']['atlas'] == "Australia":
-    #    headers = {"x-api-key": configs["galahSettings"]["ALA_API_key"]}
-    #else:
-    #    headers = {}
+    headers = {"User-Agent": "galah-python/{}".format(__version__)}
 
     # get atlas
     atlas = configs['galahSettings']['atlas']
 
     # check for identifiers or specific epithets
     if identifiers is not None or specific_epithet is not None:
 
@@ -231,62 +226,51 @@
             })
             return species_list_rename[['scientificName', 'scientificNameAuthorship', 'taxonConceptID','rank','kingdom', 
                   'phylum', 'class', 'order', 'family', 'genus', 'species','vernacularName']]
 
         else:
         
             for name in taxa:
-
+                
                 # get base URL for querying
                 baseURL, method = get_api_url(column1='called_by',column1value='search_taxa',column2='api_name',column2value='names_search_single')
 
                 # create URL, get result and concatenate result onto dataFrame
                 # make sure all the atlases are checked
                 if atlas in atlases:
                     URL = baseURL.replace("{name}","%20".join(name.split(" ")))
                 else:
                     raise ValueError("Atlas {} is not taken into account".format(atlas))
                 
                 if verbose:
                     print("\nURL being queried:\n\n{}\n".format(URL))
             
                 # get the response
-                response = requests.request(method,URL,headers=headers)
+                response = requests.request(method=method,url=URL,headers=headers)
                 response_json = response.json()
 
                 if atlas in ["Australia","ALA"] and not response_json["success"]:
                     if "homonym" in response_json["issues"]:
                         print("Warning: Search returned multiple taxa due to a homonym issue.")
                         print("Please use the `scientific_name` argument to clarify taxa.")
                         return pd.DataFrame({"search_term": taxa, "issues": response_json["issues"]})
 
-                # Check for the Swedish atlas
-                if atlas in ["Sweden"]: 
-                    raw_data = [] 
-                    if SEARCH_TAXA_ENTRIES[atlas][0] in response_json:
-                        for item in response_json[SEARCH_TAXA_ENTRIES[atlas][0]][SEARCH_TAXA_ENTRIES[atlas][1]]:
-                            if name.lower() in item['scientificName'].lower():
-                                raw_data = item
-                                break
-                    if raw_data is None:
-                        continue
-
                 # check for Austrian, Brazilian, French or Guatemalan atlas
-                elif atlas in ["Austria","Brazil","France", "Guatemala"]:
+                elif atlas in ["Austria","Brazil","France", "Guatemala","United Kingdom","UK"]: # try UK here
                     raw_data = None
                     if SEARCH_TAXA_ENTRIES[atlas][0] in response_json:
                         for item in response_json[SEARCH_TAXA_ENTRIES[atlas][0]][SEARCH_TAXA_ENTRIES[atlas][1]]:
                             if name.lower() == item['scientificName'].lower():
                                 raw_data = item
                                 break
                     if raw_data is None:
                         continue
                 
                 # check for Australian, Global, or Spanish atlas
-                elif atlas in ["Australia","Global","GBIF","Spain"]:
+                elif atlas in ["Australia","Global","GBIF","Spain","Sweden"]: # try Sweden here
                     raw_data = response_json
                     if atlas in ["Global","GBIF"]:
                         response_vernacular = requests.get("https://api.gbif.org/v1/species/{}/vernacularNames".format(raw_data[TAXONCONCEPT_NAMES[atlas]["guid"]]))
                         array_vernacular = response_vernacular.json()['results']
                 
                 # else, throw an error saying this atlas is not taken into account
                 else:
```

## galah/show_all.py

```diff
@@ -1,13 +1,14 @@
 import os
 import pandas as pd
 
 from .get_api_url import readConfig
 from .common_dictionaries import atlases as ATLASES
 from .common_functions import get_response_show_all
+from .version import __version__
 
 '''
 function is meant to show all values for possible query fields - they are defined as a boolean variable so you can see
 the large list of all the potential variables to add to your atlas query.
 '''
 def show_all(assertions=False,
              atlases=False,
@@ -71,20 +72,15 @@
     """
 
     # get configurations for different atlases
     configs = readConfig()
 
     atlas = configs['galahSettings']['atlas']
 
-    headers = {}
-
-    #if atlas in ["Australia","ALA"]:
-    #    headers = {"x-api-key": configs["galahSettings"]["ALA_API_key"]}
-    #else:
-    #    headers = {}
+    headers = {"User-Agent": "galah-python/{}".format(__version__)}
 
     # set up the option for getting back multiple values
     return_array=[]
 
     # check for assertions boolean
     if type(assertions) is bool and assertions:
 
@@ -270,18 +266,18 @@
             spatial_values = pd.DataFrame.from_dict(response.json())
             spatial_layers = pd.DataFrame()
 
             # select only the columns titled 'name', 'info', (and) 'infoUrl'
             if atlas in ["Australia","Spain"]:
 
                 # build layer id from this
-                spatial_values["type"].replace("Contextual","cl",inplace=True)
-                spatial_values["type"].replace("Environmental","el",inplace=True)
+                spatial_values.loc[spatial_values["type"] == "Contextual","type"] = "cl"
+                spatial_values.loc[spatial_values["type"] == "Environmental","type"] = "el"
                 spatial_layers["id"] =  spatial_values["type"].astype(str) + spatial_values["id"].astype(str)
-
+                
                 # build descriptions from these
                 spatial_layers["description"] = spatial_values['displayname'] + " " + spatial_values['description']
                 spatial_layers["type"] = "layers"
                 spatial_layers["link"] = ""
 
             # look only into these atlases 
             elif atlas in ["Austria","Brazil","France"]:
```

