# Comparing `tmp/nvidia_cutlass-3.4.1.0-py3-none-any.whl.zip` & `tmp/nvidia_cutlass-3.5.0.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,1099 +1,1141 @@
-Zip file size: 3808339 bytes, number of entries: 1097
--rw-r--r--  2.0 unx     6220 b- defN 24-Feb-15 21:09 cutlass/__init__.py
--rw-r--r--  2.0 unx    26509 b- defN 24-Jan-13 19:00 cutlass/library_defaults.py
--rw-r--r--  2.0 unx     5715 b- defN 24-Jan-13 19:00 cutlass/shape.py
--rw-r--r--  2.0 unx     2718 b- defN 24-Jan-13 19:00 cutlass/swizzle.py
--rw-r--r--  2.0 unx     2464 b- defN 24-Jan-13 19:00 cutlass/backend/__init__.py
--rw-r--r--  2.0 unx     5678 b- defN 24-Jan-13 19:00 cutlass/backend/arguments.py
--rw-r--r--  2.0 unx    21870 b- defN 24-Feb-15 21:09 cutlass/backend/c_types.py
--rw-r--r--  2.0 unx    18206 b- defN 24-Jan-13 19:00 cutlass/backend/compiler.py
--rw-r--r--  2.0 unx    26627 b- defN 24-Jan-13 19:00 cutlass/backend/conv2d_operation.py
--rw-r--r--  2.0 unx    17649 b- defN 24-Feb-15 21:09 cutlass/backend/epilogue.py
--rw-r--r--  2.0 unx     3813 b- defN 24-Jan-13 19:00 cutlass/backend/frontend.py
--rw-r--r--  2.0 unx    85184 b- defN 24-Feb-15 21:09 cutlass/backend/gemm_operation.py
--rw-r--r--  2.0 unx    17280 b- defN 24-Jan-13 19:00 cutlass/backend/library.py
--rw-r--r--  2.0 unx     4332 b- defN 24-Jan-13 19:00 cutlass/backend/memory_manager.py
--rw-r--r--  2.0 unx     5523 b- defN 24-Jan-13 19:00 cutlass/backend/operation.py
--rw-r--r--  2.0 unx    15701 b- defN 24-Jan-13 19:00 cutlass/backend/reduction_operation.py
--rw-r--r--  2.0 unx     1907 b- defN 24-Jan-13 19:00 cutlass/backend/type_hint.py
--rw-r--r--  2.0 unx     1920 b- defN 24-Jan-13 19:00 cutlass/backend/evt/__init__.py
--rw-r--r--  2.0 unx     7027 b- defN 24-Jan-13 19:00 cutlass/backend/evt/epilogue.py
--rw-r--r--  2.0 unx     2047 b- defN 24-Jan-13 19:00 cutlass/backend/evt/backend/__init__.py
--rw-r--r--  2.0 unx     6453 b- defN 24-Jan-13 19:00 cutlass/backend/evt/backend/emitter_base.py
--rw-r--r--  2.0 unx     2252 b- defN 24-Jan-13 19:00 cutlass/backend/evt/backend/sm80_emitter.py
--rw-r--r--  2.0 unx     7581 b- defN 24-Jan-13 19:00 cutlass/backend/evt/backend/sm80_nodes.py
--rw-r--r--  2.0 unx     3828 b- defN 24-Jan-13 19:00 cutlass/backend/evt/backend/sm90_emitter.py
--rw-r--r--  2.0 unx    10925 b- defN 24-Jan-13 19:00 cutlass/backend/evt/backend/sm90_nodes.py
--rw-r--r--  2.0 unx     1867 b- defN 24-Jan-13 19:00 cutlass/backend/evt/frontend/__init__.py
--rw-r--r--  2.0 unx     8856 b- defN 24-Feb-15 21:09 cutlass/backend/evt/frontend/frontend_base.py
--rw-r--r--  2.0 unx     6746 b- defN 24-Jan-13 19:00 cutlass/backend/evt/frontend/python_ast.py
--rw-r--r--  2.0 unx     2405 b- defN 24-Jan-13 19:00 cutlass/backend/evt/ir/__init__.py
--rw-r--r--  2.0 unx     3447 b- defN 24-Jan-13 19:00 cutlass/backend/evt/ir/compute_nodes.py
--rw-r--r--  2.0 unx     7524 b- defN 24-Jan-13 19:00 cutlass/backend/evt/ir/dag_ir.py
--rw-r--r--  2.0 unx    13261 b- defN 24-Jan-13 19:00 cutlass/backend/evt/ir/layout_algorithm.py
--rw-r--r--  2.0 unx    13267 b- defN 24-Jan-13 19:00 cutlass/backend/evt/ir/layout_nodes.py
--rw-r--r--  2.0 unx     9566 b- defN 24-Jan-13 19:00 cutlass/backend/evt/ir/load_nodes.py
--rw-r--r--  2.0 unx    10348 b- defN 24-Jan-13 19:00 cutlass/backend/evt/ir/node.py
--rw-r--r--  2.0 unx     9429 b- defN 24-Jan-13 19:00 cutlass/backend/evt/ir/store_nodes.py
--rw-r--r--  2.0 unx     4908 b- defN 24-Jan-13 19:00 cutlass/backend/evt/ir/tensor.py
--rw-r--r--  2.0 unx     2552 b- defN 24-Jan-13 19:00 cutlass/backend/evt/passes/__init__.py
--rw-r--r--  2.0 unx     5500 b- defN 24-Feb-15 21:09 cutlass/backend/evt/passes/graph_drawer.py
--rw-r--r--  2.0 unx     5291 b- defN 24-Jan-13 19:00 cutlass/backend/evt/passes/pass_argument_type.py
--rw-r--r--  2.0 unx     6809 b- defN 24-Jan-13 19:00 cutlass/backend/evt/passes/pass_dag_2_tree.py
--rw-r--r--  2.0 unx     2984 b- defN 24-Jan-13 19:00 cutlass/backend/evt/passes/pass_fix_element_d.py
--rw-r--r--  2.0 unx     4256 b- defN 24-Jan-13 19:00 cutlass/backend/evt/passes/pass_get_impl.py
--rw-r--r--  2.0 unx     9032 b- defN 24-Jan-13 19:00 cutlass/backend/evt/passes/pass_layout_elimination.py
--rw-r--r--  2.0 unx     5462 b- defN 24-Jan-13 19:00 cutlass/backend/evt/passes/pass_manager.py
--rw-r--r--  2.0 unx     2414 b- defN 24-Jan-13 19:00 cutlass/backend/evt/passes/pass_no_op_elimination.py
--rw-r--r--  2.0 unx     4456 b- defN 24-Jan-13 19:00 cutlass/backend/evt/passes/pass_preprocess_red.py
--rw-r--r--  2.0 unx     2817 b- defN 24-Jan-13 19:00 cutlass/backend/evt/passes/pass_shape_type_propagation.py
--rw-r--r--  2.0 unx     8126 b- defN 24-Jan-13 19:00 cutlass/backend/evt/passes/smem_size_calculator.py
--rw-r--r--  2.0 unx     1966 b- defN 24-Jan-13 19:00 cutlass/backend/evt/passes/util.py
--rw-r--r--  2.0 unx     1833 b- defN 24-Jan-13 19:00 cutlass/backend/utils/__init__.py
--rw-r--r--  2.0 unx     4461 b- defN 24-Jan-13 19:00 cutlass/backend/utils/device.py
--rw-r--r--  2.0 unx     1838 b- defN 24-Jan-13 19:00 cutlass/emit/__init__.py
--rw-r--r--  2.0 unx    10566 b- defN 24-Jan-13 19:00 cutlass/emit/common.py
--rw-r--r--  2.0 unx    37350 b- defN 24-Jan-13 19:00 cutlass/emit/pytorch.py
--rw-r--r--  2.0 unx     2100 b- defN 24-Jan-13 19:00 cutlass/epilogue/__init__.py
--rw-r--r--  2.0 unx     5562 b- defN 24-Jan-13 19:00 cutlass/epilogue/epilogue.py
--rw-r--r--  2.0 unx     2909 b- defN 24-Jan-13 19:00 cutlass/epilogue/evt_ops.py
--rw-r--r--  2.0 unx     1992 b- defN 24-Jan-13 19:00 cutlass/op/__init__.py
--rw-r--r--  2.0 unx    43314 b- defN 24-Jan-13 19:00 cutlass/op/conv.py
--rw-r--r--  2.0 unx    32082 b- defN 24-Jan-13 19:00 cutlass/op/gemm.py
--rw-r--r--  2.0 unx    12455 b- defN 24-Jan-13 19:00 cutlass/op/gemm_grouped.py
--rw-r--r--  2.0 unx    18566 b- defN 24-Jan-13 19:00 cutlass/op/op.py
--rw-r--r--  2.0 unx     2011 b- defN 24-Jan-13 19:00 cutlass/utils/__init__.py
--rw-r--r--  2.0 unx    12125 b- defN 24-Jan-13 19:00 cutlass/utils/check.py
--rw-r--r--  2.0 unx    12115 b- defN 24-Jan-13 19:00 cutlass/utils/datatypes.py
--rw-r--r--  2.0 unx     6905 b- defN 24-Jan-13 19:00 cutlass/utils/profiler.py
--rw-r--r--  2.0 unx     2779 b- defN 24-Jan-13 19:00 cutlass_library/__init__.py
--rw-r--r--  2.0 unx    19821 b- defN 24-Jan-13 19:00 cutlass_library/conv2d_operation.py
--rw-r--r--  2.0 unx    14390 b- defN 24-Jan-13 19:00 cutlass_library/conv3d_operation.py
--rw-r--r--  2.0 unx    52367 b- defN 24-Jan-13 19:00 cutlass_library/gemm_operation.py
--rw-r--r--  2.0 unx   249537 b- defN 24-Jan-13 19:00 cutlass_library/generator.py
--rw-r--r--  2.0 unx    33582 b- defN 24-Jan-13 19:00 cutlass_library/library.py
--rw-r--r--  2.0 unx    25597 b- defN 24-Jan-13 19:00 cutlass_library/manifest.py
--rw-r--r--  2.0 unx    16216 b- defN 24-Jan-13 19:00 cutlass_library/rank_2k_operation.py
--rw-r--r--  2.0 unx    15775 b- defN 24-Jan-13 19:00 cutlass_library/rank_k_operation.py
--rw-r--r--  2.0 unx    16164 b- defN 24-Jan-13 19:00 cutlass_library/symm_operation.py
--rw-r--r--  2.0 unx    16650 b- defN 24-Jan-13 19:00 cutlass_library/trmm_operation.py
--rw-r--r--  2.0 unx     4477 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/CMakeLists.txt
--rw-r--r--  2.0 unx     1668 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/00_basic_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    14698 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/00_basic_gemm/basic_gemm.cu
--rw-r--r--  2.0 unx     1681 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/01_cutlass_utilities/CMakeLists.txt
--rw-r--r--  2.0 unx    13255 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/01_cutlass_utilities/cutlass_utilities.cu
--rw-r--r--  2.0 unx     1693 b- defN 24-Feb-15 21:09 cutlass_library/source/examples/02_dump_reg_shmem/CMakeLists.txt
--rw-r--r--  2.0 unx     7157 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/02_dump_reg_shmem/dump_reg_shmem.cu
--rw-r--r--  2.0 unx     1789 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/03_visualize_layout/CMakeLists.txt
--rw-r--r--  2.0 unx     4478 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/03_visualize_layout/options.h
--rw-r--r--  2.0 unx     7081 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/03_visualize_layout/register_layout.cu
--rw-r--r--  2.0 unx     2691 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/03_visualize_layout/register_layout.h
--rw-r--r--  2.0 unx     5819 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/03_visualize_layout/visualize_layout.cpp
--rw-r--r--  2.0 unx    11415 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/03_visualize_layout/visualize_layout.h
--rw-r--r--  2.0 unx     1673 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/04_tile_iterator/CMakeLists.txt
--rw-r--r--  2.0 unx     8226 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/04_tile_iterator/tile_iterator.cu
--rw-r--r--  2.0 unx     1672 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/05_batched_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    15161 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/05_batched_gemm/batched_gemm.cu
--rw-r--r--  2.0 unx     1670 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/06_splitK_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    17570 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/06_splitK_gemm/splitk_gemm.cu
--rw-r--r--  2.0 unx     1686 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/07_volta_tensorop_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    18283 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu
--rw-r--r--  2.0 unx     1688 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/08_turing_tensorop_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    18228 b- defN 24-Feb-15 21:09 cutlass_library/source/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu
--rw-r--r--  2.0 unx     1703 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/09_turing_tensorop_conv2dfprop/CMakeLists.txt
--rw-r--r--  2.0 unx    28142 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu
--rw-r--r--  2.0 unx     1897 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/10_planar_complex/CMakeLists.txt
--rw-r--r--  2.0 unx    21947 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/10_planar_complex/planar_complex.cu
--rw-r--r--  2.0 unx     1921 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/11_planar_complex_array/CMakeLists.txt
--rw-r--r--  2.0 unx    23244 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/11_planar_complex_array/planar_complex_array.cu
--rw-r--r--  2.0 unx     1676 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/12_gemm_bias_relu/CMakeLists.txt
--rw-r--r--  2.0 unx    13152 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/12_gemm_bias_relu/gemm_bias_relu.cu
--rw-r--r--  2.0 unx     2757 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/CMakeLists.txt
--rw-r--r--  2.0 unx     5893 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/README.md
--rw-r--r--  2.0 unx    26102 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h
--rw-r--r--  2.0 unx    24557 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_gemm_run.h
--rw-r--r--  2.0 unx    18662 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_grouped_gemm_run.h
--rw-r--r--  2.0 unx    28268 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h
--rw-r--r--  2.0 unx    26174 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h
--rw-r--r--  2.0 unx     8756 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu
--rw-r--r--  2.0 unx     8759 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu
--rw-r--r--  2.0 unx     8712 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu
--rw-r--r--  2.0 unx     8762 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu
--rw-r--r--  2.0 unx     8782 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu
--rw-r--r--  2.0 unx     8785 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu
--rw-r--r--  2.0 unx     8711 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu
--rw-r--r--  2.0 unx     8775 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu
--rw-r--r--  2.0 unx     7269 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu
--rw-r--r--  2.0 unx     7338 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu
--rw-r--r--  2.0 unx     7294 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu
--rw-r--r--  2.0 unx     7359 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu
--rw-r--r--  2.0 unx    10064 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_grouped_f16_sm80_rf.cu
--rw-r--r--  2.0 unx     7358 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu
--rw-r--r--  2.0 unx     7424 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu
--rw-r--r--  2.0 unx    11029 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu
--rw-r--r--  2.0 unx     7619 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu
--rw-r--r--  2.0 unx     3577 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/test_run.h
--rw-r--r--  2.0 unx    12711 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/device/b2b_gemm.h
--rw-r--r--  2.0 unx    11520 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h
--rw-r--r--  2.0 unx    30457 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h
--rw-r--r--  2.0 unx     6120 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/b2b_gemm_grouped_problem_visitor.h
--rw-r--r--  2.0 unx    18151 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h
--rw-r--r--  2.0 unx     3973 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h
--rw-r--r--  2.0 unx    26762 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h
--rw-r--r--  2.0 unx    26775 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h
--rw-r--r--  2.0 unx    28422 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h
--rw-r--r--  2.0 unx    28073 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h
--rw-r--r--  2.0 unx    19973 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h
--rw-r--r--  2.0 unx    15092 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h
--rw-r--r--  2.0 unx     6380 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/grouped.h
--rw-r--r--  2.0 unx    14663 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h
--rw-r--r--  2.0 unx    31616 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h
--rw-r--r--  2.0 unx    31443 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h
--rw-r--r--  2.0 unx    21012 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h
--rw-r--r--  2.0 unx    20494 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h
--rw-r--r--  2.0 unx     7983 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h
--rw-r--r--  2.0 unx     6047 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
--rw-r--r--  2.0 unx    34515 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h
--rw-r--r--  2.0 unx    34226 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h
--rw-r--r--  2.0 unx    21820 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h
--rw-r--r--  2.0 unx    21434 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h
--rw-r--r--  2.0 unx    27144 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h
--rw-r--r--  2.0 unx    27400 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h
--rw-r--r--  2.0 unx     5719 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/grouped_threadblock_swizzle.h
--rw-r--r--  2.0 unx     1698 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/14_ampere_tf32_tensorop_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    18020 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu
--rw-r--r--  2.0 unx     1830 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/15_ampere_sparse_tensorop_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    15042 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu
--rw-r--r--  2.0 unx    15957 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm_with_visitor.cu
--rw-r--r--  2.0 unx     1703 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/16_ampere_tensorop_conv2dfprop/CMakeLists.txt
--rw-r--r--  2.0 unx    27844 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu
--rw-r--r--  2.0 unx     1694 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/17_fprop_per_channel_bias/CMakeLists.txt
--rw-r--r--  2.0 unx    12580 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu
--rw-r--r--  2.0 unx     1715 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/18_ampere_fp64_tensorop_affine2_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    14007 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu
--rw-r--r--  2.0 unx     1682 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/19_tensorop_canonical/CMakeLists.txt
--rw-r--r--  2.0 unx    13401 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/19_tensorop_canonical/tensorop_canonical.cu
--rw-r--r--  2.0 unx     1674 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/20_simt_canonical/CMakeLists.txt
--rw-r--r--  2.0 unx    12556 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/20_simt_canonical/simt_canonical.cu
--rw-r--r--  2.0 unx     1678 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/21_quaternion_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    17319 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/21_quaternion_gemm/quaternion_gemm.cu
--rw-r--r--  2.0 unx     1680 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/22_quaternion_conv/CMakeLists.txt
--rw-r--r--  2.0 unx    21423 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/22_quaternion_conv/quaternion_conv.cu
--rw-r--r--  2.0 unx     1721 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/23_ampere_gemm_operand_reduction_fusion/CMakeLists.txt
--rw-r--r--  2.0 unx    27556 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu
--rw-r--r--  2.0 unx     1674 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/24_gemm_grouped/CMakeLists.txt
--rw-r--r--  2.0 unx    50890 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/24_gemm_grouped/gemm_grouped.cu
--rw-r--r--  2.0 unx     1816 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/25_ampere_fprop_mainloop_fusion/CMakeLists.txt
--rw-r--r--  2.0 unx    26547 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu
--rw-r--r--  2.0 unx    25628 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu
--rw-r--r--  2.0 unx     1705 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/26_ampere_wgrad_mainloop_fusion/CMakeLists.txt
--rw-r--r--  2.0 unx    25538 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu
--rw-r--r--  2.0 unx    30446 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu
--rw-r--r--  2.0 unx     1733 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx     1732 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/CMakeLists.txt
--rw-r--r--  2.0 unx    28159 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu
--rw-r--r--  2.0 unx    28382 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm.cu
--rw-r--r--  2.0 unx     1754 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    27304 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/30_wgrad_split_k/30_wgrad_split_k.cu
--rw-r--r--  2.0 unx     1677 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/30_wgrad_split_k/CMakeLists.txt
--rw-r--r--  2.0 unx     1668 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/31_basic_syrk/CMakeLists.txt
--rw-r--r--  2.0 unx    15205 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/31_basic_syrk/basic_syrk.cu
--rw-r--r--  2.0 unx     1668 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/32_basic_trmm/CMakeLists.txt
--rw-r--r--  2.0 unx    15906 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/32_basic_trmm/basic_trmm.cu
--rw-r--r--  2.0 unx     1702 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/33_ampere_3xtf32_tensorop_symm/CMakeLists.txt
--rw-r--r--  2.0 unx    31803 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu
--rw-r--r--  2.0 unx    22378 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/34_transposed_conv2d/34_transposed_conv2d.cu
--rw-r--r--  2.0 unx     1684 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/34_transposed_conv2d/CMakeLists.txt
--rw-r--r--  2.0 unx     1673 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/35_gemm_softmax/CMakeLists.txt
--rw-r--r--  2.0 unx    23114 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/35_gemm_softmax/gemm_softmax.cu
--rw-r--r--  2.0 unx    16723 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h
--rw-r--r--  2.0 unx    19055 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/35_gemm_softmax/gemm_with_softmax.h
--rw-r--r--  2.0 unx     1687 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/36_gather_scatter_fusion/CMakeLists.txt
--rw-r--r--  2.0 unx    21008 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu
--rw-r--r--  2.0 unx     1689 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/CMakeLists.txt
--rw-r--r--  2.0 unx    31111 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu
--rw-r--r--  2.0 unx    13982 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h
--rw-r--r--  2.0 unx    33900 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h
--rw-r--r--  2.0 unx     1675 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/38_syr2k_grouped/CMakeLists.txt
--rw-r--r--  2.0 unx    47457 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/38_syr2k_grouped/syr2k_grouped.cu
--rw-r--r--  2.0 unx     1674 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/39_gemm_permute/CMakeLists.txt
--rw-r--r--  2.0 unx    48551 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/39_gemm_permute/gemm_permute.cu
--rw-r--r--  2.0 unx    15309 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/39_gemm_permute/layouts.h
--rw-r--r--  2.0 unx    11985 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/39_gemm_permute/permute_info.h
--rw-r--r--  2.0 unx      253 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/40_cutlass_py/README.md
--rw-r--r--  2.0 unx    15590 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/40_cutlass_py/customizable/README.md
--rw-r--r--  2.0 unx       36 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/40_cutlass_py/customizable/grouped_gemm_problem_size.csv
--rw-r--r--  2.0 unx     2390 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/CMakeLists.txt
--rw-r--r--  2.0 unx    11865 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/debug_utils.h
--rw-r--r--  2.0 unx    10832 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/default_fmha_grouped.h
--rw-r--r--  2.0 unx    37522 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/fmha_grouped.h
--rw-r--r--  2.0 unx     6666 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h
--rw-r--r--  2.0 unx    11208 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/fused_multi_head_attention_backward.cu
--rw-r--r--  2.0 unx    38218 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu
--rw-r--r--  2.0 unx    40003 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu
--rw-r--r--  2.0 unx    11075 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/gemm_kernel_utils.h
--rw-r--r--  2.0 unx    97621 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/kernel_backward.h
--rw-r--r--  2.0 unx    52615 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/kernel_forward.h
--rw-r--r--  2.0 unx    22356 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h
--rw-r--r--  2.0 unx     9162 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h
--rw-r--r--  2.0 unx     6118 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h
--rw-r--r--  2.0 unx     3994 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma.h
--rw-r--r--  2.0 unx     6248 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h
--rw-r--r--  2.0 unx    26933 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h
--rw-r--r--  2.0 unx    14105 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h
--rw-r--r--  2.0 unx     6782 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/find_default_mma.h
--rw-r--r--  2.0 unx    13959 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h
--rw-r--r--  2.0 unx    68653 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h
--rw-r--r--  2.0 unx     5776 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/default_warp_iterator_from_smem.h
--rw-r--r--  2.0 unx    23862 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h
--rw-r--r--  2.0 unx     3142 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/make_residual_last.h
--rw-r--r--  2.0 unx    64473 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h
--rw-r--r--  2.0 unx    64507 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h
--rw-r--r--  2.0 unx     2512 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h
--rw-r--r--  2.0 unx    10063 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h
--rw-r--r--  2.0 unx     3761 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h
--rw-r--r--  2.0 unx     1701 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/42_ampere_tensorop_group_conv/CMakeLists.txt
--rw-r--r--  2.0 unx    23901 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu
--rw-r--r--  2.0 unx     1690 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/43_ell_block_sparse_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    23867 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu
--rw-r--r--  2.0 unx     2589 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/README.md
--rw-r--r--  2.0 unx     1105 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/config.json
--rw-r--r--  2.0 unx    10231 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h
--rw-r--r--  2.0 unx     3745 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/utils.h
--rw-r--r--  2.0 unx     6370 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h
--rw-r--r--  2.0 unx     4099 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h
--rw-r--r--  2.0 unx     8285 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h
--rw-r--r--  2.0 unx    10439 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h
--rw-r--r--  2.0 unx     6848 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h
--rw-r--r--  2.0 unx    14747 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h
--rwxr-xr-x  2.0 unx     2346 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/ir_gen/generate.sh
--rw-r--r--  2.0 unx     1668 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/45_dual_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    12642 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/45_dual_gemm/dual_gemm.cu
--rw-r--r--  2.0 unx     2366 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/45_dual_gemm/dual_gemm_common.h
--rw-r--r--  2.0 unx    31509 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/45_dual_gemm/dual_gemm_run.h
--rw-r--r--  2.0 unx     3577 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/45_dual_gemm/test_run.h
--rw-r--r--  2.0 unx    16953 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/45_dual_gemm/device/dual_gemm.h
--rw-r--r--  2.0 unx    18413 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/45_dual_gemm/kernel/dual_gemm.h
--rw-r--r--  2.0 unx     5818 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/45_dual_gemm/thread/left_silu_and_mul.h
--rw-r--r--  2.0 unx    15613 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/45_dual_gemm/threadblock/dual_epilogue.h
--rw-r--r--  2.0 unx     7920 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/45_dual_gemm/threadblock/dual_mma_base.h
--rw-r--r--  2.0 unx    29897 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/45_dual_gemm/threadblock/dual_mma_multistage.h
--rw-r--r--  2.0 unx     1701 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/46_depthwise_simt_conv2dfprop/CMakeLists.txt
--rw-r--r--  2.0 unx    24772 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu
--rw-r--r--  2.0 unx     2030 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/47_ampere_gemm_universal_streamk/CMakeLists.txt
--rw-r--r--  2.0 unx    22676 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu
--rw-r--r--  2.0 unx    30694 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk_broadcast.cu
--rw-r--r--  2.0 unx    17286 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu
--rw-r--r--  2.0 unx     1707 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/48_hopper_warp_specialized_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    30447 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu
--rw-r--r--  2.0 unx     1751 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/49_hopper_gemm_with_collective_builder/CMakeLists.txt
--rw-r--r--  2.0 unx    18806 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu
--rw-r--r--  2.0 unx     1717 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/50_hopper_gemm_with_epilogue_swizzle/CMakeLists.txt
--rw-r--r--  2.0 unx    17340 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/51_hopper_gett/51_hopper_gett.cu
--rw-r--r--  2.0 unx     1668 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/51_hopper_gett/CMakeLists.txt
--rw-r--r--  2.0 unx     5572 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/51_hopper_gett/gett_kernel.cuh
--rw-r--r--  2.0 unx    27580 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/52_hopper_gather_scatter_fusion/52_hopper_gather_scatter_fusion.cu
--rw-r--r--  2.0 unx     1704 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/52_hopper_gather_scatter_fusion/CMakeLists.txt
--rw-r--r--  2.0 unx    17835 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_gemm.hpp
--rw-r--r--  2.0 unx     5606 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_kernel.cuh
--rw-r--r--  2.0 unx     7018 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_tensor.hpp
--rw-r--r--  2.0 unx     8533 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/52_hopper_gather_scatter_fusion/scatter_epilogue.hpp
--rw-r--r--  2.0 unx    45351 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/53_hopper_gemm_permute/53_hopper_gemm_permute.cu
--rw-r--r--  2.0 unx     1690 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/53_hopper_gemm_permute/CMakeLists.txt
--rw-r--r--  2.0 unx     4078 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/53_hopper_gemm_permute/permute_kernel.cuh
--rw-r--r--  2.0 unx    11441 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/53_hopper_gemm_permute/permute_traits.hpp
--rw-r--r--  2.0 unx    22065 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/54_hopper_fp8_warp_specialized_gemm/54_hopper_fp8_warp_specialized_gemm.cu
--rw-r--r--  2.0 unx     1712 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/54_hopper_fp8_warp_specialized_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx     5157 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/54_hopper_fp8_warp_specialized_gemm/hopper_fp8_commandline.hpp
--rw-r--r--  2.0 unx    31597 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/55_hopper_mixed_dtype_gemm/55_hopper_mixed_dtype_gemm.cu
--rw-r--r--  2.0 unx     2978 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/55_hopper_mixed_dtype_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx     2587 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/55_hopper_mixed_dtype_gemm/README.md
--rw-r--r--  2.0 unx     7838 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/55_hopper_mixed_dtype_gemm/unfused_weight_dequantize.hpp
--rw-r--r--  2.0 unx    19510 b- defN 24-Feb-15 21:09 cutlass_library/source/examples/56_hopper_ptr_array_batched_gemm/56_hopper_ptr_array_batched_gemm.cu
--rw-r--r--  2.0 unx     2802 b- defN 24-Feb-15 21:09 cutlass_library/source/examples/56_hopper_ptr_array_batched_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx    26735 b- defN 24-Feb-15 21:09 cutlass_library/source/examples/57_hopper_grouped_gemm/57_hopper_grouped_gemm.cu
--rw-r--r--  2.0 unx     3411 b- defN 24-Feb-15 21:09 cutlass_library/source/examples/57_hopper_grouped_gemm/CMakeLists.txt
--rw-r--r--  2.0 unx     2738 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/60_cutlass_import/CMakeLists.txt
--rw-r--r--  2.0 unx     2849 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/60_cutlass_import/main.cpp
--rw-r--r--  2.0 unx     4469 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/common/helper.h
--rw-r--r--  2.0 unx     1625 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/cute/CMakeLists.txt
--rw-r--r--  2.0 unx     1727 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/cute/tutorial/CMakeLists.txt
--rw-r--r--  2.0 unx    14342 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/cute/tutorial/sgemm_nt_1.cu
--rw-r--r--  2.0 unx     9550 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/cute/tutorial/tiled_copy.cu
--rw-r--r--  2.0 unx    16234 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/python/00_basic_gemm.ipynb
--rw-r--r--  2.0 unx     7350 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/python/01_epilogue.ipynb
--rw-r--r--  2.0 unx    10053 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/python/02_pytorch_extension_grouped_gemm.ipynb
--rw-r--r--  2.0 unx    18123 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/python/03_basic_conv2d.ipynb
--rw-r--r--  2.0 unx     8815 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/python/04_epilogue_visitor.ipynb
--rw-r--r--  2.0 unx      961 b- defN 24-Jan-13 19:00 cutlass_library/source/examples/python/README.md
--rw-r--r--  2.0 unx     5419 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/config.hpp
--rw-r--r--  2.0 unx    28088 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/int_tuple.hpp
--rw-r--r--  2.0 unx    58037 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/layout.hpp
--rw-r--r--  2.0 unx    17749 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/layout_composed.hpp
--rw-r--r--  2.0 unx     8403 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/pointer.hpp
--rw-r--r--  2.0 unx     8326 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/pointer_base.hpp
--rw-r--r--  2.0 unx     5510 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/pointer_flagged.hpp
--rw-r--r--  2.0 unx     6136 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/pointer_swizzle.hpp
--rw-r--r--  2.0 unx    15939 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/stride.hpp
--rw-r--r--  2.0 unx    15341 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/swizzle.hpp
--rw-r--r--  2.0 unx    21354 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/swizzle_layout.hpp
--rw-r--r--  2.0 unx    35169 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/tensor.hpp
--rw-r--r--  2.0 unx     2595 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/tensor_predicate.hpp
--rw-r--r--  2.0 unx     2279 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/tile.hpp
--rw-r--r--  2.0 unx     6203 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/underscore.hpp
--rw-r--r--  2.0 unx     3026 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/algorithm/axpby.hpp
--rw-r--r--  2.0 unx     2351 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/algorithm/clear.hpp
--rw-r--r--  2.0 unx    13582 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/algorithm/copy.hpp
--rw-r--r--  2.0 unx     2906 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/algorithm/fill.hpp
--rw-r--r--  2.0 unx    10843 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/algorithm/functional.hpp
--rw-r--r--  2.0 unx    26851 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/algorithm/gemm.hpp
--rw-r--r--  2.0 unx     2124 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/algorithm/prefer.hpp
--rw-r--r--  2.0 unx     5248 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/algorithm/tensor_algorithms.hpp
--rw-r--r--  2.0 unx    28067 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/algorithm/tuple_algorithms.hpp
--rw-r--r--  2.0 unx     7606 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/cluster_sm90.hpp
--rw-r--r--  2.0 unx     3248 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/copy.hpp
--rw-r--r--  2.0 unx     7703 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/copy_sm75.hpp
--rw-r--r--  2.0 unx     6832 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/copy_sm80.hpp
--rw-r--r--  2.0 unx     7530 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/copy_sm90.hpp
--rw-r--r--  2.0 unx    14108 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cute/arch/copy_sm90_desc.hpp
--rw-r--r--  2.0 unx    36138 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/copy_sm90_tma.hpp
--rw-r--r--  2.0 unx     2393 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/mma.hpp
--rw-r--r--  2.0 unx     3160 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/mma_sm61.hpp
--rw-r--r--  2.0 unx    12452 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/mma_sm70.hpp
--rw-r--r--  2.0 unx     4262 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/mma_sm75.hpp
--rw-r--r--  2.0 unx    68426 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/mma_sm80.hpp
--rw-r--r--  2.0 unx    47771 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/mma_sm90.hpp
--rw-r--r--  2.0 unx     6172 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/mma_sm90_desc.hpp
--rw-r--r--  2.0 unx   944978 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/mma_sm90_gmma.hpp
--rw-r--r--  2.0 unx     8212 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/arch/util.hpp
--rw-r--r--  2.0 unx    27553 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/copy_atom.hpp
--rw-r--r--  2.0 unx     5975 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/copy_traits.hpp
--rw-r--r--  2.0 unx     5087 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/copy_traits_sm75.hpp
--rw-r--r--  2.0 unx     7107 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/copy_traits_sm80.hpp
--rw-r--r--  2.0 unx     4589 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/copy_traits_sm90.hpp
--rw-r--r--  2.0 unx    51919 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/copy_traits_sm90_tma.hpp
--rw-r--r--  2.0 unx     2860 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/copy_traits_sm90_tma_swizzle.hpp
--rw-r--r--  2.0 unx    33411 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cute/atom/mma_atom.hpp
--rw-r--r--  2.0 unx     8677 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/mma_traits.hpp
--rw-r--r--  2.0 unx     2773 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/mma_traits_sm61.hpp
--rw-r--r--  2.0 unx     6092 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/mma_traits_sm70.hpp
--rw-r--r--  2.0 unx     3303 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/mma_traits_sm75.hpp
--rw-r--r--  2.0 unx    14128 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/mma_traits_sm80.hpp
--rw-r--r--  2.0 unx     5049 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/mma_traits_sm90.hpp
--rw-r--r--  2.0 unx   189975 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/atom/mma_traits_sm90_gmma.hpp
--rw-r--r--  2.0 unx     2982 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/container/alignment.hpp
--rw-r--r--  2.0 unx     9557 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/container/array.hpp
--rw-r--r--  2.0 unx     2082 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/container/array_aligned.hpp
--rw-r--r--  2.0 unx    18172 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/container/array_subbyte.hpp
--rw-r--r--  2.0 unx     5480 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/container/bit_field.hpp
--rw-r--r--  2.0 unx     4609 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/container/cuda_types.hpp
--rw-r--r--  2.0 unx    21158 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/container/tuple.hpp
--rw-r--r--  2.0 unx     4224 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/container/type_list.hpp
--rw-r--r--  2.0 unx    15563 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/arithmetic_tuple.hpp
--rw-r--r--  2.0 unx     2170 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/bfloat.hpp
--rw-r--r--  2.0 unx     2694 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/complex.hpp
--rw-r--r--  2.0 unx     2033 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/float8.hpp
--rw-r--r--  2.0 unx     1997 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/half.hpp
--rw-r--r--  2.0 unx     4871 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/int.hpp
--rw-r--r--  2.0 unx     4671 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/integer_sequence.hpp
--rw-r--r--  2.0 unx     7216 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/integer_subbyte.hpp
--rw-r--r--  2.0 unx    13004 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/integral_constant.hpp
--rw-r--r--  2.0 unx     7214 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/integral_ratio.hpp
--rw-r--r--  2.0 unx     9013 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/math.hpp
--rw-r--r--  2.0 unx     2259 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/real.hpp
--rw-r--r--  2.0 unx     2170 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/tfloat.hpp
--rw-r--r--  2.0 unx     7531 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/numeric/uint128.hpp
--rw-r--r--  2.0 unx     5010 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cute/util/debug.hpp
--rw-r--r--  2.0 unx     4310 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cute/util/print.hpp
--rw-r--r--  2.0 unx     7775 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cute/util/type_traits.hpp
--rw-r--r--  2.0 unx     3793 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/aligned_buffer.h
--rw-r--r--  2.0 unx    66810 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/array.h
--rw-r--r--  2.0 unx     3662 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/array_planar_complex.h
--rw-r--r--  2.0 unx    13552 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/array_subbyte.h
--rw-r--r--  2.0 unx    12443 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/barrier.h
--rw-r--r--  2.0 unx    14278 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/bfloat16.h
--rw-r--r--  2.0 unx     5294 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/blas3.h
--rw-r--r--  2.0 unx     3263 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/blas3_types.h
--rw-r--r--  2.0 unx     9386 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/block_striped.h
--rw-r--r--  2.0 unx     8853 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/cluster_launch.hpp
--rw-r--r--  2.0 unx    20146 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/complex.h
--rw-r--r--  2.0 unx    47943 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/constants.h
--rw-r--r--  2.0 unx    12221 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/coord.h
--rw-r--r--  2.0 unx    11385 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/core_io.h
--rw-r--r--  2.0 unx     6322 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/cuda_host_adapter.hpp
--rw-r--r--  2.0 unx     6872 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/cutlass.h
--rw-r--r--  2.0 unx     4321 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/device_kernel.h
--rw-r--r--  2.0 unx    29056 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/fast_math.h
--rw-r--r--  2.0 unx    36163 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/float8.h
--rw-r--r--  2.0 unx     2645 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/floating_point_nvrtc.h
--rw-r--r--  2.0 unx    17280 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/functional.h
--rw-r--r--  2.0 unx    10599 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm_coord.h
--rw-r--r--  2.0 unx     2875 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm_coord.hpp
--rw-r--r--  2.0 unx    24024 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/half.h
--rw-r--r--  2.0 unx     7353 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/integer_subbyte.h
--rw-r--r--  2.0 unx     3194 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/kernel_hardware_info.h
--rw-r--r--  2.0 unx     2006 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/kernel_hardware_info.hpp
--rw-r--r--  2.0 unx     2801 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/kernel_launch.h
--rw-r--r--  2.0 unx   364115 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/matrix.h
--rw-r--r--  2.0 unx     4991 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/matrix_coord.h
--rw-r--r--  2.0 unx     2726 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/matrix_shape.h
--rw-r--r--  2.0 unx   122014 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/numeric_conversion.h
--rw-r--r--  2.0 unx     3605 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/numeric_size.h
--rw-r--r--  2.0 unx     3711 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/numeric_types.h
--rw-r--r--  2.0 unx     5492 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/pitch_linear_coord.h
--rw-r--r--  2.0 unx    16279 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/predicate_vector.h
--rw-r--r--  2.0 unx    20891 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/quaternion.h
--rw-r--r--  2.0 unx     2369 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/real.h
--rw-r--r--  2.0 unx     6572 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/relatively_equal.h
--rw-r--r--  2.0 unx     3984 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/semaphore.h
--rw-r--r--  2.0 unx    38253 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/subbyte_reference.h
--rw-r--r--  2.0 unx     8964 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/tensor_coord.h
--rw-r--r--  2.0 unx    12207 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/tensor_ref.h
--rw-r--r--  2.0 unx    11201 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/tensor_ref_planar_complex.h
--rw-r--r--  2.0 unx     9509 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/tensor_view.h
--rw-r--r--  2.0 unx    10250 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/tensor_view_planar_complex.h
--rw-r--r--  2.0 unx    13017 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/tfloat32.h
--rw-r--r--  2.0 unx     2581 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/trace.h
--rw-r--r--  2.0 unx     8322 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/uint128.h
--rw-r--r--  2.0 unx     2899 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/version.h
--rw-r--r--  2.0 unx     4540 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/wmma_array.h
--rw-r--r--  2.0 unx     4964 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/workspace.h
--rw-r--r--  2.0 unx     3564 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/arch.h
--rw-r--r--  2.0 unx    19807 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/barrier.h
--rw-r--r--  2.0 unx     2691 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/cache_operation.h
--rw-r--r--  2.0 unx    18266 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/memory.h
--rw-r--r--  2.0 unx     8260 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/memory_sm75.h
--rw-r--r--  2.0 unx    15163 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/memory_sm80.h
--rw-r--r--  2.0 unx     9252 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/mma.h
--rw-r--r--  2.0 unx    11096 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/mma_sm50.h
--rw-r--r--  2.0 unx     7040 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/mma_sm60.h
--rw-r--r--  2.0 unx     4193 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/mma_sm61.h
--rw-r--r--  2.0 unx    16554 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/mma_sm70.h
--rw-r--r--  2.0 unx    31990 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/mma_sm75.h
--rw-r--r--  2.0 unx    57636 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/mma_sm80.h
--rw-r--r--  2.0 unx     8419 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/arch/mma_sm90.h
--rw-r--r--  2.0 unx    43978 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/mma_sparse_sm80.h
--rw-r--r--  2.0 unx     2621 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/reg_reconfig.h
--rw-r--r--  2.0 unx     3998 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/simd.h
--rw-r--r--  2.0 unx     3590 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/simd_sm60.h
--rw-r--r--  2.0 unx     5102 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/simd_sm61.h
--rw-r--r--  2.0 unx     8473 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/wmma.h
--rw-r--r--  2.0 unx     5286 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/wmma_sm70.h
--rw-r--r--  2.0 unx     7746 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/wmma_sm72.h
--rw-r--r--  2.0 unx     7616 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/arch/wmma_sm75.h
--rw-r--r--  2.0 unx    23019 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/conv2d_problem_size.h
--rw-r--r--  2.0 unx    16660 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/conv3d_problem_size.h
--rw-r--r--  2.0 unx     7140 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/convolution.h
--rw-r--r--  2.0 unx     9743 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/device/direct_convolution.h
--rw-r--r--  2.0 unx    12078 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/device/implicit_gemm_convolution.h
--rw-r--r--  2.0 unx    10044 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h
--rw-r--r--  2.0 unx     7671 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d.h
--rw-r--r--  2.0 unx    53546 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_dgrad.h
--rw-r--r--  2.0 unx    56838 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop.h
--rw-r--r--  2.0 unx    11953 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h
--rw-r--r--  2.0 unx     4689 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h
--rw-r--r--  2.0 unx     4659 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h
--rw-r--r--  2.0 unx    19603 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_group_fprop.h
--rw-r--r--  2.0 unx    28745 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_wgrad.h
--rw-r--r--  2.0 unx    10459 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h
--rw-r--r--  2.0 unx     9324 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_dgrad.h
--rw-r--r--  2.0 unx    14864 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop.h
--rw-r--r--  2.0 unx    11980 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h
--rw-r--r--  2.0 unx    14883 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_wgrad.h
--rw-r--r--  2.0 unx    19293 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/default_depthwise_fprop.h
--rw-r--r--  2.0 unx    18026 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/direct_convolution.h
--rw-r--r--  2.0 unx    15413 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution.h
--rw-r--r--  2.0 unx    15690 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h
--rw-r--r--  2.0 unx    17222 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h
--rw-r--r--  2.0 unx    16730 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h
--rw-r--r--  2.0 unx     9689 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/thread/depthwise_mma.h
--rw-r--r--  2.0 unx    15306 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h
--rw-r--r--  2.0 unx    19735 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h
--rw-r--r--  2.0 unx    18940 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--  2.0 unx    26136 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--  2.0 unx    10977 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h
--rw-r--r--  2.0 unx    11529 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h
--rw-r--r--  2.0 unx    11333 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h
--rw-r--r--  2.0 unx    13688 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h
--rw-r--r--  2.0 unx    10651 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h
--rw-r--r--  2.0 unx     9314 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h
--rw-r--r--  2.0 unx     9018 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h
--rw-r--r--  2.0 unx    10411 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h
--rw-r--r--  2.0 unx    30197 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_params.h
--rw-r--r--  2.0 unx    11202 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_tile_iterator.h
--rw-r--r--  2.0 unx    10349 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h
--rw-r--r--  2.0 unx    11519 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h
--rw-r--r--  2.0 unx     9043 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--  2.0 unx    10832 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--  2.0 unx     8450 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h
--rw-r--r--  2.0 unx     9569 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h
--rw-r--r--  2.0 unx    11020 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--  2.0 unx    15014 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--  2.0 unx     9634 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h
--rw-r--r--  2.0 unx    15132 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h
--rw-r--r--  2.0 unx     7945 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h
--rw-r--r--  2.0 unx     8891 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h
--rw-r--r--  2.0 unx    18249 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_params.h
--rw-r--r--  2.0 unx     9971 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h
--rw-r--r--  2.0 unx    12024 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h
--rw-r--r--  2.0 unx     8821 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--  2.0 unx    10744 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--  2.0 unx     8871 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h
--rw-r--r--  2.0 unx    10747 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h
--rw-r--r--  2.0 unx     9899 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h
--rw-r--r--  2.0 unx    20899 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h
--rw-r--r--  2.0 unx     8921 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h
--rw-r--r--  2.0 unx    12745 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h
--rw-r--r--  2.0 unx     8097 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_mma_base.h
--rw-r--r--  2.0 unx    36697 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h
--rw-r--r--  2.0 unx    30106 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h
--rw-r--r--  2.0 unx    19823 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_multistage.h
--rw-r--r--  2.0 unx    12175 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h
--rw-r--r--  2.0 unx    26320 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h
--rw-r--r--  2.0 unx    16915 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h
--rw-r--r--  2.0 unx    12476 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h
--rw-r--r--  2.0 unx     8050 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/threadblock/threadblock_swizzle.h
--rw-r--r--  2.0 unx    12392 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/warp/mma_depthwise_simt.h
--rw-r--r--  2.0 unx    30655 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h
--rw-r--r--  2.0 unx     8704 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/conv/warp/scale_bias_relu_transform.h
--rw-r--r--  2.0 unx     3048 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/detail/collective.hpp
--rw-r--r--  2.0 unx     3710 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/detail/dependent_false.hpp
--rw-r--r--  2.0 unx     5745 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/detail/helper_macros.hpp
--rw-r--r--  2.0 unx     9629 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/detail/layout.hpp
--rw-r--r--  2.0 unx     3089 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/detail/mma.hpp
--rw-r--r--  2.0 unx     6054 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/epilogue/dispatch_policy.hpp
--rw-r--r--  2.0 unx     4407 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/collective/collective_builder.hpp
--rw-r--r--  2.0 unx     2957 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/collective/collective_epilogue.hpp
--rw-r--r--  2.0 unx     9109 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/epilogue/collective/default_epilogue.hpp
--rw-r--r--  2.0 unx    10398 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/epilogue/collective/default_epilogue_array.hpp
--rw-r--r--  2.0 unx     8237 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/collective/detail.hpp
--rw-r--r--  2.0 unx    11300 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/collective/epilogue_tensor_broadcast.hpp
--rw-r--r--  2.0 unx    13967 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/collective/sm70_epilogue_vectorized.hpp
--rw-r--r--  2.0 unx    33929 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/epilogue/collective/sm90_epilogue_tma_warpspecialized.hpp
--rw-r--r--  2.0 unx     5369 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/collective/sm90_epilogue_tma_warpspecialized_bias_elementwise.hpp
--rw-r--r--  2.0 unx    28149 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/epilogue/collective/builders/sm90_builder.inl
--rw-r--r--  2.0 unx     4128 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/fusion/callbacks.hpp
--rw-r--r--  2.0 unx    12104 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/fusion/operations.hpp
--rw-r--r--  2.0 unx    49837 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/epilogue/fusion/sm90_callbacks_tma_warpspecialized.hpp
--rw-r--r--  2.0 unx    28095 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_compute_tma_warpspecialized.hpp
--rw-r--r--  2.0 unx    31364 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_load_tma_warpspecialized.hpp
--rw-r--r--  2.0 unx    45042 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_store_tma_warpspecialized.hpp
--rw-r--r--  2.0 unx    37837 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_tma_warpspecialized.hpp
--rw-r--r--  2.0 unx    17386 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/activation.h
--rw-r--r--  2.0 unx     4691 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/conversion_op.h
--rw-r--r--  2.0 unx     2281 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/detail.hpp
--rw-r--r--  2.0 unx    18989 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination.h
--rw-r--r--  2.0 unx     9323 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h
--rw-r--r--  2.0 unx    18161 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_bias_relu.h
--rw-r--r--  2.0 unx    23752 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_clamp.h
--rw-r--r--  2.0 unx     9066 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_dgelu.h
--rw-r--r--  2.0 unx    15195 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_drelu.h
--rw-r--r--  2.0 unx     3669 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_gelu.h
--rw-r--r--  2.0 unx    10056 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_generic.h
--rw-r--r--  2.0 unx     3693 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_hardswish.h
--rw-r--r--  2.0 unx     8399 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h
--rw-r--r--  2.0 unx     3046 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_params.h
--rw-r--r--  2.0 unx     9351 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_planar_complex.h
--rw-r--r--  2.0 unx    20596 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_relu.h
--rw-r--r--  2.0 unx    19458 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_relu0.h
--rw-r--r--  2.0 unx    11995 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_residual_block.h
--rw-r--r--  2.0 unx     3688 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_sigmoid.h
--rw-r--r--  2.0 unx     3669 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_silu.h
--rw-r--r--  2.0 unx     9786 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_tensor_broadcast.hpp
--rw-r--r--  2.0 unx     8662 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h
--rw-r--r--  2.0 unx     3416 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/reduction_op.h
--rw-r--r--  2.0 unx     3048 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/thread/scale_type.h
--rw-r--r--  2.0 unx     9142 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h
--rw-r--r--  2.0 unx     9441 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h
--rw-r--r--  2.0 unx     3234 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h
--rw-r--r--  2.0 unx     7209 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h
--rw-r--r--  2.0 unx    13385 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_simt.h
--rw-r--r--  2.0 unx    28290 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h
--rw-r--r--  2.0 unx     7129 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h
--rw-r--r--  2.0 unx    10846 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h
--rw-r--r--  2.0 unx     7424 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h
--rw-r--r--  2.0 unx     5763 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h
--rw-r--r--  2.0 unx     5947 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h
--rw-r--r--  2.0 unx     4409 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_thread_map_simt.h
--rw-r--r--  2.0 unx     7398 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h
--rw-r--r--  2.0 unx     7303 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h
--rw-r--r--  2.0 unx     4098 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h
--rw-r--r--  2.0 unx     4678 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h
--rw-r--r--  2.0 unx    19249 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue.h
--rw-r--r--  2.0 unx     8279 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_base.h
--rw-r--r--  2.0 unx     7455 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h
--rw-r--r--  2.0 unx    13424 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_depthwise.h
--rw-r--r--  2.0 unx    13933 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_direct_store.h
--rw-r--r--  2.0 unx     7401 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h
--rw-r--r--  2.0 unx    14610 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h
--rw-r--r--  2.0 unx     9073 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h
--rw-r--r--  2.0 unx    15321 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_streamk_with_broadcast.h
--rw-r--r--  2.0 unx    16804 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h
--rw-r--r--  2.0 unx    58727 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h
--rw-r--r--  2.0 unx    29199 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h
--rw-r--r--  2.0 unx    13454 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h
--rw-r--r--  2.0 unx    17169 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_visitor_callbacks.h
--rw-r--r--  2.0 unx     7308 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_workspace.h
--rw-r--r--  2.0 unx    14359 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/interleaved_epilogue.h
--rw-r--r--  2.0 unx     4741 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/output_iterator_parameter.h
--rw-r--r--  2.0 unx    19842 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/output_tile_thread_map.h
--rw-r--r--  2.0 unx    40972 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h
--rw-r--r--  2.0 unx    18821 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h
--rw-r--r--  2.0 unx     5636 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h
--rw-r--r--  2.0 unx    21249 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h
--rw-r--r--  2.0 unx    13873 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h
--rw-r--r--  2.0 unx    15397 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h
--rw-r--r--  2.0 unx     9146 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h
--rw-r--r--  2.0 unx    15534 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h
--rw-r--r--  2.0 unx     7487 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator.h
--rw-r--r--  2.0 unx    18100 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h
--rw-r--r--  2.0 unx     7394 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h
--rw-r--r--  2.0 unx    14544 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_2x.hpp
--rw-r--r--  2.0 unx     4387 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_compute.hpp
--rw-r--r--  2.0 unx    17161 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_load.hpp
--rw-r--r--  2.0 unx    24942 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_store.hpp
--rw-r--r--  2.0 unx     2171 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitors.hpp
--rw-r--r--  2.0 unx     7055 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h
--rw-r--r--  2.0 unx     7736 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h
--rw-r--r--  2.0 unx     5880 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_simt.h
--rw-r--r--  2.0 unx     9883 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h
--rw-r--r--  2.0 unx     8924 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h
--rw-r--r--  2.0 unx     6045 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h
--rw-r--r--  2.0 unx     4864 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/simt_policy.h
--rw-r--r--  2.0 unx     5979 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/tensor_op_policy.h
--rw-r--r--  2.0 unx    25658 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_simt.h
--rw-r--r--  2.0 unx    20290 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h
--rw-r--r--  2.0 unx    22921 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h
--rw-r--r--  2.0 unx    14250 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h
--rw-r--r--  2.0 unx     7704 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h
--rw-r--r--  2.0 unx     7485 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/volta_tensor_op_policy.h
--rw-r--r--  2.0 unx     3916 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h
--rw-r--r--  2.0 unx     9823 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/dispatch_policy.hpp
--rw-r--r--  2.0 unx     4630 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/gemm.h
--rw-r--r--  2.0 unx     3568 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/gemm_enumerated_types.h
--rw-r--r--  2.0 unx     4265 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/group_array_problem_shape.hpp
--rw-r--r--  2.0 unx     3589 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/collective/collective_builder.hpp
--rw-r--r--  2.0 unx     3668 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/collective/collective_mma.hpp
--rw-r--r--  2.0 unx     4768 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/collective/fp8_accumulation.hpp
--rw-r--r--  2.0 unx    22247 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/collective/sm70_mma_twostage.hpp
--rw-r--r--  2.0 unx    27570 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/collective/sm80_mma_multistage.hpp
--rw-r--r--  2.0 unx    33101 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_array_tma_gmma_ss_warpspecialized.hpp
--rw-r--r--  2.0 unx    28282 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_multistage_gmma_rs_warpspecialized.hpp
--rw-r--r--  2.0 unx    19361 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_multistage_gmma_ss_warpspecialized.hpp
--rw-r--r--  2.0 unx    33171 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized.hpp
--rw-r--r--  2.0 unx    66973 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized_mixed_input.hpp
--rw-r--r--  2.0 unx    22389 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss.hpp
--rw-r--r--  2.0 unx    23844 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp
--rw-r--r--  2.0 unx    23982 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8.hpp
--rw-r--r--  2.0 unx    15283 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/collective/builders/sm90_common.inl
--rw-r--r--  2.0 unx    43430 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/collective/builders/sm90_gmma_builder.inl
--rw-r--r--  2.0 unx    16881 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/base_grouped.h
--rw-r--r--  2.0 unx    24262 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/default_gemm_configuration.h
--rw-r--r--  2.0 unx    27616 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/ell_gemm.h
--rw-r--r--  2.0 unx    25202 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm.h
--rw-r--r--  2.0 unx    22367 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_array.h
--rw-r--r--  2.0 unx    22375 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_batched.h
--rw-r--r--  2.0 unx    22725 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_complex.h
--rw-r--r--  2.0 unx     2591 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_grouped.h
--rw-r--r--  2.0 unx    13736 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h
--rw-r--r--  2.0 unx    17329 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_sparse.h
--rw-r--r--  2.0 unx    11362 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_sparse_with_visitor.h
--rw-r--r--  2.0 unx    20436 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_splitk_parallel.h
--rw-r--r--  2.0 unx    15600 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_universal.h
--rw-r--r--  2.0 unx    23658 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_universal_adapter.h
--rw-r--r--  2.0 unx    15624 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_universal_base.h
--rw-r--r--  2.0 unx    14027 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_universal_streamk_with_broadcast.h
--rw-r--r--  2.0 unx    13968 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_universal_with_broadcast.h
--rw-r--r--  2.0 unx    14853 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemm_with_k_reduction.h
--rw-r--r--  2.0 unx     5961 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/gemv.h
--rw-r--r--  2.0 unx    18127 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/rank_2k.h
--rw-r--r--  2.0 unx     2747 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/rank_2k_grouped.h
--rw-r--r--  2.0 unx    16719 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/rank_k.h
--rwxr-xr-x  2.0 unx    21050 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/symm.h
--rw-r--r--  2.0 unx    26464 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/device/trmm.h
--rw-r--r--  2.0 unx    29360 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_ell_gemm.h
--rw-r--r--  2.0 unx    39181 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm.h
--rw-r--r--  2.0 unx    16130 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_complex.h
--rw-r--r--  2.0 unx    12385 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_grouped.h
--rw-r--r--  2.0 unx     6592 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h
--rw-r--r--  2.0 unx     5848 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h
--rw-r--r--  2.0 unx    11104 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h
--rw-r--r--  2.0 unx     7983 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_sparse.h
--rw-r--r--  2.0 unx     8175 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_sparse_with_visitor.h
--rw-r--r--  2.0 unx     4932 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h
--rw-r--r--  2.0 unx     5446 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_streamk_with_broadcast.h
--rw-r--r--  2.0 unx    12332 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_universal.h
--rw-r--r--  2.0 unx     5697 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_universal_with_visitor.h
--rw-r--r--  2.0 unx     8123 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h
--rw-r--r--  2.0 unx     6457 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h
--rw-r--r--  2.0 unx     8084 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_reduction.h
--rwxr-xr-x  2.0 unx     5349 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_gemv.h
--rw-r--r--  2.0 unx    11560 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k.h
--rw-r--r--  2.0 unx    20509 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k_complex.h
--rw-r--r--  2.0 unx    12470 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k_grouped.h
--rw-r--r--  2.0 unx    10620 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k_universal.h
--rw-r--r--  2.0 unx     9872 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_k.h
--rw-r--r--  2.0 unx    16990 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_k_complex.h
--rw-r--r--  2.0 unx     9444 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_k_universal.h
--rwxr-xr-x  2.0 unx    13375 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_symm.h
--rwxr-xr-x  2.0 unx    21830 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_symm_complex.h
--rwxr-xr-x  2.0 unx    10315 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_symm_universal.h
--rw-r--r--  2.0 unx    10873 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_trmm.h
--rw-r--r--  2.0 unx    10730 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_trmm_complex.h
--rw-r--r--  2.0 unx    10850 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/default_trmm_universal.h
--rw-r--r--  2.0 unx    28916 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/ell_gemm.h
--rw-r--r--  2.0 unx    13362 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm.h
--rw-r--r--  2.0 unx     8698 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_array.h
--rw-r--r--  2.0 unx     8766 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_batched.h
--rw-r--r--  2.0 unx    14692 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_grouped.h
--rw-r--r--  2.0 unx     4690 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h
--rw-r--r--  2.0 unx    15623 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h
--rw-r--r--  2.0 unx    27663 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h
--rwxr-xr-x  2.0 unx     6144 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_params.h
--rw-r--r--  2.0 unx     5150 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_pipelined.h
--rw-r--r--  2.0 unx    23352 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_planar_complex.h
--rw-r--r--  2.0 unx    18941 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_planar_complex_array.h
--rw-r--r--  2.0 unx     8142 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_splitk_parallel.h
--rw-r--r--  2.0 unx    80099 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_streamk_with_fused_epilogue.h
--rw-r--r--  2.0 unx     4291 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_transpose_operands.h
--rw-r--r--  2.0 unx    23597 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal.h
--rw-r--r--  2.0 unx     4374 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal.hpp
--rw-r--r--  2.0 unx    39277 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_streamk.h
--rw-r--r--  2.0 unx    10423 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_with_visitor.h
--rw-r--r--  2.0 unx    28721 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_with_visitor_streamk.h
--rw-r--r--  2.0 unx    48041 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h
--rw-r--r--  2.0 unx    23866 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemm_with_k_reduction.h
--rw-r--r--  2.0 unx    18393 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemv.h
--rwxr-xr-x  2.0 unx     8954 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/gemv_batched_strided.h
--rw-r--r--  2.0 unx    16765 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/grouped_problem_visitor.h
--rw-r--r--  2.0 unx     3988 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/params_sparse_base.h
--rw-r--r--  2.0 unx     8404 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/params_universal_base.h
--rw-r--r--  2.0 unx    23381 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_grouped.h
--rw-r--r--  2.0 unx    16100 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h
--rw-r--r--  2.0 unx     4334 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h
--rw-r--r--  2.0 unx    24584 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_universal.h
--rw-r--r--  2.0 unx    17989 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/rank_k_universal.h
--rw-r--r--  2.0 unx    11073 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/sm70_gemm.hpp
--rw-r--r--  2.0 unx    35090 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_array_tma_warpspecialized_cooperative.hpp
--rw-r--r--  2.0 unx    13122 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma.hpp
--rw-r--r--  2.0 unx    18498 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized.hpp
--rw-r--r--  2.0 unx    28936 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_cooperative.hpp
--rw-r--r--  2.0 unx    28129 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_pingpong.hpp
--rw-r--r--  2.0 unx    18144 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized.hpp
--rw-r--r--  2.0 unx    22969 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized_cooperative.hpp
--rw-r--r--  2.0 unx    23044 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized_pingpong.hpp
--rw-r--r--  2.0 unx     4832 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler.hpp
--rw-r--r--  2.0 unx    18485 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler_group.hpp
--rw-r--r--  2.0 unx    39022 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler_stream_k.hpp
--rw-r--r--  2.0 unx    13183 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/sparse_gemm.h
--rw-r--r--  2.0 unx     8144 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/sparse_gemm_with_visitor.h
--rw-r--r--  2.0 unx    16057 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/static_tile_scheduler.hpp
--rwxr-xr-x  2.0 unx    23881 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/symm_universal.h
--rw-r--r--  2.0 unx     4414 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/tile_scheduler.hpp
--rw-r--r--  2.0 unx    58068 b- defN 24-Feb-15 21:09 cutlass_library/source/include/cutlass/gemm/kernel/tile_scheduler_params.h
--rw-r--r--  2.0 unx    19518 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/kernel/trmm_universal.h
--rw-r--r--  2.0 unx     3567 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/thread/mma.h
--rw-r--r--  2.0 unx    15399 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/thread/mma_sm50.h
--rw-r--r--  2.0 unx    29987 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/thread/mma_sm60.h
--rw-r--r--  2.0 unx     8142 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/thread/mma_sm61.h
--rw-r--r--  2.0 unx    31930 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_ell_mma.h
--rwxr-xr-x  2.0 unx     6979 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_gemv_core.h
--rw-r--r--  2.0 unx    35582 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma.h
--rw-r--r--  2.0 unx     5123 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core.h
--rw-r--r--  2.0 unx    57426 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_simt.h
--rw-r--r--  2.0 unx    19257 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sm70.h
--rw-r--r--  2.0 unx    42310 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sm75.h
--rw-r--r--  2.0 unx   102804 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sm80.h
--rw-r--r--  2.0 unx    32106 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h
--rw-r--r--  2.0 unx    12650 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h
--rw-r--r--  2.0 unx     7387 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h
--rw-r--r--  2.0 unx    20975 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_wmma.h
--rw-r--r--  2.0 unx     7998 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h
--rw-r--r--  2.0 unx     5110 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h
--rw-r--r--  2.0 unx     4627 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h
--rw-r--r--  2.0 unx     7113 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h
--rw-r--r--  2.0 unx     6323 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_with_reduction.h
--rw-r--r--  2.0 unx     7121 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h
--rw-r--r--  2.0 unx     4959 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h
--rw-r--r--  2.0 unx    65005 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h
--rw-r--r--  2.0 unx    25495 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h
--rw-r--r--  2.0 unx     8509 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_sparse_mma.h
--rw-r--r--  2.0 unx    19515 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/default_trmm.h
--rw-r--r--  2.0 unx    24233 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/ell_mma_multistage.h
--rw-r--r--  2.0 unx    13837 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/ell_mma_pipelined.h
--rwxr-xr-x  2.0 unx     4726 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/gemv.h
--rw-r--r--  2.0 unx     3652 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/index_remat.h
--rw-r--r--  2.0 unx     7823 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/mma_base.h
--rw-r--r--  2.0 unx    27600 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/mma_blas3_multistage.h
--rw-r--r--  2.0 unx    32816 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h
--rw-r--r--  2.0 unx    27801 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/mma_multistage.h
--rw-r--r--  2.0 unx    15995 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/mma_pipelined.h
--rw-r--r--  2.0 unx     6901 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/mma_planar_complex_base.h
--rw-r--r--  2.0 unx    22839 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h
--rw-r--r--  2.0 unx    14747 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h
--rw-r--r--  2.0 unx     9864 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/mma_singlestage.h
--rw-r--r--  2.0 unx    27246 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h
--rw-r--r--  2.0 unx     9210 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/mma_sparse_base.h
--rw-r--r--  2.0 unx    25557 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/mma_sparse_multistage.h
--rw-r--r--  2.0 unx    20395 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h
--rw-r--r--  2.0 unx    15041 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/threadblock_swizzle.h
--rw-r--r--  2.0 unx    26627 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h
--rw-r--r--  2.0 unx    20553 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h
--rw-r--r--  2.0 unx     6684 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h
--rw-r--r--  2.0 unx     5178 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/default_mma_tensor_op.h
--rw-r--r--  2.0 unx    12142 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h
--rw-r--r--  2.0 unx     4053 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h
--rw-r--r--  2.0 unx     4685 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h
--rw-r--r--  2.0 unx     5691 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h
--rw-r--r--  2.0 unx     2619 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma.h
--rw-r--r--  2.0 unx    37767 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_complex_tensor_op.h
--rw-r--r--  2.0 unx    23132 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h
--rw-r--r--  2.0 unx    78519 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h
--rw-r--r--  2.0 unx    21178 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h
--rw-r--r--  2.0 unx    14589 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h
--rw-r--r--  2.0 unx    20271 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h
--rw-r--r--  2.0 unx     6144 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_planar_complex.h
--rw-r--r--  2.0 unx     8419 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_simt.h
--rw-r--r--  2.0 unx     3079 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_simt_policy.h
--rw-r--r--  2.0 unx    59793 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_simt_tile_iterator.h
--rw-r--r--  2.0 unx    13497 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_sparse_tensor_op.h
--rw-r--r--  2.0 unx    13956 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op.h
--rw-r--r--  2.0 unx    15721 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h
--rw-r--r--  2.0 unx    20472 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h
--rw-r--r--  2.0 unx     2939 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_policy.h
--rw-r--r--  2.0 unx     8966 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_sm70.h
--rw-r--r--  2.0 unx    11017 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h
--rw-r--r--  2.0 unx   135937 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h
--rw-r--r--  2.0 unx    99553 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h
--rw-r--r--  2.0 unx    75040 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h
--rw-r--r--  2.0 unx    13151 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h
--rw-r--r--  2.0 unx    27101 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h
--rw-r--r--  2.0 unx     7241 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_wmma.h
--rw-r--r--  2.0 unx    17303 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h
--rw-r--r--  2.0 unx    19101 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/scale_bias_tile_iterator.h
--rw-r--r--  2.0 unx     4610 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/softmax_scale_bias_transform.h
--rw-r--r--  2.0 unx     8728 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/gemm/warp/tile_iterator_planar_complex.h
--rw-r--r--  2.0 unx     3020 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/layout/layout.h
--rw-r--r--  2.0 unx    35129 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/layout/matrix.h
--rw-r--r--  2.0 unx    24906 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/layout/permute.h
--rw-r--r--  2.0 unx     5107 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/layout/pitch_linear.h
--rw-r--r--  2.0 unx    18798 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/layout/tensor.h
--rw-r--r--  2.0 unx    29599 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/layout/tensor_op_multiplicand_sm70.h
--rw-r--r--  2.0 unx    33137 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/layout/tensor_op_multiplicand_sm75.h
--rw-r--r--  2.0 unx    29336 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/layout/tensor_op_multiplicand_sm80.h
--rw-r--r--  2.0 unx     3354 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/layout/vector.h
--rw-r--r--  2.0 unx     2091 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/pipeline/pipeline.hpp
--rw-r--r--  2.0 unx    37270 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/pipeline/sm90_pipeline.hpp
--rw-r--r--  2.0 unx    28315 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/platform/platform.h
--rw-r--r--  2.0 unx     2936 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/reduction/threadblock_swizzle.h
--rw-r--r--  2.0 unx     6823 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/reduction/device/reduce_split_k.h
--rw-r--r--  2.0 unx     8152 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/reduction/device/tensor_reduce.h
--rw-r--r--  2.0 unx    11579 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h
--rw-r--r--  2.0 unx    11448 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/reduction/device/tensor_reduce_affine_strided.h
--rw-r--r--  2.0 unx     8762 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/reduction/kernel/reduce_softmax_final.h
--rw-r--r--  2.0 unx     7897 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/reduction/kernel/reduce_split_k.h
--rw-r--r--  2.0 unx    20685 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h
--rw-r--r--  2.0 unx    21662 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h
--rw-r--r--  2.0 unx     7208 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/reduction/thread/reduce.h
--rw-r--r--  2.0 unx     6790 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/reduction/thread/reduction_operators.h
--rw-r--r--  2.0 unx     5893 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/thread/matrix.h
--rw-r--r--  2.0 unx    33349 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/pitch_linear_thread_map.h
--rw-r--r--  2.0 unx    33948 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/collective/sm90_wgmma_transpose.hpp
--rw-r--r--  2.0 unx     3835 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/thread/transpose.h
--rw-r--r--  2.0 unx     4309 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/thread/unary_op.h
--rw-r--r--  2.0 unx     6181 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/ell_iterator.h
--rw-r--r--  2.0 unx    44443 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h
--rw-r--r--  2.0 unx    44309 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h
--rw-r--r--  2.0 unx    12890 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h
--rw-r--r--  2.0 unx    11097 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h
--rw-r--r--  2.0 unx    72537 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h
--rw-r--r--  2.0 unx    28232 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h
--rwxr-xr-x  2.0 unx    10805 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h
--rw-r--r--  2.0 unx    31412 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h
--rw-r--r--  2.0 unx    62949 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_iterator.h
--rw-r--r--  2.0 unx    27175 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h
--rw-r--r--  2.0 unx    28064 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h
--rw-r--r--  2.0 unx    13088 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h
--rw-r--r--  2.0 unx     8232 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h
--rw-r--r--  2.0 unx     2638 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator.h
--rw-r--r--  2.0 unx    13283 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h
--rw-r--r--  2.0 unx    18623 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h
--rw-r--r--  2.0 unx    27922 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h
--rw-r--r--  2.0 unx    47789 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h
--rw-r--r--  2.0 unx     2616 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator.h
--rw-r--r--  2.0 unx    16508 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h
--rw-r--r--  2.0 unx    15486 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h
--rw-r--r--  2.0 unx    36050 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h
--rw-r--r--  2.0 unx    43663 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h
--rw-r--r--  2.0 unx     5226 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/threadblock/vector_iterator.h
--rw-r--r--  2.0 unx     8828 b- defN 24-Jan-13 19:00 cutlass_library/source/include/cutlass/transform/warp/vector_fragment_iterator.h
--rw-r--r--  2.0 unx     2092 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/CMakeLists.txt
--rw-r--r--  2.0 unx    10693 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/CMakeLists.txt
--rw-r--r--  2.0 unx     4272 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/include/cutlass/library/arch_mappings.h
--rw-r--r--  2.0 unx    18334 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/include/cutlass/library/descriptions.h
--rw-r--r--  2.0 unx    16150 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/include/cutlass/library/handle.h
--rw-r--r--  2.0 unx    20135 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/include/cutlass/library/library.h
--rw-r--r--  2.0 unx     4251 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/include/cutlass/library/manifest.h
--rw-r--r--  2.0 unx    19027 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/include/cutlass/library/operation_table.h
--rw-r--r--  2.0 unx     2724 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/include/cutlass/library/singleton.h
--rw-r--r--  2.0 unx     5908 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/include/cutlass/library/types.h
--rw-r--r--  2.0 unx     8140 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/include/cutlass/library/util.h
--rw-r--r--  2.0 unx    22377 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/conv2d_operation.h
--rw-r--r--  2.0 unx    13850 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/conv3d_operation.h
--rw-r--r--  2.0 unx    42608 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/gemm_operation.h
--rw-r--r--  2.0 unx    13766 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/gemm_operation_3x.hpp
--rw-r--r--  2.0 unx    36802 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/handle.cu
--rw-r--r--  2.0 unx    13269 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/library_internal.h
--rw-r--r--  2.0 unx     3634 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/manifest.cpp
--rw-r--r--  2.0 unx     5551 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/operation_table.cu
--rw-r--r--  2.0 unx    12873 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/rank_2k_operation.h
--rw-r--r--  2.0 unx    11367 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/rank_k_operation.h
--rw-r--r--  2.0 unx     2669 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/singleton.cu
--rw-r--r--  2.0 unx    13134 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/symm_operation.h
--rw-r--r--  2.0 unx    11698 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/trmm_operation.h
--rw-r--r--  2.0 unx    46579 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/util.cu
--rw-r--r--  2.0 unx     3482 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reduction/init_reduction_operations.cu
--rw-r--r--  2.0 unx     8435 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reduction/reduction_device.cu
--rw-r--r--  2.0 unx    10269 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reduction/reduction_operation.h
--rw-r--r--  2.0 unx     6746 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/conv2d.cu
--rw-r--r--  2.0 unx     6286 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/conv3d.cu
--rw-r--r--  2.0 unx    17347 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/conv_reference_operation.h
--rw-r--r--  2.0 unx     5473 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_e4m3a_e4m3out.cu
--rw-r--r--  2.0 unx     5070 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_e4m3a_e5m2out.cu
--rw-r--r--  2.0 unx     5070 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_e5m2a_e4m3out.cu
--rw-r--r--  2.0 unx     5070 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_e5m2a_e5m2out.cu
--rw-r--r--  2.0 unx     3633 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_fp32out.cu
--rw-r--r--  2.0 unx     4262 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_fp8in_bf16out.cu
--rw-r--r--  2.0 unx     4260 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_fp8in_fp16out.cu
--rw-r--r--  2.0 unx     4260 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_fp8in_fp32out.cu
--rw-r--r--  2.0 unx     4056 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_fp_mixed_input.cu
--rw-r--r--  2.0 unx     3140 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_fp_other.cu
--rw-r--r--  2.0 unx     3729 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_int4.cu
--rw-r--r--  2.0 unx     3683 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_int8_canonical.cu
--rw-r--r--  2.0 unx     3721 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_int8_interleaved_32.cu
--rw-r--r--  2.0 unx     3744 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_int8_interleaved_64.cu
--rw-r--r--  2.0 unx    16453 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/gemm_reference_operation.h
--rw-r--r--  2.0 unx     4810 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/library/src/reference/initialize_reference_operations.cu
--rw-r--r--  2.0 unx     5548 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/CMakeLists.txt
--rw-r--r--  2.0 unx    18227 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/conv2d_operation_profiler.h
--rw-r--r--  2.0 unx    16101 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/conv3d_operation_profiler.h
--rw-r--r--  2.0 unx    10623 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/cublas_helpers.h
--rw-r--r--  2.0 unx    20435 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/cudnn_helpers.h
--rw-r--r--  2.0 unx     3233 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/cutlass_profiler.h
--rw-r--r--  2.0 unx     2454 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/debug.h
--rw-r--r--  2.0 unx     7576 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/device_allocation.h
--rw-r--r--  2.0 unx     4290 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/device_context.h
--rw-r--r--  2.0 unx     6421 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/enumerated_types.h
--rw-r--r--  2.0 unx     8749 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/gemm_operation_profiler.h
--rw-r--r--  2.0 unx     2725 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/gpu_timer.h
--rw-r--r--  2.0 unx     7924 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/operation_profiler.h
--rw-r--r--  2.0 unx     9273 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/options.h
--rw-r--r--  2.0 unx     4337 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/performance_report.h
--rw-r--r--  2.0 unx     3941 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/performance_result.h
--rw-r--r--  2.0 unx    28189 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/problem_space.h
--rw-r--r--  2.0 unx     6891 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/rank_2k_operation_profiler.h
--rw-r--r--  2.0 unx     6830 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/rank_k_operation_profiler.h
--rw-r--r--  2.0 unx     5452 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/reduction_operation_profiler.h
--rw-r--r--  2.0 unx     6471 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/sparse_gemm_operation_profiler.h
--rw-r--r--  2.0 unx     6933 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/symm_operation_profiler.h
--rw-r--r--  2.0 unx     6599 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/include/cutlass/profiler/trmm_operation_profiler.h
--rw-r--r--  2.0 unx    54272 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/conv2d_operation_profiler.cu
--rw-r--r--  2.0 unx    48776 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/conv3d_operation_profiler.cu
--rw-r--r--  2.0 unx    37138 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/cublas_helpers.cu
--rw-r--r--  2.0 unx    17066 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/cudnn_helpers.cpp
--rw-r--r--  2.0 unx     7391 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/cutlass_profiler.cu
--rw-r--r--  2.0 unx    78653 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/device_allocation.cu
--rw-r--r--  2.0 unx     8359 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/device_context.cu
--rw-r--r--  2.0 unx     8313 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/enumerated_types.cpp
--rw-r--r--  2.0 unx    43920 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/gemm_operation_profiler.cu
--rw-r--r--  2.0 unx     3892 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/gpu_timer.cpp
--rw-r--r--  2.0 unx     2374 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/main.cpp
--rw-r--r--  2.0 unx    22898 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/operation_profiler.cu
--rw-r--r--  2.0 unx    28314 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/options.cu
--rw-r--r--  2.0 unx    14227 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/performance_report.cpp
--rw-r--r--  2.0 unx     2528 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/performance_result.cu
--rw-r--r--  2.0 unx    38798 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/problem_space.cpp
--rw-r--r--  2.0 unx    25183 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/rank_2k_operation_profiler.cu
--rw-r--r--  2.0 unx    24378 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/rank_k_operation_profiler.cu
--rw-r--r--  2.0 unx    20915 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/sparse_gemm_operation_profiler.cu
--rw-r--r--  2.0 unx    26753 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/symm_operation_profiler.cu
--rw-r--r--  2.0 unx    24567 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/profiler/src/trmm_operation_profiler.cu
--rw-r--r--  2.0 unx     2297 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/CMakeLists.txt
--rw-r--r--  2.0 unx     2410 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/GPU_Clock.hpp
--rw-r--r--  2.0 unx     9777 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/command_line.h
--rw-r--r--  2.0 unx    19866 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/cublas_wrappers.hpp
--rw-r--r--  2.0 unx     5104 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/debug.h
--rw-r--r--  2.0 unx     5953 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/device_dump.h
--rw-r--r--  2.0 unx    17696 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/device_groupnorm.h
--rw-r--r--  2.0 unx    20881 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/device_layernorm.h
--rw-r--r--  2.0 unx    10561 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/device_memory.h
--rw-r--r--  2.0 unx     5219 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/device_nchw_to_nhwc.h
--rw-r--r--  2.0 unx    11075 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/device_nhwc_padding.h
--rw-r--r--  2.0 unx    18653 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/device_nhwc_pooling.h
--rw-r--r--  2.0 unx     5214 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/device_nhwc_to_nchw.h
--rw-r--r--  2.0 unx     7096 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/device_rmsnorm.h
--rw-r--r--  2.0 unx     4007 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/device_utils.h
--rw-r--r--  2.0 unx     4846 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/distribution.h
--rw-r--r--  2.0 unx     2674 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/exceptions.h
--rw-r--r--  2.0 unx    13740 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/gett_commandline.hpp
--rw-r--r--  2.0 unx     3946 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/helper_cuda.hpp
--rw-r--r--  2.0 unx     4821 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/host_reorder.h
--rw-r--r--  2.0 unx    18299 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/host_tensor.h
--rw-r--r--  2.0 unx    20354 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/host_tensor_planar_complex.h
--rw-r--r--  2.0 unx     5890 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/host_uncompress.h
--rw-r--r--  2.0 unx     1962 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/index_sequence.h
--rw-r--r--  2.0 unx     4686 b- defN 24-Feb-15 21:09 cutlass_library/source/tools/util/include/cutlass/util/packed_stride.hpp
--rw-r--r--  2.0 unx    11254 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/print_error.hpp
--rw-r--r--  2.0 unx     8341 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/tensor_view_io.h
--rw-r--r--  2.0 unx     8809 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/type_traits.h
--rw-r--r--  2.0 unx     4606 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/detail/inner_product.h
--rw-r--r--  2.0 unx     3527 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h
--rw-r--r--  2.0 unx    48350 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/convolution.h
--rw-r--r--  2.0 unx    14296 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/gemm.h
--rw-r--r--  2.0 unx    10652 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/gemm_complex.h
--rw-r--r--  2.0 unx     9652 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h
--rw-r--r--  2.0 unx     5444 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/gett.hpp
--rw-r--r--  2.0 unx    11615 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h
--rw-r--r--  2.0 unx     7278 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_compare.h
--rw-r--r--  2.0 unx    49624 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_fill.h
--rw-r--r--  2.0 unx     5454 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_foreach.h
--rw-r--r--  2.0 unx    15964 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_reduce.h
--rw-r--r--  2.0 unx     4589 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_relu.h
--rw-r--r--  2.0 unx     5381 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/kernel/gemm.h
--rw-r--r--  2.0 unx     6198 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h
--rw-r--r--  2.0 unx     5127 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h
--rw-r--r--  2.0 unx     5872 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/device/thread/gemm.h
--rw-r--r--  2.0 unx    28652 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/convolution.h
--rw-r--r--  2.0 unx     2766 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/error_metrics.h
--rw-r--r--  2.0 unx    20938 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/gemm.h
--rw-r--r--  2.0 unx     7161 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/gemm_complex.h
--rw-r--r--  2.0 unx     7708 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h
--rw-r--r--  2.0 unx    22747 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/gett.hpp
--rw-r--r--  2.0 unx     9441 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/rank_2k.h
--rw-r--r--  2.0 unx    11444 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h
--rw-r--r--  2.0 unx     8148 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/rank_k_complex.h
--rw-r--r--  2.0 unx    10509 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/symm.h
--rw-r--r--  2.0 unx    12296 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/symm_complex.h
--rw-r--r--  2.0 unx    11235 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_compare.h
--rw-r--r--  2.0 unx     3339 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_compare.hpp
--rw-r--r--  2.0 unx     8317 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_copy.h
--rw-r--r--  2.0 unx     9027 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h
--rw-r--r--  2.0 unx    46990 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_fill.h
--rw-r--r--  2.0 unx    12875 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_fill.hpp
--rw-r--r--  2.0 unx     4757 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_foreach.h
--rw-r--r--  2.0 unx     2133 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_norm.h
--rw-r--r--  2.0 unx     6111 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_reduce.h
--rw-r--r--  2.0 unx     5987 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_reduce.hpp
--rw-r--r--  2.0 unx     7670 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/trmm.h
--rw-r--r--  2.0 unx     9874 b- defN 24-Jan-13 19:00 cutlass_library/source/tools/util/include/cutlass/util/reference/host/trmm_complex.h
--rw-r--r--  2.0 unx     1889 b- defN 24-Jan-13 19:00 pycute/__init__.py
--rw-r--r--  2.0 unx     7813 b- defN 24-Jan-13 19:00 pycute/int_tuple.py
--rw-r--r--  2.0 unx    12388 b- defN 24-Jan-13 19:00 pycute/layout.py
--rw-r--r--  2.0 unx     4475 b- defN 24-Jan-13 19:00 pycute/swizzle.py
--rw-r--r--  2.0 unx     1981 b- defN 24-Jan-13 19:00 pycute/typing.py
--rw-r--r--  2.0 unx     1547 b- defN 24-Feb-15 21:16 nvidia_cutlass-3.4.1.0.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx    28723 b- defN 24-Feb-15 21:16 nvidia_cutlass-3.4.1.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Feb-15 21:16 nvidia_cutlass-3.4.1.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       31 b- defN 24-Feb-15 21:16 nvidia_cutlass-3.4.1.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx   142817 b- defN 24-Feb-15 21:16 nvidia_cutlass-3.4.1.0.dist-info/RECORD
-1097 files, 17723607 bytes uncompressed, 3565345 bytes compressed:  79.9%
+Zip file size: 3989130 bytes, number of entries: 1139
+-rw-r--r--  2.0 unx     6220 b- defN 24-Apr-12 11:22 cutlass/__init__.py
+-rw-r--r--  2.0 unx    26526 b- defN 24-Apr-12 11:22 cutlass/library_defaults.py
+-rw-r--r--  2.0 unx     5715 b- defN 24-Apr-12 11:22 cutlass/shape.py
+-rw-r--r--  2.0 unx     2718 b- defN 24-Apr-12 11:22 cutlass/swizzle.py
+-rw-r--r--  2.0 unx     2464 b- defN 24-Apr-12 11:22 cutlass/backend/__init__.py
+-rw-r--r--  2.0 unx     5678 b- defN 24-Apr-12 11:22 cutlass/backend/arguments.py
+-rw-r--r--  2.0 unx    21871 b- defN 24-Apr-12 11:22 cutlass/backend/c_types.py
+-rw-r--r--  2.0 unx    18206 b- defN 24-Apr-12 11:22 cutlass/backend/compiler.py
+-rw-r--r--  2.0 unx    26627 b- defN 24-Apr-12 11:22 cutlass/backend/conv2d_operation.py
+-rw-r--r--  2.0 unx    17657 b- defN 24-Apr-12 11:22 cutlass/backend/epilogue.py
+-rw-r--r--  2.0 unx     3813 b- defN 24-Apr-12 11:22 cutlass/backend/frontend.py
+-rw-r--r--  2.0 unx    85190 b- defN 24-Apr-12 11:22 cutlass/backend/gemm_operation.py
+-rw-r--r--  2.0 unx    17280 b- defN 24-Apr-12 11:22 cutlass/backend/library.py
+-rw-r--r--  2.0 unx     4332 b- defN 24-Apr-12 11:22 cutlass/backend/memory_manager.py
+-rw-r--r--  2.0 unx     5523 b- defN 24-Apr-12 11:22 cutlass/backend/operation.py
+-rw-r--r--  2.0 unx    15701 b- defN 24-Apr-12 11:22 cutlass/backend/reduction_operation.py
+-rw-r--r--  2.0 unx     1907 b- defN 24-Apr-12 11:22 cutlass/backend/type_hint.py
+-rw-r--r--  2.0 unx     1920 b- defN 24-Apr-12 11:22 cutlass/backend/evt/__init__.py
+-rw-r--r--  2.0 unx     7027 b- defN 24-Apr-12 11:22 cutlass/backend/evt/epilogue.py
+-rw-r--r--  2.0 unx     2047 b- defN 24-Apr-12 11:22 cutlass/backend/evt/backend/__init__.py
+-rw-r--r--  2.0 unx     6453 b- defN 24-Apr-12 11:22 cutlass/backend/evt/backend/emitter_base.py
+-rw-r--r--  2.0 unx     2252 b- defN 24-Apr-12 11:22 cutlass/backend/evt/backend/sm80_emitter.py
+-rw-r--r--  2.0 unx     7581 b- defN 24-Apr-12 11:22 cutlass/backend/evt/backend/sm80_nodes.py
+-rw-r--r--  2.0 unx     3828 b- defN 24-Apr-12 11:22 cutlass/backend/evt/backend/sm90_emitter.py
+-rw-r--r--  2.0 unx    10985 b- defN 24-Apr-12 11:22 cutlass/backend/evt/backend/sm90_nodes.py
+-rw-r--r--  2.0 unx     1867 b- defN 24-Apr-12 11:22 cutlass/backend/evt/frontend/__init__.py
+-rw-r--r--  2.0 unx     8856 b- defN 24-Apr-12 11:22 cutlass/backend/evt/frontend/frontend_base.py
+-rw-r--r--  2.0 unx     6746 b- defN 24-Apr-12 11:22 cutlass/backend/evt/frontend/python_ast.py
+-rw-r--r--  2.0 unx     2405 b- defN 24-Apr-12 11:22 cutlass/backend/evt/ir/__init__.py
+-rw-r--r--  2.0 unx     3447 b- defN 24-Apr-12 11:22 cutlass/backend/evt/ir/compute_nodes.py
+-rw-r--r--  2.0 unx     7524 b- defN 24-Apr-12 11:22 cutlass/backend/evt/ir/dag_ir.py
+-rw-r--r--  2.0 unx    13261 b- defN 24-Apr-12 11:22 cutlass/backend/evt/ir/layout_algorithm.py
+-rw-r--r--  2.0 unx    13267 b- defN 24-Apr-12 11:22 cutlass/backend/evt/ir/layout_nodes.py
+-rw-r--r--  2.0 unx     9566 b- defN 24-Apr-12 11:22 cutlass/backend/evt/ir/load_nodes.py
+-rw-r--r--  2.0 unx    10348 b- defN 24-Apr-12 11:22 cutlass/backend/evt/ir/node.py
+-rw-r--r--  2.0 unx     9429 b- defN 24-Apr-12 11:22 cutlass/backend/evt/ir/store_nodes.py
+-rw-r--r--  2.0 unx     4908 b- defN 24-Apr-12 11:22 cutlass/backend/evt/ir/tensor.py
+-rw-r--r--  2.0 unx     2552 b- defN 24-Apr-12 11:22 cutlass/backend/evt/passes/__init__.py
+-rw-r--r--  2.0 unx     5500 b- defN 24-Apr-12 11:22 cutlass/backend/evt/passes/graph_drawer.py
+-rw-r--r--  2.0 unx     5291 b- defN 24-Apr-12 11:22 cutlass/backend/evt/passes/pass_argument_type.py
+-rw-r--r--  2.0 unx     6809 b- defN 24-Apr-12 11:22 cutlass/backend/evt/passes/pass_dag_2_tree.py
+-rw-r--r--  2.0 unx     2984 b- defN 24-Apr-12 11:22 cutlass/backend/evt/passes/pass_fix_element_d.py
+-rw-r--r--  2.0 unx     4256 b- defN 24-Apr-12 11:22 cutlass/backend/evt/passes/pass_get_impl.py
+-rw-r--r--  2.0 unx     9032 b- defN 24-Apr-12 11:22 cutlass/backend/evt/passes/pass_layout_elimination.py
+-rw-r--r--  2.0 unx     5462 b- defN 24-Apr-12 11:22 cutlass/backend/evt/passes/pass_manager.py
+-rw-r--r--  2.0 unx     2414 b- defN 24-Apr-12 11:22 cutlass/backend/evt/passes/pass_no_op_elimination.py
+-rw-r--r--  2.0 unx     4456 b- defN 24-Apr-12 11:22 cutlass/backend/evt/passes/pass_preprocess_red.py
+-rw-r--r--  2.0 unx     2817 b- defN 24-Apr-12 11:22 cutlass/backend/evt/passes/pass_shape_type_propagation.py
+-rw-r--r--  2.0 unx     8126 b- defN 24-Apr-12 11:22 cutlass/backend/evt/passes/smem_size_calculator.py
+-rw-r--r--  2.0 unx     1966 b- defN 24-Apr-12 11:22 cutlass/backend/evt/passes/util.py
+-rw-r--r--  2.0 unx     1833 b- defN 24-Apr-12 11:22 cutlass/backend/utils/__init__.py
+-rw-r--r--  2.0 unx     4461 b- defN 24-Apr-12 11:22 cutlass/backend/utils/device.py
+-rw-r--r--  2.0 unx     1838 b- defN 24-Apr-12 11:22 cutlass/emit/__init__.py
+-rw-r--r--  2.0 unx    10591 b- defN 24-Apr-12 11:22 cutlass/emit/common.py
+-rw-r--r--  2.0 unx    37822 b- defN 24-Apr-12 11:22 cutlass/emit/pytorch.py
+-rw-r--r--  2.0 unx     2100 b- defN 24-Apr-12 11:22 cutlass/epilogue/__init__.py
+-rw-r--r--  2.0 unx     5562 b- defN 24-Apr-12 11:22 cutlass/epilogue/epilogue.py
+-rw-r--r--  2.0 unx     2909 b- defN 24-Apr-12 11:22 cutlass/epilogue/evt_ops.py
+-rw-r--r--  2.0 unx     1992 b- defN 24-Apr-12 11:22 cutlass/op/__init__.py
+-rw-r--r--  2.0 unx    43314 b- defN 24-Apr-12 11:22 cutlass/op/conv.py
+-rw-r--r--  2.0 unx    32080 b- defN 24-Apr-12 11:22 cutlass/op/gemm.py
+-rw-r--r--  2.0 unx    12455 b- defN 24-Apr-12 11:22 cutlass/op/gemm_grouped.py
+-rw-r--r--  2.0 unx    18566 b- defN 24-Apr-12 11:22 cutlass/op/op.py
+-rw-r--r--  2.0 unx     2011 b- defN 24-Apr-12 11:22 cutlass/utils/__init__.py
+-rw-r--r--  2.0 unx    12125 b- defN 24-Apr-12 11:22 cutlass/utils/check.py
+-rw-r--r--  2.0 unx    12115 b- defN 24-Apr-12 11:22 cutlass/utils/datatypes.py
+-rw-r--r--  2.0 unx     6905 b- defN 24-Apr-12 11:22 cutlass/utils/profiler.py
+-rw-r--r--  2.0 unx     2779 b- defN 24-Apr-12 11:22 cutlass_library/__init__.py
+-rw-r--r--  2.0 unx    24882 b- defN 24-Apr-12 11:22 cutlass_library/conv2d_operation.py
+-rw-r--r--  2.0 unx    19586 b- defN 24-Apr-12 11:22 cutlass_library/conv3d_operation.py
+-rw-r--r--  2.0 unx     9839 b- defN 24-Apr-12 11:22 cutlass_library/conv3x_emitter.py
+-rw-r--r--  2.0 unx    53775 b- defN 24-Apr-12 11:22 cutlass_library/gemm_operation.py
+-rw-r--r--  2.0 unx   277676 b- defN 24-Apr-12 11:22 cutlass_library/generator.py
+-rw-r--r--  2.0 unx    34549 b- defN 24-Apr-12 11:22 cutlass_library/library.py
+-rw-r--r--  2.0 unx    31128 b- defN 24-Apr-12 11:22 cutlass_library/manifest.py
+-rw-r--r--  2.0 unx    16216 b- defN 24-Apr-12 11:22 cutlass_library/rank_2k_operation.py
+-rw-r--r--  2.0 unx    15775 b- defN 24-Apr-12 11:22 cutlass_library/rank_k_operation.py
+-rw-r--r--  2.0 unx    16164 b- defN 24-Apr-12 11:22 cutlass_library/symm_operation.py
+-rw-r--r--  2.0 unx    16650 b- defN 24-Apr-12 11:22 cutlass_library/trmm_operation.py
+-rw-r--r--  2.0 unx     4527 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/CMakeLists.txt
+-rw-r--r--  2.0 unx     1668 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/00_basic_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    14698 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/00_basic_gemm/basic_gemm.cu
+-rw-r--r--  2.0 unx     1681 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/01_cutlass_utilities/CMakeLists.txt
+-rw-r--r--  2.0 unx    13255 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/01_cutlass_utilities/cutlass_utilities.cu
+-rw-r--r--  2.0 unx     1693 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/02_dump_reg_shmem/CMakeLists.txt
+-rw-r--r--  2.0 unx     7157 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/02_dump_reg_shmem/dump_reg_shmem.cu
+-rw-r--r--  2.0 unx     1789 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/03_visualize_layout/CMakeLists.txt
+-rw-r--r--  2.0 unx     4478 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/03_visualize_layout/options.h
+-rw-r--r--  2.0 unx     7081 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/03_visualize_layout/register_layout.cu
+-rw-r--r--  2.0 unx     2691 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/03_visualize_layout/register_layout.h
+-rw-r--r--  2.0 unx     5819 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/03_visualize_layout/visualize_layout.cpp
+-rw-r--r--  2.0 unx    11425 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/03_visualize_layout/visualize_layout.h
+-rw-r--r--  2.0 unx     1673 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/04_tile_iterator/CMakeLists.txt
+-rw-r--r--  2.0 unx     8229 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/04_tile_iterator/tile_iterator.cu
+-rw-r--r--  2.0 unx     1672 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/05_batched_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    15185 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/05_batched_gemm/batched_gemm.cu
+-rw-r--r--  2.0 unx     1670 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/06_splitK_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    17570 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/06_splitK_gemm/splitk_gemm.cu
+-rw-r--r--  2.0 unx     1686 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/07_volta_tensorop_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    18283 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu
+-rw-r--r--  2.0 unx     1688 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/08_turing_tensorop_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    18228 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu
+-rw-r--r--  2.0 unx     1703 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/09_turing_tensorop_conv2dfprop/CMakeLists.txt
+-rw-r--r--  2.0 unx    28142 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu
+-rw-r--r--  2.0 unx     1897 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/10_planar_complex/CMakeLists.txt
+-rw-r--r--  2.0 unx    21947 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/10_planar_complex/planar_complex.cu
+-rw-r--r--  2.0 unx     1921 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/11_planar_complex_array/CMakeLists.txt
+-rw-r--r--  2.0 unx    23244 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/11_planar_complex_array/planar_complex_array.cu
+-rw-r--r--  2.0 unx     1676 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/12_gemm_bias_relu/CMakeLists.txt
+-rw-r--r--  2.0 unx    13152 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/12_gemm_bias_relu/gemm_bias_relu.cu
+-rw-r--r--  2.0 unx     2757 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/CMakeLists.txt
+-rw-r--r--  2.0 unx     5893 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/README.md
+-rw-r--r--  2.0 unx    26102 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h
+-rw-r--r--  2.0 unx    24557 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_gemm_run.h
+-rw-r--r--  2.0 unx    18662 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_grouped_gemm_run.h
+-rw-r--r--  2.0 unx    28268 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h
+-rw-r--r--  2.0 unx    26174 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h
+-rw-r--r--  2.0 unx     8756 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu
+-rw-r--r--  2.0 unx     8759 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu
+-rw-r--r--  2.0 unx     8712 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu
+-rw-r--r--  2.0 unx     8762 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu
+-rw-r--r--  2.0 unx     8782 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu
+-rw-r--r--  2.0 unx     8785 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu
+-rw-r--r--  2.0 unx     8711 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu
+-rw-r--r--  2.0 unx     8775 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu
+-rw-r--r--  2.0 unx     7269 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu
+-rw-r--r--  2.0 unx     7338 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu
+-rw-r--r--  2.0 unx     7294 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu
+-rw-r--r--  2.0 unx     7359 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu
+-rw-r--r--  2.0 unx    10064 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_grouped_f16_sm80_rf.cu
+-rw-r--r--  2.0 unx     7358 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu
+-rw-r--r--  2.0 unx     7424 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu
+-rw-r--r--  2.0 unx    11029 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu
+-rw-r--r--  2.0 unx     7619 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu
+-rw-r--r--  2.0 unx     3577 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/test_run.h
+-rw-r--r--  2.0 unx    12711 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/device/b2b_gemm.h
+-rw-r--r--  2.0 unx    11520 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h
+-rw-r--r--  2.0 unx    30434 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h
+-rw-r--r--  2.0 unx     6120 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/b2b_gemm_grouped_problem_visitor.h
+-rw-r--r--  2.0 unx    18151 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h
+-rw-r--r--  2.0 unx     3973 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h
+-rw-r--r--  2.0 unx    26762 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h
+-rw-r--r--  2.0 unx    26775 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h
+-rw-r--r--  2.0 unx    28422 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h
+-rw-r--r--  2.0 unx    28073 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h
+-rw-r--r--  2.0 unx    19973 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h
+-rw-r--r--  2.0 unx    15092 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h
+-rw-r--r--  2.0 unx     6380 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/grouped.h
+-rw-r--r--  2.0 unx    14663 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h
+-rw-r--r--  2.0 unx    31616 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h
+-rw-r--r--  2.0 unx    31443 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h
+-rw-r--r--  2.0 unx    21012 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h
+-rw-r--r--  2.0 unx    20494 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h
+-rw-r--r--  2.0 unx     7983 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h
+-rw-r--r--  2.0 unx     6047 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
+-rw-r--r--  2.0 unx    34515 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h
+-rw-r--r--  2.0 unx    34226 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h
+-rw-r--r--  2.0 unx    21820 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h
+-rw-r--r--  2.0 unx    21434 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h
+-rw-r--r--  2.0 unx    27144 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h
+-rw-r--r--  2.0 unx    27400 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h
+-rw-r--r--  2.0 unx     5719 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/grouped_threadblock_swizzle.h
+-rw-r--r--  2.0 unx     1698 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/14_ampere_tf32_tensorop_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    18020 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu
+-rw-r--r--  2.0 unx     1830 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/15_ampere_sparse_tensorop_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    15042 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu
+-rw-r--r--  2.0 unx    15957 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm_with_visitor.cu
+-rw-r--r--  2.0 unx     1703 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/16_ampere_tensorop_conv2dfprop/CMakeLists.txt
+-rw-r--r--  2.0 unx    28030 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu
+-rw-r--r--  2.0 unx     1694 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/17_fprop_per_channel_bias/CMakeLists.txt
+-rw-r--r--  2.0 unx    12580 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu
+-rw-r--r--  2.0 unx     1715 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/18_ampere_fp64_tensorop_affine2_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    14007 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu
+-rw-r--r--  2.0 unx     1682 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/19_tensorop_canonical/CMakeLists.txt
+-rw-r--r--  2.0 unx    13401 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/19_tensorop_canonical/tensorop_canonical.cu
+-rw-r--r--  2.0 unx     1674 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/20_simt_canonical/CMakeLists.txt
+-rw-r--r--  2.0 unx    12556 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/20_simt_canonical/simt_canonical.cu
+-rw-r--r--  2.0 unx     1678 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/21_quaternion_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    17319 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/21_quaternion_gemm/quaternion_gemm.cu
+-rw-r--r--  2.0 unx     1680 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/22_quaternion_conv/CMakeLists.txt
+-rw-r--r--  2.0 unx    21423 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/22_quaternion_conv/quaternion_conv.cu
+-rw-r--r--  2.0 unx     1894 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/23_ampere_gemm_operand_reduction_fusion/CMakeLists.txt
+-rw-r--r--  2.0 unx    27556 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu
+-rw-r--r--  2.0 unx     1674 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/24_gemm_grouped/CMakeLists.txt
+-rw-r--r--  2.0 unx    50898 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/24_gemm_grouped/gemm_grouped.cu
+-rw-r--r--  2.0 unx     1816 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/25_ampere_fprop_mainloop_fusion/CMakeLists.txt
+-rw-r--r--  2.0 unx    26547 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu
+-rw-r--r--  2.0 unx    25628 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu
+-rw-r--r--  2.0 unx     1705 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/26_ampere_wgrad_mainloop_fusion/CMakeLists.txt
+-rw-r--r--  2.0 unx    25538 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu
+-rw-r--r--  2.0 unx    30398 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu
+-rw-r--r--  2.0 unx     1733 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx     1732 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/CMakeLists.txt
+-rw-r--r--  2.0 unx    28159 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu
+-rw-r--r--  2.0 unx    28338 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm.cu
+-rw-r--r--  2.0 unx     1754 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    27304 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/30_wgrad_split_k/30_wgrad_split_k.cu
+-rw-r--r--  2.0 unx     1677 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/30_wgrad_split_k/CMakeLists.txt
+-rw-r--r--  2.0 unx     1668 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/31_basic_syrk/CMakeLists.txt
+-rw-r--r--  2.0 unx    15202 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/31_basic_syrk/basic_syrk.cu
+-rw-r--r--  2.0 unx     1668 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/32_basic_trmm/CMakeLists.txt
+-rw-r--r--  2.0 unx    15906 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/32_basic_trmm/basic_trmm.cu
+-rw-r--r--  2.0 unx     1702 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/33_ampere_3xtf32_tensorop_symm/CMakeLists.txt
+-rw-r--r--  2.0 unx    31765 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu
+-rw-r--r--  2.0 unx    22378 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/34_transposed_conv2d/34_transposed_conv2d.cu
+-rw-r--r--  2.0 unx     1684 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/34_transposed_conv2d/CMakeLists.txt
+-rw-r--r--  2.0 unx     1673 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/35_gemm_softmax/CMakeLists.txt
+-rw-r--r--  2.0 unx    23120 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/35_gemm_softmax/gemm_softmax.cu
+-rw-r--r--  2.0 unx    16723 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h
+-rw-r--r--  2.0 unx    19055 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/35_gemm_softmax/gemm_with_softmax.h
+-rw-r--r--  2.0 unx     1687 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/36_gather_scatter_fusion/CMakeLists.txt
+-rw-r--r--  2.0 unx    21008 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu
+-rw-r--r--  2.0 unx     1689 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/CMakeLists.txt
+-rw-r--r--  2.0 unx    31111 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu
+-rw-r--r--  2.0 unx    13982 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h
+-rw-r--r--  2.0 unx    33900 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h
+-rw-r--r--  2.0 unx     1675 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/38_syr2k_grouped/CMakeLists.txt
+-rw-r--r--  2.0 unx    47468 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/38_syr2k_grouped/syr2k_grouped.cu
+-rw-r--r--  2.0 unx     1674 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/39_gemm_permute/CMakeLists.txt
+-rw-r--r--  2.0 unx    48551 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/39_gemm_permute/gemm_permute.cu
+-rw-r--r--  2.0 unx    15309 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/39_gemm_permute/layouts.h
+-rw-r--r--  2.0 unx    11985 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/39_gemm_permute/permute_info.h
+-rw-r--r--  2.0 unx      253 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/40_cutlass_py/README.md
+-rw-r--r--  2.0 unx    15590 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/40_cutlass_py/customizable/README.md
+-rw-r--r--  2.0 unx       36 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/40_cutlass_py/customizable/grouped_gemm_problem_size.csv
+-rw-r--r--  2.0 unx     2390 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/CMakeLists.txt
+-rw-r--r--  2.0 unx    11871 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/debug_utils.h
+-rw-r--r--  2.0 unx    10832 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/default_fmha_grouped.h
+-rw-r--r--  2.0 unx    37291 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/fmha_grouped.h
+-rw-r--r--  2.0 unx     6666 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h
+-rw-r--r--  2.0 unx    11208 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/fused_multi_head_attention_backward.cu
+-rw-r--r--  2.0 unx    38221 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu
+-rw-r--r--  2.0 unx    40006 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu
+-rw-r--r--  2.0 unx    11075 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/gemm_kernel_utils.h
+-rw-r--r--  2.0 unx    97640 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/kernel_backward.h
+-rw-r--r--  2.0 unx    52585 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/kernel_forward.h
+-rw-r--r--  2.0 unx    22356 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h
+-rw-r--r--  2.0 unx     9162 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h
+-rw-r--r--  2.0 unx     6118 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h
+-rw-r--r--  2.0 unx     3994 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma.h
+-rw-r--r--  2.0 unx     6248 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h
+-rw-r--r--  2.0 unx    26967 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h
+-rw-r--r--  2.0 unx    14105 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h
+-rw-r--r--  2.0 unx     6782 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/find_default_mma.h
+-rw-r--r--  2.0 unx    13959 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h
+-rw-r--r--  2.0 unx    68653 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h
+-rw-r--r--  2.0 unx     5776 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/default_warp_iterator_from_smem.h
+-rw-r--r--  2.0 unx    23862 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h
+-rw-r--r--  2.0 unx     3142 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/make_residual_last.h
+-rw-r--r--  2.0 unx    64473 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h
+-rw-r--r--  2.0 unx    64507 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h
+-rw-r--r--  2.0 unx     2512 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h
+-rw-r--r--  2.0 unx    10063 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h
+-rw-r--r--  2.0 unx     3761 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h
+-rw-r--r--  2.0 unx     1701 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/42_ampere_tensorop_group_conv/CMakeLists.txt
+-rw-r--r--  2.0 unx    23901 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu
+-rw-r--r--  2.0 unx     1690 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/43_ell_block_sparse_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    23867 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu
+-rw-r--r--  2.0 unx     2589 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/README.md
+-rw-r--r--  2.0 unx     1105 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/config.json
+-rw-r--r--  2.0 unx    10231 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h
+-rw-r--r--  2.0 unx     3745 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/utils.h
+-rw-r--r--  2.0 unx     6370 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h
+-rw-r--r--  2.0 unx     4099 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h
+-rw-r--r--  2.0 unx     8285 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h
+-rw-r--r--  2.0 unx    10439 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h
+-rw-r--r--  2.0 unx     6848 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h
+-rw-r--r--  2.0 unx    14747 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h
+-rwxr-xr-x  2.0 unx     2346 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/ir_gen/generate.sh
+-rw-r--r--  2.0 unx     1668 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/45_dual_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    12642 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/45_dual_gemm/dual_gemm.cu
+-rw-r--r--  2.0 unx     2366 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/45_dual_gemm/dual_gemm_common.h
+-rw-r--r--  2.0 unx    31509 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/45_dual_gemm/dual_gemm_run.h
+-rw-r--r--  2.0 unx     3577 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/45_dual_gemm/test_run.h
+-rw-r--r--  2.0 unx    16953 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/45_dual_gemm/device/dual_gemm.h
+-rw-r--r--  2.0 unx    18413 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/45_dual_gemm/kernel/dual_gemm.h
+-rw-r--r--  2.0 unx     5818 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/45_dual_gemm/thread/left_silu_and_mul.h
+-rw-r--r--  2.0 unx    15613 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/45_dual_gemm/threadblock/dual_epilogue.h
+-rw-r--r--  2.0 unx     7920 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/45_dual_gemm/threadblock/dual_mma_base.h
+-rw-r--r--  2.0 unx    29897 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/45_dual_gemm/threadblock/dual_mma_multistage.h
+-rw-r--r--  2.0 unx     1701 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/46_depthwise_simt_conv2dfprop/CMakeLists.txt
+-rw-r--r--  2.0 unx    24772 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu
+-rw-r--r--  2.0 unx     2030 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/47_ampere_gemm_universal_streamk/CMakeLists.txt
+-rw-r--r--  2.0 unx    22676 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu
+-rw-r--r--  2.0 unx    30694 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk_broadcast.cu
+-rw-r--r--  2.0 unx    17286 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu
+-rw-r--r--  2.0 unx     1707 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/48_hopper_warp_specialized_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    30447 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu
+-rw-r--r--  2.0 unx     1751 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/49_hopper_gemm_with_collective_builder/CMakeLists.txt
+-rw-r--r--  2.0 unx    18806 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu
+-rw-r--r--  2.0 unx     1717 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/50_hopper_gemm_with_epilogue_swizzle/CMakeLists.txt
+-rw-r--r--  2.0 unx    17340 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/51_hopper_gett/51_hopper_gett.cu
+-rw-r--r--  2.0 unx     1668 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/51_hopper_gett/CMakeLists.txt
+-rw-r--r--  2.0 unx     5599 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/51_hopper_gett/gett_kernel.cuh
+-rw-r--r--  2.0 unx    27605 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/52_hopper_gather_scatter_fusion/52_hopper_gather_scatter_fusion.cu
+-rw-r--r--  2.0 unx     1704 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/52_hopper_gather_scatter_fusion/CMakeLists.txt
+-rw-r--r--  2.0 unx    17965 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_gemm.hpp
+-rw-r--r--  2.0 unx     5606 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_kernel.cuh
+-rw-r--r--  2.0 unx     8543 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/52_hopper_gather_scatter_fusion/scatter_epilogue.hpp
+-rw-r--r--  2.0 unx    45401 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/53_hopper_gemm_permute/53_hopper_gemm_permute.cu
+-rw-r--r--  2.0 unx     1690 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/53_hopper_gemm_permute/CMakeLists.txt
+-rw-r--r--  2.0 unx     4084 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/53_hopper_gemm_permute/permute_kernel.cuh
+-rw-r--r--  2.0 unx    11441 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/53_hopper_gemm_permute/permute_traits.hpp
+-rw-r--r--  2.0 unx    22065 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/54_hopper_fp8_warp_specialized_gemm/54_hopper_fp8_warp_specialized_gemm.cu
+-rw-r--r--  2.0 unx     1712 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/54_hopper_fp8_warp_specialized_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx     5157 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/54_hopper_fp8_warp_specialized_gemm/hopper_fp8_commandline.hpp
+-rw-r--r--  2.0 unx    31597 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/55_hopper_mixed_dtype_gemm/55_hopper_mixed_dtype_gemm.cu
+-rw-r--r--  2.0 unx     2978 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/55_hopper_mixed_dtype_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx     2587 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/55_hopper_mixed_dtype_gemm/README.md
+-rw-r--r--  2.0 unx     7838 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/55_hopper_mixed_dtype_gemm/unfused_weight_dequantize.hpp
+-rw-r--r--  2.0 unx    19510 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/56_hopper_ptr_array_batched_gemm/56_hopper_ptr_array_batched_gemm.cu
+-rw-r--r--  2.0 unx     2802 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/56_hopper_ptr_array_batched_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    26735 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/57_hopper_grouped_gemm/57_hopper_grouped_gemm.cu
+-rw-r--r--  2.0 unx     3411 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/57_hopper_grouped_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx     1664 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/58_ada_fp8_gemm/CMakeLists.txt
+-rw-r--r--  2.0 unx    28799 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/58_ada_fp8_gemm/ada_fp8_gemm.cu
+-rw-r--r--  2.0 unx     1833 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/59_ampere_gather_scatter_conv/CMakeLists.txt
+-rw-r--r--  2.0 unx    11886 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/59_ampere_gather_scatter_conv/README.md
+-rw-r--r--  2.0 unx    12530 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/59_ampere_gather_scatter_conv/ampere_conv_kernel.h
+-rw-r--r--  2.0 unx    17356 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/59_ampere_gather_scatter_conv/ampere_gather_scatter_conv.cu
+-rw-r--r--  2.0 unx     2738 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/60_cutlass_import/CMakeLists.txt
+-rw-r--r--  2.0 unx     2849 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/60_cutlass_import/main.cpp
+-rw-r--r--  2.0 unx     7085 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/common/gather_tensor.hpp
+-rw-r--r--  2.0 unx     4469 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/common/helper.h
+-rw-r--r--  2.0 unx     1625 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/cute/CMakeLists.txt
+-rw-r--r--  2.0 unx     1907 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/cute/tutorial/CMakeLists.txt
+-rw-r--r--  2.0 unx    17292 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/cute/tutorial/sgemm_1.cu
+-rw-r--r--  2.0 unx    19286 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/cute/tutorial/sgemm_2.cu
+-rw-r--r--  2.0 unx    18881 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/cute/tutorial/sgemm_sm70.cu
+-rw-r--r--  2.0 unx    19981 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/cute/tutorial/sgemm_sm80.cu
+-rw-r--r--  2.0 unx     9890 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/cute/tutorial/tiled_copy.cu
+-rw-r--r--  2.0 unx    16234 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/python/00_basic_gemm.ipynb
+-rw-r--r--  2.0 unx     7350 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/python/01_epilogue.ipynb
+-rw-r--r--  2.0 unx    10053 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/python/02_pytorch_extension_grouped_gemm.ipynb
+-rw-r--r--  2.0 unx    18123 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/python/03_basic_conv2d.ipynb
+-rw-r--r--  2.0 unx     8815 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/python/04_epilogue_visitor.ipynb
+-rw-r--r--  2.0 unx      961 b- defN 24-Apr-12 11:21 cutlass_library/source/examples/python/README.md
+-rw-r--r--  2.0 unx     5368 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/config.hpp
+-rw-r--r--  2.0 unx    28072 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/int_tuple.hpp
+-rw-r--r--  2.0 unx    61599 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/layout.hpp
+-rw-r--r--  2.0 unx    18072 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/layout_composed.hpp
+-rw-r--r--  2.0 unx     8583 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/pointer.hpp
+-rw-r--r--  2.0 unx     8336 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/pointer_base.hpp
+-rw-r--r--  2.0 unx     5725 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/pointer_flagged.hpp
+-rw-r--r--  2.0 unx     6136 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/pointer_swizzle.hpp
+-rw-r--r--  2.0 unx    16790 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/stride.hpp
+-rw-r--r--  2.0 unx    15513 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/swizzle.hpp
+-rw-r--r--  2.0 unx    21354 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/swizzle_layout.hpp
+-rw-r--r--  2.0 unx    35447 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/tensor.hpp
+-rw-r--r--  2.0 unx     2595 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/tensor_predicate.hpp
+-rw-r--r--  2.0 unx     6246 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/underscore.hpp
+-rw-r--r--  2.0 unx     3276 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/algorithm/axpby.hpp
+-rw-r--r--  2.0 unx     2351 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/algorithm/clear.hpp
+-rw-r--r--  2.0 unx     9711 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/algorithm/cooperative_copy.hpp
+-rw-r--r--  2.0 unx    22774 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/algorithm/cooperative_gemm.hpp
+-rw-r--r--  2.0 unx    13575 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/algorithm/copy.hpp
+-rw-r--r--  2.0 unx     2906 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/algorithm/fill.hpp
+-rw-r--r--  2.0 unx    10890 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/algorithm/functional.hpp
+-rw-r--r--  2.0 unx    18275 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/algorithm/gemm.hpp
+-rw-r--r--  2.0 unx     2124 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/algorithm/prefer.hpp
+-rw-r--r--  2.0 unx     5742 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/algorithm/prefetch.hpp
+-rw-r--r--  2.0 unx     5238 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/algorithm/tensor_algorithms.hpp
+-rw-r--r--  2.0 unx    27396 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/algorithm/tuple_algorithms.hpp
+-rw-r--r--  2.0 unx     7642 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/cluster_sm90.hpp
+-rw-r--r--  2.0 unx     3474 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/copy.hpp
+-rw-r--r--  2.0 unx     2686 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/copy_sm50.hpp
+-rw-r--r--  2.0 unx     7739 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/copy_sm75.hpp
+-rw-r--r--  2.0 unx     6856 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/copy_sm80.hpp
+-rw-r--r--  2.0 unx     7566 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/copy_sm90.hpp
+-rw-r--r--  2.0 unx    14193 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/copy_sm90_desc.hpp
+-rw-r--r--  2.0 unx    49750 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/copy_sm90_tma.hpp
+-rw-r--r--  2.0 unx     2393 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/mma.hpp
+-rw-r--r--  2.0 unx     3172 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/mma_sm61.hpp
+-rw-r--r--  2.0 unx    12500 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/mma_sm70.hpp
+-rw-r--r--  2.0 unx     4274 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/mma_sm75.hpp
+-rw-r--r--  2.0 unx    68808 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/mma_sm80.hpp
+-rw-r--r--  2.0 unx    53627 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/mma_sm90.hpp
+-rw-r--r--  2.0 unx     6172 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/mma_sm90_desc.hpp
+-rw-r--r--  2.0 unx   946916 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/mma_sm90_gmma.hpp
+-rw-r--r--  2.0 unx     8810 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/arch/util.hpp
+-rw-r--r--  2.0 unx    27595 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/copy_atom.hpp
+-rw-r--r--  2.0 unx     5684 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/copy_traits.hpp
+-rw-r--r--  2.0 unx     2512 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/copy_traits_sm50.hpp
+-rw-r--r--  2.0 unx     5087 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/copy_traits_sm75.hpp
+-rw-r--r--  2.0 unx     7107 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/copy_traits_sm80.hpp
+-rw-r--r--  2.0 unx     4589 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/copy_traits_sm90.hpp
+-rw-r--r--  2.0 unx    37568 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/copy_traits_sm90_im2col.hpp
+-rw-r--r--  2.0 unx    56618 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/copy_traits_sm90_tma.hpp
+-rw-r--r--  2.0 unx     2860 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/copy_traits_sm90_tma_swizzle.hpp
+-rw-r--r--  2.0 unx    33597 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/mma_atom.hpp
+-rw-r--r--  2.0 unx     8749 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/mma_traits.hpp
+-rw-r--r--  2.0 unx     2773 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/mma_traits_sm61.hpp
+-rw-r--r--  2.0 unx     6092 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/mma_traits_sm70.hpp
+-rw-r--r--  2.0 unx     3303 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/mma_traits_sm75.hpp
+-rw-r--r--  2.0 unx    14088 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/mma_traits_sm80.hpp
+-rw-r--r--  2.0 unx     5049 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/mma_traits_sm90.hpp
+-rw-r--r--  2.0 unx   189998 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/atom/mma_traits_sm90_gmma.hpp
+-rw-r--r--  2.0 unx     2992 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/container/alignment.hpp
+-rw-r--r--  2.0 unx     9549 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/container/array.hpp
+-rw-r--r--  2.0 unx     2082 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/container/array_aligned.hpp
+-rw-r--r--  2.0 unx    18091 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/container/array_subbyte.hpp
+-rw-r--r--  2.0 unx     5490 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/container/bit_field.hpp
+-rw-r--r--  2.0 unx     4615 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/container/cuda_types.hpp
+-rw-r--r--  2.0 unx    20607 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/container/tuple.hpp
+-rw-r--r--  2.0 unx     4225 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/container/type_list.hpp
+-rw-r--r--  2.0 unx    14952 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/numeric/arithmetic_tuple.hpp
+-rw-r--r--  2.0 unx     2796 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/numeric/complex.hpp
+-rw-r--r--  2.0 unx     3880 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/numeric/int.hpp
+-rw-r--r--  2.0 unx     4671 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/numeric/integer_sequence.hpp
+-rw-r--r--  2.0 unx    13829 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/numeric/integral_constant.hpp
+-rw-r--r--  2.0 unx     7312 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/numeric/integral_ratio.hpp
+-rw-r--r--  2.0 unx     9038 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/numeric/math.hpp
+-rw-r--r--  2.0 unx     2949 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/numeric/numeric_types.hpp
+-rw-r--r--  2.0 unx     2259 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/numeric/real.hpp
+-rw-r--r--  2.0 unx     5010 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/util/debug.hpp
+-rw-r--r--  2.0 unx     4310 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/util/print.hpp
+-rw-r--r--  2.0 unx     8197 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cute/util/type_traits.hpp
+-rw-r--r--  2.0 unx     3793 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/aligned_buffer.h
+-rw-r--r--  2.0 unx    67892 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/array.h
+-rw-r--r--  2.0 unx     3463 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/array_planar_complex.h
+-rw-r--r--  2.0 unx    13434 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/array_subbyte.h
+-rw-r--r--  2.0 unx    12433 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/barrier.h
+-rw-r--r--  2.0 unx    13884 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/bfloat16.h
+-rw-r--r--  2.0 unx     5294 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/blas3.h
+-rw-r--r--  2.0 unx     3263 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/blas3_types.h
+-rw-r--r--  2.0 unx     9386 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/block_striped.h
+-rw-r--r--  2.0 unx     9553 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/cluster_launch.hpp
+-rw-r--r--  2.0 unx    19734 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/complex.h
+-rw-r--r--  2.0 unx    47943 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/constants.h
+-rw-r--r--  2.0 unx    11827 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/coord.h
+-rw-r--r--  2.0 unx    10992 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/core_io.h
+-rw-r--r--  2.0 unx     7249 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/cuda_host_adapter.hpp
+-rw-r--r--  2.0 unx     6478 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/cutlass.h
+-rw-r--r--  2.0 unx     4321 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/device_kernel.h
+-rw-r--r--  2.0 unx    28949 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/fast_math.h
+-rw-r--r--  2.0 unx    37129 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/float8.h
+-rw-r--r--  2.0 unx     2645 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/floating_point_nvrtc.h
+-rw-r--r--  2.0 unx    17870 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/functional.h
+-rw-r--r--  2.0 unx    10599 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm_coord.h
+-rw-r--r--  2.0 unx     2875 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm_coord.hpp
+-rw-r--r--  2.0 unx    23630 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/half.h
+-rw-r--r--  2.0 unx     7942 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/integer_subbyte.h
+-rw-r--r--  2.0 unx     2784 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/kernel_hardware_info.h
+-rw-r--r--  2.0 unx     2006 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/kernel_hardware_info.hpp
+-rw-r--r--  2.0 unx     2801 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/kernel_launch.h
+-rw-r--r--  2.0 unx   364115 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/matrix.h
+-rw-r--r--  2.0 unx     4991 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/matrix_coord.h
+-rw-r--r--  2.0 unx     2726 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/matrix_shape.h
+-rw-r--r--  2.0 unx   125928 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/numeric_conversion.h
+-rw-r--r--  2.0 unx     3129 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/numeric_size.h
+-rw-r--r--  2.0 unx     3385 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/numeric_types.h
+-rw-r--r--  2.0 unx     5492 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/pitch_linear_coord.h
+-rw-r--r--  2.0 unx    16279 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/predicate_vector.h
+-rw-r--r--  2.0 unx    20891 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/quaternion.h
+-rw-r--r--  2.0 unx     2369 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/real.h
+-rw-r--r--  2.0 unx     6572 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/relatively_equal.h
+-rw-r--r--  2.0 unx     3984 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/semaphore.h
+-rw-r--r--  2.0 unx    38253 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/subbyte_reference.h
+-rw-r--r--  2.0 unx     8964 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/tensor_coord.h
+-rw-r--r--  2.0 unx    12207 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/tensor_ref.h
+-rw-r--r--  2.0 unx    11201 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/tensor_ref_planar_complex.h
+-rw-r--r--  2.0 unx     9509 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/tensor_view.h
+-rw-r--r--  2.0 unx    10250 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/tensor_view_planar_complex.h
+-rw-r--r--  2.0 unx    13017 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/tfloat32.h
+-rw-r--r--  2.0 unx     2581 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/trace.h
+-rw-r--r--  2.0 unx     7899 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/uint128.h
+-rw-r--r--  2.0 unx     2899 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/version.h
+-rw-r--r--  2.0 unx     4540 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/wmma_array.h
+-rw-r--r--  2.0 unx     5116 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/workspace.h
+-rw-r--r--  2.0 unx     3628 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/arch.h
+-rw-r--r--  2.0 unx    20151 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/barrier.h
+-rw-r--r--  2.0 unx     2691 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/cache_operation.h
+-rw-r--r--  2.0 unx    18305 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/memory.h
+-rw-r--r--  2.0 unx     8260 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/memory_sm75.h
+-rw-r--r--  2.0 unx    15163 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/memory_sm80.h
+-rw-r--r--  2.0 unx     9800 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/mma.h
+-rw-r--r--  2.0 unx    11096 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/mma_sm50.h
+-rw-r--r--  2.0 unx     7040 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/mma_sm60.h
+-rw-r--r--  2.0 unx     4193 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/mma_sm61.h
+-rw-r--r--  2.0 unx    16554 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/mma_sm70.h
+-rw-r--r--  2.0 unx    31990 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/mma_sm75.h
+-rw-r--r--  2.0 unx    57636 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/mma_sm80.h
+-rw-r--r--  2.0 unx    11290 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/mma_sm89.h
+-rw-r--r--  2.0 unx     8419 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/mma_sm90.h
+-rw-r--r--  2.0 unx    43978 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/mma_sparse_sm80.h
+-rw-r--r--  2.0 unx    12126 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/mma_sparse_sm89.h
+-rw-r--r--  2.0 unx     2621 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/reg_reconfig.h
+-rw-r--r--  2.0 unx     3998 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/simd.h
+-rw-r--r--  2.0 unx     3590 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/simd_sm60.h
+-rw-r--r--  2.0 unx     5102 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/simd_sm61.h
+-rw-r--r--  2.0 unx     8473 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/wmma.h
+-rw-r--r--  2.0 unx     5286 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/wmma_sm70.h
+-rw-r--r--  2.0 unx     7746 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/wmma_sm72.h
+-rw-r--r--  2.0 unx     7616 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/arch/wmma_sm75.h
+-rw-r--r--  2.0 unx    23132 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/conv2d_problem_size.h
+-rw-r--r--  2.0 unx    18158 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/conv3d_problem_size.h
+-rw-r--r--  2.0 unx    22931 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/convnd_problem_shape.hpp
+-rw-r--r--  2.0 unx     7293 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/convolution.h
+-rw-r--r--  2.0 unx     3636 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/dispatch_policy.hpp
+-rw-r--r--  2.0 unx     3783 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/collective/collective_builder.hpp
+-rw-r--r--  2.0 unx     2853 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/collective/collective_conv.hpp
+-rw-r--r--  2.0 unx    10716 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/collective/detail.hpp
+-rw-r--r--  2.0 unx    27380 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/collective/sm90_implicit_gemm_gmma_ss_warpspecialized.hpp
+-rw-r--r--  2.0 unx     4145 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/collective/builders/sm90_common.inl
+-rw-r--r--  2.0 unx    10983 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/collective/builders/sm90_gmma_builder.inl
+-rw-r--r--  2.0 unx    16194 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/device/conv_universal_adapter.hpp
+-rw-r--r--  2.0 unx     9743 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/device/direct_convolution.h
+-rw-r--r--  2.0 unx    13380 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/device/implicit_gemm_convolution.h
+-rw-r--r--  2.0 unx    10044 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h
+-rw-r--r--  2.0 unx     2888 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/conv_universal.hpp
+-rw-r--r--  2.0 unx     8632 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d.h
+-rw-r--r--  2.0 unx    53546 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_dgrad.h
+-rw-r--r--  2.0 unx    57134 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop.h
+-rw-r--r--  2.0 unx    11951 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h
+-rw-r--r--  2.0 unx     4609 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_absmax.h
+-rw-r--r--  2.0 unx     6978 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h
+-rw-r--r--  2.0 unx     4657 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h
+-rw-r--r--  2.0 unx    19662 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_group_fprop.h
+-rw-r--r--  2.0 unx    28745 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_wgrad.h
+-rw-r--r--  2.0 unx    10459 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h
+-rw-r--r--  2.0 unx    20919 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_dgrad.h
+-rw-r--r--  2.0 unx    27125 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop.h
+-rw-r--r--  2.0 unx    11978 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h
+-rw-r--r--  2.0 unx     6953 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop_with_broadcast.h
+-rw-r--r--  2.0 unx    26380 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_wgrad.h
+-rw-r--r--  2.0 unx    27986 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_deconv2d.h
+-rw-r--r--  2.0 unx     8912 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_deconv2d_with_broadcast.h
+-rw-r--r--  2.0 unx    15389 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_deconv3d.h
+-rw-r--r--  2.0 unx     8917 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_deconv3d_with_broadcast.h
+-rw-r--r--  2.0 unx    19289 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/default_depthwise_fprop.h
+-rw-r--r--  2.0 unx    18078 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/direct_convolution.h
+-rw-r--r--  2.0 unx    15458 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution.h
+-rw-r--r--  2.0 unx    15729 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h
+-rw-r--r--  2.0 unx    17257 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h
+-rw-r--r--  2.0 unx    16581 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_with_absmax.h
+-rw-r--r--  2.0 unx    16738 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h
+-rw-r--r--  2.0 unx    16865 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/kernel/sm90_implicit_gemm_tma_warpspecialized.hpp
+-rw-r--r--  2.0 unx     9689 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/thread/depthwise_mma.h
+-rw-r--r--  2.0 unx    15306 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h
+-rw-r--r--  2.0 unx    19735 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h
+-rw-r--r--  2.0 unx    18940 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--  2.0 unx    26136 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--  2.0 unx    10977 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h
+-rw-r--r--  2.0 unx    11529 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h
+-rw-r--r--  2.0 unx    11333 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h
+-rw-r--r--  2.0 unx    13688 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h
+-rw-r--r--  2.0 unx    11164 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h
+-rw-r--r--  2.0 unx     9314 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h
+-rw-r--r--  2.0 unx     9018 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h
+-rw-r--r--  2.0 unx    10689 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h
+-rw-r--r--  2.0 unx    30197 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_params.h
+-rw-r--r--  2.0 unx    11202 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_tile_iterator.h
+-rw-r--r--  2.0 unx    10349 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h
+-rw-r--r--  2.0 unx    11519 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h
+-rw-r--r--  2.0 unx     9043 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--  2.0 unx    10832 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--  2.0 unx     8450 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h
+-rw-r--r--  2.0 unx     9569 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h
+-rw-r--r--  2.0 unx    11020 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--  2.0 unx    15014 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--  2.0 unx     9634 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h
+-rw-r--r--  2.0 unx    15132 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h
+-rw-r--r--  2.0 unx     8307 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h
+-rw-r--r--  2.0 unx     9126 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h
+-rw-r--r--  2.0 unx    18249 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_params.h
+-rw-r--r--  2.0 unx     9971 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h
+-rw-r--r--  2.0 unx    12024 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h
+-rw-r--r--  2.0 unx     8821 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--  2.0 unx    10744 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--  2.0 unx     8871 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h
+-rw-r--r--  2.0 unx    10747 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h
+-rw-r--r--  2.0 unx     9899 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h
+-rw-r--r--  2.0 unx    20899 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h
+-rw-r--r--  2.0 unx     8921 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h
+-rw-r--r--  2.0 unx    12745 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h
+-rw-r--r--  2.0 unx     8097 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_mma_base.h
+-rw-r--r--  2.0 unx    36697 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h
+-rw-r--r--  2.0 unx    30106 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h
+-rw-r--r--  2.0 unx    19808 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_multistage.h
+-rw-r--r--  2.0 unx    12175 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h
+-rw-r--r--  2.0 unx    26320 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h
+-rw-r--r--  2.0 unx    16915 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h
+-rw-r--r--  2.0 unx    12476 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h
+-rw-r--r--  2.0 unx     8050 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/threadblock/threadblock_swizzle.h
+-rw-r--r--  2.0 unx    12392 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/warp/mma_depthwise_simt.h
+-rw-r--r--  2.0 unx    30655 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h
+-rw-r--r--  2.0 unx     8704 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/conv/warp/scale_bias_relu_transform.h
+-rw-r--r--  2.0 unx     3048 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/detail/collective.hpp
+-rw-r--r--  2.0 unx     3710 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/detail/dependent_false.hpp
+-rw-r--r--  2.0 unx     5745 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/detail/helper_macros.hpp
+-rw-r--r--  2.0 unx    12305 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/detail/layout.hpp
+-rw-r--r--  2.0 unx     3089 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/detail/mma.hpp
+-rw-r--r--  2.0 unx     6133 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/dispatch_policy.hpp
+-rw-r--r--  2.0 unx     4425 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/collective/collective_builder.hpp
+-rw-r--r--  2.0 unx     2957 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/collective/collective_epilogue.hpp
+-rw-r--r--  2.0 unx     9243 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/collective/default_epilogue.hpp
+-rw-r--r--  2.0 unx    10494 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/collective/default_epilogue_array.hpp
+-rw-r--r--  2.0 unx     9625 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/collective/detail.hpp
+-rw-r--r--  2.0 unx    11386 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/collective/epilogue_tensor_broadcast.hpp
+-rw-r--r--  2.0 unx    14012 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/collective/sm70_epilogue_vectorized.hpp
+-rw-r--r--  2.0 unx    34828 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/collective/sm90_epilogue_tma_warpspecialized.hpp
+-rw-r--r--  2.0 unx     5427 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/collective/sm90_epilogue_tma_warpspecialized_bias_elementwise.hpp
+-rw-r--r--  2.0 unx    29518 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/collective/builders/sm90_builder.inl
+-rw-r--r--  2.0 unx     4128 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/fusion/callbacks.hpp
+-rw-r--r--  2.0 unx    12105 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/fusion/operations.hpp
+-rw-r--r--  2.0 unx    55072 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/fusion/sm90_callbacks_tma_warpspecialized.hpp
+-rw-r--r--  2.0 unx    28888 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_compute_tma_warpspecialized.hpp
+-rw-r--r--  2.0 unx    32867 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_load_tma_warpspecialized.hpp
+-rw-r--r--  2.0 unx    57933 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_store_tma_warpspecialized.hpp
+-rw-r--r--  2.0 unx    39461 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_tma_warpspecialized.hpp
+-rw-r--r--  2.0 unx    17909 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/activation.h
+-rw-r--r--  2.0 unx     4691 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/conversion_op.h
+-rw-r--r--  2.0 unx     2281 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/detail.hpp
+-rw-r--r--  2.0 unx    18989 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination.h
+-rw-r--r--  2.0 unx    13180 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h
+-rw-r--r--  2.0 unx    18161 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_bias_relu.h
+-rw-r--r--  2.0 unx    23370 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_clamp.h
+-rw-r--r--  2.0 unx     9066 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_dgelu.h
+-rw-r--r--  2.0 unx    15195 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_drelu.h
+-rw-r--r--  2.0 unx     3669 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_gelu.h
+-rw-r--r--  2.0 unx    10056 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_generic.h
+-rw-r--r--  2.0 unx    13185 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_generic_with_scaling.h
+-rw-r--r--  2.0 unx     3693 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_hardswish.h
+-rw-r--r--  2.0 unx     8399 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h
+-rw-r--r--  2.0 unx     3046 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_params.h
+-rw-r--r--  2.0 unx     9278 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_planar_complex.h
+-rw-r--r--  2.0 unx    20596 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_relu.h
+-rw-r--r--  2.0 unx    19458 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_relu0.h
+-rw-r--r--  2.0 unx    11995 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_residual_block.h
+-rw-r--r--  2.0 unx     3688 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_sigmoid.h
+-rw-r--r--  2.0 unx     3669 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_silu.h
+-rw-r--r--  2.0 unx     9786 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_tensor_broadcast.hpp
+-rw-r--r--  2.0 unx     8662 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h
+-rw-r--r--  2.0 unx     3416 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/reduction_op.h
+-rw-r--r--  2.0 unx     3048 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/thread/scale_type.h
+-rw-r--r--  2.0 unx     9142 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h
+-rw-r--r--  2.0 unx     9441 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h
+-rw-r--r--  2.0 unx     3234 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h
+-rw-r--r--  2.0 unx     7209 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h
+-rw-r--r--  2.0 unx    14255 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_simt.h
+-rw-r--r--  2.0 unx    29301 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h
+-rw-r--r--  2.0 unx     7134 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h
+-rw-r--r--  2.0 unx    10846 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h
+-rw-r--r--  2.0 unx     4264 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_with_absmax.h
+-rw-r--r--  2.0 unx    10301 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h
+-rw-r--r--  2.0 unx     5763 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h
+-rw-r--r--  2.0 unx     5947 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h
+-rw-r--r--  2.0 unx     4409 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_thread_map_simt.h
+-rw-r--r--  2.0 unx     7398 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h
+-rw-r--r--  2.0 unx     7303 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h
+-rw-r--r--  2.0 unx     4098 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h
+-rw-r--r--  2.0 unx     4678 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h
+-rw-r--r--  2.0 unx    19249 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue.h
+-rw-r--r--  2.0 unx     8279 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_base.h
+-rw-r--r--  2.0 unx     7455 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h
+-rw-r--r--  2.0 unx    13424 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_depthwise.h
+-rw-r--r--  2.0 unx    13933 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_direct_store.h
+-rw-r--r--  2.0 unx     7401 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h
+-rw-r--r--  2.0 unx    14610 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h
+-rw-r--r--  2.0 unx     9073 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h
+-rw-r--r--  2.0 unx    15321 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_streamk_with_broadcast.h
+-rw-r--r--  2.0 unx    16804 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h
+-rw-r--r--  2.0 unx    34018 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_absmax.h
+-rw-r--r--  2.0 unx    58727 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h
+-rw-r--r--  2.0 unx    29199 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h
+-rw-r--r--  2.0 unx    13454 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h
+-rw-r--r--  2.0 unx    17169 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_visitor_callbacks.h
+-rw-r--r--  2.0 unx     7308 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_workspace.h
+-rw-r--r--  2.0 unx    14359 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/interleaved_epilogue.h
+-rw-r--r--  2.0 unx     6881 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/output_iterator_parameter.h
+-rw-r--r--  2.0 unx    19842 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/output_tile_thread_map.h
+-rw-r--r--  2.0 unx    41699 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h
+-rw-r--r--  2.0 unx    18821 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h
+-rw-r--r--  2.0 unx     5636 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h
+-rw-r--r--  2.0 unx    21249 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h
+-rw-r--r--  2.0 unx    17563 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_conv.h
+-rw-r--r--  2.0 unx    13873 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h
+-rw-r--r--  2.0 unx    14985 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h
+-rw-r--r--  2.0 unx     9146 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h
+-rw-r--r--  2.0 unx    15534 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h
+-rw-r--r--  2.0 unx     7487 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator.h
+-rw-r--r--  2.0 unx    18100 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h
+-rw-r--r--  2.0 unx     7396 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_linear.h
+-rw-r--r--  2.0 unx    14524 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_2x.hpp
+-rw-r--r--  2.0 unx     4387 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_compute.hpp
+-rw-r--r--  2.0 unx    17753 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_load.hpp
+-rw-r--r--  2.0 unx    25534 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_store.hpp
+-rw-r--r--  2.0 unx     2171 b- defN 24-Apr-12 11:21 cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitors.hpp
+-rw-r--r--  2.0 unx     7055 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h
+-rw-r--r--  2.0 unx     7736 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h
+-rw-r--r--  2.0 unx     5880 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_simt.h
+-rw-r--r--  2.0 unx     9883 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h
+-rw-r--r--  2.0 unx     8924 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h
+-rw-r--r--  2.0 unx     6045 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h
+-rw-r--r--  2.0 unx     4864 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/simt_policy.h
+-rw-r--r--  2.0 unx     5979 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/tensor_op_policy.h
+-rw-r--r--  2.0 unx    25658 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_simt.h
+-rw-r--r--  2.0 unx    20290 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h
+-rw-r--r--  2.0 unx    33300 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h
+-rw-r--r--  2.0 unx    14250 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h
+-rw-r--r--  2.0 unx     7704 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h
+-rw-r--r--  2.0 unx     7485 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/volta_tensor_op_policy.h
+-rw-r--r--  2.0 unx     3916 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h
+-rw-r--r--  2.0 unx    10458 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/dispatch_policy.hpp
+-rw-r--r--  2.0 unx     4630 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/gemm.h
+-rw-r--r--  2.0 unx     3174 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/gemm_enumerated_types.h
+-rw-r--r--  2.0 unx     4265 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/group_array_problem_shape.hpp
+-rw-r--r--  2.0 unx     3771 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/collective_builder.hpp
+-rw-r--r--  2.0 unx     3668 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/collective_mma.hpp
+-rw-r--r--  2.0 unx     4768 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/fp8_accumulation.hpp
+-rw-r--r--  2.0 unx    22247 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/sm70_mma_twostage.hpp
+-rw-r--r--  2.0 unx    27955 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/sm80_mma_multistage.hpp
+-rw-r--r--  2.0 unx    33255 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_array_tma_gmma_ss_warpspecialized.hpp
+-rw-r--r--  2.0 unx    28408 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_multistage_gmma_rs_warpspecialized.hpp
+-rw-r--r--  2.0 unx    19434 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_multistage_gmma_ss_warpspecialized.hpp
+-rw-r--r--  2.0 unx    33465 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized.hpp
+-rw-r--r--  2.0 unx    63323 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized_mixed_input.hpp
+-rw-r--r--  2.0 unx    22544 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss.hpp
+-rw-r--r--  2.0 unx    24000 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp
+-rw-r--r--  2.0 unx    24138 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8.hpp
+-rw-r--r--  2.0 unx    15520 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/builders/sm90_common.inl
+-rw-r--r--  2.0 unx    43793 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/collective/builders/sm90_gmma_builder.inl
+-rw-r--r--  2.0 unx    16881 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/base_grouped.h
+-rw-r--r--  2.0 unx    27521 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/default_gemm_configuration.h
+-rw-r--r--  2.0 unx    27618 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/ell_gemm.h
+-rw-r--r--  2.0 unx    25202 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm.h
+-rw-r--r--  2.0 unx    22367 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_array.h
+-rw-r--r--  2.0 unx    22375 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_batched.h
+-rw-r--r--  2.0 unx    22725 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_complex.h
+-rw-r--r--  2.0 unx     2591 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_grouped.h
+-rw-r--r--  2.0 unx    13736 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h
+-rw-r--r--  2.0 unx    17329 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_sparse.h
+-rw-r--r--  2.0 unx    12258 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_sparse_with_absmax.h
+-rw-r--r--  2.0 unx    11362 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_sparse_with_visitor.h
+-rw-r--r--  2.0 unx    20436 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_splitk_parallel.h
+-rw-r--r--  2.0 unx    15630 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_universal.h
+-rw-r--r--  2.0 unx    24236 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_universal_adapter.h
+-rw-r--r--  2.0 unx    15624 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_universal_base.h
+-rw-r--r--  2.0 unx    14027 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_universal_streamk_with_broadcast.h
+-rw-r--r--  2.0 unx    13785 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_universal_with_absmax.h
+-rw-r--r--  2.0 unx    13968 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_universal_with_broadcast.h
+-rw-r--r--  2.0 unx    14853 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemm_with_k_reduction.h
+-rw-r--r--  2.0 unx     5961 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/gemv.h
+-rw-r--r--  2.0 unx    18127 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/rank_2k.h
+-rw-r--r--  2.0 unx     2747 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/rank_2k_grouped.h
+-rw-r--r--  2.0 unx    16719 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/rank_k.h
+-rwxr-xr-x  2.0 unx    21050 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/symm.h
+-rw-r--r--  2.0 unx    26464 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/device/trmm.h
+-rw-r--r--  2.0 unx    29360 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_ell_gemm.h
+-rw-r--r--  2.0 unx    42401 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm.h
+-rw-r--r--  2.0 unx    16130 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_complex.h
+-rw-r--r--  2.0 unx    12385 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_grouped.h
+-rw-r--r--  2.0 unx     6592 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h
+-rw-r--r--  2.0 unx     5848 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h
+-rw-r--r--  2.0 unx    11104 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h
+-rw-r--r--  2.0 unx    10526 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_sparse.h
+-rw-r--r--  2.0 unx     6102 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_sparse_with_absmax.h
+-rw-r--r--  2.0 unx     8175 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_sparse_with_visitor.h
+-rw-r--r--  2.0 unx     4932 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h
+-rw-r--r--  2.0 unx     5446 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_streamk_with_broadcast.h
+-rw-r--r--  2.0 unx    12332 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_universal.h
+-rw-r--r--  2.0 unx     5697 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_universal_with_visitor.h
+-rw-r--r--  2.0 unx     5115 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_absmax.h
+-rw-r--r--  2.0 unx     8123 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h
+-rw-r--r--  2.0 unx     6457 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h
+-rw-r--r--  2.0 unx     8084 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_reduction.h
+-rwxr-xr-x  2.0 unx     5349 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_gemv.h
+-rw-r--r--  2.0 unx    11560 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k.h
+-rw-r--r--  2.0 unx    20509 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k_complex.h
+-rw-r--r--  2.0 unx    12480 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k_grouped.h
+-rw-r--r--  2.0 unx    10630 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k_universal.h
+-rw-r--r--  2.0 unx     9872 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_k.h
+-rw-r--r--  2.0 unx    16990 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_k_complex.h
+-rw-r--r--  2.0 unx     9454 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_k_universal.h
+-rwxr-xr-x  2.0 unx    13375 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_symm.h
+-rwxr-xr-x  2.0 unx    21830 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_symm_complex.h
+-rwxr-xr-x  2.0 unx    10325 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_symm_universal.h
+-rw-r--r--  2.0 unx    10873 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_trmm.h
+-rw-r--r--  2.0 unx    10730 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_trmm_complex.h
+-rw-r--r--  2.0 unx    10860 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/default_trmm_universal.h
+-rw-r--r--  2.0 unx    28837 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/ell_gemm.h
+-rw-r--r--  2.0 unx    13362 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm.h
+-rw-r--r--  2.0 unx     8698 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_array.h
+-rw-r--r--  2.0 unx     8746 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_batched.h
+-rw-r--r--  2.0 unx    14394 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_grouped.h
+-rw-r--r--  2.0 unx     4690 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h
+-rw-r--r--  2.0 unx    15268 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h
+-rw-r--r--  2.0 unx    27550 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h
+-rwxr-xr-x  2.0 unx     5979 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_params.h
+-rw-r--r--  2.0 unx     5150 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_pipelined.h
+-rw-r--r--  2.0 unx    23348 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_planar_complex.h
+-rw-r--r--  2.0 unx    18886 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_planar_complex_array.h
+-rw-r--r--  2.0 unx     8142 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_splitk_parallel.h
+-rw-r--r--  2.0 unx    80158 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_streamk_with_fused_epilogue.h
+-rw-r--r--  2.0 unx     4291 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_transpose_operands.h
+-rw-r--r--  2.0 unx    23549 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal.h
+-rw-r--r--  2.0 unx     4374 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal.hpp
+-rw-r--r--  2.0 unx    39338 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_streamk.h
+-rw-r--r--  2.0 unx    10423 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_with_visitor.h
+-rw-r--r--  2.0 unx    28827 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_with_visitor_streamk.h
+-rw-r--r--  2.0 unx    24030 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_with_absmax.h
+-rw-r--r--  2.0 unx    47839 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h
+-rw-r--r--  2.0 unx    23866 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemm_with_k_reduction.h
+-rw-r--r--  2.0 unx    18393 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemv.h
+-rwxr-xr-x  2.0 unx     8954 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/gemv_batched_strided.h
+-rw-r--r--  2.0 unx    16765 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/grouped_problem_visitor.h
+-rw-r--r--  2.0 unx     3934 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/params_sparse_base.h
+-rw-r--r--  2.0 unx     8456 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/params_universal_base.h
+-rw-r--r--  2.0 unx    23027 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_grouped.h
+-rw-r--r--  2.0 unx    16100 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h
+-rw-r--r--  2.0 unx     4334 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h
+-rw-r--r--  2.0 unx    24214 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_universal.h
+-rw-r--r--  2.0 unx    17597 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/rank_k_universal.h
+-rw-r--r--  2.0 unx    11132 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sm70_gemm.hpp
+-rw-r--r--  2.0 unx    35170 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_array_tma_warpspecialized_cooperative.hpp
+-rw-r--r--  2.0 unx    13178 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma.hpp
+-rw-r--r--  2.0 unx    18554 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized.hpp
+-rw-r--r--  2.0 unx    29017 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_cooperative.hpp
+-rw-r--r--  2.0 unx    28200 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_pingpong.hpp
+-rw-r--r--  2.0 unx    18200 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized.hpp
+-rw-r--r--  2.0 unx    23028 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized_cooperative.hpp
+-rw-r--r--  2.0 unx    23103 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized_pingpong.hpp
+-rw-r--r--  2.0 unx     5511 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler.hpp
+-rw-r--r--  2.0 unx    18488 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler_group.hpp
+-rw-r--r--  2.0 unx    39150 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler_stream_k.hpp
+-rw-r--r--  2.0 unx    13183 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sparse_gemm.h
+-rw-r--r--  2.0 unx    16350 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sparse_gemm_with_absmax.h
+-rw-r--r--  2.0 unx     8144 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/sparse_gemm_with_visitor.h
+-rw-r--r--  2.0 unx    16041 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/static_tile_scheduler.hpp
+-rwxr-xr-x  2.0 unx    23516 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/symm_universal.h
+-rw-r--r--  2.0 unx     4414 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/tile_scheduler.hpp
+-rw-r--r--  2.0 unx    58093 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/tile_scheduler_params.h
+-rw-r--r--  2.0 unx    19233 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/kernel/trmm_universal.h
+-rw-r--r--  2.0 unx     3567 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/thread/mma.h
+-rw-r--r--  2.0 unx    15399 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/thread/mma_sm50.h
+-rw-r--r--  2.0 unx    29390 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/thread/mma_sm60.h
+-rw-r--r--  2.0 unx     8142 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/thread/mma_sm61.h
+-rw-r--r--  2.0 unx    31930 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_ell_mma.h
+-rwxr-xr-x  2.0 unx     6979 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_gemv_core.h
+-rw-r--r--  2.0 unx    35582 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma.h
+-rw-r--r--  2.0 unx     5123 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core.h
+-rw-r--r--  2.0 unx    57426 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_simt.h
+-rw-r--r--  2.0 unx    19257 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sm70.h
+-rw-r--r--  2.0 unx    44176 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sm75.h
+-rw-r--r--  2.0 unx   104675 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sm80.h
+-rw-r--r--  2.0 unx    32106 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h
+-rw-r--r--  2.0 unx    12650 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h
+-rw-r--r--  2.0 unx     7387 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h
+-rw-r--r--  2.0 unx    20975 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_wmma.h
+-rw-r--r--  2.0 unx     7998 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h
+-rw-r--r--  2.0 unx     5110 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h
+-rw-r--r--  2.0 unx     4627 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h
+-rw-r--r--  2.0 unx     7113 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h
+-rw-r--r--  2.0 unx     6323 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_with_reduction.h
+-rw-r--r--  2.0 unx     7121 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h
+-rw-r--r--  2.0 unx     4959 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h
+-rw-r--r--  2.0 unx    65005 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h
+-rw-r--r--  2.0 unx    25495 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h
+-rw-r--r--  2.0 unx     8509 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_sparse_mma.h
+-rw-r--r--  2.0 unx    19515 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/default_trmm.h
+-rw-r--r--  2.0 unx    24233 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/ell_mma_multistage.h
+-rw-r--r--  2.0 unx    13837 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/ell_mma_pipelined.h
+-rwxr-xr-x  2.0 unx     4726 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/gemv.h
+-rw-r--r--  2.0 unx     3652 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/index_remat.h
+-rw-r--r--  2.0 unx     7823 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/mma_base.h
+-rw-r--r--  2.0 unx    27600 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/mma_blas3_multistage.h
+-rw-r--r--  2.0 unx    32816 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h
+-rw-r--r--  2.0 unx    27786 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/mma_multistage.h
+-rw-r--r--  2.0 unx    15995 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/mma_pipelined.h
+-rw-r--r--  2.0 unx     6901 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/mma_planar_complex_base.h
+-rw-r--r--  2.0 unx    22839 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h
+-rw-r--r--  2.0 unx    14747 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h
+-rw-r--r--  2.0 unx     9864 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/mma_singlestage.h
+-rw-r--r--  2.0 unx    27246 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h
+-rw-r--r--  2.0 unx     9210 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/mma_sparse_base.h
+-rw-r--r--  2.0 unx    25557 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/mma_sparse_multistage.h
+-rw-r--r--  2.0 unx    20395 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h
+-rw-r--r--  2.0 unx    15041 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/threadblock_swizzle.h
+-rw-r--r--  2.0 unx    26219 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h
+-rw-r--r--  2.0 unx    20553 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h
+-rw-r--r--  2.0 unx     6684 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h
+-rw-r--r--  2.0 unx     5178 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/default_mma_tensor_op.h
+-rw-r--r--  2.0 unx    12142 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h
+-rw-r--r--  2.0 unx     4053 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h
+-rw-r--r--  2.0 unx     4685 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h
+-rw-r--r--  2.0 unx     5691 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h
+-rw-r--r--  2.0 unx     2619 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma.h
+-rw-r--r--  2.0 unx    37767 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_complex_tensor_op.h
+-rw-r--r--  2.0 unx    23132 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h
+-rw-r--r--  2.0 unx    78519 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h
+-rw-r--r--  2.0 unx    21178 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h
+-rw-r--r--  2.0 unx    14589 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h
+-rw-r--r--  2.0 unx    20271 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h
+-rw-r--r--  2.0 unx     6144 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_planar_complex.h
+-rw-r--r--  2.0 unx     8419 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_simt.h
+-rw-r--r--  2.0 unx     3079 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_simt_policy.h
+-rw-r--r--  2.0 unx    59793 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_simt_tile_iterator.h
+-rw-r--r--  2.0 unx    13497 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_sparse_tensor_op.h
+-rw-r--r--  2.0 unx    13956 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op.h
+-rw-r--r--  2.0 unx    15721 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h
+-rw-r--r--  2.0 unx    20472 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h
+-rw-r--r--  2.0 unx     2939 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_policy.h
+-rw-r--r--  2.0 unx     8966 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_sm70.h
+-rw-r--r--  2.0 unx    11017 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h
+-rw-r--r--  2.0 unx   162811 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h
+-rw-r--r--  2.0 unx    99553 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h
+-rw-r--r--  2.0 unx    75040 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h
+-rw-r--r--  2.0 unx    13151 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h
+-rw-r--r--  2.0 unx    27101 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h
+-rw-r--r--  2.0 unx     7241 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_wmma.h
+-rw-r--r--  2.0 unx    17303 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h
+-rw-r--r--  2.0 unx    19101 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/scale_bias_tile_iterator.h
+-rw-r--r--  2.0 unx     4610 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/softmax_scale_bias_transform.h
+-rw-r--r--  2.0 unx     8728 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/gemm/warp/tile_iterator_planar_complex.h
+-rw-r--r--  2.0 unx     3020 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/layout/layout.h
+-rw-r--r--  2.0 unx    34719 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/layout/matrix.h
+-rw-r--r--  2.0 unx    24906 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/layout/permute.h
+-rw-r--r--  2.0 unx     4697 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/layout/pitch_linear.h
+-rw-r--r--  2.0 unx    19044 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/layout/tensor.h
+-rw-r--r--  2.0 unx    29599 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/layout/tensor_op_multiplicand_sm70.h
+-rw-r--r--  2.0 unx    33494 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/layout/tensor_op_multiplicand_sm75.h
+-rw-r--r--  2.0 unx    29336 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/layout/tensor_op_multiplicand_sm80.h
+-rw-r--r--  2.0 unx     3354 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/layout/vector.h
+-rw-r--r--  2.0 unx     2091 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/pipeline/pipeline.hpp
+-rw-r--r--  2.0 unx    37271 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/pipeline/sm90_pipeline.hpp
+-rw-r--r--  2.0 unx    27863 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/platform/platform.h
+-rw-r--r--  2.0 unx     2936 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/reduction/threadblock_swizzle.h
+-rw-r--r--  2.0 unx     6749 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/reduction/device/reduce_split_k.h
+-rw-r--r--  2.0 unx     8152 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/reduction/device/tensor_reduce.h
+-rw-r--r--  2.0 unx    11579 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h
+-rw-r--r--  2.0 unx    11448 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/reduction/device/tensor_reduce_affine_strided.h
+-rw-r--r--  2.0 unx     8815 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/reduction/kernel/reduce_softmax_final.h
+-rw-r--r--  2.0 unx     7897 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/reduction/kernel/reduce_split_k.h
+-rw-r--r--  2.0 unx    20690 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h
+-rw-r--r--  2.0 unx    21672 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h
+-rw-r--r--  2.0 unx     7208 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/reduction/thread/reduce.h
+-rw-r--r--  2.0 unx     6790 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/reduction/thread/reduction_operators.h
+-rw-r--r--  2.0 unx     5893 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/thread/matrix.h
+-rw-r--r--  2.0 unx    33349 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/pitch_linear_thread_map.h
+-rw-r--r--  2.0 unx    33948 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/collective/sm90_wgmma_transpose.hpp
+-rw-r--r--  2.0 unx     3835 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/thread/transpose.h
+-rw-r--r--  2.0 unx     4309 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/thread/unary_op.h
+-rw-r--r--  2.0 unx     6181 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/ell_iterator.h
+-rw-r--r--  2.0 unx    44443 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h
+-rw-r--r--  2.0 unx    44309 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h
+-rw-r--r--  2.0 unx    12890 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h
+-rw-r--r--  2.0 unx    11097 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h
+-rw-r--r--  2.0 unx    72537 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h
+-rw-r--r--  2.0 unx    28232 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h
+-rwxr-xr-x  2.0 unx    10395 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h
+-rw-r--r--  2.0 unx    31412 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h
+-rw-r--r--  2.0 unx    62949 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_iterator.h
+-rw-r--r--  2.0 unx    27175 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h
+-rw-r--r--  2.0 unx    28064 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h
+-rw-r--r--  2.0 unx    13088 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h
+-rw-r--r--  2.0 unx     8232 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h
+-rw-r--r--  2.0 unx     2638 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator.h
+-rw-r--r--  2.0 unx    13283 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h
+-rw-r--r--  2.0 unx    18623 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h
+-rw-r--r--  2.0 unx    27938 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h
+-rw-r--r--  2.0 unx    47789 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h
+-rw-r--r--  2.0 unx     2616 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator.h
+-rw-r--r--  2.0 unx    16508 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h
+-rw-r--r--  2.0 unx    15486 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h
+-rw-r--r--  2.0 unx    35956 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h
+-rw-r--r--  2.0 unx    43663 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h
+-rw-r--r--  2.0 unx     5226 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/threadblock/vector_iterator.h
+-rw-r--r--  2.0 unx     8828 b- defN 24-Apr-12 11:22 cutlass_library/source/include/cutlass/transform/warp/vector_fragment_iterator.h
+-rw-r--r--  2.0 unx     2092 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/CMakeLists.txt
+-rw-r--r--  2.0 unx    10693 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/CMakeLists.txt
+-rw-r--r--  2.0 unx     4414 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/include/cutlass/library/arch_mappings.h
+-rw-r--r--  2.0 unx    18334 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/include/cutlass/library/descriptions.h
+-rw-r--r--  2.0 unx    16150 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/include/cutlass/library/handle.h
+-rw-r--r--  2.0 unx    21225 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/include/cutlass/library/library.h
+-rw-r--r--  2.0 unx     4251 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/include/cutlass/library/manifest.h
+-rw-r--r--  2.0 unx    19073 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/include/cutlass/library/operation_table.h
+-rw-r--r--  2.0 unx     2724 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/include/cutlass/library/singleton.h
+-rw-r--r--  2.0 unx     5908 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/include/cutlass/library/types.h
+-rw-r--r--  2.0 unx     8140 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/include/cutlass/library/util.h
+-rw-r--r--  2.0 unx    22377 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/conv2d_operation.h
+-rw-r--r--  2.0 unx    13850 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/conv3d_operation.h
+-rw-r--r--  2.0 unx    35804 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/conv_operation_3x.hpp
+-rw-r--r--  2.0 unx    42608 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/gemm_operation.h
+-rw-r--r--  2.0 unx    13834 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/gemm_operation_3x.hpp
+-rw-r--r--  2.0 unx    36802 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/handle.cu
+-rw-r--r--  2.0 unx    13270 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/library_internal.h
+-rw-r--r--  2.0 unx     3634 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/manifest.cpp
+-rw-r--r--  2.0 unx     5551 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/operation_table.cu
+-rw-r--r--  2.0 unx    12873 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/rank_2k_operation.h
+-rw-r--r--  2.0 unx    11367 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/rank_k_operation.h
+-rw-r--r--  2.0 unx     2669 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/singleton.cu
+-rw-r--r--  2.0 unx    13134 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/symm_operation.h
+-rw-r--r--  2.0 unx    11698 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/trmm_operation.h
+-rw-r--r--  2.0 unx    46683 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/util.cu
+-rw-r--r--  2.0 unx     3482 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reduction/init_reduction_operations.cu
+-rw-r--r--  2.0 unx     8435 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reduction/reduction_device.cu
+-rw-r--r--  2.0 unx    10269 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reduction/reduction_operation.h
+-rw-r--r--  2.0 unx     6746 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/conv2d.cu
+-rw-r--r--  2.0 unx     6286 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/conv3d.cu
+-rw-r--r--  2.0 unx    17347 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/conv_reference_operation.h
+-rw-r--r--  2.0 unx     5473 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_e4m3a_e4m3out.cu
+-rw-r--r--  2.0 unx     5070 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_e4m3a_e5m2out.cu
+-rw-r--r--  2.0 unx     5070 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_e5m2a_e4m3out.cu
+-rw-r--r--  2.0 unx     5070 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_e5m2a_e5m2out.cu
+-rw-r--r--  2.0 unx     3633 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_fp32out.cu
+-rw-r--r--  2.0 unx     4262 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_fp8in_bf16out.cu
+-rw-r--r--  2.0 unx     4260 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_fp8in_fp16out.cu
+-rw-r--r--  2.0 unx     4260 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_fp8in_fp32out.cu
+-rw-r--r--  2.0 unx     4056 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_fp_mixed_input.cu
+-rw-r--r--  2.0 unx     3140 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_fp_other.cu
+-rw-r--r--  2.0 unx     3729 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_int4.cu
+-rw-r--r--  2.0 unx     3683 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_int8_canonical.cu
+-rw-r--r--  2.0 unx     3721 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_int8_interleaved_32.cu
+-rw-r--r--  2.0 unx     3744 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_int8_interleaved_64.cu
+-rw-r--r--  2.0 unx    16453 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/gemm_reference_operation.h
+-rw-r--r--  2.0 unx     4810 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/library/src/reference/initialize_reference_operations.cu
+-rw-r--r--  2.0 unx     5548 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/CMakeLists.txt
+-rw-r--r--  2.0 unx    18227 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/conv2d_operation_profiler.h
+-rw-r--r--  2.0 unx    16101 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/conv3d_operation_profiler.h
+-rw-r--r--  2.0 unx    10623 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/cublas_helpers.h
+-rw-r--r--  2.0 unx    20435 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/cudnn_helpers.h
+-rw-r--r--  2.0 unx     3233 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/cutlass_profiler.h
+-rw-r--r--  2.0 unx     2454 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/debug.h
+-rw-r--r--  2.0 unx     7576 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/device_allocation.h
+-rw-r--r--  2.0 unx     4290 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/device_context.h
+-rw-r--r--  2.0 unx     6428 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/enumerated_types.h
+-rw-r--r--  2.0 unx     8590 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/gemm_operation_profiler.h
+-rw-r--r--  2.0 unx     2725 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/gpu_timer.h
+-rw-r--r--  2.0 unx     7924 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/operation_profiler.h
+-rw-r--r--  2.0 unx     9273 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/options.h
+-rw-r--r--  2.0 unx     4337 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/performance_report.h
+-rw-r--r--  2.0 unx     3941 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/performance_result.h
+-rw-r--r--  2.0 unx    28189 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/problem_space.h
+-rw-r--r--  2.0 unx     6891 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/rank_2k_operation_profiler.h
+-rw-r--r--  2.0 unx     6830 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/rank_k_operation_profiler.h
+-rw-r--r--  2.0 unx     5452 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/reduction_operation_profiler.h
+-rw-r--r--  2.0 unx     6471 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/sparse_gemm_operation_profiler.h
+-rw-r--r--  2.0 unx     6933 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/symm_operation_profiler.h
+-rw-r--r--  2.0 unx     6599 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/include/cutlass/profiler/trmm_operation_profiler.h
+-rw-r--r--  2.0 unx    54272 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/conv2d_operation_profiler.cu
+-rw-r--r--  2.0 unx    48776 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/conv3d_operation_profiler.cu
+-rw-r--r--  2.0 unx    37138 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/cublas_helpers.cu
+-rw-r--r--  2.0 unx    17066 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/cudnn_helpers.cpp
+-rw-r--r--  2.0 unx     7351 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/cutlass_profiler.cu
+-rw-r--r--  2.0 unx    78653 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/device_allocation.cu
+-rw-r--r--  2.0 unx     8359 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/device_context.cu
+-rw-r--r--  2.0 unx     8313 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/enumerated_types.cpp
+-rw-r--r--  2.0 unx    44294 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/gemm_operation_profiler.cu
+-rw-r--r--  2.0 unx     3892 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/gpu_timer.cpp
+-rw-r--r--  2.0 unx     2374 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/main.cpp
+-rw-r--r--  2.0 unx    26023 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/operation_profiler.cu
+-rw-r--r--  2.0 unx    28314 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/options.cu
+-rw-r--r--  2.0 unx    14227 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/performance_report.cpp
+-rw-r--r--  2.0 unx     2528 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/performance_result.cu
+-rw-r--r--  2.0 unx    38798 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/problem_space.cpp
+-rw-r--r--  2.0 unx    25183 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/rank_2k_operation_profiler.cu
+-rw-r--r--  2.0 unx    24378 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/rank_k_operation_profiler.cu
+-rw-r--r--  2.0 unx    20915 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/sparse_gemm_operation_profiler.cu
+-rw-r--r--  2.0 unx    26757 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/symm_operation_profiler.cu
+-rw-r--r--  2.0 unx    24567 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/profiler/src/trmm_operation_profiler.cu
+-rw-r--r--  2.0 unx     2297 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/CMakeLists.txt
+-rw-r--r--  2.0 unx     2410 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/GPU_Clock.hpp
+-rw-r--r--  2.0 unx     9780 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/command_line.h
+-rw-r--r--  2.0 unx    19875 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/cublas_wrappers.hpp
+-rw-r--r--  2.0 unx     5104 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/debug.h
+-rw-r--r--  2.0 unx     5958 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/device_dump.h
+-rw-r--r--  2.0 unx    17696 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/device_groupnorm.h
+-rw-r--r--  2.0 unx    20881 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/device_layernorm.h
+-rw-r--r--  2.0 unx    10561 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/device_memory.h
+-rw-r--r--  2.0 unx     5219 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/device_nchw_to_nhwc.h
+-rw-r--r--  2.0 unx    11075 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/device_nhwc_padding.h
+-rw-r--r--  2.0 unx    18653 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/device_nhwc_pooling.h
+-rw-r--r--  2.0 unx     5214 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/device_nhwc_to_nchw.h
+-rw-r--r--  2.0 unx     7237 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/device_rmsnorm.h
+-rw-r--r--  2.0 unx     4007 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/device_utils.h
+-rw-r--r--  2.0 unx     4846 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/distribution.h
+-rw-r--r--  2.0 unx     2674 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/exceptions.h
+-rw-r--r--  2.0 unx    13740 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/gett_commandline.hpp
+-rw-r--r--  2.0 unx     3946 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/helper_cuda.hpp
+-rw-r--r--  2.0 unx     4821 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/host_reorder.h
+-rw-r--r--  2.0 unx    18542 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/host_tensor.h
+-rw-r--r--  2.0 unx    20354 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/host_tensor_planar_complex.h
+-rw-r--r--  2.0 unx     5890 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/host_uncompress.h
+-rw-r--r--  2.0 unx     1962 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/index_sequence.h
+-rw-r--r--  2.0 unx    18060 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/packed_stride.hpp
+-rw-r--r--  2.0 unx    12399 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/print_error.hpp
+-rw-r--r--  2.0 unx     8341 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/tensor_view_io.h
+-rw-r--r--  2.0 unx     8809 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/type_traits.h
+-rw-r--r--  2.0 unx     4606 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/detail/inner_product.h
+-rw-r--r--  2.0 unx     3521 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h
+-rw-r--r--  2.0 unx    48350 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/convolution.h
+-rw-r--r--  2.0 unx    14296 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/gemm.h
+-rw-r--r--  2.0 unx    10652 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/gemm_complex.h
+-rw-r--r--  2.0 unx     9652 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h
+-rw-r--r--  2.0 unx     5444 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/gett.hpp
+-rw-r--r--  2.0 unx    11615 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h
+-rw-r--r--  2.0 unx     7278 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_compare.h
+-rw-r--r--  2.0 unx    49333 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_fill.h
+-rw-r--r--  2.0 unx     5454 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_foreach.h
+-rw-r--r--  2.0 unx    15983 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_reduce.h
+-rw-r--r--  2.0 unx     4589 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_relu.h
+-rw-r--r--  2.0 unx     5381 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/kernel/gemm.h
+-rw-r--r--  2.0 unx     6198 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h
+-rw-r--r--  2.0 unx     5127 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h
+-rw-r--r--  2.0 unx     5872 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/device/thread/gemm.h
+-rw-r--r--  2.0 unx    26866 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/conv.hpp
+-rw-r--r--  2.0 unx    29064 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/convolution.h
+-rw-r--r--  2.0 unx     2766 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/error_metrics.h
+-rw-r--r--  2.0 unx    20938 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/gemm.h
+-rw-r--r--  2.0 unx     7161 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/gemm_complex.h
+-rw-r--r--  2.0 unx     7708 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h
+-rw-r--r--  2.0 unx    22815 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/gett.hpp
+-rw-r--r--  2.0 unx     9441 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/rank_2k.h
+-rw-r--r--  2.0 unx    11444 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h
+-rw-r--r--  2.0 unx     8148 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/rank_k_complex.h
+-rw-r--r--  2.0 unx    10509 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/symm.h
+-rw-r--r--  2.0 unx    12296 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/symm_complex.h
+-rw-r--r--  2.0 unx    11235 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_compare.h
+-rw-r--r--  2.0 unx     3339 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_compare.hpp
+-rw-r--r--  2.0 unx     8317 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_copy.h
+-rw-r--r--  2.0 unx     9027 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h
+-rw-r--r--  2.0 unx    47111 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_fill.h
+-rw-r--r--  2.0 unx    12875 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_fill.hpp
+-rw-r--r--  2.0 unx     4757 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_foreach.h
+-rw-r--r--  2.0 unx     2133 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_norm.h
+-rw-r--r--  2.0 unx     6129 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_reduce.h
+-rw-r--r--  2.0 unx     5987 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_reduce.hpp
+-rw-r--r--  2.0 unx     7670 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/trmm.h
+-rw-r--r--  2.0 unx     9874 b- defN 24-Apr-12 11:22 cutlass_library/source/tools/util/include/cutlass/util/reference/host/trmm_complex.h
+-rw-r--r--  2.0 unx     1889 b- defN 24-Apr-12 11:22 pycute/__init__.py
+-rw-r--r--  2.0 unx     7813 b- defN 24-Apr-12 11:22 pycute/int_tuple.py
+-rw-r--r--  2.0 unx    12388 b- defN 24-Apr-12 11:22 pycute/layout.py
+-rw-r--r--  2.0 unx     4475 b- defN 24-Apr-12 11:22 pycute/swizzle.py
+-rw-r--r--  2.0 unx     1981 b- defN 24-Apr-12 11:22 pycute/typing.py
+-rw-r--r--  2.0 unx     1547 b- defN 24-Apr-12 12:04 nvidia_cutlass-3.5.0.0.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx    29356 b- defN 24-Apr-12 12:04 nvidia_cutlass-3.5.0.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-12 12:04 nvidia_cutlass-3.5.0.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       31 b- defN 24-Apr-12 12:04 nvidia_cutlass-3.5.0.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx   148425 b- defN 24-Apr-12 12:04 nvidia_cutlass-3.5.0.0.dist-info/RECORD
+1139 files, 18605303 bytes uncompressed, 3736588 bytes compressed:  79.9%
```

## zipnote {}

```diff
@@ -204,14 +204,17 @@
 
 Filename: cutlass_library/conv2d_operation.py
 Comment: 
 
 Filename: cutlass_library/conv3d_operation.py
 Comment: 
 
+Filename: cutlass_library/conv3x_emitter.py
+Comment: 
+
 Filename: cutlass_library/gemm_operation.py
 Comment: 
 
 Filename: cutlass_library/generator.py
 Comment: 
 
 Filename: cutlass_library/library.py
@@ -885,17 +888,14 @@
 
 Filename: cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_gemm.hpp
 Comment: 
 
 Filename: cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_kernel.cuh
 Comment: 
 
-Filename: cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_tensor.hpp
-Comment: 
-
 Filename: cutlass_library/source/examples/52_hopper_gather_scatter_fusion/scatter_epilogue.hpp
 Comment: 
 
 Filename: cutlass_library/source/examples/53_hopper_gemm_permute/53_hopper_gemm_permute.cu
 Comment: 
 
 Filename: cutlass_library/source/examples/53_hopper_gemm_permute/CMakeLists.txt
@@ -936,30 +936,60 @@
 
 Filename: cutlass_library/source/examples/57_hopper_grouped_gemm/57_hopper_grouped_gemm.cu
 Comment: 
 
 Filename: cutlass_library/source/examples/57_hopper_grouped_gemm/CMakeLists.txt
 Comment: 
 
+Filename: cutlass_library/source/examples/58_ada_fp8_gemm/CMakeLists.txt
+Comment: 
+
+Filename: cutlass_library/source/examples/58_ada_fp8_gemm/ada_fp8_gemm.cu
+Comment: 
+
+Filename: cutlass_library/source/examples/59_ampere_gather_scatter_conv/CMakeLists.txt
+Comment: 
+
+Filename: cutlass_library/source/examples/59_ampere_gather_scatter_conv/README.md
+Comment: 
+
+Filename: cutlass_library/source/examples/59_ampere_gather_scatter_conv/ampere_conv_kernel.h
+Comment: 
+
+Filename: cutlass_library/source/examples/59_ampere_gather_scatter_conv/ampere_gather_scatter_conv.cu
+Comment: 
+
 Filename: cutlass_library/source/examples/60_cutlass_import/CMakeLists.txt
 Comment: 
 
 Filename: cutlass_library/source/examples/60_cutlass_import/main.cpp
 Comment: 
 
+Filename: cutlass_library/source/examples/common/gather_tensor.hpp
+Comment: 
+
 Filename: cutlass_library/source/examples/common/helper.h
 Comment: 
 
 Filename: cutlass_library/source/examples/cute/CMakeLists.txt
 Comment: 
 
 Filename: cutlass_library/source/examples/cute/tutorial/CMakeLists.txt
 Comment: 
 
-Filename: cutlass_library/source/examples/cute/tutorial/sgemm_nt_1.cu
+Filename: cutlass_library/source/examples/cute/tutorial/sgemm_1.cu
+Comment: 
+
+Filename: cutlass_library/source/examples/cute/tutorial/sgemm_2.cu
+Comment: 
+
+Filename: cutlass_library/source/examples/cute/tutorial/sgemm_sm70.cu
+Comment: 
+
+Filename: cutlass_library/source/examples/cute/tutorial/sgemm_sm80.cu
 Comment: 
 
 Filename: cutlass_library/source/examples/cute/tutorial/tiled_copy.cu
 Comment: 
 
 Filename: cutlass_library/source/examples/python/00_basic_gemm.ipynb
 Comment: 
@@ -1014,26 +1044,29 @@
 
 Filename: cutlass_library/source/include/cute/tensor.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/tensor_predicate.hpp
 Comment: 
 
-Filename: cutlass_library/source/include/cute/tile.hpp
-Comment: 
-
 Filename: cutlass_library/source/include/cute/underscore.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/algorithm/axpby.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/algorithm/clear.hpp
 Comment: 
 
+Filename: cutlass_library/source/include/cute/algorithm/cooperative_copy.hpp
+Comment: 
+
+Filename: cutlass_library/source/include/cute/algorithm/cooperative_gemm.hpp
+Comment: 
+
 Filename: cutlass_library/source/include/cute/algorithm/copy.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/algorithm/fill.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/algorithm/functional.hpp
@@ -1041,26 +1074,32 @@
 
 Filename: cutlass_library/source/include/cute/algorithm/gemm.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/algorithm/prefer.hpp
 Comment: 
 
+Filename: cutlass_library/source/include/cute/algorithm/prefetch.hpp
+Comment: 
+
 Filename: cutlass_library/source/include/cute/algorithm/tensor_algorithms.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/algorithm/tuple_algorithms.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/arch/cluster_sm90.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/arch/copy.hpp
 Comment: 
 
+Filename: cutlass_library/source/include/cute/arch/copy_sm50.hpp
+Comment: 
+
 Filename: cutlass_library/source/include/cute/arch/copy_sm75.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/arch/copy_sm80.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/arch/copy_sm90.hpp
@@ -1101,23 +1140,29 @@
 
 Filename: cutlass_library/source/include/cute/atom/copy_atom.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/atom/copy_traits.hpp
 Comment: 
 
+Filename: cutlass_library/source/include/cute/atom/copy_traits_sm50.hpp
+Comment: 
+
 Filename: cutlass_library/source/include/cute/atom/copy_traits_sm75.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/atom/copy_traits_sm80.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/atom/copy_traits_sm90.hpp
 Comment: 
 
+Filename: cutlass_library/source/include/cute/atom/copy_traits_sm90_im2col.hpp
+Comment: 
+
 Filename: cutlass_library/source/include/cute/atom/copy_traits_sm90_tma.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/atom/copy_traits_sm90_tma_swizzle.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/atom/mma_atom.hpp
@@ -1167,51 +1212,36 @@
 
 Filename: cutlass_library/source/include/cute/container/type_list.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/numeric/arithmetic_tuple.hpp
 Comment: 
 
-Filename: cutlass_library/source/include/cute/numeric/bfloat.hpp
-Comment: 
-
 Filename: cutlass_library/source/include/cute/numeric/complex.hpp
 Comment: 
 
-Filename: cutlass_library/source/include/cute/numeric/float8.hpp
-Comment: 
-
-Filename: cutlass_library/source/include/cute/numeric/half.hpp
-Comment: 
-
 Filename: cutlass_library/source/include/cute/numeric/int.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/numeric/integer_sequence.hpp
 Comment: 
 
-Filename: cutlass_library/source/include/cute/numeric/integer_subbyte.hpp
-Comment: 
-
 Filename: cutlass_library/source/include/cute/numeric/integral_constant.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/numeric/integral_ratio.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/numeric/math.hpp
 Comment: 
 
-Filename: cutlass_library/source/include/cute/numeric/real.hpp
+Filename: cutlass_library/source/include/cute/numeric/numeric_types.hpp
 Comment: 
 
-Filename: cutlass_library/source/include/cute/numeric/tfloat.hpp
-Comment: 
-
-Filename: cutlass_library/source/include/cute/numeric/uint128.hpp
+Filename: cutlass_library/source/include/cute/numeric/real.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/util/debug.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cute/util/print.hpp
 Comment: 
@@ -1410,20 +1440,26 @@
 
 Filename: cutlass_library/source/include/cutlass/arch/mma_sm75.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/arch/mma_sm80.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/arch/mma_sm89.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/arch/mma_sm90.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/arch/mma_sparse_sm80.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/arch/mma_sparse_sm89.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/arch/reg_reconfig.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/arch/simd.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/arch/simd_sm60.h
@@ -1446,38 +1482,71 @@
 
 Filename: cutlass_library/source/include/cutlass/conv/conv2d_problem_size.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/conv3d_problem_size.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/conv/convnd_problem_shape.hpp
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/conv/convolution.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/conv/dispatch_policy.hpp
+Comment: 
+
+Filename: cutlass_library/source/include/cutlass/conv/collective/collective_builder.hpp
+Comment: 
+
+Filename: cutlass_library/source/include/cutlass/conv/collective/collective_conv.hpp
+Comment: 
+
+Filename: cutlass_library/source/include/cutlass/conv/collective/detail.hpp
+Comment: 
+
+Filename: cutlass_library/source/include/cutlass/conv/collective/sm90_implicit_gemm_gmma_ss_warpspecialized.hpp
+Comment: 
+
+Filename: cutlass_library/source/include/cutlass/conv/collective/builders/sm90_common.inl
+Comment: 
+
+Filename: cutlass_library/source/include/cutlass/conv/collective/builders/sm90_gmma_builder.inl
+Comment: 
+
+Filename: cutlass_library/source/include/cutlass/conv/device/conv_universal_adapter.hpp
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/conv/device/direct_convolution.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/device/implicit_gemm_convolution.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/conv/kernel/conv_universal.hpp
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/conv/kernel/default_conv2d.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_dgrad.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_absmax.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_group_fprop.h
@@ -1494,17 +1563,32 @@
 
 Filename: cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop_with_broadcast.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_wgrad.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/conv/kernel/default_deconv2d.h
+Comment: 
+
+Filename: cutlass_library/source/include/cutlass/conv/kernel/default_deconv2d_with_broadcast.h
+Comment: 
+
+Filename: cutlass_library/source/include/cutlass/conv/kernel/default_deconv3d.h
+Comment: 
+
+Filename: cutlass_library/source/include/cutlass/conv/kernel/default_deconv3d_with_broadcast.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/conv/kernel/default_depthwise_fprop.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/kernel/direct_convolution.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution.h
@@ -1512,17 +1596,23 @@
 
 Filename: cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_with_absmax.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/conv/kernel/sm90_implicit_gemm_tma_warpspecialized.hpp
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/conv/thread/depthwise_mma.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h
@@ -1767,14 +1857,17 @@
 
 Filename: cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_gelu.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_generic.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_generic_with_scaling.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_hardswish.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_params.h
@@ -1830,14 +1923,17 @@
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_with_absmax.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h
@@ -1884,14 +1980,17 @@
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_streamk_with_broadcast.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_absmax.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h
@@ -1920,14 +2019,17 @@
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_conv.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h
@@ -1938,15 +2040,15 @@
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h
 Comment: 
 
-Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h
+Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_linear.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_2x.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_compute.hpp
 Comment: 
@@ -2088,14 +2190,17 @@
 
 Filename: cutlass_library/source/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/device/gemm_sparse.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/gemm/device/gemm_sparse_with_absmax.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/gemm/device/gemm_sparse_with_visitor.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/device/gemm_splitk_parallel.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/device/gemm_universal.h
@@ -2106,14 +2211,17 @@
 
 Filename: cutlass_library/source/include/cutlass/gemm/device/gemm_universal_base.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/device/gemm_universal_streamk_with_broadcast.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/gemm/device/gemm_universal_with_absmax.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/gemm/device/gemm_universal_with_broadcast.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/device/gemm_with_k_reduction.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/device/gemv.h
@@ -2154,14 +2262,17 @@
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_sparse.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_sparse_with_absmax.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_sparse_with_visitor.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_streamk_with_broadcast.h
@@ -2169,14 +2280,17 @@
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_universal.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_universal_with_visitor.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_absmax.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_reduction.h
@@ -2280,14 +2394,17 @@
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_with_visitor.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_with_visitor_streamk.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/gemm/kernel/gemm_with_absmax.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/gemm_with_k_reduction.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/gemv.h
@@ -2355,14 +2472,17 @@
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler_stream_k.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/sparse_gemm.h
 Comment: 
 
+Filename: cutlass_library/source/include/cutlass/gemm/kernel/sparse_gemm_with_absmax.h
+Comment: 
+
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/sparse_gemm_with_visitor.h
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/static_tile_scheduler.hpp
 Comment: 
 
 Filename: cutlass_library/source/include/cutlass/gemm/kernel/symm_universal.h
@@ -2817,14 +2937,17 @@
 
 Filename: cutlass_library/source/tools/library/src/conv2d_operation.h
 Comment: 
 
 Filename: cutlass_library/source/tools/library/src/conv3d_operation.h
 Comment: 
 
+Filename: cutlass_library/source/tools/library/src/conv_operation_3x.hpp
+Comment: 
+
 Filename: cutlass_library/source/tools/library/src/gemm_operation.h
 Comment: 
 
 Filename: cutlass_library/source/tools/library/src/gemm_operation_3x.hpp
 Comment: 
 
 Filename: cutlass_library/source/tools/library/src/handle.cu
@@ -3186,14 +3309,17 @@
 
 Filename: cutlass_library/source/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h
 Comment: 
 
 Filename: cutlass_library/source/tools/util/include/cutlass/util/reference/device/thread/gemm.h
 Comment: 
 
+Filename: cutlass_library/source/tools/util/include/cutlass/util/reference/host/conv.hpp
+Comment: 
+
 Filename: cutlass_library/source/tools/util/include/cutlass/util/reference/host/convolution.h
 Comment: 
 
 Filename: cutlass_library/source/tools/util/include/cutlass/util/reference/host/error_metrics.h
 Comment: 
 
 Filename: cutlass_library/source/tools/util/include/cutlass/util/reference/host/gemm.h
@@ -3270,23 +3396,23 @@
 
 Filename: pycute/swizzle.py
 Comment: 
 
 Filename: pycute/typing.py
 Comment: 
 
-Filename: nvidia_cutlass-3.4.1.0.dist-info/LICENSE.txt
+Filename: nvidia_cutlass-3.5.0.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: nvidia_cutlass-3.4.1.0.dist-info/METADATA
+Filename: nvidia_cutlass-3.5.0.0.dist-info/METADATA
 Comment: 
 
-Filename: nvidia_cutlass-3.4.1.0.dist-info/WHEEL
+Filename: nvidia_cutlass-3.5.0.0.dist-info/WHEEL
 Comment: 
 
-Filename: nvidia_cutlass-3.4.1.0.dist-info/top_level.txt
+Filename: nvidia_cutlass-3.5.0.0.dist-info/top_level.txt
 Comment: 
 
-Filename: nvidia_cutlass-3.4.1.0.dist-info/RECORD
+Filename: nvidia_cutlass-3.5.0.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## cutlass/__init__.py

```diff
@@ -117,15 +117,15 @@
     the registry when CUTLASS is imported.
     """
     if this._option_registry is None:
         this.logger.info("Initializing option registry")
         this._option_registry = OptionRegistry(device_cc())
     return this._option_registry
 
-this.__version__ = '3.4.1'
+this.__version__ = '3.5.0'
 
 from cutlass.backend import create_memory_pool
 from cutlass.emit.pytorch import pytorch
 from cutlass.op.gemm import Gemm
 from cutlass.op.conv import Conv2d, Conv2dFprop, Conv2dDgrad, Conv2dWgrad
 from cutlass.op.gemm_grouped import GroupedGemm
 from cutlass.op.op import OperationBase
```

## cutlass/library_defaults.py

```diff
@@ -133,17 +133,17 @@
             # Reconcile A, B, and C alignments by trying to align to the minimum
             min_alignment = min(alignment_A, alignment_B, alignment_C)
             key = f"{min_alignment} {min_alignment} {min_alignment}"
             if key not in self.kernels_by_alignment:
                 # Finally, go through all available alignment combinations and find
                 # one for which all values are less than those passed in.
                 key = None
-                alignments = sorted([(int(x) for x in k.split(" ")) for k in self.kernels_by_alignment.keys()], reverse=True)
+                alignments = sorted([tuple(int(x) for x in k.split(" ")) for k in self.kernels_by_alignment.keys()], reverse=True)
                 for align_A, align_B, align_C in alignments:
-                    if align_A <= alignment_A and align_B <= alignment_B and align_C <= alignment_C:
+                    if alignment_A % align_A == 0 and alignment_B % align_B == 0 and alignment_C % align_C == 0:
                         key = f"{align_A} {align_B} {align_C}"
                         break
 
                 if key is None:
                     raise Exception(
                         f"No operations of alignment {og_key} found for data type and layout "
                         f"combination {self.datatype_comb} {self.layout_comb}. Compatible alignments "
```

## cutlass/backend/c_types.py

```diff
@@ -240,15 +240,15 @@
                 ("ptr_D", ctypes.c_void_p),
                 ("stride_D", StrideBatched_),
             ]
 
     class _HardwareInfo(ctypes.Structure):
         _fields_ = [
             ("device_id", ctypes.c_int),
-            ("sm_count", ctypes.c_int)
+            ("sm_count", ctypes.c_int),
         ]
 
     class _GemmArguments(ctypes.Structure):
         _fields_ = [
             ("mode", ctypes.c_int),
             ("problem_size", GemmCoordBatched_),
             ("mainloop", mainloop_arguments),
```

## cutlass/backend/epilogue.py

```diff
@@ -118,15 +118,15 @@
     """
     Apply a linear combination operator to an array of elements
     D = alpha * accumulator + beta * source
 
     :param element_output: data type used to load and store tensors
 
     :param epilogue_vector_length: number of elements computed per operation.
-    Usually it is 128/sizeof_bits<ElementOutput_>, but we use 64 and 32 sometimes
+    Usually it is 128/sizeof_bits_v<ElementOutput_>, but we use 64 and 32 sometimes
     when there are not enough data to store
 
     :param element_accumulator: Accumulator data type
 
     :param element_epilogue: data type used to compute linear combination
     """
 
@@ -203,15 +203,15 @@
     the output before converting to the output element type.
 
     D = alpha * accumulator + beta * source + uniform
 
     :param element_output: data type used to load and store tensors
 
     :param epilogue_vector_length: number of elements computed per operation.
-    Usually it is 128/sizeof_bits<ElementOutput_>, but we use 64 and 32 sometimes
+    Usually it is 128/sizeof_bits_v<ElementOutput_>, but we use 64 and 32 sometimes
     when there are not enough data to store
 
     :param element_accumulator: Accumulator data type
 
     :param element_epilogue: data type used to compute linear combination
     """
 
@@ -256,15 +256,15 @@
     Note: The below method only when problem_size_K <= 256 for signed int8 gemm
     or problem_size_K <= 128 for unsigned int8 gemm. The default approach is
     above.
 
     :param element_output: data type used to load and store tensors
 
     :param epilogue_vector_length: number of elements computed per operation.
-    Usually it is 128/sizeof_bits<ElementOutput_>, but we use 64 and 32 sometimes
+    Usually it is 128/sizeof_bits_v<ElementOutput_>, but we use 64 and 32 sometimes
     when there are not enough data to store
     """
 
     tag = "cutlass::epilogue::thread::FastLinearCombinationClamp"
 
     def __init__(self, element_output, epilogue_vector_length, *args) -> None:
         super().__init__()
@@ -306,15 +306,15 @@
     D = activation(alpha * accumulator + beta * source)
 
     :param activation_functor: input activation functor
 
     :param element_output: data type used to load and store tensors
 
     :param epilogue_vector_length: number of elements computed per operation.
-    Usually it is 128/sizeof_bits<ElementOutput_>, but we use 64 and 32 sometimes
+    Usually it is 128/sizeof_bits_v<ElementOutput_>, but we use 64 and 32 sometimes
     when there are not enough data to store
 
     :param element_accumulator: Accumulator data type
 
     :param element_epilogue: data type used to compute linear combination
     """
```

## cutlass/backend/gemm_operation.py

```diff
@@ -561,15 +561,17 @@
             int(self.ptr_C),
             stride_C,
             int(self.ptr_D),
             stride_D,
         )
 
         # Set hardware info
-        hw_info_ = hw_info(0, device_sm_count())
+        hw_info_ = hw_info(
+            0, device_sm_count(),
+        )
 
         self.arguments = argument_type(
             int(self.gemm_mode),
             problem_size_,
             mainloop,
             epilogue,
             hw_info_,
@@ -1296,15 +1298,15 @@
 using DeviceKernel = cutlass::gemm::device::GemmUniversalAdapter<${operation_name}${operation_suffix}>;
 """
 
     def emit(self, operation):
         # Support built-in epilogue functors or user-defined functions
 
         if operation.tile_description.stages is None or operation.tile_description.stages == 0:
-            stage_count_type = "cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename CollectiveEpilogue::SharedStorage)>"
+            stage_count_type = "cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(sizeof(typename CollectiveEpilogue::SharedStorage))>"
         else:
             stage_count_type = "_" + str(operation.tile_description.stages)
 
         if operation.emission_type == EmissionType.Kernel:
             gemm_template = self.gemm_template_kernel
         else:
             gemm_template = self.gemm_template_device
@@ -1648,15 +1650,15 @@
         return extended_name
 
     def extended_name_3x(self):
         """Generates a string representing the MMA atom. Assumes accumulator type is C type."""
         extended_name = "{core_name}_{element_a}_{element_b}_{element_acc}_{element_c}_{element_d}".format(
             element_a=DataTypeNames[self.A.element],
             element_b=DataTypeNames[self.B.element],
-            element_acc=DataTypeNames[self.tile_description.math_instruction.element_accumulator],
+            element_acc=DataTypeNames[self.accumulator_type()],
             element_c=DataTypeNames[self.C.element],
             element_d=DataTypeNames[self.epilogue_functor.element_output],
             core_name=self.core_name())
         return extended_name
 
     def layout_name(self):
         if self.is_complex() or self.is_planar_complex():
```

## cutlass/backend/evt/backend/sm90_nodes.py

```diff
@@ -295,15 +295,15 @@
         Return the string defining the type
         """
         if self._type_decl is not None:
             return self._type_decl
 
         self._type_decl = f"""
 using {self.name_camel} = cutlass::epilogue::fusion::Sm90ColReduction<
-    {op_tag(self.reg_reduce_fn)}, {op_tag(self.gmem_reduce_fn)}, 0,
+    {op_tag(self.reg_reduce_fn)}, {op_tag(self.reg_reduce_fn)}, {op_tag(self.gmem_reduce_fn)}, 0,
     typename EpilogueDescriptor::TileShape, {DataTypeTag[self.element]},
     {DataTypeTag[self.element_compute]}, {FloatRoundStyleTag[self.round_style]},
     {self.stride_mnl}
 >;
 """
         return self._type_decl
 
@@ -317,15 +317,15 @@
         Return the string defining the type
         """
         if self._type_decl is not None:
             return self._type_decl
 
         self._type_decl = f"""
 using {self.name_camel} = cutlass::epilogue::fusion::Sm90RowReduction<
-    {op_tag(self.reg_reduce_fn)}, {op_tag(self.gmem_reduce_fn)}, 0 /* Stages */,
+    {op_tag(self.reg_reduce_fn)}, {op_tag(self.reg_reduce_fn)}, {op_tag(self.gmem_reduce_fn)}, 0 /* Stages */,
     typename EpilogueDescriptor::TileShape, {DataTypeTag[self.element]},
     {DataTypeTag[self.element_compute]}, {FloatRoundStyleTag[self.round_style]},
     {self.stride_mnl}
 >;
 """
         return self._type_decl
```

## cutlass/emit/common.py

```diff
@@ -114,24 +114,26 @@
         int M, int N, int K, int L,
         const DeviceKernel::ElementA* A, const DeviceKernel::ElementB* B, const DeviceKernel::ElementC* C, DeviceKernel::ElementC* D,
         ElementCompute alpha, ElementCompute beta, const cutlass::KernelHardwareInfo& hw_info) {
 
   typename DeviceKernel::Arguments arguments{
       cutlass::gemm::GemmUniversalMode::kGemm,
       {M, N, K, L},                                                              // problem size
-      A,                                                                         // ptrA
-      cutlass::make_cute_packed_stride(StrideA{}, cute::make_shape(M, K, L)),    // stride A
-      B,                                                                         // ptrB
-      cutlass::make_cute_packed_stride(StrideB{}, cute::make_shape(N, K, L)),    // stride B
       {
+        A,                                                                         // ptrA
+        cutlass::make_cute_packed_stride(StrideA{}, cute::make_shape(M, K, L)),    // stride A
+        B,                                                                         // ptrB
+        cutlass::make_cute_packed_stride(StrideB{}, cute::make_shape(N, K, L)),    // stride B
+      },
+      {
+        {alpha, beta},
         C,                                                                       // ptrC
         cutlass::make_cute_packed_stride(StrideC{}, cute::make_shape(M, N, L)),  // stride C
         D,                                                                       // ptrD
         cutlass::make_cute_packed_stride(StrideD{}, cute::make_shape(M, N, L)),  // stride D
-        {alpha, beta},
       },
       hw_info
   };
 
   size_t workspace_size = DeviceKernel::get_workspace_size(arguments);
   cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
```

## cutlass/emit/pytorch.py

```diff
@@ -228,15 +228,15 @@
 #include "cutlass/gemm/device/gemm_universal.h"
 """,
     ApiVersion.v3x: """
 #include "cutlass/gemm/device/gemm_universal_adapter.h"
 #include "cutlass/gemm/collective/collective_builder.hpp"
 #include "cutlass/gemm/device/gemm_universal_adapter.h"
 #include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/epilogue/collective/default_epilogue.hpp"
+#include "cutlass/epilogue/collective/collective_builder.hpp"
 #include "cutlass/util/packed_stride.hpp"
 """,
 }
 
 _PYTORCH_GROUPED_GEMM_INCLUDES = """
 #include "cutlass/gemm/kernel/default_gemm_grouped.h"
 #include "cutlass/gemm/device/gemm_grouped.h"
@@ -579,36 +579,42 @@
     name='${name}',
     ext_modules=[
         CUDAExtension('${name}', [
             '${name}.cpp',
             '${name}_kernel.cu',
         ],
         include_dirs=['${cutlass_path}/include', '${cutlass_path}/tools/util/include'],
-        extra_compile_args=['-std=c++17']
+        extra_compile_args={
+            'cxx': ['-std=c++17'],
+            'nvcc': ['-std=c++17', ${extra_compile_args}],
+        },
+        libraries=['cuda']
         ),
     ],
     cmdclass={
         'build_ext': BuildExtension
     })
 
 """
 
 
-def _generate_setup(name: str, sourcedir: str):
+def _generate_setup(name: str, sourcedir: str, extra_compile_args: str=""):
     """
     Generates a setup.py file for the extension
 
     :param name: name of the module to generate
     :type name: str
     :param sourcedir: directory to which generated source files should be written
     :type sourcedir: str
+    :param extra_compile_args: additional arguments to pass to setup.py
+    :type extra_args: str
     """
     setup_py_file = os.path.join(sourcedir, "setup.py")
     setup_source = SubstituteTemplate(
-        _PYTORCH_SETUP_PY, {"name": name, "cutlass_path": CUTLASS_PATH}
+        _PYTORCH_SETUP_PY, {"name": name, "cutlass_path": CUTLASS_PATH, "extra_compile_args": extra_compile_args}
     )
     with open(setup_py_file, "w") as outfile:
         outfile.write(setup_source)
 
 
 class _ArchListSetter:
     """
@@ -692,14 +698,15 @@
             name,
             [cpp_file, cuda_file],
             extra_cuda_cflags=extra_cuda_cflags,
             extra_include_paths=[
                 os.path.join(CUTLASS_PATH, "include"),
                 os.path.join(CUTLASS_PATH, "tools/util/include"),
             ],
+            extra_ldflags=["-lcuda"],
             verbose=(logger.level == logging.DEBUG)
         )
     return jitmodule
 
 
 def _pytorch_gemm(op, name: str, cc: int, jit: bool = False, sourcedir: str = ""):
     """
@@ -755,15 +762,18 @@
     cpp_source = SubstituteTemplate(
         _PYTORCH_GEMM_CPP_TEMPLATE,
         {"name": name, "description": f"CUTLASS {op.procedural_name()} GEMM"},
     )
     with open(cpp_file, "w") as outfile:
         outfile.write(cpp_source)
 
-    _generate_setup(name, sourcedir)
+    extra_compile_args = ""
+    if cc == 90:
+        extra_compile_args = "'--generate-code=arch=compute_90a,code=[sm_90a]'"
+    _generate_setup(name, sourcedir, extra_compile_args)
 
     if jit:
         return _jit(name, cc, cpp_file, cuda_file)
 
     return None
```

## cutlass/op/gemm.py

```diff
@@ -399,15 +399,15 @@
         Returns a list of valid tile descriptions for the operations
 
         :returns: list of valid tile descriptions for the operations
         :rtype: list
         """
         tds = [datatypes.td_from_profiler_op(op) for op in self.possible_operations.all_operations]
         if self._math_operation is not None:
-            tds = [td for td in tds if td.tile_description.math_instruction == self._math_operation]
+            tds = [td for td in tds if td.math_instruction.math_operation == self._math_operation]
         return tds
 
     def construct(
         self, tile_description: TileDescription = None,
         alignment_A: int = None, alignment_B: int = None, alignment_C: int = None) -> GemmOperationUniversal:
         """
         Constructs a ``cutlass.backend.GemmUniversalOperation`` based on the input parameters and current
```

## cutlass_library/conv2d_operation.py

```diff
@@ -31,24 +31,30 @@
 #################################################################################################
 
 """
 Utilities for emitting Conv2d kernels
 """
 
 import enum
+import logging
 import os.path
 import shutil
+from string import Template
 
 try:
   import builtins
   if hasattr(builtins, "CUTLASS_IGNORE_PACKAGE") and CUTLASS_IGNORE_PACKAGE == True:
     raise ImportError("Disabling attempt to import cutlass_library")
   from cutlass_library.library import *
+  from cutlass_library.conv3x_emitter import EmitConv3xInstance, EmitConv3xIncludes
 except ImportError:
   from library import *
+  from conv3x_emitter import EmitConv3xInstance, EmitConv3xIncludes
+
+_LOGGER = logging.getLogger(__name__)
 
 ###################################################################################################
 
 #
 class Conv2dOperation:
   #
   def __init__(self, conv_kind, iterator_algorithm, arch, tile_description, A, B, C, element_epilogue, \
@@ -170,14 +176,16 @@
 #
 # Emits single instances of a CUTLASS device-wide operator
 #
 ###################################################################################################
 
 class EmitConv2dInstance:
   def __init__(self):
+    # Emitter for CUTLASS 3 convolution operations
+    self.conv3x_emitter = EmitConv3xInstance()
     self.template = """
   // Conv2d${conv_kind_name} ${iterator_algorithm_name} kernel instance "${operation_name}"
   using ${operation_name}_base =
   typename cutlass::conv::kernel::DefaultConv2d${conv_kind_name}<
     ${element_a},
     ${layout_a},
     ${element_b},
@@ -273,15 +281,26 @@
     ${iterator_algorithm},
     ${stride_support},
     cutlass::MatrixShape<${stride_r}, ${stride_s}>,
     cutlass::MatrixShape<${dilation_r}, ${dilation_s}>
   >::Kernel;
 """
 
+  def arch_number_to_type(self, arch: int):
+    return f"cutlass::arch::Sm{arch}"
+
   def emit(self, operation):
+    _LOGGER.debug("*** EmitConv2dInstance::emit")
+    _LOGGER.debug("***   operation: procedural_name()=" + operation.procedural_name())
+
+    if hasattr(operation, 'is_3x') and operation.is_3x:
+      _LOGGER.debug("***   CUTLASS 3 operation")
+      return self.conv3x_emitter.emit(operation)
+
+    _LOGGER.debug("***   CUTLASS 2 operation")
 
     warp_shape = [int(operation.tile_description.threadblock_shape[idx] / operation.tile_description.warp_count[idx]) for idx in range(3)]
 
     epilogue_vector_length = int(min(operation.C.alignment * DataTypeSize[operation.C.element], 128) / DataTypeSize[operation.C.element])
 
     values = {
       'operation_name': operation.procedural_name(),
@@ -316,17 +335,19 @@
       'math_operator': 'cutlass::arch::OpMultiplyAddComplex' if operation.is_complex() else \
       MathOperationTag[operation.tile_description.math_instruction.math_operation],
       'align_a': str(operation.A.alignment),
       'align_b': str(operation.B.alignment),
     }
 
     if operation.group_mode == GroupMode.NoneGroup:
+      _LOGGER.debug("***   group_mode=NoneGroup")
       return SubstituteTemplate(self.template, values)
 
     elif operation.group_mode == GroupMode.Depthwise:
+      _LOGGER.debug("***   group_mode=Depthwise")
       values['group_mode'] = GroupModeTag[operation.group_mode]
       # Setup other template params
       values['threadblock_output_shape_n'] = str(operation.tile_description.threadblock_output_shape[0])
       values['threadblock_output_shape_p'] = str(operation.tile_description.threadblock_output_shape[1])
       values['threadblock_output_shape_q'] = str(operation.tile_description.threadblock_output_shape[2])
 
       values['groups_per_cta'] = str(operation.tile_description.threadblock_output_shape[3])
@@ -339,25 +360,27 @@
 
       values['dilation_r'] = str(operation.tile_description.dilation[0])
       values['dilation_s'] = str(operation.tile_description.dilation[1])
 
       return SubstituteTemplate(self.template_depthwise_direct_conv, values)
 
     else:
+      _LOGGER.debug("***   group_mode=" + GroupModeTag[operation.group_mode])
       values['group_mode'] = GroupModeTag[operation.group_mode]
       return SubstituteTemplate(self.template_group_conv, values)
 
 ###################################################################################################
 #
 # Generator functions for all layouts
 #
 ###################################################################################################
 
 #
 def GenerateConv2dTensorOp(manifest, tile_descriptions, min_cc, align = 128):
+  _LOGGER.debug("*** GenerateConv2dTensorOp")
 
   for tile in tile_descriptions:
     for conv_kind in [ConvKind.Fprop, ConvKind.Dgrad, ConvKind.Wgrad]:
 
       if conv_kind == ConvKind.Fprop or (tile.math_instruction.element_accumulator in [DataType.f16, DataType.f32]):
 
         #
@@ -368,134 +391,230 @@
         for output_type in output_types:
           A = TensorDescription(tile.math_instruction.element_a, LayoutType.TensorNHWC, int(align / DataTypeSize[tile.math_instruction.element_a]))
           B = TensorDescription(tile.math_instruction.element_b, LayoutType.TensorNHWC, int(align / DataTypeSize[tile.math_instruction.element_b]))
           C = TensorDescription(output_type,  LayoutType.TensorNHWC, max(1, int(align / DataTypeSize[output_type])))
 
           manifest.append(Conv2dOperation(conv_kind, min_cc, tile, A, B, C, tile.math_instruction.element_accumulator))
 
+class EmitConv2dIncludes:
+  '''Emit includes that are specific to the operation.'''
+
+  def __init__(self):
+    self.includes = ['conv2d_operation.h']
+    self.emitter_3x = EmitConv3xIncludes()
+
+  def operation_is_3x(self, operation) -> bool:
+    """Whether operation is a CUTLASS 3 convolution (as opposed to CUTLASS 2)"""
+    return hasattr(operation, 'is_3x') and operation.is_3x
+
+  def emit(self, operation) -> str:
+    if self.operation_is_3x(operation):
+      return self.emitter_3x.emit(operation)
+
+    return '\n'.join(f"#include \"{incl}\"" for incl in self.includes) + \
+      "\n\n///////////////////////////////////////////////////////////////////////////////////////////////////"
+
 ###################################################################################################
 #
 # Emitters functions for all targets
 #
 ###################################################################################################
 
 class EmitConv2dConfigurationLibrary:
   def __init__(self, operation_path, configuration_name):
     self.configuration_name = configuration_name
     self.configuration_path = os.path.join(operation_path, "%s.cu" % configuration_name)
 
     self.instance_emitter = EmitConv2dInstance()
+    self.includes_emitter = EmitConv2dIncludes()
 
-    self.instance_template = """
-${operation_instance}
-
-// Derived class
-struct ${operation_name} :
-  public ${operation_name}_base { };
-
-///////////////////////////////////////////////////////////////////////////////////////////////////
-
-"""
     self.header_template = """
 /*
   Generated by conv2d_operation.py - Do not edit.
 */
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 #include "cutlass/cutlass.h"
 #include "cutlass/library/library.h"
 #include "cutlass/library/manifest.h"
 
 #include "library_internal.h"
-#include "conv2d_operation.h"
+"""
 
+    self.instance_template = """
+${stub_begin}
+${operation_instance}
+// Derived class
+struct ${operation_name} :
+  public ${operation_name}_base { };
+${stub_end}
 ///////////////////////////////////////////////////////////////////////////////////////////////////
+
 """
 
     self.configuration_header = """
 
 namespace cutlass {
 namespace library {
 
 // Initialize all instances
 void initialize_${configuration_name}(Manifest &manifest) {
-
 """
 
-    self.configuration_instance = """
-  using Operation_${operation_name} = cutlass::conv::device::ImplicitGemmConvolution<
+    self.configuration_instance = """${stub_begin}
+  using Operation_${operation_name} = cutlass::conv::device::${kernel_name}<
     ${operation_name}>;
 
-  manifest.append(new cutlass::library::Conv2dOperation<
-    Operation_${operation_name}>(
-      "${operation_name}"));
-
+  manifest.append(new cutlass::library::${operation_wrapper}<
+      Operation_${operation_name}
+    >(
+      "${operation_name}"
+    ));
+${stub_end}
 """
 
-    self.configuration_direct_conv_instance = """
-  using Operation_${operation_name} = cutlass::conv::device::DirectConvolution<
-    ${operation_name}>;
-
-  manifest.append(new cutlass::library::DirectConv2dOperation<
-    Operation_${operation_name}>(
-      "${operation_name}"));
-
-"""
+    self.configuration_epilogue = "}\n"
 
-    self.configuration_epilogue = """
-}
-"""
     self.epilogue_template = """
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace library
 } // namespace cutlass
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 """
 
-  #
+  def operation_is_3x(self, operation):
+    """Whether operation is a CUTLASS 3 convolution (as opposed to CUTLASS 2)"""
+    return hasattr(operation, 'is_3x') and operation.is_3x
+
   def __enter__(self):
+    """
+    Open the configuration_file, and write the "header" C++ code to it.
+
+    The "header" consists of a comment (that this is generated code,
+    so it should not be edited), and includes that are common
+    to all kinds of kernels.
+    """
+    _LOGGER.debug('*** EmitConv2dConfigurationLibrary::__enter__')
+    _LOGGER.debug('***   configuration_path (file to write): ' +
+                  str(self.configuration_path))
+    _LOGGER.debug('***   configuration_name: ' + self.configuration_name)
     self.configuration_file = open(self.configuration_path, "w")
+
     self.configuration_file.write(SubstituteTemplate(self.header_template, {
       'configuration_name': self.configuration_name
       }))
     self.operations = []
     return self
 
-  #
   def emit(self, operation):
+    """
+    Write three pieces of C++ code to the configuration_file
+    (that was opened by the __enter__ method above):
+
+    1. the header includes that are specific to the operation
+       (CUTLASS 2 vs. CUTLASS 3);
+
+    2. the "operation instance" (a "using" declaration ending in "_base"); and
+
+    3. the "operation name" (declaration and definition of a derived class
+       of the above operation instance).
+
+    The "using" declaration turns a C++ class name, possibly namespace-qualified,
+    possibly also with angle brackets, into a C-style, easily demangled identifier.
+    """
+    _LOGGER.debug('*** EmitConv2dConfigurationLibrary::emit')
+    _LOGGER.debug('***   operation.procedural_name(): ' + operation.procedural_name())
     self.operations.append(operation)
-    self.configuration_file.write(SubstituteTemplate(self.instance_template, {
+
+    self.configuration_file.write(self.includes_emitter.emit(operation))
+
+    stub_begin = ''
+    stub_end = ''
+    # It can be useful to stub (comment) out instantiations for testing.
+    # In this case, one need only set is_stub to True.
+    is_stub = False
+    if is_stub:
+      stub_begin = "// STUB for now\n#if 0"
+      stub_end = '#endif // 0'
+
+    self.configuration_file.write(Template(self.instance_template).substitute({
       'configuration_name': self.configuration_name,
       'operation_name': operation.procedural_name(),
-      'operation_instance': self.instance_emitter.emit(operation)
+      'operation_instance': self.instance_emitter.emit(operation),
+      'stub_begin': stub_begin,
+      'stub_end': stub_end
       }))
 
-  #
   def __exit__(self, exception_type, exception_value, traceback):
+    """
+    Write the rest of the C++ code to the configuration_file, and close the file.
+
+    The "rest of the C++ code" has the following components.
+
+    1. Configuration header: Open the namespace(s), and open the definition
+       of the "initialize_${configuration_name}" registration function
+       that registers the operation with the Manifest.
+       ("Registration" helps turn C++ compile-time polymorphism
+       (via template parameters) into a run-time choice of parameters.)
+
+    2. Configuration instance: In the body of the registration function,
+       make a "using" declaration Operation_${operation_name} for the
+       operation type (which uses operation_name as its template argument).
+       Then, tell the manifest about the operation via a "manifest.append" call.
+       The argument of the call is a new instance of
+       "SomethingOperation<Operation_${operation_name}>"
+       (replace Something with a specific name).
+
+    3. Configuration epilogue: Close the definition of the registration function.
+
+    4. Epilogue template: Close the namespace(s).
+    """
+
+    _LOGGER.debug('*** EmitConv2dConfigurationLibrary::__exit__')
+    _LOGGER.debug('***   configuration_path (file to write): ' +
+                  str(self.configuration_path))
+    _LOGGER.debug('***   configuration_name: ' + self.configuration_name)
 
     self.configuration_file.write(SubstituteTemplate(self.configuration_header, {
       'configuration_name': self.configuration_name
       }))
 
     for operation in self.operations:
+      stub_begin = ''
+      stub_end = ''
+      # It can be useful to stub (comment) out instantiations for testing.
+      # In this case, one need only set is_stub to True.
+      is_stub = False
+      if is_stub:
+        stub_begin = "// STUB for now\n#if 0"
+        stub_end = "#endif // 0"
+
       if operation.group_mode == GroupMode.Depthwise:
-        self.configuration_file.write(SubstituteTemplate(self.configuration_direct_conv_instance, {
-          'configuration_name': self.configuration_name,
-          'operation_name': operation.procedural_name()
-        }))
+        kernel_name = 'DirectConvolution'
+        operation_wrapper = 'DirectConv2dOperation'
       else:
-        self.configuration_file.write(SubstituteTemplate(self.configuration_instance, {
-          'configuration_name': self.configuration_name,
-          'operation_name': operation.procedural_name()
-        }))
+        kernel_name = 'ImplicitGemmConvolution'
+        operation_wrapper = 'Conv2dOperation'
+      if self.operation_is_3x(operation):
+        kernel_name = 'ConvUniversalAdapter'
+        operation_wrapper = 'ConvOperation3x'
+
+      self.configuration_file.write(SubstituteTemplate(self.configuration_instance, {
+        'configuration_name': self.configuration_name,
+        'operation_name': operation.procedural_name(),
+        'kernel_name': kernel_name,
+        'operation_wrapper': operation_wrapper,
+        'stub_begin': stub_begin,
+        'stub_end': stub_end
+      }))
 
     self.configuration_file.write(self.configuration_epilogue)
     self.configuration_file.write(self.epilogue_template)
     self.configuration_file.close()
 
 
 ###################################################################################################
```

## cutlass_library/conv3d_operation.py

```diff
@@ -31,24 +31,30 @@
 #################################################################################################
 
 """
 Utilities for emitting Conv3d kernels
 """
 
 import enum
+import logging
 import os.path
 import shutil
+from string import Template
 
 try:
   import builtins
   if hasattr(builtins, "CUTLASS_IGNORE_PACKAGE") and CUTLASS_IGNORE_PACKAGE == True:
     raise ImportError("Disabling attempt to import cutlass_library")
   from cutlass_library.library import *
+  from cutlass_library.conv3x_emitter import EmitConv3xInstance, EmitConv3xIncludes
 except ImportError:
   from library import *
+  from conv3x_emitter import EmitConv3xInstance, EmitConv3xIncludes
+
+_LOGGER = logging.getLogger(__name__)
 
 ###################################################################################################
 
 #
 class Conv3dOperation:
   #
   def __init__(self, conv_kind, iterator_algorithm, arch, tile_description, A, B, C, element_epilogue, \
@@ -144,14 +150,16 @@
 #
 # Emits single instances of a CUTLASS device-wide operator
 #
 ###################################################################################################
 
 class EmitConv3dInstance:
   def __init__(self):
+    # Emitter for CUTLASS 3 convolution operations
+    self.conv3x_emitter = EmitConv3xInstance()
     self.template = """
   // Conv3d${conv_kind_name} ${iterator_algorithm_name} kernel instance "${operation_name}"
   using ${operation_name}_base =
   typename cutlass::conv::kernel::DefaultConv3d${conv_kind_name}<
     ${element_a},
     cutlass::layout::TensorNDHWC,
     ${element_b},
@@ -174,16 +182,23 @@
     ${stages},
     cutlass::arch::OpMultiplyAdd,
     ${iterator_algorithm},
     ${stride_support}
   >::Kernel;
 """
 
-
   def emit(self, operation):
+    _LOGGER.debug("*** EmitConv3dInstance::emit")
+    _LOGGER.debug("***   operation: procedural_name()=" + operation.procedural_name())
+
+    if hasattr(operation, 'is_3x') and operation.is_3x:
+      _LOGGER.debug("***   CUTLASS 3 operation")
+      return self.conv3x_emitter.emit(operation)
+
+    _LOGGER.debug("***   CUTLASS 2 operation")
 
     warp_shape = [int(operation.tile_description.threadblock_shape[idx] / operation.tile_description.warp_count[idx]) for idx in range(3)]
 
     epilogue_vector_length = int(min(operation.C.alignment * DataTypeSize[operation.C.element], 128) / DataTypeSize[operation.C.element])
 
     values = {
       'operation_name': operation.procedural_name(),
@@ -241,120 +256,227 @@
         for output_type in output_types:
           A = TensorDescription(tile.math_instruction.element_a, LayoutType.TensorNDHWC, int(align / DataTypeSize[tile.math_instruction.element_a]))
           B = TensorDescription(tile.math_instruction.element_b, LayoutType.TensorNDHWC, int(align / DataTypeSize[tile.math_instruction.element_b]))
           C = TensorDescription(output_type,  LayoutType.TensorNDHWC, max(1, int(align / DataTypeSize[output_type])))
 
           manifest.append(Conv3dOperation(conv_kind, min_cc, tile, A, B, C, tile.math_instruction.element_accumulator))
 
+class EmitConv3dIncludes:
+  '''Emit includes that are specific to the operation.'''
+
+  def __init__(self):
+    self.includes = ['conv3d_operation.h']
+    self.emitter_3x = EmitConv3xIncludes()
+
+  def operation_is_3x(self, operation) -> bool:
+    """Whether operation is a CUTLASS 3 convolution (as opposed to CUTLASS 2)"""
+    return hasattr(operation, 'is_3x') and operation.is_3x
+
+  def emit(self, operation) -> str:
+    if self.operation_is_3x(operation):
+      return self.emitter_3x.emit(operation)
+
+    return '\n'.join(f"#include \"{incl}\"" for incl in self.includes) + \
+      "\n\n///////////////////////////////////////////////////////////////////////////////////////////////////"
+
 ###################################################################################################
 #
 # Emitters functions for all targets
 #
 ###################################################################################################
 
 class EmitConv3dConfigurationLibrary:
   def __init__(self, operation_path, configuration_name):
     self.configuration_name = configuration_name
     self.configuration_path = os.path.join(operation_path, "%s.cu" % configuration_name)
 
     self.instance_emitter = EmitConv3dInstance()
+    self.includes_emitter = EmitConv3dIncludes()
 
-    self.instance_template = """
-${operation_instance}
-
-// Derived class
-struct ${operation_name} :
-  public ${operation_name}_base { };
-
-///////////////////////////////////////////////////////////////////////////////////////////////////
-
-"""
     self.header_template = """
 /*
   Generated by conv3d_operation.py - Do not edit.
 */
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 #include "cutlass/cutlass.h"
 #include "cutlass/library/library.h"
 #include "cutlass/library/manifest.h"
 
 #include "library_internal.h"
-#include "conv3d_operation.h"
+"""
 
+    self.instance_template = """
+${stub_begin}
+${operation_instance}
+// Derived class
+struct ${operation_name} :
+  public ${operation_name}_base { };
+${stub_end}
 ///////////////////////////////////////////////////////////////////////////////////////////////////
+
 """
 
     self.configuration_header = """
 
 namespace cutlass {
 namespace library {
 
 // Initialize all instances
 void initialize_${configuration_name}(Manifest &manifest) {
-
 """
 
-    self.configuration_instance = """
-  using Operation_${operation_name} = cutlass::conv::device::ImplicitGemmConvolution<
+    self.configuration_instance = """${stub_begin}
+  using Operation_${operation_name} = cutlass::conv::device::${kernel_name}<
     ${operation_name}>;
 
-  manifest.append(new cutlass::library::Conv3dOperation<
-    Operation_${operation_name}>(
-      "${operation_name}"));
-
+  manifest.append(new cutlass::library::${operation_wrapper}<
+      Operation_${operation_name}
+    >(
+      "${operation_name}"
+    ));
+${stub_end}
 """
 
-    self.configuration_epilogue = """
-}
-"""
+    self.configuration_epilogue = "}\n"
+
     self.epilogue_template = """
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace library
 } // namespace cutlass
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 """
 
-  #
+  def operation_is_3x(self, operation):
+    """Whether operation is a CUTLASS 3 convolution (as opposed to CUTLASS 2)"""
+    return hasattr(operation, 'is_3x') and operation.is_3x
+
   def __enter__(self):
+    """
+    Open the configuration_file, and write the "header" C++ code to it.
+
+    The "header" consists of a comment (that this is generated code,
+    so it should not be edited), and includes that are common
+    to both the CUTLASS 2 and the CUTLASS 3 cases.
+    """
+    _LOGGER.debug('*** EmitConv3dConfigurationLibrary::__enter__')
+    _LOGGER.debug('***   configuration_path (file to write): ' +
+                  str(self.configuration_path))
+    _LOGGER.debug('***   configuration_name: ' + self.configuration_name)
     self.configuration_file = open(self.configuration_path, "w")
+
     self.configuration_file.write(SubstituteTemplate(self.header_template, {
       'configuration_name': self.configuration_name
       }))
     self.operations = []
     return self
 
-  #
   def emit(self, operation):
+    """
+    Write three pieces of C++ code to the configuration_file
+    (that was opened by the __enter__ method above):
+
+    1. the header includes that are specific to the operation
+       (CUTLASS 2 vs. CUTLASS 3);
+
+    2. the "operation instance" (a "using" declaration ending in "_base"); and
+
+    3. the "operation name" (declaration and definition of a derived class
+       of the above operation instance).
+
+    The "using" declaration turns a C++ class name, possibly namespace-qualified,
+    possibly also with angle brackets, into a C-style, easily demangled identifier.
+    """
+    _LOGGER.debug('*** EmitConv3dConfigurationLibrary::emit')
+    _LOGGER.debug('***   operation.procedural_name(): ' + operation.procedural_name())
     self.operations.append(operation)
-    self.configuration_file.write(SubstituteTemplate(self.instance_template, {
+
+    self.configuration_file.write(self.includes_emitter.emit(operation))
+
+    stub_begin = ''
+    stub_end = ''
+    # It can be useful to stub (comment) out instantiations for testing.
+    # In this case, one need only set is_stub to True.
+    is_stub = False
+    if is_stub:
+      stub_begin = "// STUB for now\n#if 0"
+      stub_end = '#endif // 0'
+
+    self.configuration_file.write(Template(self.instance_template).substitute({
       'configuration_name': self.configuration_name,
       'operation_name': operation.procedural_name(),
-      'operation_instance': self.instance_emitter.emit(operation)
+      'operation_instance': self.instance_emitter.emit(operation),
+      'stub_begin': stub_begin,
+      'stub_end': stub_end
       }))
 
-  #
   def __exit__(self, exception_type, exception_value, traceback):
+    """
+    Write the rest of the C++ code to the configuration_file, and close the file.
+
+    The "rest of the C++ code" has the following components.
+
+    1. Configuration header: Open the namespace(s), and open the definition
+       of the "initialize_${configuration_name}" registration function
+       that registers the operation with the Manifest.
+       ("Registration" helps turn C++ compile-time polymorphism
+       (via template parameters) into a run-time choice of parameters.)
+
+    2. Configuration instance: In the body of the registration function,
+       make a "using" declaration Operation_${operation_name} for the
+       operation type (which uses operation_name as its template argument).
+       Then, tell the manifest about the operation via a "manifest.append" call.
+       The argument of the call is a new instance of
+       "SomethingOperation<Operation_${operation_name}>"
+       (replace Something with a specific name).
+
+    3. Configuration epilogue: Close the definition of the registration function.
+
+    4. Epilogue template: Close the namespace(s).
+    """
+
+    _LOGGER.debug('*** EmitConv3dConfigurationLibrary::__exit__')
+    _LOGGER.debug('***   configuration_path (file to write): ' +
+                  str(self.configuration_path))
+    _LOGGER.debug('***   configuration_name: ' + self.configuration_name)
 
     self.configuration_file.write(SubstituteTemplate(self.configuration_header, {
       'configuration_name': self.configuration_name
       }))
 
     for operation in self.operations:
+      stub_begin = ''
+      stub_end = ''
+      # It can be useful to stub (comment) out instantiations for testing.
+      # In this case, one need only set is_stub to True.
+      is_stub = False
+      if is_stub:
+        stub_begin = "// STUB for now\n#if 0"
+        stub_end = "#endif // 0"
+
+      kernel_name = 'ImplicitGemmConvolution'
+      operation_wrapper = 'Conv3dOperation'
+      if self.operation_is_3x(operation):
+        kernel_name = 'ConvUniversalAdapter'
+        operation_wrapper = 'ConvOperation3x'
+
       self.configuration_file.write(SubstituteTemplate(self.configuration_instance, {
         'configuration_name': self.configuration_name,
-        'operation_name': operation.procedural_name()
+        'operation_name': operation.procedural_name(),
+        'kernel_name': kernel_name,
+        'operation_wrapper': operation_wrapper,
+        'stub_begin': stub_begin,
+        'stub_end': stub_end
       }))
 
     self.configuration_file.write(self.configuration_epilogue)
     self.configuration_file.write(self.epilogue_template)
     self.configuration_file.close()
 
 
 ###################################################################################################
 ###################################################################################################
-
```

## cutlass_library/gemm_operation.py

```diff
@@ -33,26 +33,29 @@
 """
 Utilities for emitting GEMM kernels
 """
 
 import collections
 import enum
 import functools
+import logging
 import operator
 import os.path
 import shutil
 
 try:
   import builtins
   if hasattr(builtins, "CUTLASS_IGNORE_PACKAGE") and CUTLASS_IGNORE_PACKAGE == True:
     raise ImportError("Disabling attempt to import cutlass_library")
   from cutlass_library.library import *
 except ImportError:
   from library import *
 
+_LOGGER = logging.getLogger(__name__)
+
 ###################################################################################################
 #
 # Data structure modeling a GEMM operation
 #
 ###################################################################################################
 
 #
@@ -135,15 +138,16 @@
 
     inst_shape = ''
     inst_operation = ''
     intermediate_type = ''
 
     math_operations_map = {
       MathOperation.xor_popc: 'xor',
-      MathOperation.and_popc: 'and'
+      MathOperation.and_popc: 'and',
+      MathOperation.multiply_add_fast_accum: 'fastaccum',
     }
 
     tensor_ops = [
       OpcodeClass.TensorOp,
       OpcodeClass.WmmaTensorOp,
       OpcodeClass.SparseTensorOp,
     ]
@@ -197,26 +201,26 @@
     return extended_name
 
   def extended_name_3x(self):
     '''Generates a string representing the MMA atom. Assumes accumulator type is C type.'''
     extended_name = "{core_name}_{element_a}_{element_b}_{element_acc}_{element_c}_{element_d}".format(
       element_a = DataTypeNames[self.A.element],
       element_b = DataTypeNames[self.B.element],
-      element_acc = DataTypeNames[self.tile_description.math_instruction.element_accumulator],
+      element_acc = DataTypeNames[self.accumulator_type()],
       element_c = DataTypeNames[self.C.element],
       element_d = DataTypeNames[self.D.element],
       core_name = self.core_name())
     return extended_name
 
   def datatype_name_3x(self):
     '''Generates a string representing the MMA atom. Assumes accumulator type is C type.'''
     datatype_name = "{element_a}_{element_b}_{element_acc}_{element_c}_{element_d}".format(
       element_a = DataTypeNames[self.A.element],
       element_b = DataTypeNames[self.B.element],
-      element_acc = DataTypeNames[self.tile_description.math_instruction.element_accumulator],
+      element_acc = DataTypeNames[self.accumulator_type()],
       element_c = DataTypeNames[self.C.element],
       element_d = DataTypeNames[self.D.element])
     return datatype_name
 
   # Generates a short string representing the AB layout tags (e.g. nt or tn)
   def layout_name(self):
     if self.is_complex() or self.is_planar_complex():
@@ -252,26 +256,22 @@
     return OpcodeClassNames[self.tile_description.math_instruction.opcode_class]
 
   # Generates the full kernel function name
   def procedural_name(self):
     ''' The full procedural name indicates architecture, extended name, tile size, and layout. '''
     opcode_class_name = OpcodeClassNames[self.tile_description.math_instruction.opcode_class]
     if self.arch >= 90:
-      kernel_name_template = "cutlass{p}_sm{ar}_{op}_{ex}_{tbm}x{tbn}x{tbk}_{cm}x{cn}x{ck}_{l}_{s}_align{al}{t}{k}{e}"
+      kernel_name_template = "cutlass{p}_sm{ar}_{op}_{ex}{ct}{cs}_{l}_{s}_align{al}{t}{k}{e}"
       return kernel_name_template.format(
           p = self.prefix,
           ar = self.arch,
           op = opcode_class_name,
           ex = self.extended_name_3x(),
-          tbm = self.tile_description.tile_shape[0],
-          tbn = self.tile_description.tile_shape[1],
-          tbk = self.tile_description.tile_shape[2],
-          cm = self.tile_description.cluster_shape[0],
-          cn = self.tile_description.cluster_shape[1],
-          ck = self.tile_description.cluster_shape[2],
+          ct = '_' + 'x'.join([str(i) for i in self.tile_description.tile_shape]) if self.tile_description.tile_shape[0] > 0 else "",
+          cs = '_' + 'x'.join([str(i) for i in self.tile_description.cluster_shape]),
           l = self.tile_description.stages,
           s = self.layout_name_3x(),
           al = str(max(self.A.alignment, self.B.alignment)),
           t = TileSchedulerSuffixes[self.tile_scheduler],
           k = self.kernel_schedule_name_3x(),
           e = self.epilogue_schedule_name_3x())
     else:
@@ -721,34 +721,34 @@
     >
 """
     self.gemm_template = """
 
 using ${operation_name}_epilogue =
   typename cutlass::epilogue::collective::CollectiveBuilder<
     ${arch}, ${opcode_class_epi},
-    cute::Shape<cute::_${tile_shape_m}, cute::_${tile_shape_n}, cute::_${tile_shape_k}>,
-    cute::Shape<cute::_${cluster_m},cute::_${cluster_n},cute::_${cluster_k}>,
+    cute::Shape<cute::_${tile_shape_epi_m}, cute::_${tile_shape_epi_n}, cute::_${tile_shape_epi_k}>,
+    cute::Shape<${cluster_shape_m}, ${cluster_shape_n}, ${cluster_shape_k}>,
     ${epi_tile_mn},
     ${element_accumulator}, ${element_epilogue},
     ${element_c}, ${layout_c}, ${align_c},
     ${element_d}, ${layout_d}, ${align_d},
     ${epilogue_schedule},
     ${epilogue_functor}
   >::CollectiveOp;
 
 using ${operation_name}_mainloop =
   typename cutlass::gemm::collective::CollectiveBuilder<
     ${arch}, ${opcode_class_main},
     ${element_a}, ${layout_a}, ${align_a},
     ${element_b}, ${layout_b}, ${align_b},
     ${element_accumulator},
-    cute::Shape<cute::_${tile_shape_m}, cute::_${tile_shape_n}, cute::_${tile_shape_k}>,
-    cute::Shape<cute::_${cluster_m},cute::_${cluster_n},cute::_${cluster_k}>,
+    cute::Shape<cute::_${tile_shape_main_m}, cute::_${tile_shape_main_n}, cute::_${tile_shape_main_k}>,
+    cute::Shape<${cluster_shape_m}, ${cluster_shape_n}, ${cluster_shape_k}>,
     ${stages},
-  ${kernel_schedule}
+    ${kernel_schedule}
   >::CollectiveOp;
 
 // Gemm operator ${operation_name}
 using ${operation_name}_base = cutlass::gemm::kernel::GemmUniversal<
     cute::Shape<int,int,int,int>,
     ${operation_name}_mainloop,
     ${operation_name}_epilogue,
@@ -769,27 +769,41 @@
       new ${gemm_kind}<GemmKernel>("${operation_name}"));
   }
 ${compile_guard_end}
 """
 
   #
   def emit(self, operation):
+    _LOGGER.debug("*** EmitGemmConfigurationLibrary::emit(operation)")
+    _LOGGER.debug("***   operation.procedural_name(): " + operation.procedural_name())
+    _LOGGER.debug("***   tile_shape: " + str(operation.tile_description.tile_shape))
+    _LOGGER.debug("***   warp_count: " + str(operation.tile_description.warp_count))
+
+    opcode_class_main = operation.tile_description.math_instruction.opcode_class
+    opcode_class_epi = opcode_class_main
 
     tile_shape = operation.tile_description.tile_shape
-    warp_count = operation.tile_description.warp_count
+    instruction_shape = operation.tile_description.math_instruction.instruction_shape
+    cluster_m = operation.tile_description.cluster_shape[0]
+    cluster_n = operation.tile_description.cluster_shape[1]
+
+    tile_shape_main_m, tile_shape_main_n, tile_shape_main_k = tile_shape
+    tile_shape_epi_m, tile_shape_epi_n, tile_shape_epi_k = tile_shape
+
+    # account for static/dynamic cluster shapes
+    cta_m = tile_shape[0] // cluster_m if cluster_m > 0 else tile_shape[0]
+    cta_n = tile_shape[1] // cluster_n if cluster_n > 0 else tile_shape[1]
+
     # stage count set to zero indicates builder automatic stage selection
     if operation.tile_description.stages > 0:
       stage_count_string = f"cutlass::gemm::collective::StageCount<{str(operation.tile_description.stages)}>"
     else:
-      stage_count_string = f"cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename {str(operation.procedural_name())}_epilogue::SharedStorage)>"
-    warp_shape = [tile_shape[idx] // warp_count[idx] for idx in range(3)]
+      stage_count_string = f"cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(sizeof(typename {str(operation.procedural_name())}_epilogue::SharedStorage))>"
 
     epi_tile_mn = "cutlass::epilogue::collective::EpilogueTileAuto"
-    opcode_class_main = operation.tile_description.math_instruction.opcode_class
-    opcode_class_epi = opcode_class_main
 
     instance_layout_A, instance_layout_B, instance_layout_C , instance_layout_D = \
       (operation.A.layout, operation.B.layout, operation.C.layout, operation.D.layout)
 
     # 3.0 profiler integration only supports trivial epilogues for now
     epilogue_vector_length = 1
 
@@ -799,19 +813,17 @@
         'element_epilogue': str(DataTypeTag[operation.element_epilogue]),
         'epilogue_functor': EpilogueFunctor3xTag[operation.epilogue_functor],
       }
       epilogue_functor = SubstituteTemplate(self.builtin_epilogue_functor_template, values)
     else:
       epilogue_functor = self.epilogue_functor.emit_declaration()
     #
-    element_a = DataTypeTag[operation.A.element]
-    element_b = DataTypeTag[operation.B.element]
-    epilogue_schedule_type = EpilogueScheduleTag[operation.epilogue_schedule]
-    element_a = DataTypeTag[operation.A.element]
-    element_b = DataTypeTag[operation.B.element]
+    # Cutlass3x complex kernels' ElementA(B) is a tuple in collective mainloop builder, e.g. cute::tuple<Element, Transform>, Transform : cute::identity / cute::conjugate.
+    element_a = DataTypeTag[operation.A.element] if not operation.is_complex() else f"cute::tuple<{str(DataTypeTag[operation.A.element])},{str(ComplexTransformTag3x[operation.A.complex_transform])}>"
+    element_b = DataTypeTag[operation.B.element] if not operation.is_complex() else f"cute::tuple<{str(DataTypeTag[operation.B.element])},{str(ComplexTransformTag3x[operation.B.complex_transform])}>"
     epilogue_schedule_type = EpilogueScheduleTag[operation.epilogue_schedule]
     values = {
       'operation_name': operation.procedural_name(),
       'operation_suffix': self.operation_suffix,
       'element_a': element_a,
       'layout_a': LayoutTag[instance_layout_A],
       'element_b': element_b,
@@ -820,26 +832,26 @@
       'layout_c': LayoutTag[instance_layout_C],
       'element_d': DataTypeTag[operation.D.element],
       'layout_d': LayoutTag[instance_layout_D],
       'element_accumulator': DataTypeTag[operation.accumulator_type()],
       'opcode_class_main': OpcodeClassTag[opcode_class_main],
       'opcode_class_epi': OpcodeClassTag[opcode_class_epi],
       'arch': "cutlass::arch::Sm%d" % operation.arch,
-      'tile_shape_m': str(operation.tile_description.tile_shape[0]),
-      'tile_shape_n': str(operation.tile_description.tile_shape[1]),
-      'tile_shape_k': str(operation.tile_description.tile_shape[2]),
-      'cluster_m': str(operation.tile_description.cluster_shape[0]),
-      'cluster_n': str(operation.tile_description.cluster_shape[1]),
-      'cluster_k': str(operation.tile_description.cluster_shape[2]),
-      'warp_shape_m': str(warp_shape[0]),
-      'warp_shape_n': str(warp_shape[1]),
-      'warp_shape_k': str(warp_shape[2]),
-      'instruction_shape_m': str(operation.tile_description.math_instruction.instruction_shape[0]),
-      'instruction_shape_n': str(operation.tile_description.math_instruction.instruction_shape[1]),
-      'instruction_shape_k': str(operation.tile_description.math_instruction.instruction_shape[2]),
+      'tile_shape_epi_m': str(tile_shape_epi_m),
+      'tile_shape_epi_n': str(tile_shape_epi_n),
+      'tile_shape_epi_k': str(tile_shape_epi_k),
+      'tile_shape_main_m': str(tile_shape_main_m),
+      'tile_shape_main_n': str(tile_shape_main_n),
+      'tile_shape_main_k': str(tile_shape_main_k),
+      'cluster_shape_m': 'cute::_' + str(operation.tile_description.cluster_shape[0]) if operation.tile_description.cluster_shape[0] > 0 else "int",
+      'cluster_shape_n': 'cute::_' + str(operation.tile_description.cluster_shape[1]) if operation.tile_description.cluster_shape[1] > 0 else "int",
+      'cluster_shape_k': 'cute::_' + str(operation.tile_description.cluster_shape[2]) if operation.tile_description.cluster_shape[2] > 0 else "int",
+      'instruction_shape_m': str(instruction_shape[0]),
+      'instruction_shape_n': str(instruction_shape[1]),
+      'instruction_shape_k': str(instruction_shape[2]),
       'kernel_schedule' : str(KernelScheduleTag[operation.kernel_schedule]),
       'epilogue_schedule' : str(epilogue_schedule_type),
       'epi_tile_mn' : epi_tile_mn,
       'epilogue_functor': epilogue_functor,
       'stages': stage_count_string,
       'align_a': str(operation.A.alignment),
       'align_b': str(operation.B.alignment),
@@ -1223,14 +1235,18 @@
 } // namespace cutlass
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 """
 
   def __enter__(self):
+    _LOGGER.debug("*** EmitGemmConfigurationLibrary::__enter__")
+    _LOGGER.debug("***   configuration_path (file to write): " +
+                  str(self.configuration_path))
+
     self.configuration_file = open(self.configuration_path, "w")
     self.configuration_file.write(self.header_template)
     self.configuration_file.write(self.separator)
 
     self.includes = collections.OrderedDict([
       ("cutlass/cutlass.h", None),
       ("cutlass/library/library.h", None),
@@ -1244,14 +1260,17 @@
     self.instance_definitions = []
     self.instance_wrappers = []
 
     self.operations = []
     return self
 
   def emit(self, operation):
+    _LOGGER.debug("*** EmitGemmConfigurationLibrary::emit(operation)")
+    _LOGGER.debug("***   operation.gemm_kind: " + str(operation.gemm_kind))
+
     emitter = self.instance_emitter[operation.gemm_kind]()
 
     for incl in emitter.includes:
       self.includes[incl] = None
 
     self.operations.append(operation)
 
@@ -1289,8 +1308,7 @@
       self.configuration_file.write(instance_wrapper)
 
     self.configuration_file.write(self.epilogue_template)
     self.configuration_file.close()
 
 ###################################################################################################
 ###################################################################################################
-
```

## cutlass_library/generator.py

```diff
@@ -36,17 +36,30 @@
 
 import argparse
 import enum
 from itertools import product
 import logging
 import os.path
 import shutil
-
 import sys
+import copy
+from typing import Any, Optional, Sequence, Tuple
+
+_LOGGER = logging.getLogger(__name__)
 
+def logging_prefix(indent_level: int = 0) -> str:
+  """String prefix for start of each debug log entry"""
+  prefix = '*** '
+  indent = '  '
+  return f"{prefix}{indent_level * indent}"
+
+def log_debug_line(line: str, indent_level: int = 0) -> None:
+  """Log one line of debug output"""
+  prefix = logging_prefix(indent_level)
+  _LOGGER.debug(prefix + line)
 
 # Certain usecases of cutlass_library nearly always prefer to run as scripts with
 # relative imports, rather than via an installed Python package. An example of this
 # is using CUTLASS's CMake system to generate a library of kernels to be profiled.
 # To make it easy to use these use cases when an existing installation of cutlass_library
 # exists, this global flag can be set to true (via command-line arguments) to ensure
 # that package-based installations are not used.
@@ -788,14 +801,368 @@
                                           group_mode=GroupMode.Depthwise)
 
           manifest.append(new_operation)
           operations.append(new_operation)
 
   return operations
 
+class ConvOperation3x:
+  """All parameters of a CUTLASS 3 convolution operation.
+
+  Unlike CUTLASS 2 convolutions, CUTLASS 3 convolutions do not
+  distinguish between 2-D and 3-D convolutions by kernel class name.
+  Instead, for CUTLASS 3 convolutions, the tensor layouts encode
+  whether the convolution is 2-D or 3-D.  Thus, this class deduces
+  the OperationKind (either Conv2d or Conv3d) from the layouts,
+  rather than taking it as a constructor parameter.
+  """
+  def __init__(self,
+               conv_kind: ConvKind,
+               tile_description: TileDescription,
+               A: TensorDescription,
+               B: TensorDescription,
+               C: TensorDescription,
+               element_compute: Optional[DataType] = None,
+               D: Optional[TensorDescription] = None,
+               kernel_schedule: KernelScheduleType = KernelScheduleType.ScheduleAuto,
+               epilogue_schedule: EpilogueScheduleType = EpilogueScheduleType.ScheduleAuto,
+               tile_scheduler: TileSchedulerType = TileSchedulerType.Default,
+               log_indent_level: int = 1):
+    log_debug_line(f'ConvOperation3x::init: conv_kind: {conv_kind}', log_indent_level)
+    log_indent_level = log_indent_level + 1
+
+    self.conv_kind = conv_kind
+    self.tile_description = tile_description
+    self.A = A
+    self.B = B
+    self.C = C
+    self.element_compute = C.element if element_compute is None else element_compute
+    self.kernel_schedule = kernel_schedule
+    self.epilogue_schedule = epilogue_schedule
+
+    self.arch = tile_description.minimum_compute_capability
+    self.tile_scheduler = tile_scheduler
+    if D == None:
+      self.D = C
+    else:
+      self.D = D
+
+    self.is_3x = True
+    self.group_mode = GroupMode.NoneGroup # CUTLASS 3 convolutions currently aren't grouped
+
+    operation_kind = None
+    for layout in (A.layout, B.layout, C.layout):
+      assert(isinstance(layout, LayoutType))
+      new_operation_kind = convolution_tensor_layout_type_to_operation_kind(layout)
+      if operation_kind is None:
+        operation_kind = new_operation_kind
+      else: # CUTLASS 3 convolutions don't permit mixing 2-D and 3-D layouts.
+        assert(operation_kind == new_operation_kind)
+    assert(operation_kind is not None)
+    self.operation_kind = operation_kind
+
+  def __str__(self):
+    return f"ConvOperation3x: operation_kind={self.operation_kind}, conv_kind={self.conv_kind}, tile_description={self.tile_description}"
+
+  def is_complex(self):
+    complex_operators = [
+      MathOperation.multiply_add_complex,
+      MathOperation.multiply_add_complex_gaussian,
+      MathOperation.multiply_add_complex_fast_f32
+    ]
+    return self.tile_description.math_instruction.math_operation in complex_operators
+
+  def is_mixed_input(self):
+    return self.A.element != self.B.element
+
+  def accumulator_type(self):
+    accum = self.tile_description.math_instruction.element_accumulator
+    if self.is_complex():
+      return get_complex_from_real(accum)
+    return accum
+
+  def short_math_name(self):
+    prefix = ''
+    if self.tile_description.math_instruction.math_operation == MathOperation.multiply_add_complex_gaussian:
+      prefix = 'g'
+    return prefix + ShortDataTypeNames[self.accumulator_type()]
+
+  def is_tensor_op(self):
+    tensor_ops = [
+      OpcodeClass.TensorOp,
+      OpcodeClass.WmmaTensorOp
+    ]
+    return self.tile_description.math_instruction.opcode_class in tensor_ops
+
+  def instruction_shape_string(self):
+    math_operations_map = {
+      MathOperation.xor_popc: 'xor',
+      MathOperation.and_popc: 'and'
+    }
+    if self.is_tensor_op():
+      is0, is1, is2 = self.tile_description.math_instruction.instruction_shape
+      math_op = self.tile_description.math_instruction.math_operation
+      math_op_string = math_operations_map[math_op] if math_op in math_operations_map.keys() else ''
+      return f"{is0}x{is1}x{is2}{math_op_string}"
+    else:
+      return ''
+
+  def intermediate_type_string(self):
+    '''
+    Name of the distinct intermediate type used by the tensor operation,
+    or the empty string if none.
+
+    Tensor ops (opcode_clas *TensorOp) may use an intermediate data type
+    that differs from the element type of A or the accumulator type.
+    '''
+    if not self.is_tensor_op():
+      return ''
+    elif self.tile_description.math_instruction.element_a == self.A.element:
+      return ''
+    elif self.tile_description.math_instruction.element_a == self.tile_description.math_instruction.element_accumulator:
+      return ''
+    else:
+      return DataTypeNames[self.tile_description.math_instruction.element_a]
+
+  def core_name(self):
+    inst_shape = self.instruction_shape_string()
+    intermediate_type = self.intermediate_type_string()
+    conv_kind_name = ConvKindNames[self.conv_kind]
+    return f"{self.short_math_name()}{inst_shape}{intermediate_type}{conv_kind_name}"
+
+  def extended_name(self):
+    core_name = self.core_name()
+    element_a = DataTypeNames[self.A.element]
+    element_b = DataTypeNames[self.B.element]
+    element_acc = DataTypeNames[self.tile_description.math_instruction.element_accumulator]
+    element_c = DataTypeNames[self.C.element]
+    element_d = DataTypeNames[self.D.element]
+    return f"{core_name}_{element_a}_{element_b}_{element_acc}_{element_c}_{element_d}"
+
+  def is_complex(self):
+    complex_operators = [
+      MathOperation.multiply_add_complex,
+      MathOperation.multiply_add_complex_gaussian,
+      MathOperation.multiply_add_complex_fast_f32
+    ]
+    return self.tile_description.math_instruction.math_operation in complex_operators
+
+  def layout_names(self):
+    '''Layout strings for A and B, respectively'''
+    if self.is_complex():
+      return (ShortComplexLayoutNames[(self.A.layout, self.A.complex_transform)],
+              ShortComplexLayoutNames[(self.B.layout, self.B.complex_transform)])
+    else:
+      return (ShortLayoutTypeNames[self.A.layout],
+              ShortLayoutTypeNames[self.B.layout])
+
+  def extended_name(self):
+    core_name = self.core_name()
+    element_a = DataTypeNames[self.A.element]
+    element_b = DataTypeNames[self.B.element]
+    element_acc = DataTypeNames[self.tile_description.math_instruction.element_accumulator]
+    element_c = DataTypeNames[self.C.element]
+    element_d = DataTypeNames[self.D.element]
+    layout_a, layout_b = self.layout_names()
+    return f"{core_name}_{element_a}{layout_a}_{element_b}{layout_b}_{element_acc}_{element_c}_{element_d}"
+
+  def configuration_name(self):
+    prefix = 'cutlass3x'
+    arch = self.arch
+    opcode_class_name = OpcodeClassNames[self.tile_description.math_instruction.opcode_class]
+    tbm = self.tile_description.tile_shape[0]
+    tbn = self.tile_description.tile_shape[1]
+    tbk = self.tile_description.tile_shape[2]
+    cm = self.tile_description.cluster_shape[0]
+    cn = self.tile_description.cluster_shape[1]
+    ck = self.tile_description.cluster_shape[2]
+    alignment = max(self.A.alignment, self.B.alignment)
+    tile_scheduler = TileSchedulerSuffixes[self.tile_scheduler]
+    kernel_schedule = KernelScheduleSuffixes[self.kernel_schedule]
+    epilogue_schedule = EpilogueScheduleSuffixes[self.epilogue_schedule]
+
+    return f"{prefix}_sm{arch}_{opcode_class_name}_{self.extended_name()}_{tbm}x{tbn}x{tbk}_{cm}x{cn}x{ck}_{self.tile_description.stages}_align{alignment}{tile_scheduler}{kernel_schedule}{epilogue_schedule}"
+
+  def procedural_name(self):
+    return self.configuration_name()
+
+def convolution_tensor_layout_type_to_operation_kind(layout: LayoutType) -> OperationKind:
+  if layout == LayoutType.TensorNHWC or layout == LayoutType.TensorKCSR:
+    return OperationKind.Conv2d
+  elif layout == LayoutType.TensorNDHWC or layout == LayoutType.TensorKCSRT:
+    return OperationKind.Conv3d
+  else:
+    raise RuntimeError(f'LayoutType {layout} does not have a corresponding OperationKind')
+
+def CreateConvOperator3x(manifest: Manifest,
+                         dims_and_alignments: Sequence[Tuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]]],
+                         tile_descriptions: Sequence[Sequence[TileDescription]],
+                         data_types,
+                         schedule_pairs: Sequence[Tuple[KernelScheduleType, KernelScheduleType]] = \
+                           [(KernelScheduleType.ScheduleAuto, EpilogueScheduleType.ScheduleAuto)],
+                         complex_transforms: Optional[Sequence[ComplexTransform]] = None,
+                         tile_schedulers: Sequence[TileSchedulerType] = [TileSchedulerType.Persistent],
+                         conv_kind: ConvKind = ConvKind.Fprop,
+                         log_indent_level: int = 1):
+  """
+  Create zero or more CUTLASS 3 two-dimensional convolution operators.
+
+  Create a CUTLASS 3 two-dimensional convolution operator
+  for all feasible combinations of the input parameters.
+  Add the operators to the manifest.
+
+  dims_and_alignments: 3-level list.  Each outer list term is a list [A, B, C].
+    Each inner list (A, B, or C) has the form [num_spatial_dimensions, alignment].
+    Both are integers; the first is the number of spatial dimensions
+    (currently, only 2 or 3 are supported), and the second is the byte alignment.
+    We deduce the operation_kind (either OperationKind.Conv2d or OperationKind.Conv3d)
+    from num_spatial_dimensions.
+
+  This function doesn't take layouts, unlike the GEMM functions.
+  CUTLASS 3 convolutions currently support three input layouts:
+
+  * TensorNWC for 1-D convolutions,
+  * TensorNHWC for 2-D convolutions, and
+  * TensorNDHWC for 3-D convolutions.
+
+  Output (C and D) layouts are the same as input layouts,
+  except for Wgrad convolutions, where the layouts are
+
+  * TensorKCS for 1-D convolutions,
+  * TensorKCSR for 2-D convolutions, and
+  * TensorKCSRT for 3-D convolutions.
+
+  The output layouts are completely constrained by the input layouts
+  and the convolution kind.
+
+  tile_descriptions: 2-level list.
+    Outer level has one list per math instruction.
+    Inner level has one TileDescription for each cluster shape.
+
+  data_types: Either a single data_type dictionary, or a list of them.
+    Keys: 'a_type', 'b_type', 'c_type', 'd_type', 'acc_type', 'epi_type'
+
+  complex_transforms: Optional list of pairs.
+    First element of each pair is the complex transform for A, and
+    second element of each pair is the complex transform for B.
+
+  schedule_pairs: [(kernel_schedule, epilogue_schedule), ...]
+
+  conv_kind: Convolution kind (Fprop, Dgrad, or Wgrad).
+  """
+  log_debug_line('CreateConvOperator3x', log_indent_level)
+  log_indent_level = log_indent_level + 1
+  log_debug_line(f'conv_kind: {conv_kind}', log_indent_level)
+
+  for triple in dims_and_alignments:
+    spatial_dimensionality = None # to be determined by loop below
+    assert(len(triple) == 3)
+    for entry in triple: # [A, B, C]
+      assert(len(entry) == 2)
+      [dim, alignment] = entry
+      assert(type(dim) is int)
+      assert(dim == 2 or dim == 3)
+      assert(type(alignment) is int)
+      assert(alignment > 0)
+      if spatial_dimensionality is None:
+        spatial_dimensionality = dim
+      else:
+        # A, B, and C need to have the same spatial dimensionality
+        assert(spatial_dimensionality == dim)
+
+  def input_and_output_layouts(spatial_dim: int, kind: ConvKind) -> Tuple[LayoutType, LayoutType]:
+    if spatial_dim == 1:
+      input_layout = LayoutType.TensorNWC
+      if kind == ConvKind.Wgrad:
+        output_layout = LayoutType.TensorKCS
+      else:
+        output_layout = input_layout
+    elif spatial_dim == 2:
+      input_layout = LayoutType.TensorNHWC
+      if kind == ConvKind.Wgrad:
+        output_layout = LayoutType.TensorKCSR
+      else:
+        output_layout = input_layout
+    elif spatial_dim == 3:
+      input_layout = LayoutType.TensorNDHWC
+      if kind == ConvKind.Wgrad:
+        output_layout = LayoutType.TensorKCSRT
+      else:
+        output_layout = input_layout
+    else:
+      assert(False)
+    return (input_layout, output_layout)
+
+  def dims_to_layouts(A_B_C: Tuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]]) -> \
+      Tuple[Tuple[LayoutType, int], Tuple[LayoutType, int], Tuple[LayoutType, int]]:
+    [A, B, C] = A_B_C
+    [spatial_dim, alignment] = A
+    [input_layout, output_layout] = input_and_output_layouts(spatial_dim, conv_kind)
+    return ((input_layout, A[1]),
+            (input_layout, B[1]),
+            (output_layout, C[1]))
+
+  # layouts: list of triples (A, B, C).
+  # Each of A, B, and C has the form [layout, alignment].
+  layouts = [dims_to_layouts(A_B_C) for A_B_C in dims_and_alignments]
+
+  if type(data_types) is dict:
+    data_types = [data_types]
+
+  for s in schedule_pairs:
+    assert(len(s) == 2)
+
+  if complex_transforms is None:
+    complex_transforms = [(ComplexTransform.none, ComplexTransform.none)]
+
+  # product produces a one-pass generator, so the loop must call it anew each time.
+  def make_combinations():
+    return product(
+      layouts,
+      tile_descriptions,
+      data_types,
+      complex_transforms,
+      schedule_pairs,
+      tile_schedulers
+    )
+
+  operations = []
+  for layout_triple, tile_description, data_type, complex_transform_pair, schedule_pair, tile_scheduler in make_combinations():
+    A_layout, A_alignment = layout_triple[0]
+    A_xform = complex_transform_pair[0]
+    B_layout, B_alignment = layout_triple[1]
+    B_xform = complex_transform_pair[1]
+    C_layout, C_alignment = layout_triple[2]
+    D_layout = C_layout
+    D_alignment = C_alignment
+
+    A = TensorDescription(data_type["a_type"], A_layout, A_alignment, A_xform)
+    B = TensorDescription(data_type["b_type"], B_layout, B_alignment, B_xform)
+    C = TensorDescription(data_type["c_type"], C_layout, C_alignment)
+    D = TensorDescription(data_type["d_type"], D_layout, D_alignment)
+    element_compute = data_type.get("epi_type", data_type["acc_type"])
+    kernel_schedule, epilogue_schedule = schedule_pair
+
+    operation = ConvOperation3x(conv_kind=conv_kind,
+                                tile_description=tile_description,
+                                A=A,
+                                B=B,
+                                C=C,
+                                element_compute=element_compute,
+                                D=D,
+                                kernel_schedule=kernel_schedule,
+                                epilogue_schedule=epilogue_schedule,
+                                tile_scheduler=tile_scheduler,
+                                log_indent_level=log_indent_level)
+    log_debug_line(f'Created ConvOperation3x: {str(operation)}', log_indent_level)
+    manifest.append(operation)
+    operations.append(operation)
+
+  return operations
+
 ###################################################################################################
 ###################################################################################################
 
 #
 def GenerateSM50_Simt(manifest, cuda_version):
   layouts = [
     (LayoutType.ColumnMajor, LayoutType.ColumnMajor, LayoutType.ColumnMajor),
@@ -2229,16 +2596,16 @@
       OpcodeClass.TensorOp,                           \
       MathOperation.multiply_add_mixed_input_upcast),
   ]
 
   min_cc = 80
   max_cc = 1024
 
-  # For mixed-input alignment constraints are a list of lists, where the 
-  # inner list contains the alignment constraints for operands/matrices 
+  # For mixed-input alignment constraints are a list of lists, where the
+  # inner list contains the alignment constraints for operands/matrices
   # [[alignA, alignB, alignC],..]
   alignment_constraints = [[16, 8, 8],]
 
   for math_inst in math_instructions:
     tile_descriptions = [
       # 128x128
       TileDescription([128, 128, 64],  4, [2, 2, 1], math_inst, min_cc, max_cc),
@@ -2273,15 +2640,15 @@
         math_inst.element_a,
         math_inst.element_b,
         math_inst.element_b,
         math_inst.element_accumulator,
       ]
 
       operations += CreateGemmOperator(manifest, layouts, tile_descriptions, \
-        data_type_mixed, alignment_constraints, None, EpilogueFunctor.LinearCombination, SwizzlingFunctor.Identity8) 
+        data_type_mixed, alignment_constraints, None, EpilogueFunctor.LinearCombination, SwizzlingFunctor.Identity8)
 
       for op in operations:
         if (DataTypeSize[op.C.element] == 16) and \
            (op.tile_description.threadblock_shape[1] <= 32):
           op.C.alignment = 4
 
 #
@@ -2316,16 +2683,16 @@
       OpcodeClass.TensorOp,                           \
       MathOperation.multiply_add_mixed_input_upcast),
   ]
 
   min_cc = 80
   max_cc = 1024
 
-  # For mixed-input alignment constraints are a list of lists, where the 
-  # inner list contains the alignment constraints for operands/matrices 
+  # For mixed-input alignment constraints are a list of lists, where the
+  # inner list contains the alignment constraints for operands/matrices
   # [[alignA, alignB, alignC],..]
   alignment_constraints = [[8, 16, 8],]
 
   for math_inst in math_instructions:
     tile_descriptions = [
       # 128x128
       TileDescription([128, 128, 64],  4, [2, 2, 1], math_inst, min_cc, max_cc),
@@ -2342,16 +2709,16 @@
       # 128x16
       TileDescription([128, 16, 64],  5, [2, 1, 1], math_inst, min_cc, max_cc),
       TileDescription([128, 16, 64],  3, [2, 1, 1], math_inst, min_cc, max_cc),
       TileDescription([128, 16, 32],  9, [2, 1, 1], math_inst, min_cc, max_cc),
       TileDescription([128, 16, 32],  5, [2, 1, 1], math_inst, min_cc, max_cc),
       TileDescription([128, 16, 32],  3, [2, 1, 1], math_inst, min_cc, max_cc),
       # 256x16
-      TileDescription([256, 16, 32],  5, [2, 1, 1], math_inst, min_cc, max_cc), 
-      TileDescription([256, 16, 32],  3, [2, 1, 1], math_inst, min_cc, max_cc), 
+      TileDescription([256, 16, 32],  5, [2, 1, 1], math_inst, min_cc, max_cc),
+      TileDescription([256, 16, 32],  3, [2, 1, 1], math_inst, min_cc, max_cc),
     ]
 
     data_type = [
       math_inst.element_a,
       math_inst.element_b,
       math_inst.element_accumulator,
       math_inst.element_accumulator,
@@ -2368,15 +2735,15 @@
         math_inst.element_a,
         math_inst.element_b,
         math_inst.element_a,
         math_inst.element_accumulator,
       ]
 
       operations = CreateGemmOperator(manifest, layouts, tile_descriptions, \
-        data_type_mixed, alignment_constraints, None, EpilogueFunctor.LinearCombination, SwizzlingFunctor.Identity8) 
+        data_type_mixed, alignment_constraints, None, EpilogueFunctor.LinearCombination, SwizzlingFunctor.Identity8)
 
       for op in operations:
         if op.tile_description.threadblock_shape[1] <= 32:
           op.C.alignment = 4
 
 #
 def GenerateSM80_TensorOp_16832_TN(manifest, cuda_version):
@@ -4322,14 +4689,249 @@
   GenerateSM80_TensorOp_168256(manifest, cuda_version)
   GenerateSM80_Simt_f32(manifest, cuda_version)
   GenerateSM80_Simt_f64(manifest, cuda_version)
   GenerateSM80_Simt_complex(manifest, cuda_version)
 
 ###################################################################################################
 
+def GenerateSM89_TensorOp_16832_fp8(manifest, cuda_version):
+  if (
+    not CudaToolkitVersionSatisfies(cuda_version, 12, 4)
+  ):
+    return
+
+  layouts = [
+    (LayoutType.RowMajor, LayoutType.ColumnMajor, LayoutType.ColumnMajor)
+  ]
+
+  math_instructions = [
+    MathInstruction(
+      [16, 8, 32],
+      DataType.e4m3, DataType.e4m3, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
+    MathInstruction(
+      [16, 8, 32],
+      DataType.e4m3, DataType.e5m2, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
+    MathInstruction(
+      [16, 8, 32],
+      DataType.e5m2, DataType.e4m3, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
+    MathInstruction(
+      [16, 8, 32],
+      DataType.e5m2, DataType.e5m2, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
+    MathInstruction(
+      [16, 8, 32],
+      DataType.e4m3, DataType.e4m3, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add_fast_accum),
+    MathInstruction(
+      [16, 8, 32],
+      DataType.e4m3, DataType.e5m2, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add_fast_accum),
+    MathInstruction(
+      [16, 8, 32],
+      DataType.e5m2, DataType.e4m3, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add_fast_accum),
+    MathInstruction(
+      [16, 8, 32],
+      DataType.e5m2, DataType.e5m2, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add_fast_accum),
+  ]
+
+  min_cc = 89
+  max_cc = 89
+
+  alignment_constraints = [16,]
+  alignment_constraints_small_channels = [16, 8, 4]
+
+  for math_inst in math_instructions:
+    tile_descriptions = [
+      TileDescription([256, 128,  64],  3, [4, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128, 256,  64],  3, [2, 4, 1], math_inst, min_cc, max_cc),
+      TileDescription([256, 128,  64],  6, [4, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128, 256,  64],  6, [2, 4, 1], math_inst, min_cc, max_cc),
+      TileDescription([256,  64,  64],  3, [4, 1, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64, 256,  64],  3, [1, 4, 1], math_inst, min_cc, max_cc),
+      TileDescription([256,  64,  64],  4, [4, 1, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64, 256,  64],  4, [1, 4, 1], math_inst, min_cc, max_cc),
+      TileDescription([256,  32,  64],  4, [4, 1, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 32, 256,  64],  4, [1, 4, 1], math_inst, min_cc, max_cc),
+      TileDescription([128, 128,  64],  3, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128, 128,  64],  4, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128, 128,  64],  5, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128, 128,  64],  6, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128,  64,  64],  6, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64, 128,  64],  6, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128,  32,  64],  6, [4, 1, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 32, 128,  64],  6, [1, 4, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64,  64,  64],  6, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64,  64,  64], 10, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([256, 128, 128],  3, [4, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128, 256, 128],  3, [2, 4, 1], math_inst, min_cc, max_cc),
+      TileDescription([256,  64, 128],  4, [4, 1, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64, 256, 128],  4, [1, 4, 1], math_inst, min_cc, max_cc),
+      TileDescription([256,  32, 128],  4, [4, 1, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 32, 256, 128],  4, [1, 4, 1], math_inst, min_cc, max_cc),
+      TileDescription([128, 128, 128],  3, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128, 128, 128],  4, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128, 128, 128],  5, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128,  64, 128],  3, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64, 128, 128],  3, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128,  64, 128],  4, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64, 128, 128],  4, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128,  32, 128],  4, [4, 1, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 32, 128, 128],  4, [1, 4, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64,  64, 128],  5, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64,  64, 128],  6, [2, 2, 1], math_inst, min_cc, max_cc),
+    ]
+
+    data_types = [
+      [
+        math_inst.element_a,
+        math_inst.element_b,
+        DataType.f32,
+        math_inst.element_accumulator
+      ],
+    ]
+
+    operations = []
+    for data_type in data_types:
+      operations += CreateGemmOperator(manifest, layouts, tile_descriptions, data_type,
+        alignment_constraints, None, EpilogueFunctor.LinearCombination)
+
+      conv_layout = (LayoutType.TensorNHWC, LayoutType.TensorNHWC, LayoutType.TensorNHWC)
+      operations += CreateConv2dOperator(manifest, conv_layout, tile_descriptions,
+        data_type, alignment_constraints, [ConvKind.Fprop], EpilogueFunctor.LinearCombination)
+
+      operations += CreateConv2dFixedChannelsOperator(manifest, conv_layout, tile_descriptions,
+        data_type, alignment_constraints_small_channels, [ConvKind.Fprop], EpilogueFunctor.LinearCombination)
+
+    for op in operations:
+      if op.tile_description.threadblock_shape[1] >= 128:
+        if op.tile_description.threadblock_shape[0] == 32:
+          op.C.alignment = 8
+        else:
+          op.C.alignment = 16
+      else:
+        op.C.alignment = 8
+
+#
+def GenerateSM89_SparseTensorOp_16864_fp8(manifest, cuda_version):
+
+  if (
+    not CudaToolkitVersionSatisfies(cuda_version, 12, 4)
+  ):
+    return
+
+  layouts = [
+    (LayoutType.RowMajor, LayoutType.ColumnMajor, LayoutType.RowMajor)
+  ]
+
+  math_instructions = [
+    MathInstruction(
+      [16, 8, 64],
+      DataType.e4m3, DataType.e4m3, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
+    MathInstruction(
+      [16, 8, 64],
+      DataType.e4m3, DataType.e5m2, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
+    MathInstruction(
+      [16, 8, 64],
+      DataType.e5m2, DataType.e4m3, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
+    MathInstruction(
+      [16, 8, 64],
+      DataType.e5m2, DataType.e5m2, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add),
+    MathInstruction(
+      [16, 8, 64],
+      DataType.e4m3, DataType.e4m3, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add_fast_accum),
+    MathInstruction(
+      [16, 8, 64],
+      DataType.e4m3, DataType.e5m2, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add_fast_accum),
+    MathInstruction(
+      [16, 8, 64],
+      DataType.e5m2, DataType.e4m3, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add_fast_accum),
+    MathInstruction(
+      [16, 8, 64],
+      DataType.e5m2, DataType.e5m2, DataType.f32,
+      OpcodeClass.TensorOp,
+      MathOperation.multiply_add_fast_accum),
+  ]
+
+  min_cc = 89
+  max_cc = 89
+
+  alignment_constraints = [16,]
+
+  for math_inst in math_instructions:
+    tile_descriptions = [
+      TileDescription([128,  64, 128],  3, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([256, 128, 128],  3, [4, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128, 256, 128],  3, [2, 4, 1], math_inst, min_cc, max_cc),
+      TileDescription([128, 128, 128],  3, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([256,  64, 128],  3, [4, 1, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64, 256, 128],  4, [1, 4, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64, 128, 128],  6, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64,  64, 128],  4, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128, 128, 256],  3, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([128,  64, 256],  4, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64, 128, 256],  3, [2, 2, 1], math_inst, min_cc, max_cc),
+      TileDescription([ 64,  64, 256],  3, [2, 2, 1], math_inst, min_cc, max_cc),
+    ]
+
+    data_types = [
+      [
+        math_inst.element_a,
+        math_inst.element_b,
+        DataType.f32,
+        math_inst.element_accumulator
+      ],
+    ]
+
+    operations = []
+    for data_type in data_types:
+      operations += CreateSparseGemmOperator(manifest, layouts, tile_descriptions, data_type,
+        alignment_constraints, None, EpilogueFunctor.LinearCombination)
+
+    for op in operations:
+      if op.tile_description.threadblock_shape[1] >= 128:
+        op.C.alignment = 16
+      else:
+        op.C.alignment = 8
+
+###################################################################################################
+
+#
+def GenerateSM89(manifest, cuda_version):
+  GenerateSM89_TensorOp_16832_fp8(manifest, cuda_version)
+  GenerateSM89_SparseTensorOp_16864_fp8(manifest, cuda_version)
+
+###################################################################################################
+
 #
 def GenerateSM90_TensorOp_16b_WGMMA_gemm(manifest, cuda_version):
   if not CudaToolkitVersionSatisfies(cuda_version, 12, 0):
     return
 
   # layouts for ABC and their alignments.
   layouts = [
@@ -4786,23 +5388,23 @@
   ]
 
   math_inst = MathInstruction(
       [64, 128, 8],
       DataType.tf32, DataType.tf32, DataType.f32,
       OpcodeClass.TensorOp,
       MathOperation.multiply_add)
-  
+
   min_cc = 90
   max_cc = 90
 
   tile_descriptions_medium = [
     TileDescription([math_inst.instruction_shape[0]*2, math_inst.instruction_shape[1], math_inst.instruction_shape[2]*4],
       0, [4, 1, 1], math_inst, min_cc, max_cc, [1,1,1])
   ]
- 
+
   tile_descriptions_small = [
     # TileDescription([math_inst.instruction_shape[0], math_inst.instruction_shape[1], math_inst.instruction_shape[2]*4],
     #   0, [4, 1, 1], math_inst, min_cc, max_cc, [1,1,1])
   ]
 
   tile_descriptions = tile_descriptions_medium + tile_descriptions_small
 
@@ -5391,15 +5993,15 @@
     else:
       schedules = [
         # [KernelScheduleType.ScheduleAuto, EpilogueScheduleType.ScheduleAuto],
         [KernelScheduleType.CpAsyncWarpSpecialized, EpilogueScheduleType.NoSmemWarpSpecialized]
       ]
       stream_k_schedules = []
 
-    
+
     for data_type in data_types:
       # With No-SMEM epilogues
       CreateGemmUniversal3xOperator(manifest, layouts, tile_descriptions, data_type, schedules)
 
       if CudaToolkitVersionSatisfies(cuda_version, 12, 1):
         # Persistent kernels with TMA epilogues
         # CreateGemmUniversal3xOperator(manifest, layouts, tile_descriptions, data_type,
@@ -6009,15 +6611,110 @@
   # HEMM computation
   CreateSymmOperator(manifest, layouts, side_modes, fill_modes, tile_descriptions, \
     data_type, alignment_constraints, BlasMode.hermitian)
 #
 
 ###################################################################################################
 
-#
+def GenerateSM90_Conv3x(manifest, cuda_version,
+                        log_indent_level: int = 0):
+  """
+  Generate CUTLASS 3 convolution kernel(s) for SM90.
+
+  This is meant to be called from GenerateSM90.
+  """
+  log_debug_line('GenerateSM90_Conv3x', log_indent_level)
+  log_indent_level = log_indent_level + 1
+
+  if not CudaToolkitVersionSatisfies(cuda_version, 12, 0):
+    return
+
+  minimum_compute_capability = 90
+  maximum_compute_capability = 90
+
+  spatial_dims = [2, 3]
+
+  def make_dims_and_alignments_triple(dim: int):
+    byte_alignment_required_by_tma = 16
+    return ((dim, byte_alignment_required_by_tma), # A
+            (dim, byte_alignment_required_by_tma), # B
+            (dim, byte_alignment_required_by_tma)) # C
+  dims_and_alignments = [make_dims_and_alignments_triple(dim) for dim in spatial_dims]
+
+  def make_math_instruction(data_types: Tuple[DataType, DataType, DataType],
+                            instruction_shape: Tuple[int, int, int]) -> MathInstruction:
+    default_opcode = OpcodeClass.TensorOp
+    default_math_op = MathOperation.multiply_add
+    [A_data_type, B_data_type, C_data_type] = data_types
+    return MathInstruction(
+      instruction_shape,
+      A_data_type, B_data_type, C_data_type,
+      default_opcode,
+      default_math_op
+    )
+  data_types_and_instruction_shapes = [
+    ((DataType.f16, DataType.f16, DataType.f16),   (64, 64, 16)),
+    ((DataType.f16, DataType.f16, DataType.f32),   (64, 64, 16)),
+    ((DataType.bf16, DataType.bf16, DataType.f32), (64, 64, 16)),
+  ]
+  math_instructions = map(lambda x: make_math_instruction(*x),
+                          data_types_and_instruction_shapes)
+
+  cluster_shapes = [
+    [2, 1, 1],
+    [1, 1, 1],
+  ]
+  conv_kinds = [
+    ConvKind.Fprop,
+    ConvKind.Dgrad
+  ]
+  mainloop_schedule = KernelScheduleType.ImplicitTmaWarpSpecializedSm90
+  stages = 0 # zero means "deduce the number of stages automatically"
+
+  # tile_descriptions is a 2-level list.
+  # Each inner list is for each cluster shape.
+  for math_inst in math_instructions:
+    tile_descriptions = []
+    for cluster_shape in cluster_shapes:
+      tile_shape = [
+        math_inst.instruction_shape[0],
+        math_inst.instruction_shape[1],
+        math_inst.instruction_shape[2] * 4
+      ]
+      warp_count = [4, 1, 1]
+      tile_description = TileDescription(
+        tile_shape, stages, warp_count, math_inst,
+        minimum_compute_capability, maximum_compute_capability,
+        cluster_shape)
+      tile_descriptions.append(tile_description)
+
+    # It's typical to get the data types from the math instruction.
+    data_type = {
+      "a_type"   : math_inst.element_a,
+      "b_type"   : math_inst.element_b,
+      "c_type"   : math_inst.element_accumulator,
+      "d_type"   : math_inst.element_accumulator,
+      "acc_type" : math_inst.element_accumulator,
+      "epi_type" : math_inst.element_accumulator
+    }
+
+    for conv_kind in conv_kinds:
+      epilogue_schedule = EpilogueScheduleType.TmaWarpSpecialized
+      schedule_pairs = [
+        (mainloop_schedule, epilogue_schedule)
+      ]
+      CreateConvOperator3x(manifest,
+                           dims_and_alignments = dims_and_alignments,
+                           tile_descriptions = tile_descriptions,
+                           data_types = data_type,
+                           schedule_pairs = schedule_pairs,
+                           tile_schedulers = [TileSchedulerType.Default], # -> void
+                           conv_kind = conv_kind,
+                           log_indent_level = log_indent_level)
+
 def GenerateSM90(manifest, cuda_version):
   GenerateSM90_TensorOp_16b_WGMMA_gemm(manifest, cuda_version)
   GenerateSM90_TensorOp_16b_WGMMA_alignx_gemm(manifest, cuda_version)
   GenerateSM90_TensorOp_tf32_WGMMA_gemm(manifest, cuda_version)
   GenerateSM90_TensorOp_tf32_WGMMA_alignx_gemm(manifest, cuda_version)
   GenerateSM90_TensorOp_int8_WGMMA_gemm(manifest, cuda_version)
   GenerateSM90_TensorOp_int8_WGMMA_alignx_gemm(manifest, cuda_version)
@@ -6031,14 +6728,15 @@
   GenerateSM90_TensorOp_1684_rank_k_complex_gaussian(manifest, cuda_version)
   GenerateSM90_TensorOp_1684_trmm(manifest, cuda_version)
   GenerateSM90_TensorOp_1684_trmm_complex(manifest, cuda_version)
   GenerateSM90_TensorOp_1684_trmm_complex_gaussian(manifest, cuda_version)
   GenerateSM90_TensorOp_1684_symm(manifest, cuda_version)
   GenerateSM90_TensorOp_1684_symm_complex(manifest, cuda_version)
   GenerateSM90_TensorOp_1684_symm_complex_gaussian(manifest, cuda_version)
+  GenerateSM90_Conv3x(manifest, cuda_version)
 
 ###################################################################################################
 
 def numeric_log_level(log_level: str) -> int:
   """
   Converts the string identifier of the log level into the numeric identifier used
   in setting the log level
@@ -6090,14 +6788,15 @@
 
   GenerateSM50(manifest, args.cuda_version)
   GenerateSM60(manifest, args.cuda_version)
   GenerateSM61(manifest, args.cuda_version)
   GenerateSM70(manifest, args.cuda_version)
   GenerateSM75(manifest, args.cuda_version)
   GenerateSM80(manifest, args.cuda_version)
+  GenerateSM89(manifest, args.cuda_version)
   GenerateSM90(manifest, args.cuda_version)
   if 'library' in args.generator_target.split(','):
     manifest.emit(GeneratorTarget.Library)
 
   if args.selected_kernel_list is not None:
     if len(manifest.selected_kernels) > 0:
       with open(args.selected_kernel_list, 'w') as file_writer:
```

## cutlass_library/library.py

```diff
@@ -35,20 +35,20 @@
 """
 
 import enum
 import re
 
 # The following block implements enum.auto() for Python 3.5 variants that don't include it such
 # as the default 3.5.2 on Ubuntu 16.04.
-# 
+#
 # https://codereview.stackexchange.com/questions/177309/reimplementing-pythons-enum-auto-for-compatibility
 
 try:
   from enum import auto as enum_auto
-except ImportError: 
+except ImportError:
   __cutlass_library_auto_enum = 0
   def enum_auto() -> int:
     global __cutlass_library_auto_enum
     i = __cutlass_library_auto_enum
     __cutlass_library_auto_enum += 1
     return i
 
@@ -246,14 +246,20 @@
 
 #
 ComplexTransformTag = {
   ComplexTransform.none: 'cutlass::ComplexTransform::kNone',
   ComplexTransform.conj: 'cutlass::ComplexTransform::kConjugate',
 }
 
+# Used for cutlass3x complex kernel collective mainloop builder instantiation
+ComplexTransformTag3x = {
+  ComplexTransform.none: 'cute::identity',
+  ComplexTransform.conj: 'cute::conjugate',
+}
+
 #
 RealComplexBijection = [
   (DataType.f16, DataType.cf16),
   (DataType.f32, DataType.cf32),
   (DataType.f64, DataType.cf64),
 ]
 
@@ -294,69 +300,79 @@
   and_popc = enum_auto()
   multiply_add_fast_bf16 = enum_auto()
   multiply_add_fast_f16 = enum_auto()
   multiply_add_fast_f32 = enum_auto()
   multiply_add_complex_fast_f32 = enum_auto()
   multiply_add_complex = enum_auto()
   multiply_add_complex_gaussian = enum_auto()
+  multiply_add_fast_accum = enum_auto()
 
 #
 MathOperationTag = {
-  MathOperation.multiply_add: 'cutlass::arch::OpMultiplyAdd', 
+  MathOperation.multiply_add: 'cutlass::arch::OpMultiplyAdd',
   MathOperation.multiply_add_saturate: 'cutlass::arch::OpMultiplyAddSaturate',
   MathOperation.multiply_add_mixed_input_upcast: 'cutlass::arch::OpMultiplyAddMixedInputUpcast',
   MathOperation.xor_popc: 'cutlass::arch::OpXorPopc',
   MathOperation.and_popc: 'cutlass::arch::OpAndPopc',
   MathOperation.multiply_add_fast_bf16: 'cutlass::arch::OpMultiplyAddFastBF16',
   MathOperation.multiply_add_fast_f16: 'cutlass::arch::OpMultiplyAddFastF16',
   MathOperation.multiply_add_fast_f32: 'cutlass::arch::OpMultiplyAddFastF32',
   MathOperation.multiply_add_complex_fast_f32: 'cutlass::arch::OpMultiplyAddComplexFastF32',
   MathOperation.multiply_add_complex: 'cutlass::arch::OpMultiplyAddComplex',
   MathOperation.multiply_add_complex_gaussian: 'cutlass::arch::OpMultiplyAddGaussianComplex',
+  MathOperation.multiply_add_fast_accum: 'cutlass::arch::OpMultiplyAddFastAccum',
 }
 
 ###################################################################################################
 
 #
 class LayoutType(enum.Enum):
   ColumnMajor = enum_auto()
   RowMajor = enum_auto()
   ColumnMajorInterleaved2 = enum_auto()
   RowMajorInterleaved2 = enum_auto()
   ColumnMajorInterleaved32 = enum_auto()
   RowMajorInterleaved32 = enum_auto()
   ColumnMajorInterleaved64 = enum_auto()
   RowMajorInterleaved64 = enum_auto()
+  TensorNWC = enum_auto()
   TensorNHWC = enum_auto()
   TensorNDHWC = enum_auto()
   TensorNCHW = enum_auto()
   TensorNGHWC = enum_auto()
   TensorNC32HW32 = enum_auto()
   TensorNC64HW64 = enum_auto()
   TensorC32RSK32 = enum_auto()
   TensorC64RSK64 = enum_auto()
+  TensorKCS = enum_auto()
+  TensorKCSR = enum_auto()
+  TensorKCSRT = enum_auto()
 
 #
 LayoutTag = {
   LayoutType.ColumnMajor: 'cutlass::layout::ColumnMajor',
   LayoutType.RowMajor: 'cutlass::layout::RowMajor',
   LayoutType.ColumnMajorInterleaved2: 'cutlass::layout::ColumnMajorInterleaved<2>',
   LayoutType.RowMajorInterleaved2: 'cutlass::layout::RowMajorInterleaved<2>',
   LayoutType.ColumnMajorInterleaved32: 'cutlass::layout::ColumnMajorInterleaved<32>',
   LayoutType.RowMajorInterleaved32: 'cutlass::layout::RowMajorInterleaved<32>',
   LayoutType.ColumnMajorInterleaved64: 'cutlass::layout::ColumnMajorInterleaved<64>',
   LayoutType.RowMajorInterleaved64: 'cutlass::layout::RowMajorInterleaved<64>',
+  LayoutType.TensorNWC: 'cutlass::layout::TensorNWC',
   LayoutType.TensorNHWC: 'cutlass::layout::TensorNHWC',
   LayoutType.TensorNDHWC: 'cutlass::layout::TensorNDHWC',
   LayoutType.TensorNCHW: 'cutlass::layout::TensorNCHW',
   LayoutType.TensorNGHWC: 'cutlass::layout::TensorNGHWC',
   LayoutType.TensorNC32HW32: 'cutlass::layout::TensorNCxHWx<32>',
   LayoutType.TensorC32RSK32: 'cutlass::layout::TensorCxRSKx<32>',
   LayoutType.TensorNC64HW64: 'cutlass::layout::TensorNCxHWx<64>',
   LayoutType.TensorC64RSK64: 'cutlass::layout::TensorCxRSKx<64>',
+  LayoutType.TensorKCS: 'cutlass::layout::TensorKCS',
+  LayoutType.TensorKCSR: 'cutlass::layout::TensorKCSR',
+  LayoutType.TensorKCSRT: 'cutlass::layout::TensorKCSRT'
 }
 
 #
 TransposedLayout = {
   LayoutType.ColumnMajor: LayoutType.RowMajor,
   LayoutType.RowMajor: LayoutType.ColumnMajor,
   LayoutType.ColumnMajorInterleaved2: LayoutType.RowMajorInterleaved2,
@@ -374,22 +390,26 @@
   LayoutType.ColumnMajorInterleaved2: 'n2',
   LayoutType.ColumnMajorInterleaved32: 'n32',
   LayoutType.ColumnMajorInterleaved64: 'n64',
   LayoutType.RowMajor: 't',
   LayoutType.RowMajorInterleaved2: 't2',
   LayoutType.RowMajorInterleaved32: 't32',
   LayoutType.RowMajorInterleaved64: 't64',
+  LayoutType.TensorNWC: 'nwc',
   LayoutType.TensorNHWC: 'nhwc',
   LayoutType.TensorNDHWC: 'ndhwc',
   LayoutType.TensorNCHW: 'nchw',
   LayoutType.TensorNGHWC: 'nghwc',
   LayoutType.TensorNC32HW32: 'nc32hw32',
   LayoutType.TensorNC64HW64: 'nc64hw64',
   LayoutType.TensorC32RSK32: 'c32rsk32',
-  LayoutType.TensorC64RSK64: 'c64rsk64'
+  LayoutType.TensorC64RSK64: 'c64rsk64',
+  LayoutType.TensorKCS: 'kcs',
+  LayoutType.TensorKCSR: 'kcsr',
+  LayoutType.TensorKCSRT: 'kcsrt'
 }
 
 #
 ShortComplexLayoutNames = {
   (LayoutType.ColumnMajor, ComplexTransform.none): 'n',
   (LayoutType.ColumnMajor, ComplexTransform.conj): 'c',
   (LayoutType.RowMajor, ComplexTransform.none): 't',
@@ -406,28 +426,30 @@
   Tma = enum_auto()
   TmaWarpSpecialized = enum_auto()
   TmaWarpSpecializedPingpong = enum_auto()
   TmaWarpSpecializedCooperative = enum_auto()
   TmaWarpSpecializedFP8FastAccum = enum_auto()
   TmaWarpSpecializedCooperativeFP8FastAccum = enum_auto()
   TmaWarpSpecializedPingpongFP8FastAccum = enum_auto()
+  ImplicitTmaWarpSpecializedSm90 = enum_auto()
 #
 KernelScheduleTag = {
   KernelScheduleType.ScheduleAuto: 'cutlass::gemm::collective::KernelScheduleAuto',
   KernelScheduleType.Multistage: 'cutlass::gemm::KernelMultistage',
   KernelScheduleType.CpAsyncWarpSpecialized: 'cutlass::gemm::KernelCpAsyncWarpSpecialized',
   KernelScheduleType.CpAsyncWarpSpecializedPingpong: 'cutlass::gemm::KernelCpAsyncWarpSpecializedPingpong',
   KernelScheduleType.CpAsyncWarpSpecializedCooperative: 'cutlass::gemm::KernelCpAsyncWarpSpecializedCooperative',
   KernelScheduleType.Tma: 'cutlass::gemm::KernelTma',
   KernelScheduleType.TmaWarpSpecialized: 'cutlass::gemm::KernelTmaWarpSpecialized',
   KernelScheduleType.TmaWarpSpecializedPingpong: 'cutlass::gemm::KernelTmaWarpSpecializedPingpong',
   KernelScheduleType.TmaWarpSpecializedCooperative: 'cutlass::gemm::KernelTmaWarpSpecializedCooperative',
   KernelScheduleType.TmaWarpSpecializedFP8FastAccum: 'cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccum',
   KernelScheduleType.TmaWarpSpecializedCooperativeFP8FastAccum: 'cutlass::gemm::KernelTmaWarpSpecializedCooperativeFP8FastAccum',
   KernelScheduleType.TmaWarpSpecializedPingpongFP8FastAccum: 'cutlass::gemm::KernelTmaWarpSpecializedPingpongFP8FastAccum',
+  KernelScheduleType.ImplicitTmaWarpSpecializedSm90: 'cutlass::conv::KernelImplicitTmaWarpSpecializedSm90',
 }
 
 #
 KernelScheduleSuffixes = {
   KernelScheduleType.ScheduleAuto: '',
   KernelScheduleType.Multistage: '_cpasync',
   KernelScheduleType.CpAsyncWarpSpecialized: '_cpasync_warpspecialized',
@@ -436,14 +458,15 @@
   KernelScheduleType.Tma: '_unspecialized',
   KernelScheduleType.TmaWarpSpecialized: '_warpspecialized',
   KernelScheduleType.TmaWarpSpecializedPingpong: '_warpspecialized_pingpong',
   KernelScheduleType.TmaWarpSpecializedCooperative: '_warpspecialized_cooperative',
   KernelScheduleType.TmaWarpSpecializedFP8FastAccum: '_warpspecialized_fp8_fastaccum',
   KernelScheduleType.TmaWarpSpecializedCooperativeFP8FastAccum: '_warpspecialized_cooperative_fp8_fastaccum',
   KernelScheduleType.TmaWarpSpecializedPingpongFP8FastAccum: '_warpspecialized_pingpong_fp8_fastaccum',
+  KernelScheduleType.ImplicitTmaWarpSpecializedSm90: '_warpspecialized',
 }
 
 class EpilogueScheduleType(enum.Enum):
   ScheduleAuto = enum_auto()
   EpilogueTransposed = enum_auto()
   NoSmemWarpSpecialized = enum_auto()
   TmaWarpSpecialized = enum_auto()
@@ -574,29 +597,29 @@
 #
 class OperationKind(enum.Enum):
   Gemm = enum_auto()
   RankK = enum_auto()
   Rank2K = enum_auto()
   Trmm = enum_auto()
   Symm = enum_auto()
-  Conv2d = enum_auto()        
-  Conv3d = enum_auto()        
+  Conv2d = enum_auto()
+  Conv3d = enum_auto()
 
 #
 OperationKindNames = {
   OperationKind.Gemm: 'gemm'
   , OperationKind.RankK: 'rank_k'
   , OperationKind.Rank2K: 'rank_2k'
   , OperationKind.Trmm: 'trmm'
   , OperationKind.Symm: 'symm'
-  , OperationKind.Conv2d: 'conv2d'  
-  , OperationKind.Conv3d: 'conv3d' 
+  , OperationKind.Conv2d: 'conv2d'
+  , OperationKind.Conv3d: 'conv3d'
 }
 
-# 
+#
 class Target(enum.Enum):
   library = enum_auto()
 #
 ArchitectureNames = {
   50: 'maxwell',
   60: 'pascal',
   61: 'pascal',
@@ -704,15 +727,15 @@
   Identity4 = enum_auto()
   Identity8 = enum_auto()
   Horizontal = enum_auto()
   StridedDgradIdentity1 = enum_auto()
   StridedDgradIdentity4 = enum_auto()
   StridedDgradHorizontal = enum_auto()
   StreamK = enum_auto()
-  
+
 #
 SwizzlingFunctorTag = {
   SwizzlingFunctor.Identity1: 'cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>',
   SwizzlingFunctor.Identity2: 'cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<2>',
   SwizzlingFunctor.Identity4: 'cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<4>',
   SwizzlingFunctor.Identity8: 'cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>',
   SwizzlingFunctor.Horizontal: 'cutlass::gemm::threadblock::GemmHorizontalThreadblockSwizzle',
@@ -830,19 +853,19 @@
   GroupMode.Depthwise: 'depthwise',
 }
 
 ###################################################################################################
 
 #
 class MathInstruction:
-  def __init__(self, 
+  def __init__(self,
       instruction_shape,                                            \
       element_a, element_b, element_accumulator,                    \
       opcode_class, math_operation = MathOperation.multiply_add     \
-    ): 
+    ):
 
     self.instruction_shape = instruction_shape
     self.element_a = element_a
     self.element_b = element_b
     self.element_accumulator = element_accumulator
     self.opcode_class = opcode_class
     self.math_operation = math_operation
@@ -883,23 +906,23 @@
     self.stride = stride
     self.dilation =  dilation
     self.math_instruction = math_instruction
     self.minimum_compute_capability = min_compute
     self.maximum_compute_capability = max_compute
 
   def procedural_name(self):
-    str_name = "%dx%dx%d_%dx%dx%dx%d_%d_filter%dx%d" % (self.threadblock_shape[0], 
-                                      self.threadblock_shape[1], 
+    str_name = "%dx%dx%d_%dx%dx%dx%d_%d_filter%dx%d" % (self.threadblock_shape[0],
+                                      self.threadblock_shape[1],
                                       self.threadblock_shape[2],
                                       self.threadblock_output_shape[0],
                                       self.threadblock_output_shape[1],
                                       self.threadblock_output_shape[2],
                                       self.threadblock_output_shape[3],
-                                      self.stages, 
-                                      self.filter_shape[0], 
+                                      self.stages,
+                                      self.filter_shape[0],
                                       self.filter_shape[1])
     # Fixed Strided and dilation
     if self.stride != [-1, -1] and self.dilation != [-1, -1]:
       str_name += "_stride%dx%d_dilation%dx%d" % (self.stride[0],
                                                   self.stride[1],
                                                   self.dilation[0],
                                                   self.dilation[1])
@@ -916,23 +939,23 @@
     self.stride = stride
     self.dilation =  dilation
     self.math_instruction = math_instruction
     self.minimum_compute_capability = min_compute
     self.maximum_compute_capability = max_compute
 
   def procedural_name(self):
-    str_name = "%dx%dx%d_%dx%dx%dx%d_%d_filter%dx%d" % (self.threadblock_shape[0], 
-                                      self.threadblock_shape[1], 
+    str_name = "%dx%dx%d_%dx%dx%dx%d_%d_filter%dx%d" % (self.threadblock_shape[0],
+                                      self.threadblock_shape[1],
                                       self.threadblock_shape[2],
                                       self.threadblock_output_shape[0],
                                       self.threadblock_output_shape[1],
                                       self.threadblock_output_shape[2],
                                       self.threadblock_output_shape[3],
-                                      self.stages, 
-                                      self.filter_shape[0], 
+                                      self.stages,
+                                      self.filter_shape[0],
                                       self.filter_shape[1])
     # Fixed Strided and dilation
     if self.stride != [-1, -1] and self.dilation != [-1, -1]:
       str_name += "_stride%dx%d_dilation%dx%d" % (self.stride[0],
                                                   self.stride[1],
                                                   self.dilation[0],
                                                   self.dilation[1])
```

## cutlass_library/manifest.py

```diff
@@ -63,14 +63,34 @@
   from conv3d_operation import *
 
 ###################################################################################################
 _LOGGER = logging.getLogger(__name__)
 
 
 class EmitOperationKindAll:
+  """
+  Emit the OperationKind-level CUTLASS library initialization code.
+  The code is generated in the {generated_path}/{operation_kind} directory
+  (e.g., tools/library/generated/gemm in the build directory,
+  for OperationKind=Gemm), in the all_{operation_kind}_operations.cu file
+  (e.g., all_gemm_operations.cu for OperationKind=Gemm).
+  That file declares several functions in namespace cutlass::library.
+  The functions all have this form,
+
+  void initialize_{configuration_name}(Manifest& manifest);
+
+  The file also _defines_ the following function in that namespace.
+
+  void initialize_all_{operation_kind}_operations(Manifest& manifest);
+
+  That function calls all of the functions declared in this file.
+  Those functions are defined in subdirectories
+  (which this class does not create).
+  """
+
   def __init__(self, generated_path, kind, args):
     self.generated_path = generated_path
     self.kind = kind
     self.args = args
 
     self.header_template ="""
 /*
@@ -105,47 +125,92 @@
 } // namespace library
 } // namespace cutlass
 
 """
 
   #
   def __enter__(self):
+    _LOGGER.debug("*** EmitOperationKindAll::__enter__")
+
     self.operation_path = os.path.join(self.generated_path, OperationKindNames[self.kind])
+    _LOGGER.debug('***   operation_path (directory to create): ' +
+                  str(self.operation_path));
     os.makedirs(self.operation_path, exist_ok=True)
 
     self.top_level_path = os.path.join(self.operation_path, f"all_{OperationKindNames[self.kind]}_operations.cu")
+    _LOGGER.debug(f"***   top_level_path (file to write): {str(self.top_level_path)}")
 
     self.top_level_file = open(self.top_level_path, "w")
     self.top_level_file.write(self.header_template)
 
     self.source_files = [self.top_level_path,]
 
     self.configurations = []
 
     return self
 
   #
   def emit(self, operations):
+    _LOGGER.debug('*** EmitOperationKindAll::emit')
+    _LOGGER.debug(f"***   len(operations): {len(operations)}")
+    _LOGGER.debug(f"***   min_cc list: {sorted(min_cc for min_cc, _ in operations.items())}")
+
     for min_cc, configurations in sorted(operations.items()):
+      _LOGGER.debug(f"***   min_cc={min_cc}")
+
       for configuration_name, _ in configurations.items():
+        _LOGGER.debug(f"***     configuration_name={configuration_name}")
         self.configurations.append(configuration_name)
         self.top_level_file.write(SubstituteTemplate(self.configuration_prototype_template, {'configuration_name': configuration_name} ))
 
   #
   def __exit__(self, exception_type, exception_value, traceback):
+    _LOGGER.debug("*** EmitOperationKindAll::__exit__")
+
     self.top_level_file.write(SubstituteTemplate(self.entry_template, {'operation_name': OperationKindNames[self.kind]}))
 
     for configuration_name in self.configurations:
       self.top_level_file.write(SubstituteTemplate(self.configuration_template, {'configuration_name': configuration_name}))
 
     self.top_level_file.write(self.epilogue_template)
     self.top_level_file.close()
 
 
 class EmitOperationKindLibrary:
+  """
+  Emit the CUTLASS library initialization code for each OperationKind.
+  The code is generated in the directory
+  {generated_path}/{operation_kind}/{min_cc}
+  (e.g., tools/library/generated/gemm/90 in the build directory,
+  for min_cc=90 and OperationKind=Gemm), in the file
+  all_sm{min_cc}_{operation_kind}_operations.cu
+  (e.g., all_sm90_gemm_operations.cu for min_cc=90 and OperationKind=Gemm).
+  The min_cc variable here indicates the minimum GPU architecture version
+  that the things to be initialized require.
+  For example, min_cc=90 indicates sm90.
+
+  That file declares several functions in namespace cutlass::library.
+  The functions all have this form,
+
+  void initialize_all_sm{min_cc}_{subclass_name}_{extended_name}_operations(Manifest& manifest);
+
+  where extended_name is operation.extended_name() for all the operations
+  given to the emit method (which see below).  (All operations for a given
+  configuration_name are guaranteed to have the same extended_name().)
+
+  The file also _defines_ the following function in that namespace.
+
+  void initialize_all_sm{min_cc}__{operation_kind}_operations(Manifest& manifest);
+
+  That function calls all of the functions declared in this file.
+  Those functions are defined in subdirectories.
+  The mapping from OperationKind to emitter handles the details
+  of what happens in each of those subdirectories.
+  """
+
   def __init__(self, generated_path, min_cc, kind, args):
     self.generated_path = generated_path
     self.min_cc = min_cc
     self.kind = kind
     self.args = args
     self.emitters = {
       OperationKind.Gemm: EmitGemmConfigurationLibrary,
@@ -190,18 +255,25 @@
 } // namespace library
 } // namespace cutlass
 
 """
 
   #
   def __enter__(self):
+    _LOGGER.debug("*** EmitOperationKindLibrary::__enter__")
+    _LOGGER.debug(f"***   generated_path: {str(self.generated_path)}")
+    _LOGGER.debug(f"***   OperationKindNames[kind]: {OperationKindNames[self.kind]}")
+    _LOGGER.debug(f"***   min_cc: {self.min_cc}")
+
     self.operation_path = os.path.join(self.generated_path, OperationKindNames[self.kind], str(self.min_cc))
+    _LOGGER.debug(f"***   operation_path (directory to make): {str(self.operation_path)}")
     os.makedirs(self.operation_path)
 
     self.top_level_path = os.path.join(self.operation_path, f"all_sm{self.min_cc}_{OperationKindNames[self.kind]}_operations.cu")
+    _LOGGER.debug(f"***   top_level_path (file to write): {str(self.top_level_path)}")
 
     self.top_level_file = open(self.top_level_path, "w")
     self.top_level_file.write(self.header_template)
 
     self.source_files = {}
 
     # Each {operation_kind x cc} combination is further decomposed by the instruction
@@ -212,48 +284,61 @@
     # Configurations in each sub class
     self.subclass_configurations = {}
 
     return self
 
   #
   def emit(self, configuration_name, operations):
+    _LOGGER.debug("*** EmitOperationKindLibrary::emit")
+    _LOGGER.debug(f"***   configuration_name: {configuration_name}")
+
     assert len(operations) > 0
 
     # The extended name for all operations of a given configuration_name is guaranteed
     # to be the same because extended_name() is used in defining configuration_name. Thus,
     # we can safely use the extended_name() of the first operation.
     extended_name = operations[0].extended_name()
+    _LOGGER.debug('***   extended_name (for all ops): ' + extended_name)
 
     # Create a directory for operations with this subclass if it does not exist
     if extended_name not in self.subclass_files:
       subclass_path = os.path.join(self.operation_path, extended_name)
+      _LOGGER.debug(f"***     subclass_path: {str(subclass_path)}")
       os.mkdir(subclass_path)
 
       self.subclass_configurations[extended_name] = []
 
       # Open a new top-level file for this sub class
       subclass_top_level_path = os.path.join(
         subclass_path, f"all_sm{self.min_cc}_{extended_name}_{OperationKindNames[self.kind]}_operations.cu")
+      _LOGGER.debug('***     subclass_top_level_path (min_cc, extended_name, ' +
+                    'OperationKind): ' + str(subclass_top_level_path))
+
       self.subclass_files[extended_name] = open(subclass_top_level_path, "w")
       self.subclass_files[extended_name].write(self.header_template)
 
       self.source_files[extended_name] = [subclass_top_level_path]
 
     subclass_dir = os.path.dirname(self.subclass_files[extended_name].name)
+    _LOGGER.debug('***   subclass_dir: ' + str(subclass_dir))
+
     with self.emitters[self.kind](subclass_dir, configuration_name) as configuration_emitter:
       for operation in operations:
         configuration_emitter.emit(operation)
 
+      _LOGGER.debug('***   configuration_emitter.configuration_path: ' +
+                    str(configuration_emitter.configuration_path))
       self.source_files[extended_name].append(configuration_emitter.configuration_path)
 
     self.subclass_configurations[extended_name].append(configuration_name)
     self.subclass_files[extended_name].write(SubstituteTemplate(self.configuration_prototype_template, {'configuration_name': configuration_name} ))
 
   #
   def __exit__(self, exception_type, exception_value, traceback):
+    _LOGGER.debug("*** EmitOperationKindLibrary::__exit__")    
     for subclass_name, subclass_file in sorted(self.subclass_files.items()):
       subclass_cfg = {
         'min_cc': str(self.min_cc),
         'subclass_name': subclass_name,
         'operation_name': OperationKindNames[self.kind]
       }
       self.top_level_file.write(SubstituteTemplate(self.subclass_prototype_template, subclass_cfg))
@@ -286,14 +371,37 @@
       # Write the call to initialize_all for this subclass to the top-level file
       self.top_level_file.write(SubstituteTemplate(self.subclass_call_template, subclass_cfg))
 
     self.top_level_file.write(self.epilogue_template)
     self.top_level_file.close()
 
 class EmitInterfaceLibrary:
+  """
+  Emit the topmost-level CUTLASS library initialization code.
+  The code is generated in the generated_path directory
+  (e.g., tools/library/generated in the build directory),
+  in the initialize_all.cpp file.
+  That file declares several functions in namespace cutlass::library.
+  The functions all have this form,
+
+  void initialize_all_{operation_kind}_operations(Manifest& manifest);
+
+  where {operation_kind} abbreviates the "kind" of operation
+  (e.g., gemm for matrix-matrix multiply, conv2d for 2-d convolution,
+  or trmm for triangular solve with multiple right-hand sides).
+  The definitions of these functions live in subdirectories.
+
+  The file also _defines_ the following function in that namespace.
+
+  void initialize_all(Manifest& manifest);
+
+  That function first prepares the manifest, and then
+  calls all of the functions declared in this file.
+  """
+
   def __init__(self, generated_path, operation_count, args):
     self.generated_path = generated_path
     self.args = args
 
     self.prototypes = []
     self.fn_calls = []
     self.operation_count = str(operation_count)
@@ -331,35 +439,43 @@
 \t} // namespace library
 } // namespace cutlass
 
 '''
 
   #
   def __enter__(self):
+    _LOGGER.debug("*** EmitInterfaceLibrary::__enter__")
+
     self.top_level_path = os.path.join(self.generated_path, 'initialize_all.cpp')
+    _LOGGER.debug("***   top_level_path: " + str(self.top_level_path))
 
     self.top_level_file = open(self.top_level_path, "w")
     self.top_level_file.write(self.top_level_hdr_template)
 
     self.source_files = [self.top_level_path,]
 
     return self
 
   #
   def emit(self, operation_name):
+    _LOGGER.debug("*** EmitInterfaceLibrary::emit")
+    _LOGGER.debug("***   operation_name: " + operation_name)
+
     self.prototypes.append(SubstituteTemplate(
        "\t\tvoid initialize_all_${operation_kind}_operations(Manifest &manifest);",
        {'operation_kind': operation_name}))
 
     self.fn_calls.append(SubstituteTemplate(
       "\t\t\tinitialize_all_${operation_kind}_operations(manifest);",
       {'operation_kind': operation_name}))
 
   #
   def __exit__(self, exception_type, exception_value, traceback):
+    _LOGGER.debug("*** EmitInterfaceLibrary::__exit__")
+
     self.top_level_file.write(SubstituteTemplate(self.top_level_prologue, {'prototypes':"\n".join(self.prototypes)}))
 
     # Write out initialize_all method
     self.top_level_file.write(SubstituteTemplate(self.top_level_initialize,
                               {'operation_count': self.operation_count, 'fn_calls':"\n".join(self.fn_calls)}))
 
     self.top_level_file.write(self.top_level_suffix)
@@ -394,16 +510,22 @@
     self.curr_build_dir = '.'
     self.filter_by_cc = True
 
     if self.args:
       self.kernel_filter = self.args.kernels
       self.curr_build_dir = args.curr_build_dir
 
+      # A common user error is to use commas instead of semicolons.
+      if ',' in args.architectures:
+        raise RuntimeError("The list of architectures (CMake option CUTLASS_NVCC_ARCHS) must be semicolon-delimited.\nDon't use commas to separate the architectures; use semicolons.\nYou specified the list as: " + args.architectures)
       architectures = args.architectures.split(';') if len(args.architectures) else ['50',]
-      architectures = [x if x != '90a' else '90' for x in architectures]
+
+      arch_conditional_cc = ['90a']
+      architectures = [x if x not in arch_conditional_cc else x.split('a')[0] for x in architectures]
+
       self.compute_capabilities = [int(x) for x in architectures]
 
       if args.filter_by_cc in ['false', 'False', '0']:
         self.filter_by_cc = False
 
     if args.operations == 'all':
       self.operations_enabled = []
@@ -677,16 +799,15 @@
       for min_cc in self.operations[kind].keys():
         source_files[kind][min_cc] = {}
 
     for operation_kind, ops in self.operations.items():
       for min_cc, configurations in sorted(ops.items()):
         with operation_emitters[target](generated_path, min_cc, operation_kind, self.args) as operation_kind_emitter:
           for configuration_name, operations in configurations.items():
-            _LOGGER.info("Emitting {config} with {num_ops} operations.".format(
-                config = configuration_name, num_ops = len(operations)))
+            _LOGGER.info(f"Emitting {configuration_name} with {len(operations)} operation{'' if len(operations) == 1 else 's'}.")
             operation_kind_emitter.emit(configuration_name, operations)
 
           for subclass, files in operation_kind_emitter.source_files.items():
             if subclass not in source_files[operation_kind][min_cc]:
               source_files[operation_kind][min_cc][subclass] = []
             source_files[operation_kind][min_cc][subclass].extend(operation_kind_emitter.source_files[subclass])
```

## cutlass_library/source/examples/CMakeLists.txt

```diff
@@ -134,12 +134,14 @@
   51_hopper_gett
   52_hopper_gather_scatter_fusion
   53_hopper_gemm_permute
   54_hopper_fp8_warp_specialized_gemm
   55_hopper_mixed_dtype_gemm
   56_hopper_ptr_array_batched_gemm
   57_hopper_grouped_gemm
+  58_ada_fp8_gemm
+  59_ampere_gather_scatter_conv
   )
 
   add_subdirectory(${EXAMPLE})
 
 endforeach()
```

## cutlass_library/source/examples/03_visualize_layout/visualize_layout.h

```diff
@@ -256,15 +256,15 @@
     //  exactly one rank is changing AND 
     //  elements are consecutive)
 
     // Don't need vectorization.
     if (options.vectorize <= 2) return std::make_pair(false, -1);
 
     // Boundary check.
-    if (i > elements.size() || (i + options.vectorize - 1) > elements.size())
+    if (i > int(elements.size()) || (i + options.vectorize - 1) > int(elements.size()))
       return std::make_pair(false, -1);
 
     // Check if either all elements are valid or invalid.
     bool all_elements_invalid = std::all_of(
         elements.begin() + i, elements.begin() + i + options.vectorize,
         [](Element const &e) { return !e.valid(); });
```

## cutlass_library/source/examples/04_tile_iterator/tile_iterator.cu

```diff
@@ -90,15 +90,15 @@
     // PredicatedTileIterator uses PitchLinear layout and therefore takes in a PitchLinearShape.
     // The contiguous dimension can be accessed via Iterator::Shape::kContiguous and the strided
     // dimension can be accessed via Iterator::Shape::kStrided
     int iterations = (extent[1] + Iterator::Shape::kStrided - 1) / Iterator::Shape::kStrided;
 
     typename Iterator::Fragment fragment;
 
-    for(int i = 0; i < fragment.size(); ++i) {
+    for(size_t i = 0; i < fragment.size(); ++i) {
       fragment[i] = 0;
     }
 
     src_iterator.load(fragment);
     dst_iterator.store(fragment);
```

## cutlass_library/source/examples/05_batched_gemm/batched_gemm.cu

```diff
@@ -203,23 +203,23 @@
   int batch_count) {
   /*
   strided batched gemm NN
   */
   
   cudaError_t result = cudaSuccess;
 
-  if (A.size() < lda * k * batch_count) {
+  if (A.size() < size_t(lda * k * batch_count)) {
     std::cout << "the size of A is too small" << std::endl;
     return cudaErrorInvalidValue;
   }
-  if (B.size() < ldb * n) {
+  if (B.size() < size_t(ldb * n)) {
     std::cout << "the size of B is too small" << std::endl;
     return cudaErrorInvalidValue;
   }
-  if (C.size() < ldc * n * batch_count) {
+  if (C.size() < size_t(ldc * n * batch_count)) {
     std::cout << "the size of C is too small" << std::endl;
     return cudaErrorInvalidValue;
   }
   
   for (int batch_idx = 0; batch_idx < batch_count; batch_idx++) {
     for (int n_idx = 0; n_idx < n; n_idx++) {
       for (int m_idx = 0; m_idx < m; m_idx++) {
```

## cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_grouped_gemm_run.h

```diff
@@ -98,15 +98,15 @@
     cutlass::TensorView<Element, Layout> view, 
     cutlass::Distribution::Kind dist_kind,
     uint64_t seed) {
 
     if (dist_kind == cutlass::Distribution::Uniform) {
 
       cutlass::reference::host::TensorFillRandomUniform(
-        view, seed, 2, -2, 0);
+        view, seed, 1, -1, 0);
     } 
     else if (dist_kind == cutlass::Distribution::Identity) {
 
       cutlass::reference::host::TensorFillIdentity(view);
     } 
     else if (dist_kind == cutlass::Distribution::Gaussian) {
```

## cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h

```diff
@@ -153,43 +153,42 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size_0;
-    GemmCoord problem_size_1;
-    typename B2bMma::IteratorA0::TensorRef ref_A0;
-    typename B2bMma::IteratorB0::TensorRef ref_B0;
-    typename Epilogue::OutputTileIterator::TensorRef ref_C0;
-    typename B2bMma::IteratorAccumulatorScaleBias::TensorRef ref_Scale0;
-    typename B2bMma::IteratorAccumulatorScaleBias::TensorRef ref_Bias0;
-    typename B2bMma::IteratorB1::TensorRef ref_B1;
-    typename Epilogue::OutputTileIterator::TensorRef ref_C1;
-    typename Epilogue::OutputTileIterator::TensorRef ref_D1;
-    int64_t batch_stride_A0;
-    int64_t batch_stride_B0;
-    int64_t batch_stride_B1;
-    int64_t batch_stride_C1;
-    int64_t batch_stride_D1;
-    int64_t batch_stride_Bias0;
-    int64_t batch_stride_Scale0;
-    typename OutputOp0::Params epilogue0;
-    typename OutputOp1::Params epilogue1;
-    int batch_count;
+    GemmUniversalMode mode = cutlass::gemm::GemmUniversalMode::kGemm;
+    GemmCoord problem_size_0{0,0,0};
+    GemmCoord problem_size_1{0,0,0};
+    typename B2bMma::IteratorA0::TensorRef ref_A0{};
+    typename B2bMma::IteratorB0::TensorRef ref_B0{};
+    typename Epilogue::OutputTileIterator::TensorRef ref_C0{};
+    typename B2bMma::IteratorAccumulatorScaleBias::TensorRef ref_Scale0{};
+    typename B2bMma::IteratorAccumulatorScaleBias::TensorRef ref_Bias0{};
+    typename B2bMma::IteratorB1::TensorRef ref_B1{};
+    typename Epilogue::OutputTileIterator::TensorRef ref_C1{};
+    typename Epilogue::OutputTileIterator::TensorRef ref_D1{};
+    int64_t batch_stride_A0{0};
+    int64_t batch_stride_B0{0};
+    int64_t batch_stride_B1{0};
+    int64_t batch_stride_C1{0};
+    int64_t batch_stride_D1{0};
+    int64_t batch_stride_Bias0{0};
+    int64_t batch_stride_Scale0{0};
+    typename OutputOp0::Params epilogue0 {};
+    typename OutputOp1::Params epilogue1 {};
+    int batch_count{1};
 
     //
     // Methods
     //
 
     /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Arguments() : mode(mode), problem_size_0(0, 0, 0), problem_size_1(0, 0, 0), batch_count(1) {}
+    Arguments() = default;
 
     /// Constructs an Arguments structure
     CUTLASS_HOST_DEVICE
     Arguments(
       GemmUniversalMode mode_,
       GemmCoord problem_size_0_,
       GemmCoord problem_size_1_,
@@ -281,55 +280,53 @@
         problem_count(problem_count),
         threadblock_count(threadblock_count)
         {}
   };
 
   /// Parameters structure
   struct Params {
-    cutlass::gemm::GemmUniversalMode mode;
-    cutlass::gemm::GemmCoord problem_size_0;
-    cutlass::gemm::GemmCoord problem_size_1;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
-    typename B2bMma::IteratorA0::Params params_A0;
-    typename B2bMma::IteratorA0::TensorRef ref_A0;
-    typename B2bMma::IteratorB0::Params params_B0;
-    typename B2bMma::IteratorB0::TensorRef ref_B0;
-    typename Epilogue::OutputTileIterator::Params params_C0;
-    typename Epilogue::OutputTileIterator::TensorRef ref_C0;
-    typename B2bMma::IteratorAccumulatorScaleBias::TensorRef ref_Scale0;
-    typename B2bMma::IteratorAccumulatorScaleBias::TensorRef ref_Bias0;
-    typename B2bMma::IteratorB1::Params params_B1;
-    typename B2bMma::IteratorB1::TensorRef ref_B1;
-    typename Epilogue::OutputTileIterator::Params params_C1;
-    typename Epilogue::OutputTileIterator::TensorRef ref_C1;
-    typename Epilogue::OutputTileIterator::Params params_D1;
-    typename Epilogue::OutputTileIterator::TensorRef ref_D1;
-    typename OutputOp0::Params output_op_0;
-    typename OutputOp1::Params output_op_1;
-    int64_t batch_stride_A0;
-    int64_t batch_stride_B0;
-    int64_t batch_stride_B1;
-    int64_t batch_stride_C1;
-    int64_t batch_stride_D1;
-    int64_t batch_stride_Bias0;
-    int64_t batch_stride_Scale0;
-    int *semaphore;
-    int gemm_k_iterations_0;
-    int gemm_k_size_0;
-    int gemm_k_iterations_1;
-    int gemm_k_size_1;
+    cutlass::gemm::GemmUniversalMode mode = cutlass::gemm::GemmUniversalMode::kGemm;
+    cutlass::gemm::GemmCoord problem_size_0{};
+    cutlass::gemm::GemmCoord problem_size_1{};
+    cutlass::gemm::GemmCoord grid_tiled_shape{};
+    int swizzle_log_tile{0};
+    typename B2bMma::IteratorA0::Params params_A0{};
+    typename B2bMma::IteratorA0::TensorRef ref_A0{};
+    typename B2bMma::IteratorB0::Params params_B0{};
+    typename B2bMma::IteratorB0::TensorRef ref_B0{};
+    typename Epilogue::OutputTileIterator::Params params_C0{};
+    typename Epilogue::OutputTileIterator::TensorRef ref_C0{};
+    typename B2bMma::IteratorAccumulatorScaleBias::TensorRef ref_Scale0{};
+    typename B2bMma::IteratorAccumulatorScaleBias::TensorRef ref_Bias0{};
+    typename B2bMma::IteratorB1::Params params_B1{};
+    typename B2bMma::IteratorB1::TensorRef ref_B1{};
+    typename Epilogue::OutputTileIterator::Params params_C1{};
+    typename Epilogue::OutputTileIterator::TensorRef ref_C1{};
+    typename Epilogue::OutputTileIterator::Params params_D1{};
+    typename Epilogue::OutputTileIterator::TensorRef ref_D1{};
+    typename OutputOp0::Params output_op_0{};
+    typename OutputOp1::Params output_op_1{};
+    int64_t batch_stride_A0{0};
+    int64_t batch_stride_B0{0};
+    int64_t batch_stride_B1{0};
+    int64_t batch_stride_C1{0};
+    int64_t batch_stride_D1{0};
+    int64_t batch_stride_Bias0{0};
+    int64_t batch_stride_Scale0{0};
+    int *semaphore = nullptr;
+    int gemm_k_iterations_0{0};
+    int gemm_k_size_0{0};
+    int gemm_k_iterations_1{0};
+    int gemm_k_size_1{0};
 
     //
     // Methods
     //
 
-    CUTLASS_HOST_DEVICE
-    Params(): mode(mode), swizzle_log_tile(0), semaphore(0), gemm_k_iterations_0(0), gemm_k_size_0(0),
-        gemm_k_iterations_1(0), gemm_k_size_1(0) { }
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(
       cutlass::gemm::GemmUniversalMode mode,
       cutlass::gemm::GemmCoord const & problem_size_0,
       cutlass::gemm::GemmCoord const & problem_size_1,
       cutlass::gemm::GemmCoord const & grid_tiled_shape,
```

## cutlass_library/source/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu

```diff
@@ -261,14 +261,18 @@
 
 // Number of pipeline stages to use
 constexpr int NumStages = 3;
 
 // Which iterator algorithm to use: Analytic or Optimized
 static cutlass::conv::IteratorAlgorithm const IteratorAlgorithm = cutlass::conv::IteratorAlgorithm::kOptimized;
 
+// Is the output packed or strided
+// Use kStride if using strided output
+static cutlass::conv::StrideSupport const OutputStride = cutlass::conv::StrideSupport::kUnity;
+
 // The epilogue part of the kernel
 using EpilogueOp = cutlass::epilogue::thread::LinearCombination<
     ElementOutput,                                     // Data type of output matrix.
     128 / cutlass::sizeof_bits<ElementOutput>::value,  // The number of elements per vectorized
                                                        // memory access. This becomes the vector width of
                                                        // math instructions in the epilogue too.
     ElementAccumulator,                                // Data type of accumulator
@@ -285,15 +289,16 @@
   ThreadblockShape,
   WarpShape,
   InstructionShape,
   EpilogueOp,
   SwizzleThreadBlock,
   NumStages,
   cutlass::arch::OpMultiplyAdd,
-  IteratorAlgorithm
+  IteratorAlgorithm,
+  OutputStride
 >::Kernel;
 
 // Type of the actual kernel
 using ImplicitGemm = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/examples/23_ampere_gemm_operand_reduction_fusion/CMakeLists.txt

```diff
@@ -23,14 +23,18 @@
 # FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 # DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 # SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 # CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 # OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
-
+set(TEST_STANDARD --m=1024 --n=1024 --k=1024)
+set(TEST_LARGE_PERFCHECK --m=4096 --n=3456 --k=4096 --perf-check)
 
 cutlass_example_add_executable(
   23_ampere_gemm_operand_reduction_fusion
   ampere_gemm_operand_reduction_fusion.cu
+  TEST_COMMAND_OPTIONS
+  TEST_STANDARD
+  TEST_LARGE_PERFCHECK
   )
```

## cutlass_library/source/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu

```diff
@@ -373,30 +373,30 @@
   cutlass::HostTensor<ElementOutput, LayoutGemmKReduction> tensor_reduction({reduce_vector_length, 1});
   cutlass::HostTensor<ElementOutput, LayoutGemmKReduction> tensor_ref_reduction({reduce_vector_length, 1});
 
   // Fill input and output matrices on host using CUTLASS helper functions
   cutlass::reference::host::TensorFillRandomUniform(
       tensor_a.host_view(),
       1997,
-      ElementInputA(2),
-      ElementInputA(-2),
+      ElementInputA(1),
+      ElementInputA(-1),
       0);  // <- Fill tensor A on host with uniform-distribution random data
 
   cutlass::reference::host::TensorFillRandomUniform(
       tensor_b.host_view(),
       2003,
-      ElementInputB(2),
-      ElementInputB(-2),
+      ElementInputB(1),
+      ElementInputB(-1),
       0);  // <- Fill tensor B on host with uniform-distribution random data
 
   cutlass::reference::host::TensorFillRandomUniform(
       tensor_c.host_view(),
       2017,
-      ElementOutput(2),
-      ElementOutput(-2),
+      ElementOutput(1),
+      ElementOutput(-1),
       0);  // <- Fill matrix C on host with uniform-distribution random data
   cutlass::reference::host::TensorFill(
       tensor_d.host_view());  // <- fill matrix D on host with zeros
   cutlass::reference::host::TensorFill(
       tensor_ref_d.host_view());  // <- fill matrix D for reference on host with zeros
 
   cutlass::reference::host::TensorFill(
```

## cutlass_library/source/examples/24_gemm_grouped/gemm_grouped.cu

```diff
@@ -785,15 +785,15 @@
         << bin.first.m() << "-by-" << bin.first.n() << "-by-" << bin.first.k()
         << ", batch count: " << bin.second.size() << "\n";
 
       ++bin_idx;
       problem_count_check += bin.second.size();
     }
 
-    if (problem_count_check != this->problem_count()) {
+    if (problem_count_check != size_t(this->problem_count())) {
       std::cout << "\n***\nERROR in BINNING LOGIC!\n***\n" << std::endl;
     }
 
     std::cout << std::endl;
   }
 
   /// Executes a batched kernel and measures runtime
```

## cutlass_library/source/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu

```diff
@@ -32,28 +32,28 @@
 /**
 NVIDIA Ampere architecture starts supporting tfloat32 (see include/cutlass/tfloat32.h)
 data types in tensor cores.  One big advantage is that we can load in fp32 data and convert them
 implicitly to tf32 inside the GEMM kernel which means no change is needed to accelerate traditional
 fp32 data by using NVIDIA Ampere architecture.
 
 We can use the tf32 mode of tensor core to emulate a fast accurate SGEMM kernel which is accelerated
-using Ampere Tensor Cores (see include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h). 
+using Ampere Tensor Cores (see include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h).
 
 The trick is very simple
   a x b = (a_big + a_small) x (b_big + b_small) = a_big x b_big + a_big x b_small + a_small x b_big
   big = convert_to_tf32(fp32)
   small = convert_to_tf32(fp32 - big)
 
 a_small x b_small is discarded because they are too small.
 
-This example demonstrates usage of this kernel, along with accuracy measurements w.r.t. actual FP32 
+This example demonstrates usage of this kernel, along with accuracy measurements w.r.t. actual FP32
 results (SGEMM using SIMT) and against FP64 results (DGEMM)
 
-To enable this feature, the only change needs to make is to change the default OpMultiplyAdd to 
-OpMultiplyAddFastF32. 
+To enable this feature, the only change needs to make is to change the default OpMultiplyAdd to
+OpMultiplyAddFastF32.
 
 Now, we have several different flavors of sgemm now in the profiler for Ampere.  Here are the difference
 
   sgemm           // CUDA core SIMT kernel.  FP32 in, accumulated in FP32, FP32 out.
   s1688gemm       // Use 3xTF32 to emulate FP32.  FP32 in, converted in TF32-big and TF32-small internally,
                   // accumulated in FP32, FP32 out.
   s1688tf32gemm   // Use 1xTF32.  FP32 in, converted to one TF32 internally, accumulated in FP32, FP32 out.
@@ -93,22 +93,22 @@
 
   int m, n, k;
   double l2_norm_3xtf32_vs_fp64;
   double l2_norm_1xtf32_vs_fp64;
   double l2_norm_fp32_vs_fp64;
 
   // ctor
-  Result(  
+  Result(
     int m, int n, int k,
     double runtime_ms, double gflops,
     double l2_norm_3xtf32_vs_fp64,
     double l2_norm_1xtf32_vs_fp64,
-    double l2_norm_fp32_vs_fp64) : 
+    double l2_norm_fp32_vs_fp64) :
     m(m), n(n), k(k),
-    runtime_ms(runtime_ms), gflops(gflops), 
+    runtime_ms(runtime_ms), gflops(gflops),
     l2_norm_3xtf32_vs_fp64(l2_norm_3xtf32_vs_fp64),
     l2_norm_1xtf32_vs_fp64(l2_norm_1xtf32_vs_fp64),
     l2_norm_fp32_vs_fp64(l2_norm_fp32_vs_fp64)   {}
 
   Result() {}
 
   //
@@ -143,15 +143,15 @@
   float alpha;
   float beta;
   std::string rand_mode;
 
   int iterations;
   int seed;
   bool benchmark;
-  
+
   Options():
     help(false),
     problem_size({3456, 4096, 4096}),
     iterations(20),
     seed(1),
     alpha(1),
     beta(),
@@ -186,15 +186,15 @@
 
     cmd.get_cmd_line_argument("m", problem_size.m());
     cmd.get_cmd_line_argument("n", problem_size.n());
     cmd.get_cmd_line_argument("k", problem_size.k());
 
     cmd.get_cmd_line_argument("alpha", alpha);
     cmd.get_cmd_line_argument("beta", beta);
-    
+
     cmd.get_cmd_line_argument("iterations", iterations);
     cmd.get_cmd_line_argument("seed", seed);
     cmd.get_cmd_line_argument("rand_mode", rand_mode);
 
     if (cmd.check_cmd_line_flag("benchmark")) {
       benchmark = true;
     }
@@ -223,17 +223,17 @@
 
     return out;
   }
 
   /// Compute performance in GFLOP/s
   double gflops(double runtime_s) const {
 
-    // Number of real-valued multiply-adds 
+    // Number of real-valued multiply-adds
     int64_t fmas = problem_size.product();
-    
+
     // Two flops per multiply-add
     return 2.0 * double(fmas) / double(1.0e9) / runtime_s;
   }
 };
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -268,18 +268,18 @@
                                                        // elements. This becomes the vector width of
                                                        // math instructions in the epilogue too
     float,                                   // <- data type of accumulator
     float>;                                  // <- data type for alpha/beta in linear combination function
 
 // Number of pipelines you want to use
 constexpr int NumStages = 3;
-// Alignment 
+// Alignment
 constexpr int Alignment = 4;
 
-// 
+//
 // Gemm Operators (Gemm_3xTF32, Gemm_1xTF32, GEMM_F32, GEMM_F64)
 //
 
 // Gemm_3xTF32
 using Gemm_3xTF32 = cutlass::gemm::device::Gemm<
                                               float,
                                               LayoutInputA,
@@ -292,15 +292,15 @@
                                               SmArch,
                                               ShapeMMAThreadBlock,
                                               ShapeMMAWarp,
                                               ShapeMMAOp,
                                               EpilogueOp,
                                               SwizzleThreadBlock,
                                               NumStages,
-                                              Alignment, 
+                                              Alignment,
                                               Alignment,
                                               false,
                                               cutlass::arch::OpMultiplyAddFastF32>;
 
 // Gemm_1xTF32
 using Gemm_1xTF32 = cutlass::gemm::device::Gemm<
                                               float,
@@ -314,15 +314,15 @@
                                               SmArch,
                                               ShapeMMAThreadBlock,
                                               ShapeMMAWarp,
                                               ShapeMMAOp,
                                               EpilogueOp,
                                               SwizzleThreadBlock,
                                               NumStages,
-                                              Alignment, 
+                                              Alignment,
                                               Alignment,
                                               false,
                                               cutlass::arch::OpMultiplyAdd>;
 
 // Gemm_F64
 using Gemm_F64 = cutlass::reference::device::Gemm<
                                               double,
@@ -352,15 +352,15 @@
 
   ////////////////////////////////////////////////////////////////////////////////
   /// 1. Initialize F32 Precision input tensors using CUTLASS helper functions
   ////////////////////////////////////////////////////////////////////////////////
   cutlass::HostTensor<float, LayoutInputA> tensor_a_F32(problem_size.mk());  // <- Create matrix A with dimensions M x K
   cutlass::HostTensor<float, LayoutInputB> tensor_b_F32(problem_size.kn());  // <- Create matrix B with dimensions K x N
   cutlass::HostTensor<float, LayoutOutput> tensor_c_F32(problem_size.mn());  // <- Create matrix C with dimensions M x N
-  cutlass::HostTensor<float, LayoutOutput> tensor_d_F32(problem_size.mn());  // <- Create matrix D with dimensions M x N 
+  cutlass::HostTensor<float, LayoutOutput> tensor_d_F32(problem_size.mn());  // <- Create matrix D with dimensions M x N
 
   if (options.rand_mode == "uniform") {
     const float min = -1;
     const float max =  1;
     // Fill input and output matrices on host using CUTLASS helper functions
     cutlass::reference::host::TensorFillRandomUniform(
         tensor_a_F32.host_view(),
@@ -393,44 +393,44 @@
         tensor_c_F32.host_view(),
         options.seed,
         double(0),
         double(5));      // <- Fill matrix C on host with gaussian-distribution random data
   }
   cutlass::reference::host::TensorFill(
       tensor_d_F32.host_view());  // <- fill matrix D on host with zeros
-  
+
   // Copy data from host to GPU
   tensor_a_F32.sync_device();
   tensor_b_F32.sync_device();
   tensor_c_F32.sync_device();
   tensor_d_F32.sync_device();
 
   ////////////////////////////////////////////////////////////////////////////////
   /// 2. Initialize F64 tensors using the same values used for F32
   ////////////////////////////////////////////////////////////////////////////////
   // Gemm input operands (A, B, C)
   cutlass::HostTensor<double, LayoutInputA> tensor_a_F64(problem_size.mk());  // <- Create matrix A with dimensions M x K
   cutlass::HostTensor<double, LayoutInputB> tensor_b_F64(problem_size.kn());  // <- Create matrix B with dimensions K x N
   cutlass::HostTensor<double, LayoutOutput> tensor_c_F64(problem_size.mn());  // <- Create matrix C with dimensions M x N
-  
+
   // Gemm output (D) for GEMM_F64
   cutlass::HostTensor<double, LayoutOutput> tensor_d_F64(problem_size.mn());  // <- Create matrix D with dimensions M x N
   // Gemm output (D) for GEMM_3xTF32
   cutlass::HostTensor<float, LayoutOutput> tensor_d_3xTF32(problem_size.mn());  // <- Create matrix D with dimensions M x N
   // Gemm output (D) for GEMM_1xTF32
   cutlass::HostTensor<float, LayoutOutput> tensor_d_1xTF32(problem_size.mn());  // <- Create matrix D with dimensions M x N
 
   // Copy values from the DP tensors
   cutlass::reference::host::TensorCopy(tensor_a_F64.host_view(), tensor_a_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_b_F64.host_view(), tensor_b_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_c_F64.host_view(), tensor_c_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_d_F64.host_view(), tensor_d_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_d_3xTF32.host_view(), tensor_d_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_d_1xTF32.host_view(), tensor_d_F32.host_view());
-  
+
   // Copy data from host to GPU
   tensor_a_F64.sync_device();
   tensor_b_F64.sync_device();
   tensor_c_F64.sync_device();
   tensor_d_F64.sync_device();
   tensor_d_3xTF32.sync_device();
   tensor_d_1xTF32.sync_device();
@@ -460,15 +460,15 @@
 
   // Allocate workspace memory
   cutlass::device_memory::allocation<uint8_t> workspace_3xtf32(workspace_size_3xtf32);
 
   // Instantiate CUTLASS kernel depending on templates
   Gemm_3xTF32 gemm_op_3xTF32;
 
-  // Check the problem size is supported or not 
+  // Check the problem size is supported or not
   cutlass::Status status_3xtf32 = gemm_op_3xTF32.can_implement(arguments_3xtf32);
   CUTLASS_CHECK(status_3xtf32);
 
   // Initialize CUTLASS kernel with arguments and workspace pointer
   status_3xtf32 = gemm_op_3xTF32.initialize(arguments_3xtf32, workspace_3xtf32.get());
   CUTLASS_CHECK(status_3xtf32);
 
@@ -564,15 +564,15 @@
 
   // Allocate workspace memory
   cutlass::device_memory::allocation<uint8_t> workspace_1xtf32(workspace_size_1xtf32);
 
   // Instantiate CUTLASS kernel depending on templates
   Gemm_1xTF32 gemm_op_1xtf32;
 
-  // Check the problem size is supported or not 
+  // Check the problem size is supported or not
   cutlass::Status status_1xtf32 = gemm_op_1xtf32.can_implement(arguments_1xtf32);
   CUTLASS_CHECK(status_1xtf32);
 
   // Initialize CUTLASS kernel with arguments and workspace pointer
   status_1xtf32 = gemm_op_1xtf32.initialize(arguments_1xtf32, workspace_1xtf32.get());
   CUTLASS_CHECK(status_1xtf32);
 
@@ -623,15 +623,15 @@
   // Wait for kernels to finish
   cudaDeviceSynchronize();
 
   // Copy output data from CUTLASS and reference kernel to host for comparison
   tensor_d_F32.sync_host();
 
   ////////////////////////////////////////////////////////////////////////////////
-  ///////               Compute l2 norms 
+  ///////               Compute l2 norms
   ////////////////////////////////////////////////////////////////////////////////
 
   // l2 norm 3xTF32 vs F64
   cutlass::HostTensor<double, LayoutOutput> tensor_d_3xTF32_in_F64(problem_size.mn());
   cutlass::reference::host::TensorCopy(tensor_d_3xTF32_in_F64.host_view(), tensor_d_3xTF32.host_view());
 
   result.l2_norm_3xtf32_vs_fp64 = cutlass::reference::host::TensorRelativeErrorMetric(
@@ -660,41 +660,41 @@
   std::cout << std::fixed;
   std::cout.precision(4);
   std::cout << "Runtime: " << result.runtime_ms << " ms" << std::endl;
   std::cout.precision(2);
   std::cout << "GFLOPs: " << result.gflops << std::endl;
   std::cout << "Normalized L2 norm of" << std::endl;
   std::cout.precision(8);
-  std::cout << std::scientific 
+  std::cout << std::scientific
             << " - 3xTF32 error with FP64 reference : " << result.l2_norm_3xtf32_vs_fp64 << std::endl
             << " - 1xTF32 error with FP64 reference : " << result.l2_norm_1xtf32_vs_fp64 << std::endl
             << " - FP32 error with FP64 reference   : " << result.l2_norm_fp32_vs_fp64 << std::endl;
 
   return true;
 }
 
 int main(int argc, const char **argv) {
-  
+
   bool notSupported = false;
 
   // Ampere Tensor Core operations exposed with mma.sync and ldmatrix are first available
-  // in CUDA 11.0. 
+  // in CUDA 11.0.
   //
   // CUTLASS must be compiled with CUDA 11.0 Toolkit to run these examples.
   if (!(__CUDACC_VER_MAJOR__ >= 11)) {
     std::cerr << "Ampere Tensor Core operations must be compiled with CUDA 11.0 Toolkit or later." << std::endl;
     notSupported = true;
   }
 
   cudaDeviceProp props;
 
   cudaError_t error = cudaGetDeviceProperties(&props, 0);
   if (error != cudaSuccess) {
     std::cerr << "cudaGetDeviceProperties() returned an error: " << cudaGetErrorString(error) << std::endl;
-    return false;
+    return -1;
   }
 
   if (!((props.major * 10 + props.minor) >= 80)) {
     std::cerr << "Ampere Tensor Core operations must be run on a machine with compute capability at least 80."
               << std::endl;
     notSupported = true;
   }
@@ -712,25 +712,25 @@
     return 0;
   }
 
   bool result = true;
 
   if (options.benchmark) {
     for (int k = 4; k <= 65536; k *= 2) {
-  
+
       options.problem_size[2] = k;
-  
+
       printf("Gemm problem size: %d x %d x %d\n", \
         options.problem_size.m(), options.problem_size.n(), options.problem_size.k());
-  
+
       if (!options.valid()) {
         std::cerr << "Invalid problem." << std::endl;
         return -1;
       }
-  
+
       result &= run(options);
     }
   } else {
     // Execute one problem size
     if (!options.valid()) {
       std::cerr << "Invalid problem." << std::endl;
       return -1;
```

## cutlass_library/source/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm.cu

```diff
@@ -30,15 +30,15 @@
  **************************************************************************************************/
 
 /**
   This example is almost the same as example 27 which uses 3xTF32 to run GEMM.  The only
   difference is that this example uses 3xtf32 on complex gemm.
 
   To enable this feature, the only change needs to make is to change OpMultiplyAddComplex
-  to OpMultiplyAddComplexFastF32. 
+  to OpMultiplyAddComplexFastF32.
 */
 
 #include <iostream>
 #include <vector>
 #include <limits>
 
 #include "cutlass/cutlass.h"
@@ -70,22 +70,22 @@
 
   int m, n, k;
   double l2_norm_3xtf32_vs_fp64;
   double l2_norm_1xtf32_vs_fp64;
   double l2_norm_fp32_vs_fp64;
 
   // ctor
-  Result(  
+  Result(
     int m, int n, int k,
     double runtime_ms, double gflops,
     double l2_norm_3xtf32_vs_fp64,
     double l2_norm_1xtf32_vs_fp64,
-    double l2_norm_fp32_vs_fp64) : 
+    double l2_norm_fp32_vs_fp64) :
     m(m), n(n), k(k),
-    runtime_ms(runtime_ms), gflops(gflops), 
+    runtime_ms(runtime_ms), gflops(gflops),
     l2_norm_3xtf32_vs_fp64(l2_norm_3xtf32_vs_fp64),
     l2_norm_1xtf32_vs_fp64(l2_norm_1xtf32_vs_fp64),
     l2_norm_fp32_vs_fp64(l2_norm_fp32_vs_fp64)   {}
 
   Result() {}
 
   //
@@ -120,15 +120,15 @@
   float alpha;
   float beta;
   std::string rand_mode;
 
   int iterations;
   int seed;
   bool benchmark;
-  
+
   Options():
     help(false),
     problem_size({3456, 4096, 4096}),
     iterations(20),
     seed(1),
     alpha(1),
     beta(),
@@ -149,15 +149,15 @@
 
     cmd.get_cmd_line_argument("m", problem_size.m());
     cmd.get_cmd_line_argument("n", problem_size.n());
     cmd.get_cmd_line_argument("k", problem_size.k());
 
     cmd.get_cmd_line_argument("alpha", alpha);
     cmd.get_cmd_line_argument("beta", beta);
-    
+
     cmd.get_cmd_line_argument("iterations", iterations);
     cmd.get_cmd_line_argument("seed", seed);
     cmd.get_cmd_line_argument("rand_mode", rand_mode);
 
     if (cmd.check_cmd_line_flag("benchmark")) {
       benchmark = true;
     }
@@ -186,17 +186,17 @@
 
     return out;
   }
 
   /// Compute performance in GFLOP/s
   double gflops(double runtime_s) const {
 
-    // Number of real-valued multiply-adds 
+    // Number of real-valued multiply-adds
     int64_t fmas = problem_size.product();
-    
+
     // Two flops per multiply-add
     return 2.0 * double(fmas) / double(1.0e9) / runtime_s;
   }
 };
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -235,15 +235,15 @@
 
 // Number of pipelines you want to use
 constexpr int NumStages = 3;
 // Transform
 constexpr cutlass::ComplexTransform TransformA = cutlass::ComplexTransform::kNone;
 constexpr cutlass::ComplexTransform TransformB = cutlass::ComplexTransform::kNone;
 
-// 
+//
 // Gemm Operators (Gemm_3xTF32, Gemm_1xTF32, GEMM_F32, GEMM_F64)
 //
 
 // Gemm_3xTF32
 using Gemm_3xTF32 = cutlass::gemm::device::GemmComplex<
                                               cutlass::complex<float>,
                                               LayoutInputA,
@@ -256,15 +256,15 @@
                                               SmArch,
                                               ShapeMMAThreadBlock,
                                               ShapeMMAWarp,
                                               ShapeMMAOp,
                                               EpilogueOp,
                                               SwizzleThreadBlock,
                                               NumStages,
-                                              TransformA, 
+                                              TransformA,
                                               TransformB,
                                               cutlass::arch::OpMultiplyAddComplexFastF32>;
 
 // Gemm_1xTF32
 using Gemm_1xTF32 = cutlass::gemm::device::GemmComplex<
                                               cutlass::complex<float>,
                                               LayoutInputA,
@@ -277,30 +277,30 @@
                                               SmArch,
                                               ShapeMMAThreadBlock,
                                               ShapeMMAWarp,
                                               ShapeMMAOp,
                                               EpilogueOp,
                                               SwizzleThreadBlock,
                                               NumStages,
-                                              TransformA, 
+                                              TransformA,
                                               TransformB,
                                               cutlass::arch::OpMultiplyAddComplex>;
 
 bool run(Options &options) {
 
   // Create a tuple of problem size for matrix multiplication
   cutlass::gemm::GemmCoord problem_size = options.problem_size;
 
   ////////////////////////////////////////////////////////////////////////////////
   /// 1. Initialize F32 Precision input tensors using CUTLASS helper functions
   ////////////////////////////////////////////////////////////////////////////////
   cutlass::HostTensor<cutlass::complex<float>, LayoutInputA> tensor_a_F32(problem_size.mk());  // <- Create matrix A with dimensions M x K
   cutlass::HostTensor<cutlass::complex<float>, LayoutInputB> tensor_b_F32(problem_size.kn());  // <- Create matrix B with dimensions K x N
   cutlass::HostTensor<cutlass::complex<float>, LayoutOutput> tensor_c_F32(problem_size.mn());  // <- Create matrix C with dimensions M x N
-  cutlass::HostTensor<cutlass::complex<float>, LayoutOutput> tensor_d_F32(problem_size.mn());  // <- Create matrix D with dimensions M x N 
+  cutlass::HostTensor<cutlass::complex<float>, LayoutOutput> tensor_d_F32(problem_size.mn());  // <- Create matrix D with dimensions M x N
 
   if (options.rand_mode == "uniform") {
     const float min = -1;
     const float max =  1;
     // Fill input and output matrices on host using CUTLASS helper functions
     cutlass::reference::host::TensorFillRandomUniform(
         tensor_a_F32.host_view(),
@@ -333,44 +333,44 @@
         tensor_c_F32.host_view(),
         options.seed,
         double(0),
         double(5));      // <- Fill matrix C on host with gaussian-distribution random data
   }
   cutlass::reference::host::TensorFill(
       tensor_d_F32.host_view());  // <- fill matrix D on host with zeros
-  
+
   // Copy data from host to GPU
   tensor_a_F32.sync_device();
   tensor_b_F32.sync_device();
   tensor_c_F32.sync_device();
   tensor_d_F32.sync_device();
 
   ////////////////////////////////////////////////////////////////////////////////
   /// 2. Initialize F64 tensors using the same values used for F32
   ////////////////////////////////////////////////////////////////////////////////
   // Gemm input operands (A, B, C)
   cutlass::HostTensor<cutlass::complex<double>, LayoutInputA> tensor_a_F64(problem_size.mk());  // <- Create matrix A with dimensions M x K
   cutlass::HostTensor<cutlass::complex<double>, LayoutInputB> tensor_b_F64(problem_size.kn());  // <- Create matrix B with dimensions K x N
   cutlass::HostTensor<cutlass::complex<double>, LayoutOutput> tensor_c_F64(problem_size.mn());  // <- Create matrix C with dimensions M x N
-  
+
   // Gemm output (D) for GEMM_F64
   cutlass::HostTensor<cutlass::complex<double>, LayoutOutput> tensor_d_F64(problem_size.mn());  // <- Create matrix D with dimensions M x N
   // Gemm output (D) for GEMM_3xTF32
   cutlass::HostTensor<cutlass::complex<float>, LayoutOutput> tensor_d_3xTF32(problem_size.mn());  // <- Create matrix D with dimensions M x N
   // Gemm output (D) for GEMM_1xTF32
   cutlass::HostTensor<cutlass::complex<float>, LayoutOutput> tensor_d_1xTF32(problem_size.mn());  // <- Create matrix D with dimensions M x N
 
   // Copy values from the DP tensors
   cutlass::reference::host::TensorCopy(tensor_a_F64.host_view(), tensor_a_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_b_F64.host_view(), tensor_b_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_c_F64.host_view(), tensor_c_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_d_F64.host_view(), tensor_d_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_d_3xTF32.host_view(), tensor_d_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_d_1xTF32.host_view(), tensor_d_F32.host_view());
-  
+
   // Copy data from host to GPU
   tensor_a_F64.sync_device();
   tensor_b_F64.sync_device();
   tensor_c_F64.sync_device();
   tensor_d_F64.sync_device();
   tensor_d_3xTF32.sync_device();
   tensor_d_1xTF32.sync_device();
@@ -400,15 +400,15 @@
 
   // Allocate workspace memory
   cutlass::device_memory::allocation<uint8_t> workspace_3xtf32(workspace_size_3xtf32);
 
   // Instantiate CUTLASS kernel depending on templates
   Gemm_3xTF32 gemm_op;
 
-  // Check the problem size is supported or not 
+  // Check the problem size is supported or not
   cutlass::Status status_3xtf32 = gemm_op.can_implement(arguments_3xtf32);
   CUTLASS_CHECK(status_3xtf32);
 
   // Initialize CUTLASS kernel with arguments and workspace pointer
   status_3xtf32 = gemm_op.initialize(arguments_3xtf32, workspace_3xtf32.get());
   CUTLASS_CHECK(status_3xtf32);
 
@@ -504,15 +504,15 @@
 
   // Allocate workspace memory
   cutlass::device_memory::allocation<uint8_t> workspace_1xtf32(workspace_size_1xtf32);
 
   // Instantiate CUTLASS kernel depending on templates
   Gemm_1xTF32 gemm_op_1xtf32;
 
-  // Check the problem size is supported or not 
+  // Check the problem size is supported or not
   cutlass::Status status_1xtf32 = gemm_op_1xtf32.can_implement(arguments_1xtf32);
   CUTLASS_CHECK(status_1xtf32);
 
   // Initialize CUTLASS kernel with arguments and workspace pointer
   status_1xtf32 = gemm_op_1xtf32.initialize(arguments_1xtf32, workspace_1xtf32.get());
   CUTLASS_CHECK(status_1xtf32);
 
@@ -565,15 +565,15 @@
   // Wait for kernels to finish
   cudaDeviceSynchronize();
 
   // Copy output data from CUTLASS and reference kernel to host for comparison
   tensor_d_F32.sync_host();
 
   ////////////////////////////////////////////////////////////////////////////////
-  ///////               Compute l2 norms 
+  ///////               Compute l2 norms
   ////////////////////////////////////////////////////////////////////////////////
 
   // l2 norm 3xTF32 vs F64
   cutlass::HostTensor<cutlass::complex<double>, LayoutOutput> tensor_d_3xTF32_in_F64(problem_size.mn());
   cutlass::reference::host::TensorCopy(tensor_d_3xTF32_in_F64.host_view(), tensor_d_3xTF32.host_view());
 
   result.l2_norm_3xtf32_vs_fp64 = cutlass::reference::host::TensorRelativeErrorMetric(
@@ -602,41 +602,41 @@
   std::cout << std::fixed;
   std::cout.precision(4);
   std::cout << "Runtime: " << result.runtime_ms << " ms" << std::endl;
   std::cout.precision(2);
   std::cout << "GFLOPs: " << result.gflops << std::endl;
   std::cout << "Normalized L2 norm of" << std::endl;
   std::cout.precision(8);
-  std::cout << std::scientific 
+  std::cout << std::scientific
             << " - 3xTF32 error with FP64 reference : " << result.l2_norm_3xtf32_vs_fp64 << std::endl
             << " - 1xTF32 error with FP64 reference : " << result.l2_norm_1xtf32_vs_fp64 << std::endl
             << " - FP32 error with FP64 reference   : " << result.l2_norm_fp32_vs_fp64 << std::endl;
 
   return true;
 }
 
 int main(int argc, const char **argv) {
-  
+
   bool notSupported = false;
 
   // Ampere Tensor Core operations exposed with mma.sync and ldmatrix are first available
-  // in CUDA 11.0. 
+  // in CUDA 11.0.
   //
   // CUTLASS must be compiled with CUDA 11.0 Toolkit to run these examples.
   if (!(__CUDACC_VER_MAJOR__ >= 11)) {
     std::cerr << "Ampere Tensor Core operations must be compiled with CUDA 11.0 Toolkit or later." << std::endl;
     notSupported = true;
   }
 
   cudaDeviceProp props;
 
   cudaError_t error = cudaGetDeviceProperties(&props, 0);
   if (error != cudaSuccess) {
     std::cerr << "cudaGetDeviceProperties() returned an error: " << cudaGetErrorString(error) << std::endl;
-    return false;
+    return -1;
   }
 
   if (!((props.major * 10 + props.minor) >= 80)) {
     std::cerr << "Ampere Tensor Core operations must be run on a machine with compute capability at least 80."
               << std::endl;
     notSupported = true;
   }
@@ -654,25 +654,25 @@
     return 0;
   }
 
   bool result = true;
 
   if (options.benchmark) {
     for (int k = 4; k <= 65536; k *= 2) {
-  
+
       options.problem_size[2] = k;
-  
+
       printf("Gemm problem size: %d x %d x %d\n", \
         options.problem_size.m(), options.problem_size.n(), options.problem_size.k());
-  
+
       if (!options.valid()) {
         std::cerr << "Invalid problem." << std::endl;
         return -1;
       }
-  
+
       result &= run(options);
     }
   } else {
     // Execute one problem size
     if (!options.valid()) {
       std::cerr << "Invalid problem." << std::endl;
       return -1;
```

## cutlass_library/source/examples/31_basic_syrk/basic_syrk.cu

```diff
@@ -109,18 +109,18 @@
       double,
       1,
       double,
       double
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
     5,     // Stages
-    1,     // AligmentA
+    1,     // AlignmentA
     false, // SplitKSerail
-    cutlass::arch::OpMultiplyAdd, 
-    cutlass::ComplexTransform::kNone, 
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::ComplexTransform::kNone,
     cutlass::BlasMode::kSymmetric
   >;
 
   // Define a CUTLASS SYRK type
   CutlassSyrk syrk_operator;
 
   // Construct the CUTLASS SYRK arguments object.
@@ -145,15 +145,15 @@
                               lda,
                               ldc,
                               ldc);
 
   //
   // Launch the CUTLASS SYRK kernel.
   //
-  
+
   cutlass::Status status = syrk_operator(args);
 
   //
   // Return a cudaError_t if the CUTLASS SYRK operator returned an error code.
   //
 
   if (status != cutlass::Status::kSuccess) {
```

## cutlass_library/source/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu

```diff
@@ -32,28 +32,28 @@
 /**
 NVIDIA Ampere architecture starts supporting tfloat32 (see include/cutlass/tfloat32.h)
 data types in tensor cores.  One big advantage is that we can load in F32 data and convert them
 implicitly to tf32 inside the SYMM kernel which means no change is needed to accelerate traditional
 F32 data by using NVIDIA Ampere architecture.
 
 We can use the tf32 mode of tensor core to emulate a fast accurate SYMM kernel which is accelerated
-using Ampere Tensor Cores (see include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h). 
+using Ampere Tensor Cores (see include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h).
 
 The trick is very simple
   a x b = (a_big + a_small) x (b_big + b_small) = a_big x b_big + a_big x b_small + a_small x b_big
   big = convert_to_tf32(F32)
   small = convert_to_tf32(F32 - big)
 
 a_small x b_small is discarded because they are too small.
 
-This example demonstrates usage of this kernel, along with accuracy measurements w.r.t. actual F32 
+This example demonstrates usage of this kernel, along with accuracy measurements w.r.t. actual F32
 results (SSYMM from cuBLAS) and against F64 results (DSYMM from CUTLASS)
 
-To enable this feature, the only change needs to make is to change the default OpMultiplyAdd to 
-OpMultiplyAddFastF32. 
+To enable this feature, the only change needs to make is to change the default OpMultiplyAdd to
+OpMultiplyAddFastF32.
 
 Now, we have two different flavors of SSYMM in the profiler for Ampere:
 
   s1688symm       // Use 3xTF32 to emulate F32.  F32 in, converted in TF32-big and TF32-small internally,
                   // accumulated in F32, F32 out.
   s1688tf32symm   // Use 1xTF32.  F32 in, converted to one TF32 internally, accumulated in F32, F32 out.
 */
@@ -91,15 +91,15 @@
   bool help;
 
   cutlass::gemm::GemmCoord problem_size;
   float alpha;
   float beta;
   std::string rand_mode;
   int seed;
-  
+
   Options():
     help(false),
     problem_size({4096, 4096, 4096}),
     seed(1),
     alpha(1),
     beta(),
     rand_mode("uniform") { }
@@ -133,15 +133,15 @@
     cmd.get_cmd_line_argument("m", problem_size.m());
     cmd.get_cmd_line_argument("n", problem_size.n());
     // Since the kernels in this example are in Left Side Mode
     cmd.get_cmd_line_argument("m", problem_size.k());
 
     cmd.get_cmd_line_argument("alpha", alpha);
     cmd.get_cmd_line_argument("beta", beta);
-    
+
     cmd.get_cmd_line_argument("seed", seed);
     cmd.get_cmd_line_argument("rand_mode", rand_mode);
 
   }
 
   /// Prints the usage statement.
   std::ostream & print_usage(std::ostream &out) const {
@@ -203,18 +203,18 @@
                                                        // elements. This becomes the vector width of
                                                        // math instructions in the epilogue too
     float,                                   // <- data type of accumulator
     float>;                                  // <- data type for alpha/beta in linear combination function
 
 // Number of pipelines you want to use
 constexpr int NumStages = 3;
-// Alignment 
+// Alignment
 constexpr int Alignment = 4;
 
-// 
+//
 // CUTLASS Symm Operators (SSYM: Symm_3xTF32, Symm_1xTF32, DSYMM: Symm_F64)
 //
 
 // Symm_3xTF32
 using Symm_3xTF32 = cutlass::gemm::device::Symm<
                                               float,
                                               LayoutInputA,
@@ -229,15 +229,15 @@
                                               SmArch,
                                               ShapeMMAThreadBlock,
                                               ShapeMMAWarp,
                                               ShapeMMAOp,
                                               EpilogueOp,
                                               SwizzleThreadBlock,
                                               NumStages,
-                                              1, // Symmetric matrix is always align 1 
+                                              1, // Symmetric matrix is always align 1
                                               Alignment,
                                               false,
                                               cutlass::arch::OpMultiplyAddFastF32>;
 
 // Symm_1xTF32
 using Symm_1xTF32 = cutlass::gemm::device::Symm<
                                               float,
@@ -253,15 +253,15 @@
                                               SmArch,
                                               ShapeMMAThreadBlock,
                                               ShapeMMAWarp,
                                               ShapeMMAOp,
                                               EpilogueOp,
                                               SwizzleThreadBlock,
                                               NumStages,
-                                              1, // Symmetric matrix is always align 1 
+                                              1, // Symmetric matrix is always align 1
                                               Alignment,
                                               false,
                                               cutlass::arch::OpMultiplyAdd>;
 
 // Symm_F64
 using Symm_F64 = cutlass::gemm::device::Symm<
                                               double,
@@ -294,15 +294,15 @@
 
   ////////////////////////////////////////////////////////////////////////////////
   /// 1. Initialize F32 Precision input tensors using CUTLASS helper functions
   ////////////////////////////////////////////////////////////////////////////////
   cutlass::HostTensor<float, LayoutInputA> tensor_a_F32(problem_size.mk());  // <- Create matrix A with dimensions M x K
   cutlass::HostTensor<float, LayoutInputB> tensor_b_F32(problem_size.kn());  // <- Create matrix B with dimensions K x N
   cutlass::HostTensor<float, LayoutOutput> tensor_c_F32(problem_size.mn());  // <- Create matrix C with dimensions M x N
-  cutlass::HostTensor<float, LayoutOutput> tensor_d_F32(problem_size.mn());  // <- Create matrix D with dimensions M x N 
+  cutlass::HostTensor<float, LayoutOutput> tensor_d_F32(problem_size.mn());  // <- Create matrix D with dimensions M x N
 
   if (options.rand_mode == "uniform") {
     const float min = -1;
     const float max =  1;
     // Fill input and output matrices on host using CUTLASS helper functions
     cutlass::reference::host::TensorFillRandomUniform(
         tensor_a_F32.host_view(),
@@ -335,29 +335,29 @@
         tensor_c_F32.host_view(),
         options.seed,
         double(0),
         double(5));      // <- Fill matrix C on host with gaussian-distribution random data
   }
   cutlass::reference::host::TensorFill(
       tensor_d_F32.host_view());  // <- fill matrix D on host with zeros
-  
+
   // Copy data from host to GPU
   tensor_a_F32.sync_device();
   tensor_b_F32.sync_device();
   tensor_c_F32.sync_device();
   tensor_d_F32.sync_device();
 
   ////////////////////////////////////////////////////////////////////////////////
   /// 2. Initialize F64 tensors, Output tensors and setup arguments
   ////////////////////////////////////////////////////////////////////////////////
   // Symm F64 input operands (A, B, C)
   cutlass::HostTensor<double, LayoutInputA> tensor_a_F64(problem_size.mk());  // <- Create matrix A with dimensions M x K
   cutlass::HostTensor<double, LayoutInputB> tensor_b_F64(problem_size.kn());  // <- Create matrix B with dimensions K x N
   cutlass::HostTensor<double, LayoutOutput> tensor_c_F64(problem_size.mn());  // <- Create matrix C with dimensions M x N
-  
+
   // Symm output (D) for SYMM_3xTF32
   cutlass::HostTensor<float, LayoutOutput> tensor_d_3xTF32(problem_size.mn());  // <- Create matrix D with dimensions M x N
   // Symm output (D) for SYMM_1xTF32
   cutlass::HostTensor<float, LayoutOutput> tensor_d_1xTF32(problem_size.mn());  // <- Create matrix D with dimensions M x N
   // Symm output (D) for SYMM_F64
   cutlass::HostTensor<double, LayoutOutput> tensor_d_F64(problem_size.mn());  // <- Create matrix D with dimensions M x N
 #if CUTLASS_ENABLE_CUBLAS
@@ -371,15 +371,15 @@
   cutlass::reference::host::TensorCopy(tensor_c_F64.host_view(), tensor_c_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_d_F64.host_view(), tensor_d_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_d_3xTF32.host_view(), tensor_d_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_d_1xTF32.host_view(), tensor_d_F32.host_view());
 #if CUTLASS_ENABLE_CUBLAS
   cutlass::reference::host::TensorCopy(tensor_d_cublasF32.host_view(), tensor_d_F32.host_view());
 #endif
-  
+
   // Copy data from host to GPU
   tensor_a_F64.sync_device();
   tensor_b_F64.sync_device();
   tensor_c_F64.sync_device();
   tensor_d_F64.sync_device();
   tensor_d_3xTF32.sync_device();
   tensor_d_1xTF32.sync_device();
@@ -426,15 +426,15 @@
 
   // Allocate workspace memory
   cutlass::device_memory::allocation<uint8_t> workspace_3xtf32(workspace_size_3xtf32);
 
   // Instantiate CUTLASS kernel depending on templates
   Symm_3xTF32 symm_op_3xtf32;
 
-  // Check the problem size is supported or not 
+  // Check the problem size is supported or not
   cutlass::Status status_3xtf32 = symm_op_3xtf32.can_implement(arguments_3xtf32);
   CUTLASS_CHECK(status_3xtf32);
 
   // Initialize CUTLASS kernel with arguments and workspace pointer
   status_3xtf32 = symm_op_3xtf32.initialize(arguments_3xtf32, workspace_3xtf32.get());
   CUTLASS_CHECK(status_3xtf32);
 
@@ -473,15 +473,15 @@
 
   // Allocate workspace memory
   cutlass::device_memory::allocation<uint8_t> workspace_1xtf32(workspace_size_1xtf32);
 
   // Instantiate CUTLASS kernel depending on templates
   Symm_1xTF32 symm_op_1xtf32;
 
-  // Check the problem size is supported or not 
+  // Check the problem size is supported or not
   cutlass::Status status_1xtf32 = symm_op_1xtf32.can_implement(arguments_1xtf32);
   CUTLASS_CHECK(status_1xtf32);
 
   // Initialize CUTLASS kernel with arguments and workspace pointer
   status_1xtf32 = symm_op_1xtf32.initialize(arguments_1xtf32, workspace_1xtf32.get());
   CUTLASS_CHECK(status_1xtf32);
 
@@ -520,15 +520,15 @@
 
   // Allocate workspace memory
   cutlass::device_memory::allocation<uint8_t> workspace_f64(workspace_size_f64);
 
   // Instantiate CUTLASS kernel depending on templates
   Symm_F64 symm_op_f64;
 
-  // Check the problem size is supported or not 
+  // Check the problem size is supported or not
   cutlass::Status status_f64 = symm_op_f64.can_implement(arguments_f64);
   CUTLASS_CHECK(status_f64);
 
   // Initialize CUTLASS kernel with arguments and workspace pointer
   status_f64 = symm_op_f64.initialize(arguments_f64, workspace_f64.get());
   CUTLASS_CHECK(status_f64);
 
@@ -564,23 +564,23 @@
       static_cast<const float*>(tensor_a_F32.device_data()),
       int(tensor_a_F32.layout().stride(0)),
       static_cast<const float*>(tensor_b_F32.device_data()),
       int(tensor_b_F32.layout().stride(0)),
       static_cast<const float*>(&beta),
       static_cast<float*>(tensor_d_cublasF32.device_data()),
       int(tensor_d_cublasF32.layout().stride(0))
-    );   
+    );
 
   cudaDeviceSynchronize();
 
   tensor_d_cublasF32.sync_host();
 #endif
 
   ////////////////////////////////////////////////////////////////////////////////
-  /// 7. Compute l2 norms 
+  /// 7. Compute l2 norms
   ////////////////////////////////////////////////////////////////////////////////
 
 #if CUTLASS_ENABLE_CUBLAS
   // l2 norm cuBLAS F32 vs F64
   cutlass::HostTensor<double, LayoutOutput> tensor_d_cublasF32_in_F64(problem_size.mn());
   cutlass::reference::host::TensorCopy(tensor_d_cublasF32_in_F64.host_view(), tensor_d_cublasF32.host_view());
 
@@ -601,60 +601,60 @@
     tensor_d_1xTF32_in_F64.host_view(), tensor_d_F64.host_view());
 
 #if CUTLASS_ENABLE_CUBLAS
   // l2 norm 3xTF32 vs cuBLAS F32
   double l2_norm_3xtf32_vs_cublasf32 = cutlass::reference::host::TensorRelativeErrorMetric(
     tensor_d_3xTF32.host_view(), tensor_d_cublasF32.host_view());
 #endif
-  
+
   // l2 norm 3xTF32 vs 1xTF32
   double l2_norm_3xtf32_vs_1xtf32 = cutlass::reference::host::TensorRelativeErrorMetric(
     tensor_d_3xTF32.host_view(), tensor_d_1xTF32.host_view());
 
   ///////////////////////////////////////////////////////////////////////////////
 
-  // Print kernel info and L2 norms 
+  // Print kernel info and L2 norms
   std::cout << "Problem Size: (" << problem_size.m() << "," << problem_size.n() << "," << problem_size.k() << ") "
             << "Alpha: "  << alpha << "," << " Beta: "  << beta << std::endl;
   std::cout << std::fixed;
   std::cout << "Normalized L2 norm of" << std::endl;
   std::cout.precision(8);
-  std::cout << std::scientific 
+  std::cout << std::scientific
 #if CUTLASS_ENABLE_CUBLAS
             << " - cuBLAS F32 error with F64 reference    : " << l2_norm_cublasf32_vs_f64 << std::endl
 #endif
             << " - 3xTF32 error with F64 reference        : " << l2_norm_3xtf32_vs_f64 << std::endl
             << " - 1xTF32 error with F64 reference        : " << l2_norm_1xtf32_vs_f64 << std::endl
 #if CUTLASS_ENABLE_CUBLAS
             << " - 3xTF32 error with cuBLAS F32 reference : " << l2_norm_3xtf32_vs_cublasf32 << std::endl
 #endif
             << " - 3xTF32 error with 1xTF32 reference     : " << l2_norm_3xtf32_vs_1xtf32 << std::endl;
 
   return true;
 }
 
 int main(int argc, const char **argv) {
-  
+
   bool notSupported = false;
 
   // Ampere Tensor Core operations exposed with mma.sync and ldmatrix are first available
-  // in CUDA 11.0. 
+  // in CUDA 11.0.
   //
   // CUTLASS must be compiled with CUDA 11.0 Toolkit to run these examples.
   if (!(__CUDACC_VER_MAJOR__ >= 11)) {
     std::cerr << "Ampere Tensor Core operations must be compiled with CUDA 11.0 Toolkit or later." << std::endl;
     notSupported = true;
   }
 
   cudaDeviceProp props;
 
   cudaError_t error = cudaGetDeviceProperties(&props, 0);
   if (error != cudaSuccess) {
     std::cerr << "cudaGetDeviceProperties() returned an error: " << cudaGetErrorString(error) << std::endl;
-    return false;
+    return -1;
   }
 
   if (!((props.major * 10 + props.minor) >= 80)) {
     std::cerr << "Ampere Tensor Core operations must be run on a machine with compute capability at least 80."
               << std::endl;
     notSupported = true;
   }
```

## cutlass_library/source/examples/35_gemm_softmax/gemm_softmax.cu

```diff
@@ -452,15 +452,15 @@
     return status;
   }
 
   template<typename Element>
   bool verify_tensor(std::vector<Element> vector_Input, \
                        std::vector<Element> vector_Input_Ref) {
 
-    int64_t size = (vector_Input.size() < vector_Input_Ref.size()) ? vector_Input.size() : vector_Input_Ref.size();
+    auto size = int64_t((vector_Input.size() < vector_Input_Ref.size()) ? vector_Input.size() : vector_Input_Ref.size());
     float abs_tol = options.tolerance;
     float rel_tol = options.tolerance;
     
     for (int64_t i = 0; i < size; ++i) {
       float diff = (float)(vector_Input.at(i) - vector_Input_Ref.at(i));
       float abs_diff = fabs(diff);
       float abs_ref = fabs((float)vector_Input_Ref.at(i));
```

## cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu

```diff
@@ -450,56 +450,56 @@
 
   /// Random initialization
   void initialize() {
 
     cutlass::reference::host::TensorFillRandomUniform(
       tensor_A0.host_view(),
         options.seed,
-        ElementInputA0(5),
-        ElementInputA0(-5),
+        ElementInputA0(4),
+        ElementInputA0(-4),
         0
       );
 
     cutlass::reference::host::TensorFillRandomUniform(
       tensor_B0.host_view(),
         options.seed + 1,
-        ElementInputB0(5),
-        ElementInputB0(-5),
+        ElementInputB0(4),
+        ElementInputB0(-4),
         0
       );
 
     cutlass::reference::host::TensorFillRandomUniform(
       tensor_A1.host_view(),
         options.seed + 2,
-        ElementInputA1(5),
-        ElementInputA1(-5),
+        ElementInputA1(4),
+        ElementInputA1(-4),
         0
       );
 
     cutlass::reference::host::TensorFillRandomUniform(
       tensor_Beta.host_view(),
         options.seed + 3,
-        ElementInputScaleBias(5),
-        ElementInputScaleBias(-5),
+        ElementInputScaleBias(4),
+        ElementInputScaleBias(-4),
         0
       );
 
     cutlass::reference::host::TensorFillRandomUniform(
       tensor_Gamma.host_view(),
         options.seed + 4,
-        ElementInputScaleBias(5),
-        ElementInputScaleBias(-5),
+        ElementInputScaleBias(4),
+        ElementInputScaleBias(-4),
         0
       );
 
     cutlass::reference::host::TensorFillRandomUniform(
       tensor_Shifted_K.host_view(),
         options.seed + 5,
-        ElementOutput(5),
-        ElementOutput(-6),
+        ElementOutput(4),
+        ElementOutput(-5),
         0
       );
 
     tensor_A0.sync_device();
     tensor_B0.sync_device();
     tensor_A1.sync_device();
     tensor_Beta.sync_device();
```

## cutlass_library/source/examples/38_syr2k_grouped/syr2k_grouped.cu

```diff
@@ -799,15 +799,15 @@
     else {
       cuda_streams.push_back(nullptr);
     }
 
     // Use 'D' for the in/out workspace
     this->block_D.copy_from_device(this->block_C.get());
 
-    for (int i = 0; i < this->options.problem_sizes.size(); ++i) {
+    for (size_t i = 0; i < this->options.problem_sizes.size(); ++i) {
       cutlass::gemm::GemmCoord const & problem = this->options.problem_sizes[i];
       int32_t batch_count = 1;
       int64_t lda = this->lda_host.at(i);
       int64_t ldb = this->ldb_host.at(i);
       int64_t ldc = this->ldc_host.at(i);
       typename Rank2K::ElementA* ptrA = this->block_A.get() + this->offset_A.at(i);
       typename Rank2K::ElementB* ptrB = this->block_B.get() + this->offset_B.at(i);
@@ -900,18 +900,18 @@
       return result;
     }
 
     //
     // Run profiling loop
     //
 
-    int last_stream_idx = 0;
+    size_t last_stream_idx = 0;
 
     for (int iter = 0; iter < this->options.iterations; ++iter) {
-      for (int i = 0; i < this->options.problem_sizes.size(); ++i) {
+      for (size_t i = 0; i < this->options.problem_sizes.size(); ++i) {
         cutlass::gemm::GemmCoord const & problem = this->options.problem_sizes[i];
         int32_t batch_count = 1;
         int64_t lda = this->lda_host.at(i);
         int64_t ldb = this->ldb_host.at(i);
         int64_t ldc = this->ldc_host.at(i);
         typename Rank2K::ElementA* ptrA = this->block_A.get() + this->offset_A.at(i);
         typename Rank2K::ElementB* ptrB = this->block_B.get() + this->offset_B.at(i);
@@ -1142,15 +1142,15 @@
       this->ldb.get(),
       this->ldc.get(),
       this->ldd.get(),
       this->options.problem_sizes.data()
     );
 
     // Initialize the Rank2K object
-    Rank2K rank2k;
+    Rank2K rank2k{};
     size_t workspace_size = rank2k.get_workspace_size(args);
     cutlass::DeviceAllocation<uint8_t> workspace(workspace_size);
 
     result.status = rank2k.initialize(args, workspace.get());
 
     if (result.status != cutlass::Status::kSuccess) {
       std::cerr << "Failed to initialize CUTLASS Grouped Rank2K kernel." << std::endl;
```

## cutlass_library/source/examples/41_fused_multi_head_attention/debug_utils.h

```diff
@@ -36,15 +36,15 @@
 
 ////////////////////////////////////////////////////////////////////////////////
 // Debugging functions
 ////////////////////////////////////////////////////////////////////////////////
 // Nans & inf detection
 #define NANCHECK(frag)                         \
   {                                            \
-    for (int _i = 0; _i < frag.size(); ++_i) { \
+    for (size_t _i = 0; _i < frag.size(); ++_i) { \
       assert(std::isfinite(float(frag[_i])));  \
       assert(!std::isnan(float(frag[_i])));    \
     }                                          \
   }
 
 // Print on the first thread of the first block
 #if 1
@@ -143,15 +143,15 @@
       float(accum[start + 6]),                        \
       float(accum[start + 7]));
 #define PRINT_ACCUM8_T0_L0(name, accum) PRINT_ACCUM8_T0_L0_START(name, accum, 0)
 #define PRINT_FRAG_T0_L0(name, frag)                          \
   {                                                           \
     auto typeStr = __get_type_name<decltype(frag)>();         \
     PRINT_B0_T0("printing %s (%s)", name, typeStr.data);      \
-    for (int _start = 0; _start < frag.size(); _start += 8) { \
+    for (size_t _start = 0; _start < frag.size(); _start += 8) { \
       PRINT_ACCUM8_T0_L0_START("  ", frag, _start);           \
     }                                                         \
     /*__syncthreads();                                        \
     NANCHECK(frag); */                                        \
   }
 #define PRINT_ARRAY_T0_L0_INCR(name, array, length, incr)   \
   {                                                         \
```

## cutlass_library/source/examples/41_fused_multi_head_attention/fmha_grouped.h

```diff
@@ -163,66 +163,47 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    GemmCoord *problem_sizes0;
-    GemmCoord *problem_sizes1;
+    GemmCoord *problem_sizes0{nullptr};
+    GemmCoord *problem_sizes1{nullptr};
 
-    int problem_count;
-    int threadblock_count;
+    int problem_count{0};
+    int threadblock_count{0};
 
-    ElementQ ** ptr_Q;
-    ElementK ** ptr_K;
-    ElementP ** ptr_P;
-    ElementV ** ptr_V;
-    ElementO ** ptr_O;
-    ElementOAccum ** ptr_O_accum;
-
-    typename LayoutQ::Stride::LongIndex *ldq;
-    typename LayoutK::Stride::LongIndex *ldk;
-    typename LayoutP::Stride::LongIndex *ldv;
-    typename LayoutO::Stride::LongIndex *ldo;
-
-    // Scale
-    ElementAccumulator scale;
+    ElementQ ** ptr_Q{nullptr};
+    ElementK ** ptr_K{nullptr};
+    ElementP ** ptr_P{nullptr};
+    ElementV ** ptr_V{nullptr};
+    ElementO ** ptr_O{nullptr};
+    ElementOAccum ** ptr_O_accum{nullptr};
+
+    typename LayoutQ::Stride::LongIndex *ldq{nullptr};
+    typename LayoutK::Stride::LongIndex *ldk{nullptr};
+    typename LayoutP::Stride::LongIndex *ldv{nullptr};
+    typename LayoutO::Stride::LongIndex *ldo{nullptr};
 
     // Whether causal masking is to be performed
-    bool causal;
+    bool causal{false};
+
+    // Scale
+    ElementAccumulator scale{0};
 
     // Only used by device-level operator
-    GemmCoord *host_problem_sizes;
+    GemmCoord *host_problem_sizes{nullptr};
 
     //
     // Methods
     //
-
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Arguments():
-      problem_count(0),
-      threadblock_count(0),
-      ptr_Q(nullptr),
-      ptr_K(nullptr),
-      ptr_P(nullptr),
-      ptr_V(nullptr),
-      ptr_O(nullptr),
-      ptr_O_accum(nullptr),
-      ldq(nullptr),
-      ldk(nullptr),
-      ldv(nullptr),
-      ldo(nullptr),
-      scale(0),
-      causal(false),
-      host_problem_sizes(nullptr)
-    {
-
-    }
+  
+      /// Default ctor
+    Arguments() = default;
 
     /// Ctor
     CUTLASS_HOST_DEVICE
     Arguments(
       GemmCoord *problem_sizes0,
       GemmCoord *problem_sizes1,
       int problem_count,
```

## cutlass_library/source/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu

```diff
@@ -282,15 +282,15 @@
 
   /// Compute performance in GFLOP/s
   double gflops(double runtime_s) const {
 
     // Number of real-valued multiply-adds 
     int64_t fops = int64_t();
 
-    for (int i = 0; i < problem_sizes0.size(); ++i) {
+    for (size_t i = 0; i < problem_sizes0.size(); ++i) {
       auto const& problem0 = problem_sizes0[i];
       auto const& problem1 = problem_sizes1[i];
       for (int row = 0; row < problem0.m(); ++row) {
         int num_cols0 = problem0.n();
         if (causal) {
           num_cols0 = std::min(row + 1, num_cols0);
         }
```

## cutlass_library/source/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu

```diff
@@ -336,15 +336,15 @@
 
   /// Compute performance in GFLOP/s
   double gflops(double runtime_s) const {
 
     // Number of real-valued multiply-adds 
     int64_t fops = int64_t();
 
-    for (int i = 0; i < problem_sizes0.size(); ++i) {
+    for (size_t i = 0; i < problem_sizes0.size(); ++i) {
       auto const& problem0 = problem_sizes0[i];
       auto const& problem1 = problem_sizes1[i];
 
       for (int row = 0; row < problem0.m(); ++row) {
         int num_cols0 = problem0.n();
         if (causal) {
           num_cols0 = std::min(row + 1, num_cols0);
```

## cutlass_library/source/examples/41_fused_multi_head_attention/kernel_backward.h

```diff
@@ -1442,15 +1442,15 @@
       int32_t query_start,
       int32_t key_start,
       const curandStatePhilox4_32_10_t& curand_state_init,
       uint8_t warp_id,
       uint8_t lane_id) {
     cutlass::Array<cutlass::uint1b_t, MatmulDOIVJ::Mma::FragmentC::kElements>
         dropout_keep_mask_doivj;
-    dropout_keep_mask_doivj.fill(1);
+    dropout_keep_mask_doivj.fill(cutlass::uint1b_t{1});
     const float dropout_scale =
         kApplyDropout ? 1.0 / (1.0 - p.dropout_prob) : 1.0f;
 
     cutlass::MatrixCoord no_offset{0, 0};
     accum_t scale = p.scale;
     int16_t thread_id = 32 * warp_id + lane_id;
 
@@ -1740,15 +1740,15 @@
         auto lane_offset = MatmulDOIVJ::AccumLambdaIterator::get_lane_offset(
             lane_id, warp_id, output_tile_coords_doivj);
         MatmulDOIVJ::AccumLambdaIterator::iterateRows(
             lane_offset,
             [&](int accum_m) {},
             [&](int accum_m /*q*/, int accum_n /*k*/, int idx) {
               if (zij.at({accum_n, accum_m}) == scalar_t(0)) {
-                dropout_keep_mask_doivj[idx] = cutlass::uint1b_t(0);
+                dropout_keep_mask_doivj[idx] = cutlass::uint1b_t{0};
               }
             },
             [&](int accum_m) {});
       }
       __syncthreads();
     }
     rematerializeThreadIds();
```

## cutlass_library/source/examples/41_fused_multi_head_attention/kernel_forward.h

```diff
@@ -36,15 +36,14 @@
 #include <ATen/cuda/CUDAGraphsUtils.cuh>
 #endif
 
 #include <curand_kernel.h>
 #include <cmath>
 #include <vector>
 
-#include "cutlass/bfloat16.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/layout/vector.h"
 #include "cutlass/matrix.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/tensor_ref.h"
```

## cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h

```diff
@@ -240,19 +240,21 @@
             thread_idx,
             warp_idx,
             lane_idx) {}
 
   CUTLASS_DEVICE
   bool set_prologue_done(bool value) {
     prologue_done_ = value;
+    return true;
   }
 
   CUTLASS_DEVICE
   bool set_zero_outside_bounds(bool value) {
     zero_outside_bounds_ = value;
+    return true;
   }
 
   template <bool kLoadA = true, bool kLoadB = true>
   CUTLASS_DEVICE static void prologue(
       typename Base::SharedStorage& shared_storage,
       ///< iterator over A operand in global memory
       IteratorA iterator_A,
```

## cutlass_library/source/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu

```diff
@@ -448,15 +448,15 @@
   /// Returns the number of threadblocks to launch if the kernel can run on the target
   /// device. Otherwise, returns zero.
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
-    int smem_size = int(sizeof(typename Gemm::GemmKernel::SharedStorage));
+    size_t smem_size = sizeof(typename Gemm::GemmKernel::SharedStorage);
 
     cudaDeviceProp properties;
     int device_idx;
     cudaError_t result = cudaGetDevice(&device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDevice() API call failed.");
@@ -505,15 +505,15 @@
       options.a_ell_num_columns,
       options.a_ell_blocksize,
       options.a_base,
       epilogue_op 
     );
 
     // Initialize the GEMM object
-    Gemm gemm;
+    Gemm gemm{};
 
     result.status = gemm.initialize(args);
 
     if (result.status != cutlass::Status::kSuccess) {
       std::cerr << "Failed to initialize CUTLASS BlockedEll SpMM kernel." << std::endl;
       return result;
     }
```

## cutlass_library/source/examples/51_hopper_gett/gett_kernel.cuh

```diff
@@ -98,15 +98,16 @@
   // CollectiveMma for GETTs can be built using the CollectiveBuilders
   using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
       cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
       ElementA, StrideA, 128 / cutlass::sizeof_bits<ElementA>::value,
       ElementB, StrideB, 128 / cutlass::sizeof_bits<ElementB>::value,
       ElementAccumulator,
       TileShape, Shape<_1,_2,_1>,
-      cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename CollectiveEpilogue::SharedStorage)>,
+      cutlass::gemm::collective::StageCountAutoCarveout<
+        static_cast<int>(sizeof(typename CollectiveEpilogue::SharedStorage))>,
       cutlass::gemm::collective::KernelScheduleAuto
     >::CollectiveOp;
 
   // The GETT kernel is a composition of a collective mainloop and epilogue, just like any 3.x GEMM
   using GettKernel = cutlass::gemm::kernel::GemmUniversal<
       ProblemShapeMNKL,
       CollectiveMainloop,
```

## cutlass_library/source/examples/52_hopper_gather_scatter_fusion/52_hopper_gather_scatter_fusion.cu

```diff
@@ -285,15 +285,16 @@
   using MainloopOpt = typename cutlass::gemm::collective::CollectiveBuilder<
     cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
     ElementA, LayoutA, 128 / cutlass::sizeof_bits<ElementA>::value,
     ElementB, LayoutB, 128 / cutlass::sizeof_bits<ElementB>::value,
     ElementAccumulator,
     Shape<_128,_128,_64>,
     Shape<_2,_2,_1>,
-    cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename EpilogueOpt::SharedStorage)>,
+    cutlass::gemm::collective::StageCountAutoCarveout<
+      static_cast<int>(sizeof(typename EpilogueOpt::SharedStorage))>,
     cutlass::gemm::collective::KernelScheduleAuto
   >::CollectiveOp;
 
   using KernelOpt = cutlass::gemm::kernel::GemmUniversal<
     ProblemShape,
     MainloopOpt,
     EpilogueOpt,
```

## cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_gemm.hpp

```diff
@@ -35,14 +35,19 @@
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/dispatch_policy.hpp"
 
 #include "cute/tensor.hpp"
 
 #include "gather_tensor.hpp"
 
+namespace cutlass {
+  ///Forward declaration
+  struct CudaHostAdapter;
+}
+
 namespace cutlass::gemm::kernel {
 
 ///////////////////////////////////////////////////////////////////////////////
 
 template <
   class ProblemShape_,
   class CollectiveMainloop_,
@@ -139,18 +144,18 @@
     TileSchedulerArguments scheduler{};
     GatherA gather_A{};
     GatherB gather_B{};
   };
 
   // Kernel entry point API
   struct Params {
-    GemmUniversalMode mode;
-    ProblemShape problem_shape;
-    MainloopParams mainloop;
-    EpilogueParams epilogue;
+    GemmUniversalMode mode{};
+    ProblemShape problem_shape{};
+    MainloopParams mainloop{};
+    EpilogueParams epilogue{};
     GatherA gather_A{};
     GatherB gather_B{};
   };
 
   //
   // Methods
   //
@@ -187,22 +192,23 @@
     }
     implementable &= CollectiveMainloop::can_implement(args.problem_shape, args.mainloop);
     implementable &= CollectiveEpilogue::can_implement(args.problem_shape, args.epilogue);
     return implementable;
   }
 
   static
-  int
+  size_t
   get_workspace_size(Arguments const& args) {
     return 0;
   }
 
   static
   cutlass::Status
-  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr) {
+  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return Status::kSuccess;
   }
 
   // Computes the kernel launch grid shape based on runtime parameters
   static dim3
   get_grid_shape(Params const& params) {
     auto cluster_shape = Shape<_1,_1,_1>{};
```

## cutlass_library/source/examples/52_hopper_gather_scatter_fusion/scatter_epilogue.hpp

```diff
@@ -35,15 +35,15 @@
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/dispatch_policy.hpp"
 #include "cutlass/epilogue/collective/detail.hpp"
 
 #include "cute/tensor.hpp"
-#include "cute/numeric/int.hpp"
+#include "cute/numeric/numeric_types.hpp"
 
 #include "gather_tensor.hpp"
 
 namespace cutlass::epilogue::collective {
 
 /// Applies an element wise operation to all elements within the fragment
 /// and scatter-writes them out to destination storage.
```

## cutlass_library/source/examples/53_hopper_gemm_permute/53_hopper_gemm_permute.cu

```diff
@@ -389,25 +389,27 @@
 
   using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
     cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
     ElementA, StrideA, 128 / cutlass::sizeof_bits<ElementA>::value,
     ElementB, StrideB, 128 / cutlass::sizeof_bits<ElementB>::value,
     ElementAccumulator,
     TileShape, ClusterShape,
-    cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename CollectiveEpilogue::SharedStorage)>,
+    cutlass::gemm::collective::StageCountAutoCarveout<
+      static_cast<int>(sizeof(typename CollectiveEpilogue::SharedStorage))>,
     cutlass::gemm::collective::KernelScheduleAuto
   >::CollectiveOp;
 
   using CollectiveMainloopPermute = typename cutlass::gemm::collective::CollectiveBuilder<
     cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
     ElementA, StrideAPermute, 128 / cutlass::sizeof_bits<ElementA>::value,
     ElementB, StrideBPermute, 128 / cutlass::sizeof_bits<ElementB>::value,
     ElementAccumulator,
     TileShapePermute, ClusterShape,
-    cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename CollectiveEpiloguePermute::SharedStorage)>,
+    cutlass::gemm::collective::StageCountAutoCarveout<
+      static_cast<int>(sizeof(typename CollectiveEpiloguePermute::SharedStorage))>,
     cutlass::gemm::collective::KernelScheduleAuto
   >::CollectiveOp;
 
   using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
     ProblemShape,
     CollectiveMainloop,
     CollectiveEpilogue
```

## cutlass_library/source/examples/53_hopper_gemm_permute/permute_kernel.cuh

```diff
@@ -33,15 +33,15 @@
     \brief Simple permutation kernel implementation.
 */
 
 #include "cutlass/layout/pitch_linear.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/tensor_view.h"
 #include "cutlass/fast_math.h"
-#include "cute/numeric/uint128.hpp"
+#include "cute/numeric/numeric_types.hpp"
 
 namespace example
 {
 
 /**
  * Assumes column-major input (M mode is contiguous, N mode is strided).
  * For row major, the inputs must be switched accordingly.
```

## cutlass_library/source/examples/cute/tutorial/CMakeLists.txt

```diff
@@ -25,16 +25,31 @@
 # SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 # CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 # OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 
 cutlass_example_add_executable(
-  sgemm_nt_1
-  sgemm_nt_1.cu
+  sgemm_1
+  sgemm_1.cu
+)
+
+cutlass_example_add_executable(
+  sgemm_2
+  sgemm_2.cu
+)
+
+cutlass_example_add_executable(
+  sgemm_sm70
+  sgemm_sm70.cu
+)
+
+cutlass_example_add_executable(
+  sgemm_sm80
+  sgemm_sm80.cu
 )
 
 cutlass_example_add_executable(
   tiled_copy
   tiled_copy.cu
 )
```

## cutlass_library/source/examples/cute/tutorial/tiled_copy.cu

```diff
@@ -63,53 +63,54 @@
 // does not perform predication.
 
 
 /// Simple copy kernel.
 //
 // Uses local_partition() to partition a tile among threads arranged as (THR_M, THR_N).
 template <class TensorS, class TensorD, class ThreadLayout>
-__global__ void copy_kernel(TensorS S, TensorD D, ThreadLayout) 
+__global__ void copy_kernel(TensorS S, TensorD D, ThreadLayout)
 {
   using namespace cute;
 
   // Slice the tiled tensors
   Tensor tile_S = S(make_coord(_,_), blockIdx.x, blockIdx.y);   // (BlockShape_M, BlockShape_N)
   Tensor tile_D = D(make_coord(_,_), blockIdx.x, blockIdx.y);   // (BlockShape_M, BlockShape_N)
 
   // Construct a partitioning of the tile among threads with the given thread arrangement.
 
-  // Concept:                       Tensor    Layout          Index
-  Tensor thr_tile_S = local_partition(tile_S, ThreadLayout{}, threadIdx.x);
-  Tensor thr_tile_D = local_partition(tile_D, ThreadLayout{}, threadIdx.x);
+  // Concept:                         Tensor  ThrLayout       ThrIndex
+  Tensor thr_tile_S = local_partition(tile_S, ThreadLayout{}, threadIdx.x);  // (ThrValM, ThrValN)
+  Tensor thr_tile_D = local_partition(tile_D, ThreadLayout{}, threadIdx.x);  // (ThrValM, ThrValN)
 
   // Construct a register-backed Tensor with the same shape as each thread's partition
-  auto fragment = make_fragment_like(thr_tile_S);
+  // Use make_tensor to try to match the layout of thr_tile_S
+  Tensor fragment = make_tensor_like(thr_tile_S);               // (ThrValM, ThrValN)
 
   // Copy from GMEM to RMEM and from RMEM to GMEM
   copy(thr_tile_S, fragment);
   copy(fragment, thr_tile_D);
 }
 
 /// Vectorized copy kernel.
 ///
 /// Uses `make_tiled_copy()` to perform a copy using vector instructions. This operation
 /// has the precondition that pointers are aligned to the vector size.
 ///
 template <class TensorS, class TensorD, class ThreadLayout, class VecLayout>
-__global__ void copy_kernel_vectorized(TensorS S, TensorD D, ThreadLayout, VecLayout) 
+__global__ void copy_kernel_vectorized(TensorS S, TensorD D, ThreadLayout, VecLayout)
 {
   using namespace cute;
   using Element = typename TensorS::value_type;
 
   // Slice the tensors to obtain a view into each tile.
-  Tensor tile_S = S(make_coord(_, _), blockIdx.x, blockIdx.y);   // (BlockShape_M, BlockShape_N)
-  Tensor tile_D = D(make_coord(_, _), blockIdx.x, blockIdx.y);   // (BlockShape_M, BlockShape_N)
+  Tensor tile_S = S(make_coord(_, _), blockIdx.x, blockIdx.y);  // (BlockShape_M, BlockShape_N)
+  Tensor tile_D = D(make_coord(_, _), blockIdx.x, blockIdx.y);  // (BlockShape_M, BlockShape_N)
 
   // Define `AccessType` which controls the size of the actual memory access.
-  using AccessType = cutlass::AlignedArray<Element, size(shape(VecLayout{}))>;
+  using AccessType = cutlass::AlignedArray<Element, size(VecLayout{})>;
 
   // A copy atom corresponds to one hardware memory access.
   using Atom = Copy_Atom<UniversalCopy<AccessType>, Element>;
 
   // Construct tiled copy, a tiling of copy atoms.
   //
   // Note, this assumes the vector and thread layouts are aligned with contigous data
@@ -121,114 +122,106 @@
       Atom{},                       // access size
       ThreadLayout{},               // thread layout
       VecLayout{});                 // vector layout (e.g. 4x1)
 
   // Construct a Tensor corresponding to each thread's slice.
   auto thr_copy = tiled_copy.get_thread_slice(threadIdx.x);
 
-  Tensor thr_tile_S = thr_copy.partition_S(tile_S);
-  Tensor thr_tile_D = thr_copy.partition_D(tile_D);
+  Tensor thr_tile_S = thr_copy.partition_S(tile_S);             // (CopyOp, CopyM, CopyN)
+  Tensor thr_tile_D = thr_copy.partition_D(tile_D);             // (CopyOp, CopyM, CopyN)
 
   // Construct a register-backed Tensor with the same shape as each thread's partition
-  auto fragment = make_fragment_like(thr_tile_D);
+  // Use make_fragment because the first mode is the instruction-local mode
+  Tensor fragment = make_fragment_like(thr_tile_D);             // (CopyOp, CopyM, CopyN)
 
   // Copy from GMEM to RMEM and from RMEM to GMEM
   copy(tiled_copy, thr_tile_S, fragment);
   copy(tiled_copy, fragment, thr_tile_D);
 }
 
-/// Helper to convert a shape to a dim3
-template <class Shape>
-dim3 shape_to_dim3(Shape shape)
-{
-  using namespace cute;
-
-  CUTE_STATIC_ASSERT_V(rank(shape) <= Int<3>{});
-  auto result = append<3>(product_each(shape), 1u);
-
-  return dim3(get<0>(result), get<1>(result), get<2>(result));
-}
-
 /// Main function
 int main(int argc, char** argv)
 {
   //
   // Given a 2D shape, perform an efficient copy
   //
 
   using namespace cute;
   using Element = float;
 
   // Define a tensor shape with dynamic extents (m, n)
   auto tensor_shape = make_shape(256, 512);
 
-  thrust::host_vector<Element> h_S(size(tensor_shape));
-  thrust::host_vector<Element> h_D(size(tensor_shape));
-
   //
-  // Initialize
+  // Allocate and initialize
   //
 
+  thrust::host_vector<Element> h_S(size(tensor_shape));
+  thrust::host_vector<Element> h_D(size(tensor_shape));
+
   for (size_t i = 0; i < h_S.size(); ++i) {
     h_S[i] = static_cast<Element>(i);
     h_D[i] = Element{};
   }
 
   thrust::device_vector<Element> d_S = h_S;
   thrust::device_vector<Element> d_D = h_D;
 
   //
   // Make tensors
   //
 
-  Tensor tensor_S = make_tensor(make_gmem_ptr(d_S.data().get()), make_layout(tensor_shape));  
-  Tensor tensor_D = make_tensor(make_gmem_ptr(d_D.data().get()), make_layout(tensor_shape));
+  Tensor tensor_S = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_S.data())), make_layout(tensor_shape));
+  Tensor tensor_D = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_D.data())), make_layout(tensor_shape));
 
   //
-  // Partition
+  // Tile tensors
   //
 
-
   // Define a statically sized block (M, N).
-  //
   // Note, by convention, capital letters are used to represent static modes.
   auto block_shape = make_shape(Int<128>{}, Int<64>{});
 
-  if ((get<0>(tensor_shape) % get<0>(block_shape)) || (get<1>(tensor_shape) % get<1>(block_shape))) {
+  if ((size<0>(tensor_shape) % size<0>(block_shape)) || (size<1>(tensor_shape) % size<1>(block_shape))) {
     std::cerr << "The tensor shape must be divisible by the block shape." << std::endl;
     return -1;
   }
+  // Equivalent check to the above
+  if (not weakly_compatible(block_shape, tensor_shape)) {
+    std::cerr << "Expected the tensors to be weakly compatible with the block_shape." << std::endl;
+    return -1;
+  }
 
-  // Tile the tensor (m, m) ==> ((M, N), m', n') where (M, N) is the static tile
+  // Tile the tensor (m, n) ==> ((M, N), m', n') where (M, N) is the static tile
   // shape, and modes (m', n') correspond to the number of tiles.
-  // 
-  // These will be used to determine the CUDA kernel grid dimensinos.
-  Tensor tiled_tensor_S = tiled_divide(tensor_S, block_shape);
-  Tensor tiled_tensor_D = tiled_divide(tensor_D, block_shape);
+  //
+  // These will be used to determine the CUDA kernel grid dimensions.
+  Tensor tiled_tensor_S = tiled_divide(tensor_S, block_shape);      // ((M, N), m', n')
+  Tensor tiled_tensor_D = tiled_divide(tensor_D, block_shape);      // ((M, N), m', n')
 
   // Thread arrangement
-  Layout thr_layout = make_layout(make_shape(Int<32>{}, Int< 8>{}));
+  Layout thr_layout = make_layout(make_shape(Int<32>{}, Int<8>{}));
 
   // Vector dimensions
   Layout vec_layout = make_layout(make_shape(Int<4>{}, Int<1>{}));
 
   //
   // Determine grid and block dimensions
   //
 
-  dim3 gridDim = shape_to_dim3(select<1,2>(shape(tiled_tensor_D))); // Grid shape corresponds to  modes m' and n'
-  dim3 blockDim(size(shape(thr_layout)));
+  dim3 gridDim (size<1>(tiled_tensor_D), size<2>(tiled_tensor_D));   // Grid shape corresponds to modes m' and n'
+  dim3 blockDim(size(thr_layout));
 
   //
   // Launch the kernel
   //
   copy_kernel_vectorized<<< gridDim, blockDim >>>(
-    tiled_tensor_S, 
-    tiled_tensor_D, 
-    thr_layout, 
+    tiled_tensor_S,
+    tiled_tensor_D,
+    thr_layout,
     vec_layout);
 
   cudaError result = cudaDeviceSynchronize();
   if (result != cudaSuccess) {
     std::cerr << "CUDA Runtime error: " << cudaGetErrorString(result) << std::endl;
     return -1;
   }
```

## cutlass_library/source/include/cute/config.hpp

```diff
@@ -87,80 +87,76 @@
 // constexpr ... else" statement must actually return.  Thus, GCC
 // emits spurious "missing return statement" build warnings.
 // Developers can suppress these warnings by using the
 // CUTE_GCC_UNREACHABLE macro, which must be followed by a semicolon.
 // It's harmless to use the macro for other GCC versions or other
 // compilers, but it has no effect.
 #if ! defined(CUTE_GCC_UNREACHABLE)
-#  if defined(__clang__) || defined(__GNUC__)
+#  if defined(__GNUC__)
 #    define CUTE_GCC_UNREACHABLE __builtin_unreachable()
 #  else
 #    define CUTE_GCC_UNREACHABLE
 #  endif
 #endif
 
-#ifdef _MSC_VER
+#if defined(_MSC_VER)
 // Provides support for alternative operators 'and', 'or', and 'not'
-#include <iso646.h>
+#  include <iso646.h>
 #endif // _MSC_VER
 
 #if defined(__CUDACC_RTC__)
-#define CUTE_STL_NAMESPACE cuda::std
-#define CUTE_STL_NAMESPACE_IS_CUDA_STD
+#  define CUTE_STL_NAMESPACE cuda::std
+#  define CUTE_STL_NAMESPACE_IS_CUDA_STD
 #else
-#define CUTE_STL_NAMESPACE std
+#  define CUTE_STL_NAMESPACE std
 #endif
 
 //
 // Assertion helpers
 //
 
 #if defined(__CUDACC_RTC__)
-#include <cuda/std/cassert>
+#  include <cuda/std/cassert>
 #else
-#include <cassert>
+#  include <cassert>
 #endif
 
 #define CUTE_STATIC_V(x)            decltype(x)::value
 
 #define CUTE_STATIC_ASSERT          static_assert
 #define CUTE_STATIC_ASSERT_V(x,...) static_assert(decltype(x)::value, ##__VA_ARGS__)
 
+// Fail and print a message. Typically used for notification of a compiler misconfiguration.
 #if defined(__CUDA_ARCH__)
-#  define CUTE_RUNTIME_ASSERT(x) __brkpt()
+#  define CUTE_INVALID_CONTROL_PATH(x) assert(0 && x); printf(x); __brkpt()
 #else
-#  define CUTE_RUNTIME_ASSERT(x) assert(0 && x)
+#  define CUTE_INVALID_CONTROL_PATH(x) assert(0 && x); printf(x)
 #endif
 
 //
 // IO
 //
 
 #if !defined(__CUDACC_RTC__)
-#include <cstdio>
-#include <iostream>
-#include <iomanip>
+#  include <cstdio>
+#  include <iostream>
+#  include <iomanip>
 #endif
 
 //
 // Support
 //
 
 #include <cute/util/type_traits.hpp>
 
 //
 // Basic types
 //
 
-#include <cute/numeric/int.hpp>
-#include <cute/numeric/real.hpp>
-#include <cute/numeric/half.hpp>
-#include <cute/numeric/float8.hpp>
-#include <cute/numeric/bfloat.hpp>
-#include <cute/numeric/tfloat.hpp>
-#include <cute/numeric/complex.hpp>
+#include <cute/numeric/numeric_types.hpp>
+
 //
 // Debugging utilities
 //
 
 #include <cute/util/print.hpp>
 #include <cute/util/debug.hpp>
```

## cutlass_library/source/include/cute/int_tuple.hpp

```diff
@@ -214,29 +214,29 @@
 template <class Tuple>
 static constexpr int depth_v = depth_t<Tuple>::value;
 
 //
 // product
 //
 
-// Implementation of product (see below) as a function object
+// Implementation of product as a function object
 struct Product
 {
   template <class IntTuple>
   CUTE_HOST_DEVICE constexpr
   auto
   operator()(IntTuple const& a) const
   {
     if constexpr (is_tuple<IntTuple>::value) {
       if constexpr (tuple_size<IntTuple>::value == 0) {
         return Int<1>{};
       } else {
         return cute::transform_apply(a, Product{}, multiplies_unary_lfold{});
       }
-    } else {
+    } else if constexpr (cute::is_integral<IntTuple>::value) {
       return a;
     }
 
     CUTE_GCC_UNREACHABLE;
   }
 };
 // Callable product function object
@@ -244,15 +244,15 @@
 
 // Return a rank(t) tuple @a result such that get<i>(@a result) = product(get<i>(@a t))
 template <class Tuple>
 CUTE_HOST_DEVICE constexpr
 auto
 product_each(Tuple const& t)
 {
-  return transform(wrap(t), [](auto const& x) { return product(x); });
+  return transform(wrap(t), product);
 }
 
 // Take the product of Tuple at the leaves of TupleG
 template <class Tuple, class TupleG>
 CUTE_HOST_DEVICE constexpr
 auto
 product_like(Tuple const& tuple, TupleG const& guide)
@@ -321,18 +321,29 @@
 //
 
 template <class IntTupleA, class IntTupleB>
 CUTE_HOST_DEVICE constexpr
 auto
 ceil_div(IntTupleA const& a, IntTupleB const& b)
 {
-  if constexpr (is_tuple<IntTupleA>::value && is_tuple<IntTupleB>::value) {
-    static_assert(tuple_size<IntTupleA>::value >= tuple_size<IntTupleB>::value, "Mismatched ranks");
-    constexpr int R = tuple_size<IntTupleA>::value;        // Missing ranks in TupleB are implicitly 1
-    return transform(a, append<R>(b,Int<1>{}), [](auto const& x, auto const& y) { return ceil_div(x,y); });
+  if constexpr (is_tuple<IntTupleA>::value) {
+    if constexpr (is_tuple<IntTupleB>::value) {  // tuple tuple
+      static_assert(tuple_size<IntTupleA>::value >= tuple_size<IntTupleB>::value, "Mismatched ranks");
+      constexpr int R = tuple_size<IntTupleA>::value;        // Missing ranks in TupleB are implicitly 1
+      return transform(a, append<R>(b,Int<1>{}), [](auto const& x, auto const& y) { return ceil_div(x,y); });
+    } else {                                     // tuple int
+      auto const [result, rest] = fold(a, cute::make_tuple(cute::make_tuple(), b),
+        [] (auto const& init, auto const& ai) {
+          return cute::make_tuple(append(get<0>(init), ceil_div(ai, get<1>(init))), ceil_div(get<1>(init), ai));
+        });
+      return result;
+    }
+  } else
+  if constexpr (is_tuple<IntTupleB>::value) {    // int tuple
+    return ceil_div(a, product(b));
   } else {
     return (a + b - Int<1>{}) / b;
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
@@ -390,15 +401,15 @@
   if constexpr (is_tuple<IntTupleB>::value) {    // int tuple
     return shape_div(a, product(b));
   } else
   if constexpr (is_static<IntTupleA>::value && is_static<IntTupleB>::value) {
     static_assert(IntTupleA::value % IntTupleB::value == 0 || IntTupleB::value % IntTupleA::value == 0, "Static shape_div failure");
     return C<shape_div(IntTupleA::value, IntTupleB::value)>{};
   } else {                                       // int int
-    //assert(a % b == 0 || b % a == 0);          // Wave dynamic assertion
+    //assert(a % b == 0 || b % a == 0);          // Waive dynamic assertion
     return a / b != 0 ? a / b : signum(a) * signum(b);  // Division with rounding away from zero
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
 /** Minimum for Shapes
@@ -851,78 +862,63 @@
 template <class T, class U>
 CUTE_HOST_DEVICE constexpr
 auto
 elem_geq(T const& t, U const& u) {
   return !elem_less(t, u);
 }
 
+namespace detail {
+
 /** Increment a (dynamic) coord lexicographically within a shape
+ * @pre is_congruent<Coord,Shape>::value
  * \code
  *    auto shape = make_shape(1,2,make_shape(2,3),3);
  *
  *   int i = 0;
  *   for (auto coord = repeat_like(shape, 0); back(coord) != back(shape); increment(coord, shape)) {
  *      std::cout << i++ << ": " << coord << std::endl;
  *   }
  *   assert(i == size(shape));
  * \endcode
  */
-template <class Coord, class Shape>
-CUTE_HOST_DEVICE constexpr
-void
-increment(Coord& coord, Shape const& shape);
-
-namespace detail {
-
-template <class Coord, class Shape, int I0, int... Is>
-CUTE_HOST_DEVICE constexpr
-void
-increment(Coord& coord, Shape const& shape, seq<I0,Is...>)
-{
-  cute::increment(get<I0>(coord), get<I0>(shape));
-  if constexpr (sizeof...(Is) != 0) {
-    if (back(get<I0>(coord)) == back(get<I0>(shape))) {
-      back(get<I0>(coord)) = 0;
-      increment(coord, shape, seq<Is...>{});
-    }
-  }
-}
-
-} // end namespace detail
-
-template <class Coord, class Shape>
+template <int I = 0, class Coord, class Shape>
 CUTE_HOST_DEVICE constexpr
 void
 increment(Coord& coord, Shape const& shape)
 {
-  if constexpr (is_integral<Coord>::value && is_integral<Shape>::value) {
+  if constexpr (is_integral<Coord>::value) {
     ++coord;
-  } else if constexpr (is_tuple<Coord>::value && is_tuple<Shape>::value) {
-    static_assert(tuple_size<Coord>::value == tuple_size<Shape>::value, "Mismatched ranks");
-    detail::increment(coord, shape, tuple_seq<Coord>{});
   } else {
-    static_assert(sizeof(Coord) == 0, "Invalid parameters");
+    increment(get<I>(coord), get<I>(shape));
+    if constexpr (I+1 < tuple_size<Coord>::value) {
+      if (back(get<I>(coord)) == back(get<I>(shape))) {
+        back(get<I>(coord)) = 0;
+        increment<I+1>(coord, shape);
+      }
+    }
   }
 }
 
+} // end namespace detail
+
 struct ForwardCoordIteratorSentinal
 {};
 
 // A forward iterator for a starting coordinate in a shape's domain, and a shape.
 // The starting coordinate may be zero but need not necessarily be.
 template <class Coord, class Shape>
 struct ForwardCoordIterator
 {
   static_assert(is_congruent<Coord, Shape>::value);
 
   CUTE_HOST_DEVICE constexpr
   Coord const& operator*() const { return coord; }
 
   CUTE_HOST_DEVICE constexpr
-  ForwardCoordIterator& operator++() { increment(coord, shape); return *this; }
+  ForwardCoordIterator& operator++() { detail::increment(coord, shape); return *this; }
 
   // Sentinel for the end of the implied range
   CUTE_HOST_DEVICE constexpr
   bool operator< (ForwardCoordIteratorSentinal const&) const { return back(coord) <  back(shape); }
   CUTE_HOST_DEVICE constexpr
   bool operator==(ForwardCoordIteratorSentinal const&) const { return back(coord) == back(shape); }
   CUTE_HOST_DEVICE constexpr
```

## cutlass_library/source/include/cute/layout.hpp

```diff
@@ -52,14 +52,17 @@
 
 template <class... Strides>
 using Step = cute::tuple<Strides...>;
 
 template <class... Coords>
 using Coord = cute::tuple<Coords...>;
 
+template <class... Layouts>
+using Tile = cute::tuple<Layouts...>;
+
 template <class... Ts>
 CUTE_HOST_DEVICE constexpr
 Shape<Ts...>
 make_shape(Ts const&... t) {
   return {t...};
 }
 template <class... Ts>
@@ -76,15 +79,25 @@
 }
 template <class... Ts>
 CUTE_HOST_DEVICE constexpr
 Coord<Ts...>
 make_coord(Ts const&... t) {
   return {t...};
 }
+template <class... Ts>
+CUTE_HOST_DEVICE constexpr
+Tile<Ts...>
+make_tile(Ts const&... t)
+{
+  return {t...};
+}
 
+//
+// Layout
+//
 
 template <class Shape, class Stride = LayoutLeft::Apply<Shape> >
 struct Layout
     : private cute::tuple<Shape, Stride>   // EBO for static layouts
 {
   // Expensive in compilation time...
   //static_assert(is_congruent<Shape, Stride>::value, "Shape and Stride must be congruent");
@@ -362,67 +375,65 @@
 CUTE_HOST_DEVICE constexpr
 auto
 make_layout(Shape const& shape, GenRowMajor)
 {
   return make_layout(shape, compact_row_major(shape));
 }
 
-// Follow the same ordering induced by the strides, but make the layout compact
+//
+// Advanced Layout constructions
+//
+
+// Make a compact layout with shape @a shape and strides following the order induced by @a order.
+// Dynamic values in @a order are ignored, considered large, and considered ordered from left to right.
+// Example:
+//   make_ordered_layout(Shape<_2,_2,_2,_2>{}, Step<_0,_2,_3,_1>{})
+//     ->  (_2,_2,_2,_2):(_1,_4,_8,_2)
+//   make_ordered_layout(make_shape(2,3,4,5), make_step(Int<2>{}, 67, 42, Int<50>{}))
+//     -> (2,3,4,5):(_1,10,30,2)
 template <class Shape, class Order>
 CUTE_HOST_DEVICE constexpr
 auto
 make_ordered_layout(Shape const& shape, Order const& order)
 {
-  static_assert(is_static<Order>::value);
   return make_layout(shape, compact_order(shape, order));
 }
 
-template <class Shape, class Stride>
-CUTE_HOST_DEVICE constexpr
-auto
-make_ordered_layout(Layout<Shape,Stride> const& layout)
-{
-  return make_ordered_layout(layout.shape(), layout.stride());
-}
-
-// Make a layout of the same shape that is either ordered or colmajor depending on staticness
+// Make a compact layout with the same shape as @a layout
+//   and strides following the order induced by @a layout.stride().
+// Static-0 strides in the input @a layout are preserved in the output.
+// Example:
+//   make_layout_like(Layout<Shape<_2,_2,_2,_2>, Stride<_0,_2,_4,_1>>{})
+//     ->  (_2,_2,_2,_2):(_0,_2,_4,_1)
+//   make_layout_like(make_layout(make_shape(2,3,4,5), make_stride(Int<0>{},42,Int<1>{},Int<0>{})))
+//     -> (2,3,4,5):(_0,4,_1,_0)
 template <class Shape, class Stride>
 CUTE_HOST_DEVICE constexpr
 auto
 make_layout_like(Layout<Shape,Stride> const& layout)
 {
-  auto any_zero = any_of(layout.stride(), [](auto d) { return is_constant<0, decltype(d)>{}; });
-  if constexpr (any_zero) {
-    // If there are static-0 strides, then make a col-major layout that keeps those 0s
-    return make_layout(layout.shape(),
-                       compact_col_major(filter_zeros(layout.stride(), layout.shape())));
-  } else
-  if constexpr (is_static<Shape>::value && is_static<Stride>::value) {
-    // If the layout is fully static, then make a layout that follows the same order as the strides
-    // Assumes the strides are unique
-    return make_ordered_layout(layout.shape(), layout.stride());
-  } else {
-    return make_layout(layout.shape());
-  }
-
-  CUTE_GCC_UNREACHABLE;
+  return make_layout(layout.shape(),
+                     compact_order(filter_zeros(layout.stride(), layout.shape()), layout.stride()));
 }
 
-//
-// Make a layout of the same shape,
-//   with mode-0 being colmajor then following the mode order in layout
-//
+// Make a compact layout with the same shape as @a layout
+//   and strides following the order induced by @a layout.stride(),
+//   except mode-0 is always stride-1 and generated column-major.
+// The 0th mode is commonly used for MMA_Atoms or Copy_Atoms so this
+//   generates the 0th mode with LayoutLeft (preserving stride-0s) regardless of the reference layout
 template <class Shape, class Stride>
 CUTE_HOST_DEVICE constexpr
 auto
 make_fragment_like(Layout<Shape,Stride> const& layout)
 {
   constexpr int R = Layout<Shape,Stride>::rank;
-  if constexpr (R > 1 && is_static<Shape>::value && is_static<Stride>::value) {
-    return tiled_product(make_layout(shape<0>(layout)), make_ordered_layout(take<1,R>(layout)));
+  if constexpr (R > 1 && is_static<Shape>::value) {
+    return tiled_product(make_layout(get<0>(layout.shape()),
+                                     compact_col_major(filter_zeros(get<0>(layout.stride()), get<0>(layout.shape())))),
+                         make_ordered_layout(take<1,R>(layout.shape()), take<1,R>(layout.stride())));
   } else {
     return make_layout(layout.shape());
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
@@ -454,27 +465,27 @@
 // Return the Is...th sublayout.
 // For Is... = <I0,I1,...,IN>, equivalent to get<IN>(...get<I1>(get<I0>(layout)))
 template <size_t... Is, class Shape, class Stride>
 CUTE_HOST_DEVICE constexpr
 auto
 get(Layout<Shape,Stride> const& layout)
 {
-  return make_layout(get<Is...>(layout.shape()), 
+  return make_layout(get<Is...>(layout.shape()),
                      get<Is...>(layout.stride()));
 }
 
-// Return a new layout with only the modes in the range [B,E) 
+// Return a new layout with only the modes in the range [B,E)
 template <int B, int E, class Shape, class Stride>
 CUTE_HOST_DEVICE constexpr
 auto
 take(Layout<Shape,Stride> const& layout)
 {
   static_assert(B < E, "take: empty range error");
   static_assert(0 <= B && E <= Layout<Shape,Stride>::rank, "take: range out of bounds");
-  return make_layout(take<B,E>(layout.shape()), 
+  return make_layout(take<B,E>(layout.shape()),
                      take<B,E>(layout.stride()));
 }
 
 // Return a new layout with only the modes Is... = <I0,I1,...,IN>
 template <int... Is, class Shape, class Stride>
 CUTE_HOST_DEVICE constexpr
 auto
@@ -486,15 +497,15 @@
 
 // Return a layout with depth at most 1
 template <class Shape, class Stride>
 CUTE_HOST_DEVICE constexpr
 auto
 flatten(Layout<Shape,Stride> const& layout)
 {
-  return make_layout(flatten(layout.shape()), 
+  return make_layout(flatten(layout.shape()),
                      flatten(layout.stride()));
 }
 
 // Return a layout whose profile is congruent to TargetProfile
 // @pre Input layout is flat, flatten(@a layout) == @a layout
 // @pre Input layout can be folded to profile, rank(@a layout) == rank(flatten(@a target_profile))
 // @post congruent(@a result, @a target_profile)
@@ -743,29 +754,69 @@
     }
   } else if constexpr (is_constant<1, decltype(get<I>(old_shape))>::value) {
     // shape<I>(layout) == _1, skip it and continue
     return bw_coalesce<I-1>(old_shape, old_stride, new_shape, new_stride);
   } else if constexpr (is_constant<1, NewShape>::value) {
     // Replace our shape-1 with anything (Can only happen on input new_shape/new_stride)
     return bw_coalesce<I-1>(old_shape, old_stride, get<I>(old_shape), get<I>(old_stride));
-  } else if constexpr (is_constant<true, decltype(get<I>(old_shape) * get<I>(old_stride) == get<0>(new_stride))>::value) {
+  } else if constexpr (is_static<decltype(get<0>(new_shape))>::value &&
+                       is_constant<true, decltype(get<I>(old_shape) * get<I>(old_stride) == get<0>(new_stride))>::value) {
     // Merge modes because the shapes and strides match
     return bw_coalesce<I-1>(old_shape, old_stride,
                             replace_front(new_shape,  get<I>(old_shape) * get<0>(new_shape)),
                             replace_front(new_stride, get<I>(old_stride)));
   } else {
     // Can't replace or merge, so prepend a new mode
     return bw_coalesce<I-1>(old_shape, old_stride,
                             prepend(new_shape,  get<I>(old_shape)),
                             prepend(new_stride, get<I>(old_stride)));
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
+// cute::coalesce promises to not change the Layout as a function from integers to codomain.
+// It accomplishes this inside of the Layout's domain, but not always outside of the domain.
+//   Example: (_4,_1):(_1,_0) coalesces to _4:_1.
+// detail::coalesce_x preserves the Layout function inside its domain and outside.
+//
+// @post depth(@a result) <= 1
+// @post for all i, 0 <= i, @a layout(i) == @a result(i)
+template <class Shape, class Stride>
+CUTE_HOST_DEVICE constexpr
+auto
+coalesce_x(Layout<Shape,Stride> const& layout)
+{
+  auto flat_shape  = flatten(layout.shape());
+  auto flat_stride = flatten(layout.stride());
+
+  constexpr int R = decltype(rank(flat_shape))::value;
+  if constexpr (is_constant<1, decltype(get<R-1>(flat_shape))>::value) {
+    return detail::bw_coalesce<R-2>(flat_shape, flat_stride,             Int<2>{}, get<R-1>(flat_stride));
+  } else {
+    return detail::bw_coalesce<R-2>(flat_shape, flat_stride, get<R-1>(flat_shape), get<R-1>(flat_stride));
+  }
+}
+
+// Apply coalesce_x at the terminals of trg_profile
+template <class Shape, class Stride, class IntTuple>
+CUTE_HOST_DEVICE constexpr
+auto
+coalesce_x(Layout<Shape,Stride> const& layout, IntTuple const& trg_profile)
+{
+  if constexpr (is_tuple<IntTuple>::value) {
+    static_assert(tuple_size<IntTuple>::value <= Layout<Shape,Stride>::rank);
+    return cute::transform_layout(layout, trg_profile, [](auto const& l, auto const& t) { return coalesce_x(l,t); });
+  } else {
+    return coalesce_x(layout);
+  }
+
+  CUTE_GCC_UNREACHABLE;
+}
+
 } // end namespace detail
 
 // "Simplify" the layout by combining modes that are possible to combine
 // Does not respect the shape of the layout, but does preserve total size
 // @post size(@a result) == size(@a layout)
 // @post depth(@a result) <= 1
 // @post for all i, 0 <= i < size(@a layout), @a layout(i) == @a result(i)
@@ -793,14 +844,33 @@
   } else {
     return coalesce(layout);
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
+// Combine static and dynamic modes of a shape.
+// @post size(@a result) == size(@a shape)
+// @post depth(@a result) <= 1
+template <class Shape>
+CUTE_HOST_DEVICE constexpr
+auto
+coalesce(Shape const& shape)
+{
+  static_assert(is_integral<Shape>::value || is_tuple<Shape>::value);
+
+  return cute::fold_first(flatten(shape), [](auto const& init, auto const& a) {
+    if constexpr (is_static<decltype(back(init))>::value == is_static<decltype(a)>::value) {
+      return replace_back(init, back(init) * a);  // Both static or both dynamic, coalesce and replace
+    } else {
+      return append(init, a);                     // Can't coalesce, so append
+    }
+  });
+}
+
 // Replace the modes in layout that have a 0-stride with a 1-size
 template <class Shape, class Stride>
 CUTE_HOST_DEVICE constexpr
 auto
 filter_zeros(Layout<Shape,Stride> const& layout)
 {
   return make_layout(filter_zeros(layout.stride(), layout.shape()), layout.stride());
@@ -904,93 +974,89 @@
 
 namespace detail {
 
 template <class LShape, class LStride,
           class RShape, class RStride>
 CUTE_HOST_DEVICE constexpr
 auto
-composition_impl(Layout<LShape,LStride> const& lhs,
+composition_impl(LShape const& lhs_shape, LStride const& lhs_stride,
                  RShape const& rhs_shape, RStride const& rhs_stride)
 {
   if constexpr (is_tuple<RShape>::value) {
     // Apply the right-distributivity of Layout composition
-    return transform_layout(rhs_shape, rhs_stride, [&](auto const& s, auto const& d) { return composition_impl(lhs, s, d); });
+    return transform_layout(rhs_shape, rhs_stride, [&](auto const& s, auto const& d) {
+      return composition_impl(lhs_shape, lhs_stride, s, d);
+    });
   } else
   if constexpr (is_scaled_basis<RStride>::value) {
     // Special case for a ScaledBasis stride
-    return composition_impl(get<RStride::mode()>(lhs), rhs_shape, rhs_stride.value());
+    return composition_impl(basis_get(rhs_stride, lhs_shape), basis_get(rhs_stride, lhs_stride),
+                            rhs_shape, basis_value(rhs_stride));
+  } else
+  if constexpr (is_constant<0, RStride>::value) {
+    // Special case shortcut for any static stride-0
+    return Layout<RShape, RStride>{rhs_shape, rhs_stride};
   } else
-  if constexpr (is_integral<RStride>::value) {
-    // Integral Rstride (and RShape)
+  if constexpr (is_integral<decltype(lhs_shape)>::value) {
+    // Special case shortcut for any integral LShape
+    return Layout{rhs_shape, rhs_stride * lhs_stride};
+  } else
+  if constexpr (is_constant<1, RStride>::value) {
+    // Special case shortcut for any static stride-1
+    constexpr int R  = rank_v<LShape>;
+    auto result_shape_0  = take<0,R-1>(lhs_shape);
+
+    // Mod out the rhs_shape from the lhs_shape
+    auto const [result_shape_1, rest_shape]  = fold(result_shape_0, cute::make_tuple(cute::make_tuple(), rhs_shape),
+      [] (auto const& init, auto const& si) {
+        return cute::make_tuple(append(get<0>(init), shape_min(abs(si), get<1>(init))), shape_div(get<1>(init), abs(si)));
+      });
 
-    // NOTE: Should only flatten once for efficiency
-    auto flat_shape  = flatten(lhs.shape());
-    [[maybe_unused]] auto flat_stride = flatten(lhs.stride());
-    [[maybe_unused]] constexpr int R  = rank(flat_shape);
-
-    if constexpr (is_constant<0, RStride>::value) {
-      // Special case shortcut for any static stride-0
-      return Layout<RShape, RStride>{rhs_shape, rhs_stride};
-    } else
-    if constexpr (is_integral<decltype(flat_shape)>::value) {
-      // Special case shortcut for any integral LShape
-      auto result_stride = rhs_stride * flat_stride;
-      return Layout<RShape, decltype(result_stride)>{rhs_shape, result_stride};
-    } else
-    if constexpr (is_constant<1, RStride>::value) {
-      // Special case shortcut for any static stride-1
-      auto result_shape_0  = take<0,R-1>(flat_shape);
-
-      // Mod out the rhs_shape from the lhs.shape()
-      auto const [result_shape_1, rest_shape]  = fold(result_shape_0, cute::make_tuple(cute::make_tuple(), rhs_shape),
-        [] (auto const& init, auto const& si) {
-          return cute::make_tuple(append(get<0>(init), shape_min(abs(si), get<1>(init))), shape_div(get<1>(init), abs(si)));
-        });
-
-      // Jump into coalesce and append (rest_shape, get<R-1>(lhs.stride())
-      return detail::bw_coalesce<R-2>(result_shape_1, flat_stride, rest_shape, get<R-1>(flat_stride));
-    } else
-    {
-      // General case
-      auto result_shape_0  = take<0,R-1>(flat_shape);
-      auto result_stride_0 = take<0,R-1>(flat_stride);
-
-      // Divide out the rhs_stride from the lhs.shape()
-      auto const [result_shape_1, rest_stride] = fold(result_shape_0, cute::make_tuple(cute::make_tuple(), rhs_stride),
-        [] (auto const& init, auto const& di) {
-          return cute::make_tuple(append(get<0>(init), shape_div(di, get<1>(init))), shape_div(get<1>(init), di));
-        });
-
-      // Apply any lhs.shape() changes to the stride
-      auto result_stride_1 = elem_scale(result_stride_0, shape_div(result_shape_0, result_shape_1));
-
-      // Mod out the rhs_shape from the lhs.shape()
-      auto const [result_shape_2, rest_shape] = fold(result_shape_1, cute::make_tuple(cute::make_tuple(), rhs_shape),
-        [] (auto const& init, auto const& si) {
-          return cute::make_tuple(append(get<0>(init), shape_min(abs(si), get<1>(init))), shape_div(get<1>(init), abs(si)));
-        });
+    // Jump into coalesce and append (rest_shape, get<R-1>(lhs_stride))
+    return detail::bw_coalesce<R-2>(result_shape_1, lhs_stride, rest_shape, get<R-1>(lhs_stride));
+  } else {
+    // General case: integral RShape and RStride, tuple LShape and LStride
+    constexpr int R  = rank_v<LShape>;
+    auto result_shape_0  = take<0,R-1>(lhs_shape);
+    auto result_stride_0 = take<0,R-1>(lhs_stride);
+
+    // Divide out the rhs_stride from the lhs_shape
+    auto const [result_shape_1, rest_stride] = fold(result_shape_0, cute::make_tuple(cute::make_tuple(), rhs_stride),
+      [] (auto const& init, auto const& di) {
+        return cute::make_tuple(append(get<0>(init), shape_div(di, get<1>(init))), shape_div(get<1>(init), di));
+      });
+
+    // Apply any lhs_shape changes to the stride
+    auto result_stride_1 = elem_scale(result_stride_0, shape_div(result_shape_0, result_shape_1));
+
+    // Mod out the rhs_shape from the lhs_shape
+    auto const [result_shape_2, rest_shape] = fold(result_shape_1, cute::make_tuple(cute::make_tuple(), rhs_shape),
+      [] (auto const& init, auto const& si) {
+        return cute::make_tuple(append(get<0>(init), shape_min(abs(si), get<1>(init))), shape_div(get<1>(init), abs(si)));
+      });
 
-      // Jump into coalesce and append (rest_shape, rest_stride * get<R-1>(lhs.stride())
-      return detail::bw_coalesce<R-2>(result_shape_2, result_stride_1, rest_shape, rest_stride * get<R-1>(flat_stride));
-    }
+    // Jump into coalesce and append (rest_shape, rest_stride * get<R-1>(lhs_stride))
+    return detail::bw_coalesce<R-2>(result_shape_2, result_stride_1, rest_shape, rest_stride * get<R-1>(lhs_stride));
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
 } // end namespace detail
 
 template <class LShape, class LStride,
           class RShape, class RStride>
 CUTE_HOST_DEVICE constexpr
 auto
 composition(Layout<LShape,LStride> const& lhs,
             Layout<RShape,RStride> const& rhs)
 {
-  return detail::composition_impl(lhs, rhs.shape(), rhs.stride());
+  auto coprofile = repeat_like(decltype(coshape(rhs)){}, Int<0>{});
+  auto flat_lhs = detail::coalesce_x(lhs, coprofile);
+  return detail::composition_impl(flat_lhs.shape(), flat_lhs.stride(), rhs.shape(), rhs.stride());
 }
 
 template <class LShape, class LStride, class Tiler>
 CUTE_HOST_DEVICE constexpr
 auto
 composition(Layout<LShape,LStride> const& lhs,
             Tiler                  const& rhs)
@@ -998,15 +1064,16 @@
   if constexpr (is_tuple<Tiler>::value) {
     static_assert(tuple_size<Tiler>::value <= Layout<LShape,LStride>::rank);
     // Drop any modes of lhs that aren't hit by rhs
     return detail::transform_layout(lhs, rhs, [](auto const& l, auto const& r) { return composition(l,r); }, make_seq<tuple_size<Tiler>::value>{}, seq<>{}, seq<>{});
   } else if constexpr (is_underscore<Tiler>::value) {
     return lhs;
   } else if constexpr (is_integral<Tiler>::value) {
-    return detail::composition_impl(lhs, rhs, Int<1>{});
+    auto flat_lhs = detail::coalesce_x(lhs);
+    return detail::composition_impl(flat_lhs.shape(), flat_lhs.stride(), rhs, Int<1>{});
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
 //
 // Complement
@@ -1018,22 +1085,22 @@
 //           For all j in [0, size(@a layout)),
 //               @a result(i) != @a layout(j)
 //
 
 namespace detail {
 
 // @pre @a layout has been filtered (flattened and no stride-0 or size-1 modes).
-template <class Shape, class Stride, class CoSizeHi>
+template <class Shape, class Stride, class CoTarget>
 CUTE_HOST_DEVICE constexpr
 auto
-complement(Shape const& shape, Stride const& stride, CoSizeHi const& cosize_hi)
+complement(Shape const& shape, Stride const& stride, CoTarget const& cotarget)
 {
   if constexpr (is_constant<0, Stride>::value) {
     // Special case for irreducible rank-1 stride-0 layout
-    return make_layout(cosize_hi);
+    return make_layout(coalesce(cotarget));
   } else {
     // General case
     constexpr int R = rank_v<Shape>;
     static_assert(R == 1 || is_static<Stride>::value,
                   "Dynamic-stride complement only for rank-1 layouts");
 
     // Should just be a sort and a fold...
@@ -1041,51 +1108,52 @@
     auto [shape_, stride_, result_shape_, result_stride] =
       fold(make_seq<R-1>{},
            cute::make_tuple(shape, stride, cute::make_tuple(), cute::make_tuple(Int<1>{})),
            [](auto const& init, auto i)
            {
               auto [shape, stride, result_shape, result_stride] = init;
               auto min_stride = cute::min(stride);
-              auto min_idx    = find(stride, min_stride);
+              auto min_idx    = cute::find(stride, min_stride);
               auto new_shape  = min_stride / get<i>(result_stride);
-              auto new_stride = get<min_idx>(shape) * min_stride;
+              auto new_stride = min_stride * get<min_idx>(shape);
               static_assert(not is_constant<0, decltype(new_shape)>::value, "Non-injective Layout detected in complement.");
 
               return cute::make_tuple(remove<min_idx>(shape),              // Remove the min_idx from shape
                                       remove<min_idx>(stride),             // Remove the min_idx from stride
                                       append(result_shape , new_shape ),   // new shape  = min_stride / last_stride
-                                      append(result_stride, new_stride));  // new stride = curr_shape * min_stride
+                                      append(result_stride, new_stride));  // new stride = min_stride * curr_shape
             });
 
     // Append the last shape mode
-    auto new_shape    = get<0>(stride_) / get<R-1>(result_stride);
+    auto new_shape    = get<0>(stride_) / get<R-1>(result_stride);         // new shape  = min_stride / last_stride
     static_assert(not is_constant<0, decltype(new_shape)>::value, "Non-injective Layout detected in complement.");
-    auto result_shape = append(result_shape_, new_shape);                  // new shape  = min_stride / last_stride
+    auto result_shape = append(result_shape_, new_shape);
 
     // Compute the rest_shape and rest_stride
-    auto rest_stride = get<0>(shape_) * get<0>(stride_);
-    auto rest_shape  = ceil_div(cosize_hi, rest_stride);
-
-    // Jump into coalesce and append (rest_shape, rest_stride)
-    return detail::bw_coalesce<R-1>(result_shape, result_stride, rest_shape, rest_stride);
+    auto new_stride  = get<0>(stride_) * get<0>(shape_);                   // new stride = min_stride * curr_shape
+    auto rest_shape  = coalesce(ceil_div(cotarget, new_stride));
+    auto rest_stride = compact_col_major(rest_shape, new_stride);
+
+    // Coalesce and append (rest_shape, rest_stride)
+    return coalesce(make_layout(make_shape (result_shape , rest_shape ),
+                                make_stride(result_stride, rest_stride)));
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
 } // end namespace detail
 
-template <class Shape, class Stride, class CoSizeHi>
+template <class Shape, class Stride, class CoTarget>
 CUTE_HOST_DEVICE constexpr
 auto
-complement(Layout<Shape,Stride> const& layout, CoSizeHi const& cosize_hi)
+complement(Layout<Shape,Stride> const& layout, CoTarget const& cotarget)
 {
-  static_assert(cute::is_integral<CoSizeHi>::value, "Expected integral codomain size in complement.");
   auto filter_layout = filter(layout);
-  return detail::complement(filter_layout.shape(), filter_layout.stride(), cosize_hi);
+  return detail::complement(filter_layout.shape(), filter_layout.stride(), shape(cotarget));
 }
 
 template <class Shape, class Stride>
 CUTE_HOST_DEVICE constexpr
 auto
 complement(Layout<Shape,Stride> const& layout)
 {
@@ -1351,15 +1419,15 @@
 template <class LShape, class LStride,
           class TShape, class TStride>
 CUTE_HOST_DEVICE constexpr
 auto
 logical_divide(Layout<LShape,LStride> const& layout,
                Layout<TShape,TStride> const& tiler)
 {
-  return composition(layout, make_layout(tiler, complement(tiler, size(layout))));
+  return composition(layout, make_layout(tiler, complement(tiler, shape(layout))));
 }
 
 template <class LShape, class LStride, class Tiler>
 CUTE_HOST_DEVICE constexpr
 auto
 logical_divide(Layout<LShape,LStride> const& layout,
                Tiler                  const& tiler)
@@ -1372,14 +1440,31 @@
   } else if constexpr (is_integral<Tiler>::value) {
     return logical_divide(layout, make_layout(tiler));
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
+// Generalization of ceil_div for Layout lhs
+//   is effectively the "rest mode" of logical_divide.
+// Occurs in the calculation of gridDim, for example, for generalized tilers
+// Example:
+//   dim3 gridDim(size(ceil_div(problem_shape_M, cta_tiler_M)),
+//                size(ceil_div(problem_shape_N, cta_tiler_N)));
+// This does not consider compositional acceptance, so it may be the case that
+//   ceil_div produces a result while logical_divide (and friends) do not.
+template <class Target, class TShape, class TStride>
+CUTE_HOST_DEVICE constexpr
+auto
+ceil_div(Target                 const& target,
+         Layout<TShape,TStride> const& tiler)
+{
+  return complement(tiler, size(target));
+}
+
 //
 // Convenience operator
 //   that produces layouts like ((BLK_A,BLK_B,...),(a,b,...,x,y))
 //   by gathering the tile modes and residuals into a rank-2 result.
 //
 
 template <class LShape, class LStride,
@@ -1421,15 +1506,14 @@
   return result(repeat<R0>(_), repeat<R1>(_));
 }
 
 //
 // Logical product
 //
 
-// @post compatible()
 template <class LShape, class LStride,
           class TShape, class TStride>
 CUTE_HOST_DEVICE constexpr
 auto
 logical_product(Layout<LShape,LStride> const& block,
                 Layout<TShape,TStride> const& tiler)
 {
@@ -1497,15 +1581,15 @@
   auto R0 = rank<0>(result);
   auto R1 = rank<1>(result);
   return result(repeat<R0>(_), repeat<R1>(_));
 }
 
 //
 // Rank-sensitive products
-// 
+//
 
 // blocked_product -- Reproduce a block over a tiler.
 // Think of every element of "tiler" as a "block"
 //   and return the layout of the resulting structure.
 // @post rank(@a result) == cute::max(rank(@a block), rank(@a tiler))
 template <class TShape, class TStride,
           class UShape, class UStride>
@@ -1513,15 +1597,15 @@
 auto
 blocked_product(Layout<TShape,TStride> const& block,
                 Layout<UShape,UStride> const& tiler)
 {
   constexpr int R = cute::max(rank_v<TShape>, rank_v<UShape>);
 
   auto result = logical_product(append<R>(block), append<R>(tiler));
-  
+
   return coalesce(zip(get<0>(result), get<1>(result)), tuple_repeat<R>(Int<1>{}));
 }
 
 // raked_product -- Reproduce a block over a tiler with block-interleaving.
 // Think of every element of "tiler" as a "block", interleave those blocks,
 //   and return the layout of the resulting structure.
 // @post rank(@a result) == cute::max(rank(@a block), rank(@a tiler))
@@ -1541,15 +1625,15 @@
 
 // tile_to_shape -- Perform a product of a layout so that the result matches a target shape.
 // This is similar to blocked_product, but specifies the result shape instead of the
 //   product shape, which is more convenient in certain circumstances.
 // @param block The layout to repeat
 // @param trg_shape The target shape of the result
 // @param ord_shape The order of the modes of @a trg_shape to tile @a layout with.
-//                  Defaults to GenColMajor, so @a layout will repeat 
+//                  Defaults to GenColMajor, so @a layout will repeat
 //                    across the first mode first, the second mode second, etc
 //                  E.g. Step<_2,_1,_3> will cause @a layout to repeat
 //                    across the second mode first, the first mode second, and the third mode last.
 // @pre rank(@a block) <= rank(@a trg_shape)
 // @post compatible(@a trg_shape, shape(@a result))
 template <class Shape, class Stride,
           class TrgShape, class ModeOrder = LayoutLeft>
@@ -1655,15 +1739,15 @@
   using scale = decltype(trait_ratio(sizeof_bits<NewType>{}, sizeof_bits<OldType>{}));
   if constexpr (scale::num == 1 && scale::den == 1) {
     return layout;
   }
   else if constexpr (scale::num == 1) {
     return downcast<scale::den>(layout);
   }
-  else if constexpr (scale::den == 1) { 
+  else if constexpr (scale::den == 1) {
     return upcast<scale::num>(layout);
   }
   else {
     static_assert(dependent_false<scale>, "Recast not supported.");
   }
 
   CUTE_GCC_UNREACHABLE;
```

## cutlass_library/source/include/cute/layout_composed.hpp

```diff
@@ -174,14 +174,24 @@
 
   template <class... Layouts>
   CUTE_HOST_DEVICE constexpr
   auto
   tile(Layouts const&... layouts) const {
     return tiled_divide(*this, make_tile(layouts...));
   }
+
+  // Equality, return a static or dynamic boolean
+  template <class... Args>
+  CUTE_HOST_DEVICE constexpr
+  auto
+  operator==(ComposedLayout<Args...> const& other) const {
+    return this->layout_a() == other.layout_a() &&
+           this->layout_b() == other.layout_b() &&
+           this->offset()   == other.offset();
+  }
 };
 
 template <class A, class O, class B>
 struct is_layout<ComposedLayout<A,O,B>> : true_type {};
 
 template <class T>
 struct is_composed_layout : false_type {};
@@ -378,20 +388,20 @@
   return composition(composition(a, b.layout_a()), b.layout_b());
 }
 
 //
 // complement
 //
 
-template <class A, class O, class B, class CoSizeHi>
+template <class A, class O, class B, class CoTarget>
 CUTE_HOST_DEVICE constexpr
 auto
-complement(ComposedLayout<A,O,B> const& layout, CoSizeHi const& cosize_hi)
+complement(ComposedLayout<A,O,B> const& layout, CoTarget const& cotarget)
 {
-  return complement(layout.layout_b(), cosize_hi);
+  return complement(layout.layout_b(), cotarget);
 }
 
 template <class A, class O, class B>
 CUTE_HOST_DEVICE constexpr
 auto
 complement(ComposedLayout<A,O,B> const& layout)
 {
@@ -596,15 +606,15 @@
   using scale = decltype(trait_ratio(sizeof_bits<NewType>{}, sizeof_bits<OldType>{}));
   if constexpr (scale::num == 1 && scale::den == 1) {
     return layout;
   }
   else if constexpr (scale::num == 1) {
     return downcast<scale::den>(layout);
   }
-  else if constexpr (scale::den == 1) { 
+  else if constexpr (scale::den == 1) {
     return upcast<scale::num>(layout);
   }
   else {
     static_assert(dependent_false<scale>, "Recast not supported.");
   }
   CUTE_GCC_UNREACHABLE;
 }
```

## cutlass_library/source/include/cute/pointer.hpp

```diff
@@ -29,23 +29,22 @@
  *
  **************************************************************************************************/
 #pragma once
 
 #include <cute/config.hpp>
 
 #include <cute/util/type_traits.hpp>
-#include <cute/numeric/int.hpp>        // sizeof_bits
+#include <cute/numeric/numeric_types.hpp>        // sizeof_bits
 #include <cute/numeric/math.hpp>
 #include <cute/numeric/integral_constant.hpp>
 
 #include <cute/container/array_subbyte.hpp>
 
 #include <cute/pointer_base.hpp>
 #include <cute/pointer_swizzle.hpp>
-#include <cute/layout.hpp>
 namespace cute
 {
 
 //
 // recast_ptr<T> -- Create an iterator over values of type T.
 // For most types this will simply be T*, but certain types require more care.
 // Subbyte Types: uint2_t, uint4_t, etc
@@ -54,28 +53,28 @@
 //
 
 template <class NewT>
 CUTE_HOST_DEVICE constexpr
 auto
 recast_ptr(void* ptr)
 {
-  if constexpr (is_subbyte<NewT>::value) {
+  if constexpr (cute::is_subbyte_v<NewT>) {
     return subbyte_iterator<NewT>(ptr);
   } else {
     return reinterpret_cast<NewT*>(ptr);
   }
   CUTE_GCC_UNREACHABLE;
 }
 
 template <class NewT>
 CUTE_HOST_DEVICE constexpr
 auto
 recast_ptr(void const* ptr)
 {
-  if constexpr (is_subbyte<NewT>::value) {
+  if constexpr (cute::is_subbyte_v<NewT>) {
     return subbyte_iterator<NewT const>(ptr);
   } else {
     return reinterpret_cast<NewT const*>(ptr);
   }
   CUTE_GCC_UNREACHABLE;
 }
 
@@ -98,14 +97,16 @@
 
 template <class T, class = void>
 struct is_gmem : false_type {};
 template <class P>                     // Found the gmem
 struct is_gmem<gmem_ptr<P>> : true_type {};
 template <class P>                     // Recurse on ::iterator, if possible
 struct is_gmem<P, void_t<typename P::iterator>> : is_gmem<typename P::iterator> {};
+template <class P>
+constexpr bool is_gmem_v = is_gmem<P>::value;
 
 // Idempotent gmem tag on an iterator
 template <class Iterator>
 CUTE_HOST_DEVICE constexpr
 auto
 make_gmem_ptr(Iterator iter) {
   if constexpr (is_gmem<Iterator>::value) {
@@ -159,14 +160,16 @@
 
 template <class T, class = void>
 struct is_smem : false_type {};
 template <class P>                     // Found the smem
 struct is_smem<smem_ptr<P>> : true_type {};
 template <class P>                     // Recurse on ::iterator, if possible
 struct is_smem<P, void_t<typename P::iterator>> : is_smem<typename P::iterator> {};
+template <class P>
+constexpr bool is_smem_v = is_smem<P>::value;
 
 // Idempotent smem tag on an iterator
 template <class Iterator>
 CUTE_HOST_DEVICE constexpr
 auto
 make_smem_ptr(Iterator iter) {
   if constexpr (is_smem<Iterator>::value) {
@@ -220,14 +223,16 @@
 };
 
 // Anything that is not gmem or smem is rmem
 template <class T, class = void>
 struct is_rmem : bool_constant<not (is_gmem<T>::value || is_smem<T>::value)> {};
 template <class P>
 struct is_rmem<rmem_ptr<P>> : true_type {};
+template <class P>
+constexpr bool is_rmem_v = is_rmem<P>::value;
 
 // Idempotent rmem tag on an iterator
 template <class Iterator>
 CUTE_HOST_DEVICE constexpr
 auto
 make_rmem_ptr(Iterator iter) {
   if constexpr (is_rmem<Iterator>::value) {
```

## cutlass_library/source/include/cute/pointer_base.hpp

```diff
@@ -29,15 +29,15 @@
  *
  **************************************************************************************************/
 #pragma once
 
 #include <cute/config.hpp>
 
 #include <cute/util/type_traits.hpp>
-#include <cute/numeric/int.hpp>        // sizeof_bits
+#include <cute/numeric/numeric_types.hpp>        // sizeof_bits
 
 namespace cute
 {
 
 //
 // C++20 <iterator> iterator_traits
 //
```

## cutlass_library/source/include/cute/pointer_flagged.hpp

```diff
@@ -85,15 +85,15 @@
   return composition(layout.layout_a(), smem_ptr_flag_bits<B/N>{}, downcast<N>(layout.layout_b()));
 }
 
 //
 // Conversion with swizzle_layout
 //
 
-template <class T, class SwizzleFn, int B, class Layout>
+template <class SwizzleFn, int B, class Layout>
 CUTE_HOST_DEVICE
 auto
 as_position_independent_swizzle_layout(ComposedLayout<SwizzleFn,smem_ptr_flag_bits<B>,Layout> const& layout)
 {
   return composition(recast_layout<uint8_t,uint_bit_t<B>>(layout.layout_a()), Int<0>{}, layout.layout_b());
 }
 
@@ -105,37 +105,45 @@
   static_assert(is_smem<remove_cvref_t<Tensor>>::value, "Expected smem tensor.");
   using SwizzleFn = get_swizzle_t<remove_cvref_t<Tensor>>;
   if constexpr (SwizzleFn::num_bits == 0) {
     return tensor;
   } else {
 #if !defined(NDEBUG)
     {
-    uint32_t address = cast_smem_ptr_to_uint(raw_pointer_cast(std::forward<Tensor>(tensor).data()));
+    uint32_t address = cast_smem_ptr_to_uint(raw_pointer_cast(static_cast<Tensor&&>(tensor).data()));
     uint32_t mask    = ((uint32_t(1) << SwizzleFn::num_base) - 1) | SwizzleFn::swizzle_code;
     assert((address & mask) == 0);  // Alignment to the Base, Z, and Y of Swizzle
     }
 #endif
     using T = typename remove_cvref_t<Tensor>::value_type;
     // Recast swizzle from acting on byte-addressed pointers to elements of type-T
     auto new_swizzle = recast_layout<uint8_t, T>(SwizzleFn{});
     // Strip off everything and create a new smem_ptr for type-T
-    auto new_ptr = make_smem_ptr<T>(raw_pointer_cast(std::forward<Tensor>(tensor).data()));
+    auto new_ptr = make_smem_ptr<T>(raw_pointer_cast(static_cast<Tensor&&>(tensor).data()));
     return make_tensor(new_ptr, composition(new_swizzle, Int<0>{}, tensor.layout()));
   }
   CUTE_GCC_UNREACHABLE;
 }
 
 //
 // Display utilities
 //
 
 // Capture and cast smem_ptr_flag Layouts to offset-0 layouts
 template <class SwizzleFn, int B, class Layout>
 CUTE_HOST_DEVICE
 void
+print_layout(ComposedLayout<SwizzleFn,smem_ptr_flag_bits<B>,Layout> const& layout)
+{
+  print_layout(as_position_independent_swizzle_layout(layout));
+}
+
+template <class SwizzleFn, int B, class Layout>
+CUTE_HOST_DEVICE
+void
 print_latex(ComposedLayout<SwizzleFn,smem_ptr_flag_bits<B>,Layout> const& layout)
 {
   print_latex(as_position_independent_swizzle_layout(layout));
 }
 
 template <int B>
 CUTE_HOST_DEVICE void print(smem_ptr_flag_bits<B> ptr)
```

## cutlass_library/source/include/cute/stride.hpp

```diff
@@ -390,49 +390,74 @@
 
 //
 // Compact Order -- compute a compact stride based on an ordering of the modes
 //
 
 namespace detail {
 
-template <class Shape, class Order, class OrigShape, class OrigOrder>
+// @pre weakly_congruent(order, shape)
+// @pre is_congruent<RefShape, RefOrder>
+// @pre is_static<Order>
+// @pre is_static<RefOrder>
+template <class Shape, class Order, class RefShape, class RefOrder>
 CUTE_HOST_DEVICE constexpr
 auto
 compact_order(Shape const& shape, Order const& order,
-              OrigShape const& orig_shape, OrigOrder const& orig_order)
+              RefShape const& ref_shape, RefOrder const& ref_order)
 {
   if constexpr (is_tuple<Order>::value) {
-    return transform(shape, order, [&](auto const& x, auto const& y) { return compact_order(x, y, orig_shape, orig_order); });
+    static_assert(tuple_size<Shape>::value == tuple_size<Order>::value, "Need equal rank of shape and order");
+    return transform(shape, order, [&](auto const& s, auto const& o) { return compact_order(s, o, ref_shape, ref_order); });
   } else {
-    auto d = product(transform(orig_shape, orig_order,
-                               [&](auto const& s, auto const& o) {
-                                  return conditional_return(o < order, product(s), Int<1>{});
-                                }));
-    return compact_col_major(shape, d);
+    // Compute the starting stride for this shape by accumulating all shapes corresponding to lesser orders
+    auto stride_start = product(transform(ref_shape, ref_order,
+                                          [&](auto const& s, auto const& o) {
+                                            return conditional_return(o < order, s, Int<1>{});
+                                          }));
+    return compact_col_major(shape, stride_start);
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
 } // end namespace detail
 
 template <class Shape, class Order>
 CUTE_HOST_DEVICE constexpr
 auto
 compact_order(Shape const& shape, Order const& order)
 {
-  if constexpr(is_congruent<Shape,Order>::value) {
-    return detail::compact_order(shape, order, flatten_to_tuple(shape), flatten_to_tuple(order));
-  }
-  else
-  {
-    // Here we only want to apply order to top-level subshapes and default (col-major) order on other levels
-    static_assert(rank(Shape{}) == rank(Order{}), "Need equal rank of shape and order");
-    return detail::compact_order(shape, order, shape, order);
-  }
+  auto ref_shape = flatten_to_tuple(product_like(shape, order));
+
+  auto flat_order = flatten_to_tuple(order);
+  // Find the largest static element of order
+  auto max_order = cute::fold(flat_order, Int<0>{}, [](auto v, auto order) {
+    if constexpr (is_constant<true, decltype(v < order)>::value) {  
+      return order;
+    } else {
+      return v;
+    }
+
+    CUTE_GCC_UNREACHABLE;
+  });
+  // Replace any dynamic elements within order with large-static elements
+  auto max_seq = make_range<max_order+1, max_order+1+rank(flat_order)>{};
+  auto ref_order = cute::transform(max_seq, flat_order, [](auto seq_v, auto order) {
+    if constexpr (is_static<decltype(order)>::value) {
+      return order;
+    } else {
+      return seq_v;
+    }
+
+    CUTE_GCC_UNREACHABLE;
+  });
+
+  auto new_order = unflatten(ref_order, order);
+
+  return detail::compact_order(shape, new_order, ref_shape, ref_order);
 }
 
 template <class Shape>
 CUTE_HOST_DEVICE constexpr
 auto
 compact_order(Shape const& shape, GenColMajor const& major)
 {
```

## cutlass_library/source/include/cute/swizzle.hpp

```diff
@@ -83,14 +83,22 @@
   template <class Offset>
   CUTE_HOST_DEVICE constexpr
   auto
   operator()(Offset const& offset) const
   {
     return apply(offset);
   }
+
+  template <int B, int M, int S>
+  CUTE_HOST_DEVICE constexpr
+  auto
+  operator==(Swizzle<B,M,S> const&) const
+  {
+    return B == BBits && M == MBase && S == SShift;
+  }
 };
 
 //
 // make_swizzle<0b1000, 0b0100>()         ->  Swizzle<1,2,1>
 // make_swizzle<0b11000000, 0b00000110>() ->  Swizzle<2,1,5>
 //
```

## cutlass_library/source/include/cute/tensor.hpp

```diff
@@ -38,15 +38,14 @@
 
 #include <cute/container/tuple.hpp>
 #include <cute/container/array_aligned.hpp>
 #include <cute/container/array_subbyte.hpp>
 
 #include <cute/pointer.hpp>
 #include <cute/layout.hpp>
-#include <cute/tile.hpp>
 
 namespace cute
 {
 
 //
 // Engine -- owning or non-owning data store
 //
@@ -313,14 +312,16 @@
   cute::tuple<layout_type, engine_type> rep_;
 };
 
 template <class T>
 struct is_tensor : false_type {};
 template <class Engine, class Layout>
 struct is_tensor<Tensor<Engine,Layout>> : true_type {};
+template <class T>
+constexpr bool is_tensor_v = is_tensor<T>::value;
 
 // Customization point for creation of owning and non-owning Tensors
 template <class T>
 struct MakeTensor
 {
   template <class Layout,
             __CUTE_REQUIRES(not has_dereference<T>::value &&
@@ -477,34 +478,24 @@
 // Return the subtensor of a mode
 template <class Tensor,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 decltype(auto)
 tensor(Tensor&& tensor)
 {
-  return std::forward<Tensor>(tensor);
+  return static_cast<Tensor&&>(tensor);
 }
 
 template <int I, int... Is, class Tensor,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 decltype(auto)
 tensor(Tensor&& tensor)
 {
-  return make_tensor(std::forward<Tensor>(tensor).data(), get<I,Is...>(tensor.layout()));
-}
-
-// Return the subtensor of a range of modes
-template <int B, int E, class Tensor,
-          __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
-CUTE_HOST_DEVICE constexpr
-decltype(auto)
-take(Tensor&& tensor)
-{
-  return make_tensor(std::forward<Tensor>(tensor).data(), take<B,E>(tensor.layout()));
+  return make_tensor(static_cast<Tensor&&>(tensor).data(), get<I,Is...>(tensor.layout()));
 }
 
 // Return the layout of a mode
 template <int... Is, class Engine, class Layout>
 CUTE_HOST_DEVICE constexpr
 decltype(auto)
 layout(Tensor<Engine,Layout> const& tensor)
@@ -563,86 +554,96 @@
 
 template <class Tensor,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 flatten(Tensor&& tensor)
 {
-  return make_tensor(std::forward<Tensor>(tensor).data(), flatten(tensor.layout()));
+  return make_tensor(static_cast<Tensor&&>(tensor).data(), flatten(tensor.layout()));
 }
 
 template <class Tensor,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 coalesce(Tensor&& tensor)
 {
-  return make_tensor(std::forward<Tensor>(tensor).data(), coalesce(tensor.layout()));
+  return make_tensor(static_cast<Tensor&&>(tensor).data(), coalesce(tensor.layout()));
 }
 
 template <class Tensor, class Profile,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 coalesce(Tensor&& tensor, Profile const& profile)
 {
-  return make_tensor(std::forward<Tensor>(tensor).data(), coalesce(tensor.layout(), profile));
+  return make_tensor(static_cast<Tensor&&>(tensor).data(), coalesce(tensor.layout(), profile));
 }
 
 template <class Tensor,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 filter_zeros(Tensor&& tensor)
 {
-  return make_tensor(cute::forward<Tensor>(tensor).data(), filter_zeros(tensor.layout()));
+  return make_tensor(static_cast<Tensor&&>(tensor).data(), filter_zeros(tensor.layout()));
 }
 
 template <class Tensor,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 filter(Tensor&& tensor)
 {
-  return make_tensor(std::forward<Tensor>(tensor).data(), filter(tensor.layout()));
+  return make_tensor(static_cast<Tensor&&>(tensor).data(), filter(tensor.layout()));
 }
 
 template <class Tensor, class Profile,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 filter(Tensor&& tensor, Profile const& profile)
 {
-  return make_tensor(std::forward<Tensor>(tensor).data(), filter(tensor.layout(), profile));
+  return make_tensor(static_cast<Tensor&&>(tensor).data(), filter(tensor.layout(), profile));
 }
 
 // Return a tensor with the same shape as input but offset by a given coordinate
 template <class Coord, class Tensor,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 domain_offset(Coord const& coord, Tensor&& tensor)
 {
   auto [layout, ptr_offset] = domain_offset(coord, tensor.layout());
-  return make_tensor(std::forward<Tensor>(tensor).data() + ptr_offset, layout);
+  return make_tensor(static_cast<Tensor&&>(tensor).data() + ptr_offset, layout);
 }
 
 // Group the modes [B,E) into a single mode
 // e.g. group<2,4>(make_tensor<int>(Layout<Shape<_1,_2,_3,_4,_5,_6>>{}))
 //      => make_tensor<int>(Layout<Shape<_1,_2,Shape<_3,_4>,_5,_6>>{})
 template <int B, int E, class Tensor,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 group_modes(Tensor&& tensor)
 {
-  return make_tensor(std::forward<Tensor>(tensor).data(),
+  return make_tensor(static_cast<Tensor&&>(tensor).data(),
                      group<B,E>(tensor.layout()));
 }
 
+// Return the subtensor of a range of modes
+template <int B, int E, class Tensor,
+          __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
+CUTE_HOST_DEVICE constexpr
+decltype(auto)
+take(Tensor&& tensor)
+{
+  return make_tensor(static_cast<Tensor&&>(tensor).data(), take<B,E>(tensor.layout()));
+}
+
 //
 // Recast
 //
 
 // NOTE: This is very dangerous to do
 //   -- doesn't check dynamic integer divisibility
 //   -- doesn't check alignment
@@ -658,17 +659,17 @@
 
   // If this is an upcast of a normal Layout with static negative strides, then offset as well
   if constexpr (sizeof(OldType) < sizeof(NewType) && not is_composed_layout<decltype(old_layout)>::value) {
     auto shape_diff = transform(flatten(old_layout.shape()), flatten(new_layout.shape()), minus{});
     auto extent_diff = transform(shape_diff, flatten(old_layout.stride()), multiplies{});
     auto offset = fold(extent_diff, Int<0>{}, [](auto const& i, auto const& a) { return i + cute::min(a,Int<0>{}); });
 
-    return make_tensor(recast_ptr<NewType>(std::forward<Tensor>(tensor).data() + offset), new_layout);
+    return make_tensor(recast_ptr<NewType>(static_cast<Tensor&&>(tensor).data() + offset), new_layout);
   } else {
-    return make_tensor(recast_ptr<NewType>(std::forward<Tensor>(tensor).data()         ), new_layout);
+    return make_tensor(recast_ptr<NewType>(static_cast<Tensor&&>(tensor).data()         ), new_layout);
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
 //
 // max_common_vector
@@ -784,53 +785,53 @@
 template <class Tensor, class Tiler,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 logical_divide(Tensor    && tensor,
                Tiler const& tiler)   // Layout or Tile<Layout...> or Shape
 {
-  return make_tensor(std::forward<Tensor>(tensor).data(),
+  return make_tensor(static_cast<Tensor&&>(tensor).data(),
                      logical_divide(tensor.layout(), tiler));
 }
 
 // zipped_divide is logical_divide with Tiler modes and Rest modes gathered together: (Tiler,Rest)
 // When Tiler is Layout, this has no effect as logical_divide results in the same.
 // When Tiler is Tile<Layout...> or Shape, this zips modes into standard form ((BLK_A,BLK_B),(a,b,x,y))
 template <class Tensor, class Tiler,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 zipped_divide(Tensor    && tensor,
               Tiler const& tiler)    // Layout or Tile<Layout...> or Shape
 {
-  return make_tensor(std::forward<Tensor>(tensor).data(),
+  return make_tensor(static_cast<Tensor&&>(tensor).data(),
                      zipped_divide(tensor.layout(), tiler));
 }
 
 // tiled_divide is zipped_divide with the second output mode flattened ((BLK_A,BLK_B),a,b,x,y)
 template <class Tensor, class Tiler,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
-tiled_divide(Tensor   && tensor,
+tiled_divide(Tensor    && tensor,
              Tiler const& tiler)     // Layout or Tile<Layout...> or Shape
 {
-  return make_tensor(std::forward<Tensor>(tensor).data(),
+  return make_tensor(static_cast<Tensor&&>(tensor).data(),
                      tiled_divide(tensor.layout(), tiler));
 }
 
 // flat_divide is zipped_divide with the both modes flattened (BLK_A,BLK_B,a,b,x,y)
 template <class Tensor, class Tiler,
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 flat_divide(Tensor    && tensor,
             Tiler const& tiler)      // Layout or Tile<Layout...> or Shape
 {
-  return make_tensor(std::forward<Tensor>(tensor).data(),
+  return make_tensor(static_cast<Tensor&&>(tensor).data(),
                      flat_divide(tensor.layout(), tiler));
 }
 
 // logical_product on a Tensor doesn't make sense since it often increases cosize
 //   though this might make sense for creating Tensors with broadcasted (stride-0) modes
 
 //
@@ -846,15 +847,15 @@
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 inner_partition(Tensor    && tensor,
                 Tiler const& tiler,
                 Coord const& coord)
 {
-  auto tensor_tiled = zipped_divide(std::forward<Tensor>(tensor), tiler);
+  auto tensor_tiled = zipped_divide(static_cast<Tensor&&>(tensor), tiler);
   constexpr int R0 = decltype(rank<0>(tensor_tiled))::value;
 
   // The coord slices into the second mode (the "rest" mode), flatten the first
   if constexpr (is_tuple<Coord>::value) {
     // Append trailing modes if coord is tuple
     constexpr int R1 = decltype(rank<1>(tensor_tiled))::value;;
     return tensor_tiled(repeat<R0>(_), append<R1>(coord,_));
@@ -873,15 +874,15 @@
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 outer_partition(Tensor    && tensor,
                 Tiler const& tiler,
                 Coord const& coord)
 {
-  auto tensor_tiled = zipped_divide(std::forward<Tensor>(tensor), tiler);
+  auto tensor_tiled = zipped_divide(static_cast<Tensor&&>(tensor), tiler);
   constexpr int R1 = decltype(rank<1>(tensor_tiled))::value;
 
   // The coord slices into the first mode (the "tile" mode), flatten the second
   if constexpr (is_tuple<Coord>::value) {
     // Append trailing modes if coord is tuple
     constexpr int R0 = decltype(rank<0>(tensor_tiled))::value;
     return tensor_tiled(append<R0>(coord,_), repeat<R1>(_));
@@ -899,15 +900,15 @@
           __CUTE_REQUIRES(is_tensor<remove_cvref_t<Tensor>>::value)>
 CUTE_HOST_DEVICE constexpr
 auto
 local_tile(Tensor    && tensor,
            Tiler const& tiler,   // tiler to apply
            Coord const& coord)   // coord to slice into "remainder"
 {
-  return inner_partition(std::forward<Tensor>(tensor),
+  return inner_partition(static_cast<Tensor&&>(tensor),
                          tiler,
                          coord);
 }
 
 // Same as above, but with a projection parameter to strip out unwanted tiling modes for convenience
 //   when using projections of the same tiler.
 // This is typical at the CTA level where tiles of data are extracted as projections:
@@ -924,15 +925,15 @@
 CUTE_HOST_DEVICE
 auto
 local_tile(Tensor    && tensor,
            Tiler const& tiler,   // tiler to apply
            Coord const& coord,   // coord to slice into "remainder"
            Proj  const& proj)    // projection to apply to tiler and coord
 {
-  return local_tile(std::forward<Tensor>(tensor),
+  return local_tile(static_cast<Tensor&&>(tensor),
                     dice(proj, tiler),
                     dice(proj, coord));
 }
 
 // Tile a tensor according to the flat shape of a layout that provides the coordinate of the target index.
 // This is typical at the Thread level where data is partitioned across repeated patterns of threads:
 //   Tensor data = ...                                                            // (_16,_64)
@@ -942,15 +943,15 @@
 CUTE_HOST_DEVICE
 auto
 local_partition(Tensor                     && tensor,
                 Layout<LShape,LStride> const& tile,    // coord -> index
                 Index                  const& index)   // index to slice for
 {
   static_assert(is_integral<Index>::value);
-  return outer_partition(std::forward<Tensor>(tensor),
+  return outer_partition(static_cast<Tensor&&>(tensor),
                          product_each(shape(tile)),
                          tile.get_flat_coord(index));
 }
 
 // Same as above, but with a projection parameter to strip out unwanted tiling modes for convenience
 //   when using projections of the same tiler.
 // This is typical at the Thread level where data is partitioned across projected layouts of threads:
@@ -966,15 +967,15 @@
 CUTE_HOST_DEVICE
 auto
 local_partition(Tensor                     && tensor,
                 Layout<LShape,LStride> const& tile,   // coord -> index
                 Index                  const& index,  // index to slice for
                 Projection             const& proj)
 {
-  return local_partition(std::forward<Tensor>(tensor),
+  return local_partition(static_cast<Tensor&&>(tensor),
                          dice(proj, tile),
                          index);
 }
 
 //
 // Display utilities
 //
@@ -982,17 +983,19 @@
 template <class Engine, class Layout>
 CUTE_HOST_DEVICE void print(Tensor<Engine,Layout> const& tensor)
 {
   print(tensor.data()); print(" o "); print(tensor.layout());
 }
 
 template <class Engine, class Layout>
-CUTE_HOST_DEVICE void print_tensor(Tensor<Engine,Layout> const& tensor)
+CUTE_HOST_DEVICE void print_tensor(Tensor<Engine,Layout> const& tensor, bool print_type = true)
 {
-  print(tensor); print(":\n");
+  if (print_type) {
+    print(tensor); print(":\n");
+  }
 
   if constexpr (Layout::rank == 1)
   {
     for (int m = 0; m < size(tensor); ++m) {
       pretty_print(tensor(m));
       printf("\n");
     }
@@ -1004,26 +1007,26 @@
         pretty_print(tensor(m,n));
       }
       printf("\n");
     }
   } else
   if constexpr (Layout::rank == 3)
   {
-    print_tensor(tensor(_,_,0));
+    print_tensor(tensor(_,_,0), false);
     for (int k = 1; k < size<2>(tensor); ++k) {
       for (int i = 0; i < 5*size<1>(tensor); ++i) { print("-"); } print("\n");
-      print_tensor(tensor(_,_,k));
+      print_tensor(tensor(_,_,k), false);
     }
   } else
   if constexpr (Layout::rank == 4)
   {
-    print_tensor(tensor(_,_,_,0));
+    print_tensor(tensor(_,_,_,0), false);
     for (int p = 1; p < size<3>(tensor); ++p) {
       for (int i = 0; i < 5*size<1>(tensor); ++i) { print("="); } print("\n");
-      print_tensor(tensor(_,_,_,p));
+      print_tensor(tensor(_,_,_,p), false);
     }
   }
 }
 
 #if !defined(__CUDACC_RTC__)
 template <class Engine, class Layout>
 CUTE_HOST std::ostream& print_tensor_os(std::ostream& os, Tensor<Engine,Layout> const& tensor)
@@ -1077,18 +1080,21 @@
 
 //
 // Extended Engines
 //
 
 #include <cute/pointer_swizzle.hpp>
 #include <cute/pointer_flagged.hpp>
-
 //
 // Tensor Algorithms
 //
 
 #include <cute/algorithm/tensor_algorithms.hpp>
 #include <cute/algorithm/fill.hpp>
 #include <cute/algorithm/clear.hpp>
 #include <cute/algorithm/copy.hpp>
+#include <cute/algorithm/prefetch.hpp>
 #include <cute/algorithm/axpby.hpp>
 #include <cute/algorithm/gemm.hpp>
+
+#include <cute/algorithm/cooperative_copy.hpp>
+#include <cute/algorithm/cooperative_gemm.hpp>
```

## cutlass_library/source/include/cute/underscore.hpp

```diff
@@ -41,14 +41,17 @@
 {
 
 // For slicing
 struct Underscore : Int<0> {};
 
 CUTE_INLINE_CONSTANT Underscore _;
 
+// Convenient alias
+using X = Underscore;
+
 // Treat Underscore as an integral like integral_constant
 template <>
 struct is_integral<Underscore> : true_type {};
 
 template <class T>
 struct is_underscore : false_type {};
 template <>
```

## cutlass_library/source/include/cute/algorithm/axpby.hpp

```diff
@@ -29,60 +29,67 @@
  *
  **************************************************************************************************/
 #pragma once
 
 #include <cute/config.hpp>
 
 #include <cute/tensor.hpp>
+#include <cute/tensor_predicate.hpp>
 
 namespace cute
 {
 
 //
 // Accept mutable temporaries
 //
 template <class Alpha,
           class XEngine, class XLayout,
           class Beta,
-          class YEngine, class YLayout>
+          class YEngine, class YLayout,
+          class PrdTensor = TrivialPredTensor>
 CUTE_HOST_DEVICE
 void
 axpby(Alpha                    const& alpha,
       Tensor<XEngine, XLayout> const& x,
       Beta                     const& beta,
-      Tensor<YEngine, YLayout>     && y)
+      Tensor<YEngine, YLayout>     && y,
+      PrdTensor                const& p = {})
 {
-  return axpby(alpha, x, beta, y);
+  return axpby(alpha, x, beta, y, p);
 }
 
 //
 // AXPBY
 //
 template <class Alpha,
           class XEngine, class XLayout,
           class Beta,
-          class YEngine, class YLayout>
+          class YEngine, class YLayout,
+          class PrdTensor = TrivialPredTensor>
 CUTE_HOST_DEVICE
 void
 axpby(Alpha                    const& alpha,
       Tensor<XEngine, XLayout> const& x,
       Beta                     const& beta,
-      Tensor<YEngine, YLayout>      & y)
+      Tensor<YEngine, YLayout>      & y,
+      PrdTensor                const& p = {})
 {
   auto isBetaZero = [&] () {
     if constexpr (is_complex<Beta>::value) {
       return beta.real() == Int<0>{} && beta.imag() == Int<0>{};
     }
     else {
       return beta == Int<0>{};
     }
 
     CUTE_GCC_UNREACHABLE;
   } ();
 
   CUTE_UNROLL
   for (int i = 0; i < size(x); ++i) {
-    y(i) = (isBetaZero ? alpha * x(i) : alpha * x(i) + beta * y(i));
+    if (p(i)) {
+      y(i) = (isBetaZero ? alpha * x(i) : alpha * x(i) + beta * y(i));
+    }
   }
 }
 
 } // end namespace cute
```

## cutlass_library/source/include/cute/algorithm/copy.hpp

```diff
@@ -141,18 +141,18 @@
 //
 // copy_if -- Predicated CopyAtom
 //
 
 namespace detail {
 
 // Trait that detects if atom's traits has a member function with(bool)
-template<typename, typename Enable = void>
+template <class, class Enable = void>
 constexpr bool has_with_bool = false;
 
-template<typename T>
+template <class T>
 constexpr bool has_with_bool<T, cute::void_t<decltype(declval<typename T::Traits>().with(declval<bool>()))>> = true;
 
 } // end namespace detail
 
 template <class... CopyArgs,
           class PredTensor,
           class SrcEngine, class SrcLayout,
```

## cutlass_library/source/include/cute/algorithm/functional.hpp

```diff
@@ -29,28 +29,29 @@
  *
  **************************************************************************************************/
 #pragma once
 
 #include <cute/config.hpp>
 
 #include <cute/util/type_traits.hpp>
+#include <cute/numeric/complex.hpp>
 
 /** C++14 <functional> extensions */
 
 namespace cute {
 
 /**************/
 /** Identity **/
 /**************/
 
 struct identity {
   template <class T>
   CUTE_HOST_DEVICE constexpr
   decltype(auto) operator()(T&& arg) const {
-    return std::forward<T>(arg);
+    return static_cast<T&&>(arg);
   }
 };
 
 template <class R>
 struct constant_fn {
   template <class... T>
   CUTE_HOST_DEVICE constexpr
@@ -65,31 +66,31 @@
 /***********/
 
 #define CUTE_LEFT_UNARY_OP(NAME,OP)                                  \
   struct NAME {                                                      \
     template <class T>                                               \
     CUTE_HOST_DEVICE constexpr                                       \
     decltype(auto) operator()(T&& arg) const {                       \
-      return OP std::forward<T>(arg);                                \
+      return OP static_cast<T&&>(arg);                                \
     }                                                                \
   }
 #define CUTE_RIGHT_UNARY_OP(NAME,OP)                                 \
   struct NAME {                                                      \
     template <class T>                                               \
     CUTE_HOST_DEVICE constexpr                                       \
     decltype(auto) operator()(T&& arg) const {                       \
-      return std::forward<T>(arg) OP ;                               \
+      return static_cast<T&&>(arg) OP ;                               \
     }                                                                \
   }
 #define CUTE_NAMED_UNARY_OP(NAME,OP)                                 \
   struct NAME {                                                      \
     template <class T>                                               \
     CUTE_HOST_DEVICE constexpr                                       \
     decltype(auto) operator()(T&& arg) const {                       \
-      return OP (std::forward<T>(arg));                              \
+      return OP (static_cast<T&&>(arg));                              \
     }                                                                \
   }
 
 CUTE_LEFT_UNARY_OP(unary_plus,       +);
 CUTE_LEFT_UNARY_OP(negate,           -);
 CUTE_LEFT_UNARY_OP(bit_not,          ~);
 CUTE_LEFT_UNARY_OP(logical_not,      !);
@@ -111,47 +112,47 @@
 template <int Shift_>
 struct shift_right_const {
   static constexpr int Shift = Shift_;
 
   template <class T>
   CUTE_HOST_DEVICE constexpr
   decltype(auto) operator()(T&& arg) const {
-    return std::forward<T>(arg) >> Shift;
+    return static_cast<T&&>(arg) >> Shift;
   }
 };
 
 template <int Shift_>
 struct shift_left_const {
   static constexpr int Shift = Shift_;
 
   template <class T>
   CUTE_HOST_DEVICE constexpr
   decltype(auto) operator()(T&& arg) const {
-    return std::forward<T>(arg) << Shift;
+    return static_cast<T&&>(arg) << Shift;
   }
 };
 
 /************/
 /** Binary **/
 /************/
 
 #define CUTE_BINARY_OP(NAME,OP)                                      \
   struct NAME {                                                      \
     template <class T, class U>                                      \
     CUTE_HOST_DEVICE constexpr                                       \
     decltype(auto) operator()(T&& lhs, U&& rhs) const {              \
-      return std::forward<T>(lhs) OP std::forward<U>(rhs);           \
+      return static_cast<T&&>(lhs) OP static_cast<U&&>(rhs);           \
     }                                                                \
   }
 #define CUTE_NAMED_BINARY_OP(NAME,OP)                                \
   struct NAME {                                                      \
     template <class T, class U>                                      \
     CUTE_HOST_DEVICE constexpr                                       \
     decltype(auto) operator()(T&& lhs, U&& rhs) const {              \
-      return OP (std::forward<T>(lhs), std::forward<U>(rhs));        \
+      return OP (static_cast<T&&>(lhs), static_cast<U&&>(rhs));        \
     }                                                                \
   }
 
 
 CUTE_BINARY_OP(plus,                 +);
 CUTE_BINARY_OP(minus,                -);
 CUTE_BINARY_OP(multiplies,           *);
@@ -269,15 +270,15 @@
 template <class Fn, class Arg>
 struct bound_fn {
 
   template <class T>
   CUTE_HOST_DEVICE constexpr
   decltype(auto)
   operator()(T&& arg) {
-    return fn_(arg_, std::forward<T>(arg));
+    return fn_(arg_, static_cast<T&&>(arg));
   }
 
   Fn fn_;
   Arg arg_;
 };
 
 template <class Fn, class Arg>
```

## cutlass_library/source/include/cute/algorithm/gemm.hpp

```diff
@@ -248,15 +248,15 @@
   CUTE_STATIC_ASSERT_V(size<1>(A) == size<1>(B));  // AK == BK
   CUTE_STATIC_ASSERT_V(size<0>(C) == size<0>(D) && size<1>(C) == size<1>(D));
 
   // Assert this is a 1-value MMA
   CUTE_STATIC_ASSERT_V(size<1>(typename MMA_Atom<MMA>::LayoutC_TV{}) == Int<1>{});
   CUTE_STATIC_ASSERT_V(size<1>(typename MMA_Atom<MMA>::LayoutA_TV{}) == Int<1>{});
   CUTE_STATIC_ASSERT_V(size<1>(typename MMA_Atom<MMA>::LayoutB_TV{}) == Int<1>{});
-  
+
   gemm(mma,
        make_tensor(D.data(), prepend<3>(D.layout())),      // (1,M,N)
        make_tensor(A.data(), prepend<3>(A.layout())),      // (1,M,K)
        make_tensor(B.data(), prepend<3>(B.layout())),      // (1,N,K)
        make_tensor(C.data(), prepend<3>(C.layout())));     // (1,M,N)
 }
 
@@ -447,14 +447,15 @@
   CUTE_STATIC_ASSERT_V(size<1>(A) == size<1>(B));  // AK == BK
   CUTE_STATIC_ASSERT_V(size<0>(C) == size<0>(D) && size<1>(C) == size<1>(D));
 
   // Assert this is a 1-value MMA
   CUTE_STATIC_ASSERT_V(size<1>(typename MMA_Atom<MMA>::LayoutC_TV{}) == Int<1>{});
   CUTE_STATIC_ASSERT_V(size<1>(typename MMA_Atom<MMA>::LayoutA_TV{}) == Int<1>{});
   CUTE_STATIC_ASSERT_V(size<1>(typename MMA_Atom<MMA>::LayoutB_TV{}) == Int<1>{});
+
   gemm(mma,
        make_tensor(D.data(), prepend<3>(D.layout())),      // (1,M,N)
        make_tensor(A.data(), prepend<3>(A.layout())),      // (1,M,K)
        make_tensor(B.data(), prepend<3>(B.layout())),      // (1,N,K)
        make_tensor(C.data(), prepend<3>(C.layout())));     // (1,M,N)
 }
 
@@ -492,249 +493,8 @@
     copy(A(_,_,k), rA(_,_,k));
     copy(B(_,_,k), rB(_,_,k));
     // Thread-level register gemm for k
     gemm(mma, D, rA(_,_,k), rB(_,_,k), C);
   }
 }
 
-//
-// Collective Shared-Memory GEMMs
-//
-
-template <class... Args,
-          class Alpha, class TA, class ALayout, class TB, class BLayout,
-          class Beta,  class TC, class CLayout,
-          class ALoadTransformOp, class BLoadTransformOp,
-          __CUTE_REQUIRES(ALayout::rank == 2 && is_smem<TA>::value &&
-                          BLayout::rank == 2 && is_smem<TB>::value &&
-                          CLayout::rank == 2 && is_smem<TC>::value)>
-CUTE_HOST_DEVICE
-void
-gemm(ThrMMA<Args...> const& thr_mma,
-     Alpha const& alpha,
-     Tensor<TA, ALayout> sA,
-     Tensor<TB, BLayout> sB,
-     Beta  const& beta,
-     Tensor<TC, CLayout> sC,
-     ALoadTransformOp const& sA_load_op /* transforms A values before used in GEMM */,
-     BLoadTransformOp const& sB_load_op /* transforms B values before used in GEMM */)
-{
-  CUTE_STATIC_ASSERT_V(size<0>(sA) == size<0>(sC));  // AM == CM
-  CUTE_STATIC_ASSERT_V(size<0>(sB) == size<1>(sC));  // BN == CN
-  CUTE_STATIC_ASSERT_V(size<1>(sA) == size<1>(sB));  // AK == BK
-
-  using TypeA = typename TA::value_type;
-  using TypeB = typename TB::value_type;
-  using TypeC = typename TC::value_type;
-
-  static_assert(is_same_v<decay_t<invoke_result_t<ALoadTransformOp, TypeA>>, TypeA>,
-    "ALoadTransformOp functor must accept and return value of type TA::value_type");
-  static_assert(is_same_v<decay_t<invoke_result_t<BLoadTransformOp, TypeB>>, TypeB>,
-    "BLoadTransformOp functor must accept and return value of type TB::value_type");
-
-  // Original, static size of the problem
-  auto M = size<0>(sC);
-  auto N = size<1>(sC);
-  auto K = size<1>(sA);
-
-  // Block size of the compute tile
-  auto BLK_M = tile_size<0>(thr_mma);
-  auto BLK_N = tile_size<1>(thr_mma);
-  auto BLK_K = tile_size<2>(thr_mma);
-
-  // Compute the "residues"
-  auto m_residue = M - BLK_M * (ceil_div(M, BLK_M) - Int<1>{});  //  (0,BLK_M]
-  auto n_residue = N - BLK_N * (ceil_div(N, BLK_N) - Int<1>{});  //  (0,BLK_N]
-  auto k_residue = K - BLK_K * (ceil_div(K, BLK_K)           );  // (-BLK_K,0]
-
-  // Shift the origin so k_residue is zeroth tile
-  sA.data() = &sA(0,k_residue);
-  sB.data() = &sB(0,k_residue);
-
-#if 0
-  if (thread0()) {
-    printf("%d in BLK_M (%d)\n", int(m_residue), int(BLK_M));
-    printf("%d in BLK_N (%d)\n", int(n_residue), int(BLK_N));
-    printf("%d in BLK_K (%d)\n", int(k_residue), int(BLK_K));
-  }
-#endif
-
-  //
-  // MMA Partitioning
-  //
-
-  // Round the layout extents up to BLK_X
-  Tensor rounded_sA = sA.compose(make_shape(ceil_div(M, BLK_M) * BLK_M, ceil_div(K, BLK_K) * BLK_K));
-  Tensor rounded_sB = sB.compose(make_shape(ceil_div(N, BLK_N) * BLK_N, ceil_div(K, BLK_K) * BLK_K));
-  Tensor rounded_sC = sC.compose(make_shape(ceil_div(M, BLK_M) * BLK_M, ceil_div(N, BLK_N) * BLK_N));
-
-#if 0
-  if (thread0()) {
-    print(rounded_sA.layout()); print("\n");
-    print(rounded_sB.layout()); print("\n");
-    print(rounded_sC.layout()); print("\n");
-  }
-#endif
-
-  // Partition the sA and sB tiles across the threads for the MMA
-  Tensor tCsA = thr_mma.partition_A(rounded_sA);                    // (MMA,MMA_M,MMA_K)
-  Tensor tCsB = thr_mma.partition_B(rounded_sB);                    // (MMA,MMA_N,MMA_K)
-  Tensor tCsC = thr_mma.partition_C(rounded_sC);                    // (MMA,MMA_M,MMA_N)
-  // Create register tensors for the MMA to operate on
-  Tensor tCrA = thr_mma.make_fragment_A(tCsA);                      // (MMA,MMA_M,MMA_K)
-  Tensor tCrB = thr_mma.make_fragment_B(tCsB);                      // (MMA,MMA_N,MMA_K)
-  Tensor tCrC = thr_mma.make_fragment_C(tCsC);                      // (MMA,MMA_M,MMA_N)
-
-#if 0
-  if (thread0()) {
-    print(tCsA.layout()); print("\n");
-    print(tCsB.layout()); print("\n");
-    print(tCsC.layout()); print("\n");
-    print(tCrA.layout()); print("\n");
-    print(tCrB.layout()); print("\n");
-    print(tCrC.layout()); print("\n");
-  }
-#endif
-
-  //
-  // PREDICATION
-  //
-
-  // Allocate the preds for only the MMA-mode of tCsA and tCsB
-  Tensor tCpA = make_tensor<bool>(size<0>(tCsA));
-  Tensor tCpB = make_tensor<bool>(size<0>(tCsB));
-
-  // Create coordinate tensors on a single compute block for predication
-  Tensor cA = make_identity_tensor(make_shape(BLK_M, BLK_K));        // (BLK_M,BLK_K) -> (blk_m,blk_k)
-  Tensor cB = make_identity_tensor(make_shape(BLK_N, BLK_K));        // (BLK_M,BLK_K) -> (blk_n,blk_k)
-
-  // Repeat partitioning with thr_mma
-  Tensor tCcA = thr_mma.partition_A(cA);                             // (MMA,1,1) -> (blk_m,blk_k)
-  Tensor tCcB = thr_mma.partition_B(cB);                             // (MMA,1,1) -> (blk_n,blk_k)
-
-  // Populate the m and n predicates
-  CUTE_UNROLL
-  for (int i = 0; i < size(tCpA); ++i) {
-    tCpA(i) = elem_less(get<0>(tCcA(i)), m_residue);
-  }
-  CUTE_UNROLL
-  for (int i = 0; i < size(tCpB); ++i) {
-    tCpB(i) = elem_less(get<0>(tCcB(i)), n_residue);
-  }
-
-#if 0
-  printf("Thr %d: A(%d,%d):%d  B(%d,%d):%d\n",
-         threadIdx.x,
-         int(get<0>(tCcA(0))), int(get<1>(tCcA(0))), int(tCpA(0)),
-         int(get<0>(tCcB(0))), int(get<1>(tCcB(0))), int(tCpB(0)));
-#endif
-
-  //
-  // PREFETCH k_block = 0 (with k-predication)
-  //
-
-  CUTE_UNROLL
-  for (int i = 0; i < size<0>(tCsA); ++i) {                // Copy MMA_I
-    if (k_residue == 0 || get<1>(tCcA(i)) >= -k_residue) { // k_block = 0, predicated on k
-      CUTE_UNROLL
-      for (int m = 0; m < size<1>(tCsA); ++m) {            // Copy MMA_M, predicated on m
-        tCrA(i,m,0) = (m_residue == BLK_M || m < size<1>(tCsA)-1 || tCpA(i)) ? sA_load_op(tCsA(i,m,0)) : TypeA{};
-      }
-    }
-  }
-
-  CUTE_UNROLL
-  for (int i = 0; i < size<0>(tCsB); ++i) {                // Copy MMA_I
-    if (k_residue == 0 || get<1>(tCcB(i)) >= -k_residue) { // k_block = 0, predicated on k
-      CUTE_UNROLL
-      for (int n = 0; n < size<1>(tCsB); ++n) {            // Copy MMA_N, predicated on n
-        tCrB(i,n,0) = (n_residue == BLK_N || n < size<1>(tCsB)-1 || tCpB(i)) ? sB_load_op(tCsB(i,n,0)) : TypeB{};
-      }
-    }
-  }
-  //
-  // MAINLOOP
-  //
-
-  // Clear accumulators
-  clear(tCrC);
-
-  constexpr int K_BLOCK_MAX = size<2>(tCrA);
-
-  CUTE_UNROLL
-  for (int k_block = 0; k_block < K_BLOCK_MAX; ++k_block)
-  {
-    // static-if load the next k_block. No k-predication required on these loads.
-    if (k_block < K_BLOCK_MAX-1)
-    {
-      // Load the next k_block
-      int k_next = k_block + 1;
-
-      CUTE_UNROLL
-      for (int m = 0; m < size<1>(tCsA); ++m) {            // Copy MMA_M
-        CUTE_UNROLL
-        for (int i = 0; i < size<0>(tCsA); ++i) {          // Copy_if MMA_I predicated on m
-          tCrA(i,m,k_next) = (m_residue == BLK_M || m < size<1>(tCsA)-1 || tCpA(i)) ? sA_load_op(tCsA(i,m,k_next)) : TypeA{};
-        }
-      }
-
-      CUTE_UNROLL
-      for (int n = 0; n < size<1>(tCsB); ++n) {            // Copy MMA_N
-        CUTE_UNROLL
-        for (int i = 0; i < size<0>(tCsB); ++i) {          // Copy MMA_I predicated on n
-          tCrB(i,n,k_next) = (n_residue == BLK_N || n < size<1>(tCsB)-1 || tCpB(i)) ? sB_load_op(tCsB(i,n,k_next)) : TypeB{};
-        }
-      }
-    }
-
-    // GEMM on k_block in registers
-    gemm(thr_mma, tCrA(_,_,k_block), tCrB(_,_,k_block), tCrC);
-  }
-
-  //
-  // Epilogue
-  //
-
-  Tensor cC   = make_identity_tensor(make_shape(BLK_M, BLK_N));      // (BLK_M,BLK_N) -> (blk_m,blk_n)
-  Tensor tCcC = thr_mma.partition_C(cC);                             // (MMA, 1, 1)   -> (blk_m,blk_n)
-
-  const bool isBetaZero = (beta == Beta{});
-
-  // Custom axpby_if for now
-  CUTE_UNROLL
-  for (int m = 0; m < size<1>(tCsC); ++m)
-  {
-    CUTE_UNROLL
-    for (int n = 0; n < size<2>(tCsC); ++n)
-    {
-      CUTE_UNROLL
-      for (int i = 0; i < size<0>(tCsC); ++i)
-      {
-        if ((m_residue == BLK_M || m < size<1>(tCrC)-1 || get<0>(tCcC(i)) < m_residue) &&
-            (n_residue == BLK_N || n < size<2>(tCrC)-1 || get<1>(tCcC(i)) < n_residue))
-        {
-          tCsC(i,m,n) = isBetaZero ? alpha * tCrC(i,m,n) : alpha * tCrC(i,m,n) + beta * tCsC(i,m,n);
-        }
-      }
-    }
-  }
-}
-
-template <class... Args,
-          class Alpha, class TA, class ALayout, class TB, class BLayout,
-          class Beta,  class TC, class CLayout,
-          __CUTE_REQUIRES(ALayout::rank == 2 && is_smem<TA>::value &&
-                          BLayout::rank == 2 && is_smem<TB>::value &&
-                          CLayout::rank == 2 && is_smem<TC>::value)>
-CUTE_HOST_DEVICE
-void
-gemm(ThrMMA<Args...> const& thr_mma,
-     Alpha const& alpha,
-     Tensor<TA, ALayout> sA,
-     Tensor<TB, BLayout> sB,
-     Beta  const& beta,
-     Tensor<TC, CLayout> sC)
-{
-  gemm(thr_mma, alpha, sA, sB, beta, sC, identity() /* sA_load_op */, identity() /* sB_load_op */);
-}
-
 } // end namespace cute
```

## cutlass_library/source/include/cute/algorithm/tensor_algorithms.hpp

```diff
@@ -46,113 +46,122 @@
 template <class Engine, class Layout, class UnaryOp>
 CUTE_HOST_DEVICE constexpr
 void
 for_each(Tensor<Engine,Layout> const& tensor, UnaryOp&& op)
 {
   CUTE_UNROLL
   for (int i = 0; i < size(tensor); ++i) {
-    static_cast<UnaryOp&&>(op)(tensor(i));
+    op(tensor(i));
   }
 }
 
 template <class Engine, class Layout, class UnaryOp>
 CUTE_HOST_DEVICE constexpr
 void
 for_each(Tensor<Engine,Layout>& tensor, UnaryOp&& op)
 {
   CUTE_UNROLL
   for (int i = 0; i < size(tensor); ++i) {
-    static_cast<UnaryOp&&>(op)(tensor(i));
+    op(tensor(i));
   }
 }
 
 // Accept mutable temporaries
 template <class Engine, class Layout, class UnaryOp>
 CUTE_HOST_DEVICE constexpr
 void
 for_each(Tensor<Engine,Layout>&& tensor, UnaryOp&& op)
 {
-  return for_each(tensor, static_cast<UnaryOp&&>(op));
+  return for_each(tensor, op);
 }
 
 //
 // transform
 //
 
 // Similar to std::transform but does not return number of elements affected
 template <class Engine, class Layout, class UnaryOp>
 CUTE_HOST_DEVICE constexpr
 void
 transform(Tensor<Engine,Layout>& tensor, UnaryOp&& op)
 {
   CUTE_UNROLL
   for (int i = 0; i < size(tensor); ++i) {
-    tensor(i) = static_cast<UnaryOp&&>(op)(tensor(i));
+    tensor(i) = op(tensor(i));
   }
 }
 
 // Accept mutable temporaries
 template <class Engine, class Layout, class UnaryOp>
 CUTE_HOST_DEVICE constexpr
 void
 transform(Tensor<Engine,Layout>&& tensor, UnaryOp&& op)
 {
-  return transform(tensor, std::forward<UnaryOp>(op));
+  return transform(tensor, op);
 }
 
 // Similar to std::transform transforms one tensors and assigns it to another
-template <class EngineIn, class LayoutIn, class EngineOut, class LayoutOut, class UnaryOp>
+template <class EngineIn, class LayoutIn, 
+          class EngineOut, class LayoutOut, 
+          class UnaryOp>
 CUTE_HOST_DEVICE constexpr
 void
-transform(Tensor<EngineIn,LayoutIn>& tensor_in, Tensor<EngineOut,LayoutOut>& tensor_out, UnaryOp&& op)
+transform(Tensor<EngineIn, LayoutIn > const& tensor_in, 
+          Tensor<EngineOut,LayoutOut>      & tensor_out, 
+          UnaryOp&& op)
 {
   CUTE_UNROLL
   for (int i = 0; i < size(tensor_in); ++i) {
-    tensor_out(i) = static_cast<UnaryOp&&>(op)(tensor_in(i));
+    tensor_out(i) = op(tensor_in(i));
   }
 }
 
 // Accept mutable temporaries
 template <class EngineIn, class LayoutIn,
-          class EngineOut, class LayoutOut, class UnaryOp>
+          class EngineOut, class LayoutOut, 
+          class UnaryOp>
 CUTE_HOST_DEVICE constexpr
 void
-transform(Tensor<EngineIn,LayoutIn>&& tensor_in, Tensor<EngineOut,LayoutOut>&& tensor_out, UnaryOp&& op)
+transform(Tensor<EngineIn, LayoutIn > const& tensor_in, 
+          Tensor<EngineOut,LayoutOut>     && tensor_out, 
+          UnaryOp&& op)
 {
   return transform(tensor_in, tensor_out, op);
 }
 
 // Similar to std::transform with a binary operation
 // Takes two tensors as input and one tensor as output. 
 // Applies the binary_op to tensor_in1 and tensor_in2 and
 // assigns it to tensor_out
 template <class EngineIn1, class LayoutIn1,
           class EngineIn2, class LayoutIn2,
-          class EngineOut, class LayoutOut, class BinaryOp>
+          class EngineOut, class LayoutOut, 
+          class BinaryOp>
 CUTE_HOST_DEVICE constexpr
 void
-transform(Tensor<EngineIn1,LayoutIn1>& tensor_in1,
-          Tensor<EngineIn2,LayoutIn2>& tensor_in2,
-          Tensor<EngineOut,LayoutOut>& tensor_out, 
+transform(Tensor<EngineIn1,LayoutIn1> const& tensor_in1,
+          Tensor<EngineIn2,LayoutIn2> const& tensor_in2,
+          Tensor<EngineOut,LayoutOut>      & tensor_out, 
           BinaryOp&& op)
 {
   CUTE_UNROLL
   for (int i = 0; i < size(tensor_in1); ++i) {
-    tensor_out(i) = static_cast<BinaryOp&&>(op)(tensor_in1(i), tensor_in2(i));
+    tensor_out(i) = op(tensor_in1(i), tensor_in2(i));
   }
 }
 
 // Accept mutable temporaries
 template <class EngineIn1, class LayoutIn1,
           class EngineIn2, class LayoutIn2,
-          class EngineOut, class LayoutOut, class BinaryOp>
+          class EngineOut, class LayoutOut, 
+          class BinaryOp>
 CUTE_HOST_DEVICE constexpr
 void
-transform(Tensor<EngineIn1,LayoutIn1>&& tensor_in1, 
-          Tensor<EngineIn2,LayoutIn2>&& tensor_in2,
-          Tensor<EngineOut,LayoutOut>&& tensor_out,
+transform(Tensor<EngineIn1,LayoutIn1> const& tensor_in1, 
+          Tensor<EngineIn2,LayoutIn2> const& tensor_in2,
+          Tensor<EngineOut,LayoutOut>     && tensor_out,
           BinaryOp&& op)
 {
   return transform(tensor_in1, tensor_in2, tensor_out, op);
 }
 
 } // end namespace cute
```

## cutlass_library/source/include/cute/algorithm/tuple_algorithms.hpp

```diff
@@ -201,44 +201,14 @@
     return f(static_cast<T&&>(t));
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
 //
-// For Sequence
-// (s, t, f) => (f(t[s_0]),f(t[s_1]),...,f(t[s_n]))
-//
-
-namespace detail {
-
-template <int... I, class F>
-CUTE_HOST_DEVICE constexpr
-void
-for_sequence(seq<I...> const&, F&& f) {
-  (f(Int<I>{}), ...);
-}
-
-}; // end namespace detail
-
-template <int... I, class T, class F>
-CUTE_HOST_DEVICE constexpr
-void
-for_sequence(seq<I...> const& s, T&& t, F&& f) {
-  detail::for_sequence(s, [&](auto&& i){ f(get<remove_cvref_t<decltype(i)>::value>(static_cast<T&&>(t))); });
-}
-
-template <int I, class T, class F>
-CUTE_HOST_DEVICE constexpr
-void
-for_sequence(T&& t, F&& f) {
-  for_sequence(make_seq<I>{}, static_cast<T&&>(t), static_cast<F&&>(f));
-}
-
-//
 // Transform
 // (t, f) => (f(t_0),f(t_1),...,f(t_n))
 //
 
 template <class T, class F>
 CUTE_HOST_DEVICE constexpr
 auto
@@ -547,23 +517,23 @@
 //
 // Select tuple elements with given indices.
 //
 
 template <int... I, class T>
 CUTE_HOST_DEVICE constexpr
 auto
-select(T const & t)
+select(T const& t)
 {
   return cute::make_tuple(get<I>(t)...);
 }
 
-template <class T, typename Indices>
+template <class T, class Indices>
 CUTE_HOST_DEVICE constexpr
 auto
-select(T const & t, Indices const & indices)
+select(T const& t, Indices const& indices)
 {
   if constexpr (is_tuple<Indices>::value) {
     return cute::transform(indices, [&t](auto i) { return select(t, i); });
   } else {
     static_assert(is_static<Indices>::value, "Order must be static");
     return get<Indices::value>(t);
   }
@@ -651,15 +621,15 @@
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
 namespace detail {
 
-template<class FlatTuple, class TargetProfile>
+template <class FlatTuple, class TargetProfile>
 CUTE_HOST_DEVICE constexpr
 auto
 unflatten_impl(FlatTuple const& flat_tuple, TargetProfile const& target_profile)
 {
   if constexpr (is_tuple<TargetProfile>::value) {
     return fold(target_profile, cute::make_tuple(cute::make_tuple(), flat_tuple), [](auto const& v, auto const& t) {
       auto [result, remaining_tuple] = v;
@@ -676,15 +646,15 @@
 }  // end namespace detail
 
 // Unflatten a flat tuple into a hierarchical tuple
 // @pre flatten(@a flat_tuple) == @a flat_tuple
 // @pre rank(flatten(@a target_profile)) == rank(@a flat_tuple)
 // @post congruent(@a result, @a target_profile)
 // @post flatten(@a result) == @a flat_tuple
-template<class FlatTuple, class TargetProfile>
+template <class FlatTuple, class TargetProfile>
 CUTE_HOST_DEVICE constexpr
 auto
 unflatten(FlatTuple const& flat_tuple, TargetProfile const& target_profile)
 {
   auto [unflatten_tuple, flat_remainder] = detail::unflatten_impl(flat_tuple, target_profile);
   CUTE_STATIC_ASSERT_V(rank(flat_remainder) == Int<0>{});
   return unflatten_tuple;
@@ -861,14 +831,15 @@
     } else {
       return detail::construct(cute::make_tuple(a), x, seq<0>{}, make_seq<N-1>{}, seq<>{});
     }
   }
 
   CUTE_GCC_UNREACHABLE;
 }
+
 template <class T, class X>
 CUTE_HOST_DEVICE constexpr
 auto
 append(T const& a, X const& x)
 {
   if constexpr (is_tuple<T>::value) {
     return detail::construct(a, x, make_seq<tuple_size<T>::value>{}, seq<0>{}, seq<>{});
@@ -898,14 +869,15 @@
       static_assert(N > 1);
       return detail::construct(cute::make_tuple(a), x, seq<>{}, make_seq<N-1>{}, seq<0>{});
     }
   }
 
   CUTE_GCC_UNREACHABLE;
 }
+
 template <class T, class X>
 CUTE_HOST_DEVICE constexpr
 auto
 prepend(T const& a, X const& x)
 {
   if constexpr (is_tuple<T>::value) {
     return detail::construct(a, x, seq<>{}, seq<0>{}, make_seq<tuple_size<T>::value>{});
@@ -1101,20 +1073,19 @@
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
 /// @return A tuple of the elements of @c t in reverse order.
 template <class T>
-CUTE_HOST_DEVICE constexpr auto
-reverse(T const& t) {
+CUTE_HOST_DEVICE constexpr 
+auto
+reverse(T const& t) 
+{
   if constexpr (is_tuple<T>::value) {
-    return detail::apply(t, [] (auto const&... a) {
-        return cute::make_tuple(a...);
-      }, tuple_rseq<T>{});
-  }
-  else {
+    return detail::apply(t, [](auto const&... a){ return cute::make_tuple(a...); }, tuple_rseq<T>{});
+  } else {
     return t;
   }
 }
 
 } // end namespace cute
```

## cutlass_library/source/include/cute/arch/cluster_sm90.hpp

```diff
@@ -45,43 +45,43 @@
 namespace cute {
 
 CUTE_DEVICE void cluster_arrive_relaxed()
 {
 #if defined(CUTE_ARCH_CLUSTER_SM90_ENABLED)
   asm volatile("barrier.cluster.arrive.relaxed.aligned;\n" : : );
 #else
-  CUTE_RUNTIME_ASSERT("CUTE_ARCH_CLUSTER_SM90_ENABLED is not defined");
+  CUTE_INVALID_CONTROL_PATH("CUTE_ARCH_CLUSTER_SM90_ENABLED is not defined");
 #endif
 }
 
 CUTE_DEVICE void cluster_arrive()
 {
 #if defined(CUTE_ARCH_CLUSTER_SM90_ENABLED)
   asm volatile("barrier.cluster.arrive.aligned;\n" : : );
 #else
-  CUTE_RUNTIME_ASSERT("CUTE_ARCH_CLUSTER_SM90_ENABLED is not defined");
+  CUTE_INVALID_CONTROL_PATH("CUTE_ARCH_CLUSTER_SM90_ENABLED is not defined");
 #endif
 }
 
 CUTE_DEVICE void cluster_wait()
 {
 #if defined(CUTE_ARCH_CLUSTER_SM90_ENABLED)
   asm volatile("barrier.cluster.wait.aligned;\n" : : );
 #else
-  CUTE_RUNTIME_ASSERT("CUTE_ARCH_CLUSTER_SM90_ENABLED is not defined");
+  CUTE_INVALID_CONTROL_PATH("CUTE_ARCH_CLUSTER_SM90_ENABLED is not defined");
 #endif
 }
 
 CUTE_DEVICE void cluster_sync()
 {
 #if defined(CUTE_ARCH_CLUSTER_SM90_ENABLED)
   cluster_arrive();
   cluster_wait();
 #else
-  CUTE_RUNTIME_ASSERT("CUTE_ARCH_CLUSTER_SM90_ENABLED is not defined");
+  CUTE_INVALID_CONTROL_PATH("CUTE_ARCH_CLUSTER_SM90_ENABLED is not defined");
 #endif
 }
 
 // Returns the dim3 grid size in terms of number of clusters.
 CUTE_DEVICE dim3 cluster_grid_dims()
 {
 #if defined(CUTE_ARCH_CLUSTER_SM90_ENABLED)
@@ -90,15 +90,15 @@
   asm volatile("mov.u32 %0, %%nclusterid.y;\n" : "=r"(y) : );
   asm volatile("mov.u32 %0, %%nclusterid.z;\n" : "=r"(z) : );
   return {x, y, z};
 #elif defined(__CUDA_ARCH__)
   // MSVC requires protecting use of gridDim with __CUDA_ARCH__.
   return gridDim;
 #elif defined(_MSC_VER)
-  CUTE_RUNTIME_ASSERT("cluster_grid_dims() can only be called on device");
+  CUTE_INVALID_CONTROL_PATH("cluster_grid_dims() can only be called on device");
   return {0, 0, 0};
 #else
   return {0, 0, 0};
 #endif
 }
 
 // Returns the dim3 cluster rank in the grid.
@@ -110,15 +110,15 @@
   asm volatile("mov.u32 %0, %%clusterid.y;\n" : "=r"(y) : );
   asm volatile("mov.u32 %0, %%clusterid.z;\n" : "=r"(z) : );
   return {x, y, z};
 #elif defined(__CUDA_ARCH__)
   // MSVC requires protecting use of blockIdx with __CUDA_ARCH__.
   return blockIdx;
 #elif defined(_MSC_VER)
-  CUTE_RUNTIME_ASSERT("cluster_id_in_grid() can only be called on device");
+  CUTE_INVALID_CONTROL_PATH("cluster_id_in_grid() can only be called on device");
   return {0, 0, 0};
 #else
   return {0, 0, 0};
 #endif
 }
 
 // Returns the relative dim3 block rank local to the cluster.
```

## cutlass_library/source/include/cute/arch/copy.hpp

```diff
@@ -29,15 +29,15 @@
  *
  **************************************************************************************************/
 #pragma once
 
 #include <cute/config.hpp>
 
 #include <cute/arch/util.hpp>
-#include <cute/numeric/int.hpp>
+#include <cute/numeric/numeric_types.hpp>
 
 namespace cute
 {
 
 //
 // Direct Copy for any type
 //
@@ -85,8 +85,21 @@
 //
 
 using AutoVectorizingCopy = AutoVectorizingCopyWithAssumedAlignment<8>;
 
 // Alias
 using DefaultCopy = AutoVectorizingCopy;
 
+
+//
+// Global memory prefetch into L2
+//
+
+CUTE_HOST_DEVICE static void
+prefetch(void const* gmem_ptr)
+{
+#if defined(__CUDA_ARCH__)
+  asm volatile("prefetch.global.L2 [%0];\n" : : "l"(gmem_ptr) : "memory");
+#endif
+}
+
 } // end namespace cute
```

## cutlass_library/source/include/cute/arch/copy_sm75.hpp

```diff
@@ -74,15 +74,15 @@
   {
 #if defined(CUTE_ARCH_LDSM_SM75_ACTIVATED)
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_src);
     asm volatile ("ldmatrix.sync.aligned.x1.m8n8.shared.b16 {%0}, [%1];\n"
         : "=r"(dst)
         :  "r"(smem_int_ptr));
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use ldmatrix without CUTE_ARCH_LDSM_SM75_ACTIVATED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use ldmatrix without CUTE_ARCH_LDSM_SM75_ACTIVATED.");
 #endif
   }
 };
 
 struct SM75_U32x2_LDSM_N
 {
   using SRegisters = uint128_t[1];
@@ -94,15 +94,15 @@
   {
 #if defined(CUTE_ARCH_LDSM_SM75_ACTIVATED)
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_src);
     asm volatile ("ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%0, %1}, [%2];\n"
         : "=r"(dst0), "=r"(dst1)
         :  "r"(smem_int_ptr));
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use ldmatrix without CUTE_ARCH_LDSM_SM75_ACTIVATED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use ldmatrix without CUTE_ARCH_LDSM_SM75_ACTIVATED.");
 #endif
   }
 };
 
 struct SM75_U32x4_LDSM_N
 {
   using SRegisters = uint128_t[1];
@@ -114,15 +114,15 @@
   {
 #if defined(CUTE_ARCH_LDSM_SM75_ACTIVATED)
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_src);
     asm volatile ("ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%0, %1, %2, %3}, [%4];\n"
         : "=r"(dst0), "=r"(dst1), "=r"(dst2), "=r"(dst3)
         :  "r"(smem_int_ptr));
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use ldmatrix without CUTE_ARCH_LDSM_SM75_ACTIVATED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use ldmatrix without CUTE_ARCH_LDSM_SM75_ACTIVATED.");
 #endif
   }
 };
 
 struct SM75_U16x2_LDSM_T
 {
   using SRegisters = uint128_t[1];
@@ -134,15 +134,15 @@
   {
 #if defined(CUTE_ARCH_LDSM_SM75_ACTIVATED)
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_src);
     asm volatile ("ldmatrix.sync.aligned.x1.trans.m8n8.shared.b16 {%0}, [%1];\n"
         : "=r"(dst)
         :  "r"(smem_int_ptr));
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use ldmatrix without CUTE_ARCH_LDSM_SM75_ACTIVATED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use ldmatrix without CUTE_ARCH_LDSM_SM75_ACTIVATED.");
 #endif
   }
 };
 
 struct SM75_U16x4_LDSM_T
 {
   using SRegisters = uint128_t[1];
@@ -154,15 +154,15 @@
   {
 #if defined(CUTE_ARCH_LDSM_SM75_ACTIVATED)
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_src);
     asm volatile ("ldmatrix.sync.aligned.x2.trans.m8n8.shared.b16 {%0, %1}, [%2];\n"
         : "=r"(dst0), "=r"(dst1)
         :  "r"(smem_int_ptr));
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use ldmatrix without CUTE_ARCH_LDSM_SM75_ACTIVATED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use ldmatrix without CUTE_ARCH_LDSM_SM75_ACTIVATED.");
 #endif
   }
 };
 
 struct SM75_U16x8_LDSM_T
 {
   using SRegisters = uint128_t[1];
@@ -174,15 +174,15 @@
   {
 #if defined(CUTE_ARCH_LDSM_SM75_ACTIVATED)
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_src);
     asm volatile ("ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%0, %1, %2, %3}, [%4];\n"
         : "=r"(dst0), "=r"(dst1), "=r"(dst2), "=r"(dst3)
         :  "r"(smem_int_ptr));
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use ldmatrix without CUTE_ARCH_LDSM_SM75_ACTIVATED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use ldmatrix without CUTE_ARCH_LDSM_SM75_ACTIVATED.");
 #endif
   }
 };
 
 //
 // Legacy LDSM interfaces that aren't very useful
 //
```

## cutlass_library/source/include/cute/arch/copy_sm80.hpp

```diff
@@ -60,15 +60,15 @@
     TS const* gmem_ptr    = &gmem_src;
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_dst);
     asm volatile("cp.async.ca.shared.global.L2::128B [%0], [%1], %2;\n"
         :: "r"(smem_int_ptr),
            "l"(gmem_ptr),
            "n"(sizeof(TS)));
 #else
-    CUTE_RUNTIME_ASSERT("Support for cp.async instructions has not been enabled");
+    CUTE_INVALID_CONTROL_PATH("Support for cp.async instructions has not been enabled");
 #endif
   }
 };
 
 /// Copy via cp.async with caching at global level
 template <class TS, class TD = TS>
 struct SM80_CP_ASYNC_CACHEGLOBAL
@@ -87,15 +87,15 @@
     TS const* gmem_ptr    = &gmem_src;
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_dst);
     asm volatile("cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\n"
         :: "r"(smem_int_ptr),
            "l"(gmem_ptr),
            "n"(sizeof(TS)));
 #else
-    CUTE_RUNTIME_ASSERT("Support for cp.async instructions has not been enabled");
+    CUTE_INVALID_CONTROL_PATH("Support for cp.async instructions has not been enabled");
 #endif
   }
 };
 
 /// Copy via cp.async with caching at all levels
 template <class TS, class TD = TS>
 struct SM80_CP_ASYNC_CACHEALWAYS_ZFILL
@@ -117,15 +117,15 @@
     int src_size = pred ? sizeof(TS) : 0;
     asm volatile("cp.async.ca.shared.global.L2::128B [%0], [%1], %2, %3;\n"
         :: "r"(smem_int_ptr),
            "l"(gmem_ptr),
            "n"(sizeof(TS)),
            "r"(src_size));
 #else
-    CUTE_RUNTIME_ASSERT("Support for cp.async instructions has not been enabled");
+    CUTE_INVALID_CONTROL_PATH("Support for cp.async instructions has not been enabled");
 #endif
   }
 };
 
 /// Copy via cp.async with caching at global level
 template <class TS, class TD = TS>
 struct SM80_CP_ASYNC_CACHEGLOBAL_ZFILL
@@ -147,15 +147,15 @@
     int src_size = pred ? sizeof(TS) : 0;
     asm volatile("cp.async.cg.shared.global.L2::128B [%0], [%1], %2, %3;\n"
         :: "r"(smem_int_ptr),
            "l"(gmem_ptr),
            "n"(sizeof(TS)),
            "r"(src_size));
 #else
-    CUTE_RUNTIME_ASSERT("Support for cp.async instructions has not been enabled");
+    CUTE_INVALID_CONTROL_PATH("Support for cp.async instructions has not been enabled");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Establishes an ordering w.r.t previously issued cp.async instructions. Does not block.
```

## cutlass_library/source/include/cute/arch/copy_sm90.hpp

```diff
@@ -59,15 +59,15 @@
   {
 #if defined(CUTE_ARCH_STSM_SM90_ENABLED)
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_dst);
     asm volatile ("stmatrix.sync.aligned.x1.m8n8.shared.b16 [%0], {%1};\n"
         :: "r"(smem_int_ptr),
            "r"(src));
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use stmatrix without CUTE_ARCH_STSM_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use stmatrix without CUTE_ARCH_STSM_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_U32x2_STSM_N
 {
   using SRegisters = uint32_t[2];
@@ -79,15 +79,15 @@
   {
 #if defined(CUTE_ARCH_STSM_SM90_ENABLED)
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_dst);
     asm volatile ("stmatrix.sync.aligned.x2.m8n8.shared.b16 [%0], {%1, %2};\n"
         :: "r"(smem_int_ptr),
            "r"(src0), "r"(src1));
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use stmatrix without CUTE_ARCH_STSM_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use stmatrix without CUTE_ARCH_STSM_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_U32x4_STSM_N
 {
   using SRegisters = uint32_t[4];
@@ -99,15 +99,15 @@
   {
 #if defined(CUTE_ARCH_STSM_SM90_ENABLED)
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_dst);
     asm volatile ("stmatrix.sync.aligned.x4.m8n8.shared.b16 [%0], {%1, %2, %3, %4};\n"
         :: "r"(smem_int_ptr),
           "r"(src0), "r"(src1), "r"(src2), "r"(src3));
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use stmatrix without CUTE_ARCH_STSM_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use stmatrix without CUTE_ARCH_STSM_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_U16x2_STSM_T
 {
   using SRegisters = uint32_t[1];
@@ -119,15 +119,15 @@
   {
 #if defined(CUTE_ARCH_STSM_SM90_ENABLED)
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_dst);
     asm volatile ("stmatrix.sync.aligned.x1.trans.m8n8.shared.b16 [%0], {%1};\n"
         :: "r"(smem_int_ptr),
            "r"(src));
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use stmatrix without CUTE_ARCH_STSM_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use stmatrix without CUTE_ARCH_STSM_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_U16x4_STSM_T
 {
   using SRegisters = uint32_t[2];
@@ -139,15 +139,15 @@
   {
 #if defined(CUTE_ARCH_STSM_SM90_ENABLED)
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_dst);
     asm volatile ("stmatrix.sync.aligned.x2.trans.m8n8.shared.b16 [%0], {%1, %2};\n"
         :: "r"(smem_int_ptr),
            "r"(src0), "r"(src1));
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use stmatrix without CUTE_ARCH_STSM_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use stmatrix without CUTE_ARCH_STSM_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_U16x8_STSM_T
 {
   using SRegisters = uint32_t[4];
@@ -159,15 +159,15 @@
   {
 #if defined(CUTE_ARCH_STSM_SM90_ENABLED)
     uint32_t smem_int_ptr = cast_smem_ptr_to_uint(&smem_dst);
     asm volatile ("stmatrix.sync.aligned.x4.trans.m8n8.shared.b16 [%0], {%1, %2, %3, %4};\n"
         :: "r"(smem_int_ptr),
           "r"(src0), "r"(src1), "r"(src2), "r"(src3));
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use stmatrix without CUTE_ARCH_STSM_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use stmatrix without CUTE_ARCH_STSM_SM90_ENABLED.");
 #endif
   }
 };
 
 //
 // Legacy STSM interfaces that aren't very useful
 //
```

## cutlass_library/source/include/cute/arch/copy_sm90_desc.hpp

```diff
@@ -39,16 +39,15 @@
 
 #include <cute/arch/copy.hpp>
 #include <cute/arch/copy_sm90.hpp>
 
 #include <cute/container/alignment.hpp>
 #include <cute/container/bit_field.hpp>
 #include <cute/container/array.hpp>
-#include <cute/numeric/int.hpp>   // to_Format<[u]intX>
-#include <cute/numeric/half.hpp>  // to_Format<half_t>
+#include <cute/numeric/numeric_types.hpp>
 
 namespace cute
 {
 
 //////////////////////////////////////////////////////////////////////////////////////////////////////
 /// Barriers are 64-bit of user-managed information used in broadly two types syncronization patterns
 /// 1) arrive/wait on threads (usage: cp.async and warp-specialized kernels)
@@ -173,16 +172,18 @@
 
 #endif // (__CUDACC_VER_MAJOR__ >= 12)
 
 } // end namespace TMA
 
 #if (__CUDACC_VER_MAJOR__ >= 12) && !defined(__CUDACC_RTC__)
   using TmaDescriptor = CUtensorMap;
+  using Im2ColTmaDescriptor = CUtensorMap;
 #else
   using TmaDescriptor = struct alignas(64) { char bytes[128]; };
+  using Im2ColTmaDescriptor = struct alignas(64) { char bytes[128]; };
 #endif
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 /// Initiates a TensorMap Prefetch
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 CUTE_HOST_DEVICE
 void
@@ -193,57 +194,57 @@
   // Prefetch TMA Descriptor using generic addressing (i.e. no specific state space: const or param)
   asm volatile (
     "prefetch.tensormap [%0];"
     :
     : "l"(gmem_int_desc)
     : "memory");
 #else
-  CUTE_RUNTIME_ASSERT("Trying to use TMA Descriptor Prefetch without CUTE_ARCH_TMA_SM90_ENABLED.");
+  CUTE_INVALID_CONTROL_PATH("Trying to use TMA Descriptor Prefetch without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 /// Perform a TensorMap modification (by each field)
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // Replace tensor pointer directly in GMEM
 CUTE_HOST_DEVICE
 void
-tma_descriptor_replace_addr_in_global_mem(TmaDescriptor const* desc_ptr, 
+tma_descriptor_replace_addr_in_global_mem(TmaDescriptor const* desc_ptr,
                                           void const* const new_tensor_ptr)
 {
 #if defined(CUTE_ARCH_DEVICE_MODIFIABLE_TMA_SM90_ENABLED)
   uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
   uint64_t const new_desc_addr = reinterpret_cast<uint64_t>(new_tensor_ptr);
   asm volatile (
     "tensormap.replace.tile.global_address.global.b1024.b64 [%0], %1;"
     :: "l"(gmem_int_desc), "l"(new_desc_addr));
 #else
-  CUTE_RUNTIME_ASSERT("Using TMA Descriptor modification without CUTE_ARCH_TMA_SM90_ENABLED and CUDA 12.3");
+  CUTE_INVALID_CONTROL_PATH("Using TMA Descriptor modification without CUTE_ARCH_TMA_SM90_ENABLED and CUDA 12.3");
 #endif
 }
 
 // Replace tensor pointer by bringing the tensormap from GMEM into the shared memory
 CUTE_HOST_DEVICE
 void
-tma_descriptor_replace_addr_in_shared_mem(TmaDescriptor& smem_desc, 
+tma_descriptor_replace_addr_in_shared_mem(TmaDescriptor& smem_desc,
                                           void const* const new_tensor_ptr)
 {
 #if defined(CUTE_ARCH_DEVICE_MODIFIABLE_TMA_SM90_ENABLED)
   uint32_t smem_int_desc = cast_smem_ptr_to_uint(&smem_desc);
   uint64_t const new_desc_addr = reinterpret_cast<uint64_t>(new_tensor_ptr);
   uint64_t const smem_int64_desc = 0;
   asm volatile (
     "cvt.u64.u32 %0, %1;"
     :: "l"(smem_int64_desc), "r"(smem_int_desc));
   asm volatile (
     "tensormap.replace.tile.global_address.shared::cta.b1024.b64 [%0], %1;"
     :: "l"(smem_int64_desc), "l"(new_desc_addr));
 #else
-  CUTE_RUNTIME_ASSERT("Using TMA Descriptor modification without CUTE_ARCH_TMA_SM90_ENABLED and CUDA 12.3");
+  CUTE_INVALID_CONTROL_PATH("Using TMA Descriptor modification without CUTE_ARCH_TMA_SM90_ENABLED and CUDA 12.3");
 #endif
 }
 
 // Replace tensor dims and strides for GEMMs by bringing the tensormap from GMEM into the shared memory
 CUTE_HOST_DEVICE
 void
 tma_descriptor_replace_dims_strides_in_shared_mem(TmaDescriptor                 & smem_desc,
@@ -269,15 +270,15 @@
     asm volatile (
       "tensormap.replace.tile.global_stride.shared::cta.b1024.b64 [%0], 0, %1;"
       :: "l"(smem_int64_desc), "l"(prob_stride[1] >> 4));
     asm volatile (
       "tensormap.replace.tile.global_stride.shared::cta.b1024.b64 [%0], 1, %1;"
       :: "l"(smem_int64_desc), "l"(prob_stride[2] >> 4));
 #else
-  CUTE_RUNTIME_ASSERT("Using TMA Descriptor modification without CUTE_ARCH_TMA_SM90_ENABLED and CUDA 12.3");
+  CUTE_INVALID_CONTROL_PATH("Using TMA Descriptor modification without CUTE_ARCH_TMA_SM90_ENABLED and CUDA 12.3");
 #endif
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 /// Perform a fused copy and fence operation (needed when modifying tensormap in shared memory)
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -288,30 +289,30 @@
 #if defined(CUTE_ARCH_DEVICE_MODIFIABLE_TMA_SM90_ENABLED)
   uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(gmem_desc_ptr);
   uint32_t smem_int_desc = cast_smem_ptr_to_uint(&smem_desc);
   asm volatile (
     "tensormap.cp_fenceproxy.global.shared::cta.tensormap::generic.release.gpu.sync.aligned [%0], [%1], 128;"
     :: "l"(gmem_int_desc), "r"(smem_int_desc));
 #else
-  CUTE_RUNTIME_ASSERT("Using TMA Descriptor modification without CUTE_ARCH_TMA_SM90_ENABLED and CUDA 12.3");
+  CUTE_INVALID_CONTROL_PATH("Using TMA Descriptor modification without CUTE_ARCH_TMA_SM90_ENABLED and CUDA 12.3");
 #endif
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 /// Perform a release fence operation (needed when modifying tensormap directly in GMEM)
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 CUTE_HOST_DEVICE
 void
 tma_descriptor_fence_release()
 {
 #if defined(CUTE_ARCH_DEVICE_MODIFIABLE_TMA_SM90_ENABLED)
   asm volatile ("fence.proxy.tensormap::generic.release.gpu;");
 #else
-  CUTE_RUNTIME_ASSERT("Using TMA Descriptor modification without CUTE_ARCH_TMA_SM90_ENABLED and CUDA 12.3");
+  CUTE_INVALID_CONTROL_PATH("Using TMA Descriptor modification without CUTE_ARCH_TMA_SM90_ENABLED and CUDA 12.3");
 #endif
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 /// Perform a acquire fence operation
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -328,14 +329,14 @@
     : "memory");
   asm volatile (
     "cvta.global.u64 %0, %0;"
     :
     : "l"(gmem_int_desc), "l"(gmem_int_desc)
     : "memory");
 #else
-  CUTE_RUNTIME_ASSERT("Using TMA Descriptor modification without CUTE_ARCH_TMA_SM90_ENABLED and CUDA 12.3");
+  CUTE_INVALID_CONTROL_PATH("Using TMA Descriptor modification without CUTE_ARCH_TMA_SM90_ENABLED and CUDA 12.3");
 #endif
 }
 
 ///////////////////////////////////////////////////////////////////////////////
 
 } // end namespace cute
```

## cutlass_library/source/include/cute/arch/copy_sm90_tma.hpp

```diff
@@ -40,905 +40,1319 @@
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 /// TMA_LOAD : Initiates a TMA copy from global memory to shared memory
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct SM90_TMA_LOAD_1D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& crd0)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes"
       " [%0], [%1, {%3}], [%2];"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "r"(crd0)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
+
+  struct PREFETCH
+  {
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& crd0)
+    {
+  #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+      uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
+      asm volatile (
+        "cp.async.bulk.prefetch.tensor.1d.L2.global"
+        " [%0, {%1}];"
+        :
+        : "l"(gmem_int_desc),
+          "r"(crd0)
+        : "memory");
+  #else
+      CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+  #endif
+    }
+  };
 };
 
 struct SM90_TMA_LOAD_2D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes"
       " [%0], [%1, {%3, %4}], [%2];"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "r"(crd0), "r"(crd1)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
+
+  struct PREFETCH
+  {
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& crd0, int32_t const& crd1)
+    {
+  #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+      uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
+      asm volatile (
+        "cp.async.bulk.prefetch.tensor.2d.L2.global"
+        " [%0, {%1, %2}];"
+        :
+        : "l"(gmem_int_desc),
+          "r"(crd0), "r"(crd1)
+        : "memory");
+  #else
+      CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+  #endif
+    }
+  };
 };
 
 struct SM90_TMA_LOAD_3D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.3d.shared::cluster.global.mbarrier::complete_tx::bytes"
       " [%0], [%1, {%3, %4, %5}], [%2];"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "r"(crd0), "r"(crd1), "r"(crd2)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
+
+  struct PREFETCH
+  {
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& crd0, int32_t const& crd1, int32_t const& crd2)
+    {
+  #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+      uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
+      asm volatile (
+        "cp.async.bulk.prefetch.tensor.3d.L2.global"
+        " [%0, {%1, %2, %3}];"
+        :
+        : "l"(gmem_int_desc),
+          "r"(crd0), "r"(crd1), "r"(crd2)
+        : "memory");
+  #else
+      CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+  #endif
+    }
+  };
 };
 
 struct SM90_TMA_LOAD_4D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier::complete_tx::bytes"
       " [%0], [%1, {%3, %4, %5, %6}], [%2];"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "r"(crd0), "r"(crd1), "r"(crd2), "r"(crd3)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
+
+  struct PREFETCH
+  {
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3)
+    {
+  #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+      uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
+      asm volatile (
+        "cp.async.bulk.prefetch.tensor.4d.L2.global"
+        " [%0, {%1, %2, %3, %4}];"
+        :
+        : "l"(gmem_int_desc),
+          "r"(crd0), "r"(crd1), "r"(crd2), "r"(crd3)
+        : "memory");
+  #else
+      CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+  #endif
+    }
+  };
 };
 
 struct SM90_TMA_LOAD_5D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3, int32_t const& crd4)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.5d.shared::cluster.global.mbarrier::complete_tx::bytes"
       " [%0], [%1, {%3, %4, %5, %6, %7}], [%2];"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "r"(crd0), "r"(crd1), "r"(crd2), "r"(crd3), "r"(crd4)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
+
+  struct PREFETCH
+  {
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3, int32_t const& crd4)
+    {
+  #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+      uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
+      asm volatile (
+        "cp.async.bulk.prefetch.tensor.5d.L2.global"
+        " [%0, {%1, %2, %3, %4, %5}];"
+        :
+        : "l"(gmem_int_desc),
+          "r"(crd0), "r"(crd1), "r"(crd2), "r"(crd3), "r"(crd4)
+        : "memory");
+  #else
+      CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+  #endif
+    }
+  };
 };
 
 struct SM90_TMA_LOAD
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& crd0)
   {
-    return SM90_TMA_LOAD_1D::copy(desc_ptr, smem_mbar, smem_ptr, crd0);
+    return SM90_TMA_LOAD_1D::copy(desc_ptr, mbar_ptr, smem_ptr, crd0);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1)
   {
-    return SM90_TMA_LOAD_2D::copy(desc_ptr, smem_mbar, smem_ptr, crd0, crd1);
+    return SM90_TMA_LOAD_2D::copy(desc_ptr, mbar_ptr, smem_ptr, crd0, crd1);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2)
   {
-    return SM90_TMA_LOAD_3D::copy(desc_ptr, smem_mbar, smem_ptr, crd0, crd1, crd2);
+    return SM90_TMA_LOAD_3D::copy(desc_ptr, mbar_ptr, smem_ptr, crd0, crd1, crd2);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3)
   {
-    return SM90_TMA_LOAD_4D::copy(desc_ptr, smem_mbar, smem_ptr, crd0, crd1, crd2, crd3);
+    return SM90_TMA_LOAD_4D::copy(desc_ptr, mbar_ptr, smem_ptr, crd0, crd1, crd2, crd3);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3, int32_t const& crd4)
   {
-    return SM90_TMA_LOAD_5D::copy(desc_ptr, smem_mbar, smem_ptr, crd0, crd1, crd2, crd3, crd4);
+    return SM90_TMA_LOAD_5D::copy(desc_ptr, mbar_ptr, smem_ptr, crd0, crd1, crd2, crd3, crd4);
   }
+
+  struct PREFETCH
+  {
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& crd0)
+    {
+      return SM90_TMA_LOAD_1D::PREFETCH::copy(desc_ptr, crd0);
+    }
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& crd0, int32_t const& crd1)
+    {
+      return SM90_TMA_LOAD_2D::PREFETCH::copy(desc_ptr, crd0, crd1);
+    }
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& crd0, int32_t const& crd1, int32_t const& crd2)
+    {
+      return SM90_TMA_LOAD_3D::PREFETCH::copy(desc_ptr, crd0, crd1, crd2);
+    }
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3)
+    {
+      return SM90_TMA_LOAD_4D::PREFETCH::copy(desc_ptr, crd0, crd1, crd2, crd3);
+    }
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3, int32_t const& crd4)
+    {
+      return SM90_TMA_LOAD_5D::PREFETCH::copy(desc_ptr, crd0, crd1, crd2, crd3, crd4);
+    }
+  };
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 /// TMA_LOAD im2col: Initiates a TMA copy, in im2col mode, from global memory to shared memory
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct SM90_TMA_LOAD_IM2COL_3D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_n,
        uint16_t const& offset_w)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     // Copy from global to shared::cluster.
     asm volatile (
       "cp.async.bulk.tensor.3d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes"
       " [%0], [%1, {%3, %4, %5}], [%2], {%6};"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "r"(coord_c), "r"(coord_w), "r"(coord_n),
         "h"(offset_w)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
+
+  struct PREFETCH
+  {
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_n,
+         uint16_t const& offset_w)
+    {
+  #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+      uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
+      asm volatile (
+        "cp.async.bulk.prefetch.tensor.3d.L2.global.im2col"
+        " [%0, {%1, %2, %3}], {%4};"
+        :
+        : "l"(gmem_int_desc),
+          "r"(coord_c), "r"(coord_w), "r"(coord_n),
+          "h"(offset_w)
+        : "memory");
+  #else
+      CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+  #endif
+    }
+  };
 };
 
 struct SM90_TMA_LOAD_IM2COL_4D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_n,
-       uint16_t const& offset_w,
-       uint16_t const& offset_h)
+       uint16_t const& offset_w, uint16_t const& offset_h)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     // Copy from global to shared::cluster.
     asm volatile (
       "cp.async.bulk.tensor.4d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes"
       " [%0], [%1, {%3, %4, %5, %6}], [%2], {%7, %8};"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "r"(coord_c), "r"(coord_w), "r"(coord_h), "r"(coord_n),
         "h"(offset_w), "h"(offset_h)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
+
+  struct PREFETCH
+  {
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_n,
+         uint16_t const& offset_w, uint16_t const& offset_h)
+    {
+  #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+      uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
+      asm volatile (
+        "cp.async.bulk.prefetch.tensor.4d.L2.global.im2col"
+        " [%0, {%1, %2, %3, %4}], {%5, %6};"
+        :
+        : "l"(gmem_int_desc),
+          "r"(coord_c), "r"(coord_w), "r"(coord_h), "r"(coord_n),
+          "h"(offset_w), "h"(offset_h)
+        : "memory");
+  #else
+      CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+  #endif
+    }
+  };
 };
 
 struct SM90_TMA_LOAD_IM2COL_5D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_d, int32_t const& coord_n,
-       uint16_t const& offset_w,
-       uint16_t const& offset_h,
-       uint16_t const& offset_d)
+       uint16_t const& offset_w, uint16_t const& offset_h, uint16_t const& offset_d)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     // Copy from global to shared::cluster.
     asm volatile (
       "cp.async.bulk.tensor.5d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes"
       " [%0], [%1, {%3, %4, %5, %6, %7}], [%2], {%8, %9, %10};"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "r"(coord_c), "r"(coord_w), "r"(coord_h), "r"(coord_d), "r"(coord_n),
         "h"(offset_w), "h"(offset_h), "h"(offset_d)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
+
+  struct PREFETCH
+  {
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_d, int32_t const& coord_n,
+         uint16_t const& offset_w, uint16_t const& offset_h, uint16_t const& offset_d)
+    {
+  #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+      uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
+      asm volatile (
+        "cp.async.bulk.prefetch.tensor.5d.L2.global.im2col"
+        " [%0, {%1, %2, %3, %4, %5}], {%6, %7, %8};"
+        :
+        : "l"(gmem_int_desc),
+          "r"(coord_c), "r"(coord_w), "r"(coord_h), "r"(coord_d), "r"(coord_n),
+          "h"(offset_w), "h"(offset_h), "h"(offset_d)
+        : "memory");
+  #else
+      CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+  #endif
+    }
+  };
 };
 
 struct SM90_TMA_LOAD_IM2COL
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_n,
        uint16_t const& offset_w)
   {
-    return SM90_TMA_LOAD_IM2COL_3D::copy(desc_ptr, smem_mbar, smem_ptr,
+    return SM90_TMA_LOAD_IM2COL_3D::copy(desc_ptr, mbar_ptr, smem_ptr,
                                          coord_c, coord_w, coord_n,
                                          offset_w);
   }
-
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_n,
-       uint16_t const& offset_w,
-       uint16_t const& offset_h)
+       uint16_t const& offset_w, uint16_t const& offset_h)
   {
-    return SM90_TMA_LOAD_IM2COL_4D::copy(desc_ptr, smem_mbar, smem_ptr,
-					 coord_c, coord_w, coord_h, coord_n,
-					 offset_w, offset_h);
+    return SM90_TMA_LOAD_IM2COL_4D::copy(desc_ptr, mbar_ptr, smem_ptr,
+                                         coord_c, coord_w, coord_h, coord_n,
+                                         offset_w, offset_h);
   }
-
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_d, int32_t const& coord_n,
-       uint16_t const& offset_w,
-       uint16_t const& offset_h,
-       uint16_t const& offset_d)
-  {
-    return SM90_TMA_LOAD_IM2COL_5D::copy(desc_ptr, smem_mbar, smem_ptr,
-					 coord_c, coord_w, coord_h, coord_d, coord_n,
-					 offset_w, offset_h, offset_d);
-  }
+       uint16_t const& offset_w, uint16_t const& offset_h, uint16_t const& offset_d)
+  {
+    return SM90_TMA_LOAD_IM2COL_5D::copy(desc_ptr, mbar_ptr, smem_ptr,
+                                         coord_c, coord_w, coord_h, coord_d, coord_n,
+                                         offset_w, offset_h, offset_d);
+  }
+
+  struct PREFETCH
+  {
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_n,
+         uint16_t const& offset_w)
+    {
+      return SM90_TMA_LOAD_IM2COL_3D::PREFETCH::copy(desc_ptr,
+                                                     coord_c, coord_w, coord_n,
+                                                     offset_w);
+    }
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_n,
+         uint16_t const& offset_w, uint16_t const& offset_h)
+    {
+      return SM90_TMA_LOAD_IM2COL_4D::PREFETCH::copy(desc_ptr,
+                                                     coord_c, coord_w, coord_h, coord_n,
+                                                     offset_w, offset_h);
+    }
+    CUTE_HOST_DEVICE static void
+    copy(void const* desc_ptr,
+         int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_d, int32_t const& coord_n,
+         uint16_t const& offset_w, uint16_t const& offset_h, uint16_t const& offset_d)
+    {
+      return SM90_TMA_LOAD_IM2COL_5D::PREFETCH::copy(desc_ptr,
+                                                     coord_c, coord_w, coord_h, coord_d, coord_n,
+                                                     offset_w, offset_h, offset_d);
+    }
+  };
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 /// TMA_LOAD_MULTICAST: Initiates a TMA copy from global memory to shared memory
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct SM90_TMA_LOAD_MULTICAST_1D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& crd0)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster"
       " [%0], [%1, {%4}], [%2], %3;"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "h"(multicast_mask),
         "r"(crd0)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_LOAD_MULTICAST_2D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster"
       " [%0], [%1, {%4, %5}], [%2], %3;"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "h"(multicast_mask),
         "r"(crd0), "r"(crd1)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_LOAD_MULTICAST_3D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.3d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster"
       " [%0], [%1, {%4, %5, %6}], [%2], %3;"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "h"(multicast_mask),
         "r"(crd0), "r"(crd1), "r"(crd2)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_LOAD_MULTICAST_4D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster"
       " [%0], [%1, {%4, %5, %6, %7}], [%2], %3;"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "h"(multicast_mask),
         "r"(crd0), "r"(crd1), "r"(crd2),  "r"(crd3)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_LOAD_MULTICAST_5D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3, int32_t const& crd4)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.5d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster"
       " [%0], [%1, {%4, %5, %6, %7, %8}], [%2], %3;"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "h"(multicast_mask),
         "r"(crd0), "r"(crd1), "r"(crd2), "r"(crd3), "r"(crd4)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_LOAD_MULTICAST
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& crd0)
   {
-    return SM90_TMA_LOAD_MULTICAST_1D::copy(desc_ptr, smem_mbar, multicast_mask, smem_ptr, crd0);
+    return SM90_TMA_LOAD_MULTICAST_1D::copy(desc_ptr, mbar_ptr, multicast_mask, smem_ptr, crd0);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1)
   {
-    return SM90_TMA_LOAD_MULTICAST_2D::copy(desc_ptr, smem_mbar, multicast_mask, smem_ptr, crd0, crd1);
+    return SM90_TMA_LOAD_MULTICAST_2D::copy(desc_ptr, mbar_ptr, multicast_mask, smem_ptr, crd0, crd1);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2)
   {
-    return SM90_TMA_LOAD_MULTICAST_3D::copy(desc_ptr, smem_mbar, multicast_mask, smem_ptr, crd0, crd1, crd2);
+    return SM90_TMA_LOAD_MULTICAST_3D::copy(desc_ptr, mbar_ptr, multicast_mask, smem_ptr, crd0, crd1, crd2);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3)
   {
-    return SM90_TMA_LOAD_MULTICAST_4D::copy(desc_ptr, smem_mbar, multicast_mask, smem_ptr, crd0, crd1, crd2, crd3);
+    return SM90_TMA_LOAD_MULTICAST_4D::copy(desc_ptr, mbar_ptr, multicast_mask, smem_ptr, crd0, crd1, crd2, crd3);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3, int32_t const& crd4)
   {
-    return SM90_TMA_LOAD_MULTICAST_5D::copy(desc_ptr, smem_mbar, multicast_mask, smem_ptr, crd0, crd1, crd2, crd3, crd4);
+    return SM90_TMA_LOAD_MULTICAST_5D::copy(desc_ptr, mbar_ptr, multicast_mask, smem_ptr, crd0, crd1, crd2, crd3, crd4);
   }
+
+  using PREFETCH = typename SM90_TMA_LOAD::PREFETCH;
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 /// TMA_LOAD_MULTICAST im2col: Initiates a TMA copy, in im2col mode, from global memory to shared memory
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct SM90_TMA_LOAD_IM2COL_MULTICAST_3D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_n,
        uint16_t const& offset_w)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     // Copy from global to shared::cluster.
     asm volatile (
       "cp.async.bulk.tensor.3d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes.multicast::cluster"
       " [%0], [%1, {%3, %4, %5}], [%2], {%6}, %7;"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "r"(coord_c), "r"(coord_w), "r"(coord_n),
         "h"(offset_w),
-	"h"(multicast_mask)
+        "h"(multicast_mask)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_LOAD_IM2COL_MULTICAST_4D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_n,
        uint16_t const& offset_w, uint16_t const& offset_h)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     // Copy from global to shared::cluster.
     asm volatile (
       "cp.async.bulk.tensor.4d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes.multicast::cluster"
       " [%0], [%1, {%3, %4, %5, %6}], [%2], {%7, %8}, %9;"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "r"(coord_c), "r"(coord_w), "r"(coord_h), "r"(coord_n),
         "h"(offset_w), "h"(offset_h),
-	"h"(multicast_mask)
+        "h"(multicast_mask)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_LOAD_IM2COL_MULTICAST_5D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_d, int32_t const& coord_n,
        uint16_t const& offset_w, uint16_t const& offset_h, uint16_t const& offset_d)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     // Copy from global to shared::cluster.
     asm volatile (
       "cp.async.bulk.tensor.5d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes.multicast::cluster"
       " [%0], [%1, {%3, %4, %5, %6, %7}], [%2], {%8, %9, %10}, %11;"
       :
       : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
         "r"(coord_c), "r"(coord_w), "r"(coord_h), "r"(coord_d), "r"(coord_n),
         "h"(offset_w), "h"(offset_h), "h"(offset_d),
-	"h"(multicast_mask)
+        "h"(multicast_mask)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_LOAD_IM2COL_MULTICAST
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_n,
        uint16_t const& offset_w)
   {
-    return SM90_TMA_LOAD_IM2COL_MULTICAST_3D::copy(desc_ptr, smem_mbar, multicast_mask, 
+    return SM90_TMA_LOAD_IM2COL_MULTICAST_3D::copy(desc_ptr, mbar_ptr, multicast_mask,
                                                    smem_ptr,
                                                    coord_c, coord_w, coord_n,
                                                    offset_w);
   }
 
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_n,
        uint16_t const& offset_w, uint16_t const& offset_h)
   {
-    return SM90_TMA_LOAD_IM2COL_MULTICAST_4D::copy(desc_ptr, smem_mbar, multicast_mask, 
+    return SM90_TMA_LOAD_IM2COL_MULTICAST_4D::copy(desc_ptr, mbar_ptr, multicast_mask,
                                                    smem_ptr,
                                                    coord_c, coord_w, coord_h, coord_n,
                                                    offset_w, offset_h);
   }
 
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr, uint64_t& smem_mbar, uint16_t multicast_mask,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask,
+       void      * smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_d, int32_t const& coord_n,
        uint16_t const& offset_w, uint16_t const& offset_h, uint16_t const& offset_d)
   {
-    return SM90_TMA_LOAD_IM2COL_MULTICAST_5D::copy(desc_ptr, smem_mbar, multicast_mask, 
+    return SM90_TMA_LOAD_IM2COL_MULTICAST_5D::copy(desc_ptr, mbar_ptr, multicast_mask,
                                                    smem_ptr,
                                                    coord_c, coord_w, coord_h, coord_d, coord_n,
                                                    offset_w, offset_h, offset_d);
   }
+
+  using PREFETCH = typename SM90_TMA_LOAD_IM2COL::PREFETCH;
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 /// TMA_STORE : Initiates a TMA copy from shared memory to global memory
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct SM90_TMA_STORE_1D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& crd0)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.1d.global.shared::cta.bulk_group [%0, {%2}], [%1];"
       :
       : "l"(gmem_int_desc), "r"(smem_int_ptr),
         "r"(crd0)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_STORE_2D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& crd0, int32_t const& crd1)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.2d.global.shared::cta.bulk_group [%0, {%2, %3}], [%1];"
       :
       : "l"(gmem_int_desc), "r"(smem_int_ptr),
         "r"(crd0), "r"(crd1)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_STORE_3D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.3d.global.shared::cta.bulk_group [%0, {%2, %3, %4}], [%1];"
       :
       : "l"(gmem_int_desc), "r"(smem_int_ptr),
         "r"(crd0), "r"(crd1), "r"(crd2)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_STORE_4D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.4d.global.shared::cta.bulk_group [%0, {%2, %3, %4, %5}], [%1];"
       :
       : "l"(gmem_int_desc), "r"(smem_int_ptr),
         "r"(crd0), "r"(crd1), "r"(crd2), "r"(crd3)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_STORE_5D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3, int32_t const& crd4)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.5d.global.shared::cta.bulk_group [%0, {%2, %3, %4, %5, %6}], [%1];"
       :
       : "l"(gmem_int_desc), "r"(smem_int_ptr),
         "r"(crd0), "r"(crd1), "r"(crd2), "r"(crd3), "r"(crd4)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_STORE
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& crd0)
   {
     return SM90_TMA_STORE_1D::copy(desc_ptr, smem_ptr, crd0);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& crd0, int32_t const& crd1)
   {
     return SM90_TMA_STORE_2D::copy(desc_ptr, smem_ptr, crd0, crd1);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2)
   {
     return SM90_TMA_STORE_3D::copy(desc_ptr, smem_ptr, crd0, crd1, crd2);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3)
   {
     return SM90_TMA_STORE_4D::copy(desc_ptr, smem_ptr, crd0, crd1, crd2, crd3);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3, int32_t const& crd4)
   {
     return SM90_TMA_STORE_5D::copy(desc_ptr, smem_ptr, crd0, crd1, crd2, crd3, crd4);
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 /// TMA_STORE im2col: Initiates a TMA copy, in im2col mode, from shared memory to global memory
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct SM90_TMA_STORE_IM2COL_3D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_n)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.3d.global.shared::cta.im2col_no_offs.bulk_group"
       " [%0, {%2, %3, %4}], [%1];"
       :
       : "l"(gmem_int_desc), "r"(smem_int_ptr),
         "r"(coord_c), "r"(coord_w), "r"(coord_n)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_STORE_IM2COL_4D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_n)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.4d.global.shared::cta.im2col_no_offs.bulk_group"
       " [%0, {%2, %3, %4, %5}], [%1];"
       :
       : "l"(gmem_int_desc), "r"(smem_int_ptr),
         "r"(coord_c), "r"(coord_w), "r"(coord_h), "r"(coord_n)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_STORE_IM2COL_5D
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_d, int32_t const& coord_n)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile (
       "cp.async.bulk.tensor.5d.global.shared::cta.im2col_no_offs.bulk_group"
       " [%0, {%2, %3, %4, %5, %6}], [%1];"
       :
       : "l"(gmem_int_desc), "r"(smem_int_ptr),
         "r"(coord_c), "r"(coord_w), "r"(coord_h), "r"(coord_d), "r"(coord_n)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_TMA_STORE_IM2COL
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_n)
   {
     return SM90_TMA_STORE_IM2COL_3D::copy(desc_ptr, smem_ptr, coord_c, coord_w, coord_n);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_n)
   {
     return SM90_TMA_STORE_IM2COL_4D::copy(desc_ptr, smem_ptr, coord_c, coord_w, coord_h, coord_n);
   }
   CUTE_HOST_DEVICE static void
-  copy(void const* const desc_ptr,
-       void const* const smem_ptr,
+  copy(void const* desc_ptr,
+       void const* smem_ptr,
        int32_t const& coord_c, int32_t const& coord_w, int32_t const& coord_h, int32_t const& coord_d, int32_t const& coord_n)
   {
     return SM90_TMA_STORE_IM2COL_5D::copy(desc_ptr, smem_ptr, coord_c, coord_w, coord_h, coord_d, coord_n);
   }
 };
 
+// Fence for smem stores for subsequent TMA_STORE
+CUTE_HOST_DEVICE static void
+tma_store_fence() {
+#if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+    asm volatile ("fence.proxy.async.shared::cta;");
+#elif defined(__CUDA_ARCH__)
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+#endif
+}
+
 // Indicate arrival of warp issuing TMA_STORE
 CUTE_HOST_DEVICE static void
 tma_store_arrive() {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     asm volatile("cp.async.bulk.commit_group;");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
 }
 
 // Wait until at most Count committed TMA_STOREs are pending and all prior commits are complete
 template <int Count>
 CUTE_HOST_DEVICE static void
 tma_store_wait() {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     asm volatile(
       "cp.async.bulk.wait_group.read %0;"
       :
       : "n"(Count)
       : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
+/// TMA_REDUCE_ADD : Initiates a TMA reduce-add from shared memory to global memory
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
+struct SM90_TMA_REDUCE_ADD_1D
+{
+  CUTE_HOST_DEVICE static void
+  copy(void const* const desc_ptr,
+       void const* const smem_ptr,
+       int32_t const& crd0)
+  {
+#if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+    uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
+    uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
+    asm volatile (
+      "cp.reduce.async.bulk.tensor.1d.global.shared::cta.add.bulk_group [%0, {%2}], [%1];"
+      :
+      : "l"(gmem_int_desc), "r"(smem_int_ptr),
+        "r"(crd0)
+      : "memory");
+#else
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+#endif
+  }
+};
+
+struct SM90_TMA_REDUCE_ADD_2D
+{
+  CUTE_HOST_DEVICE static void
+  copy(void const* const desc_ptr,
+       void const* const smem_ptr,
+       int32_t const& crd0, int32_t const& crd1)
+  {
+#if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+    uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
+    uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
+    asm volatile (
+      "cp.reduce.async.bulk.tensor.2d.global.shared::cta.add.bulk_group [%0, {%2, %3}], [%1];"
+      :
+      : "l"(gmem_int_desc), "r"(smem_int_ptr),
+        "r"(crd0), "r"(crd1)
+      : "memory");
+#else
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+#endif
+  }
+};
+
+struct SM90_TMA_REDUCE_ADD_3D
+{
+  CUTE_HOST_DEVICE static void
+  copy(void const* const desc_ptr,
+       void const* const smem_ptr,
+       int32_t const& crd0, int32_t const& crd1, int32_t const& crd2)
+  {
+#if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+    uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
+    uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
+    asm volatile (
+      "cp.reduce.async.bulk.tensor.3d.global.shared::cta.add.bulk_group [%0, {%2, %3, %4}], [%1];"
+      :
+      : "l"(gmem_int_desc), "r"(smem_int_ptr),
+        "r"(crd0), "r"(crd1), "r"(crd2)
+      : "memory");
+#else
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+#endif
+  }
+};
+
+struct SM90_TMA_REDUCE_ADD_4D
+{
+  CUTE_HOST_DEVICE static void
+  copy(void const* const desc_ptr,
+       void const* const smem_ptr,
+       int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3)
+  {
+#if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+    uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
+    uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
+    asm volatile (
+      "cp.reduce.async.bulk.tensor.4d.global.shared::cta.add.bulk_group [%0, {%2, %3, %4, %5}], [%1];"
+      :
+      : "l"(gmem_int_desc), "r"(smem_int_ptr),
+        "r"(crd0), "r"(crd1), "r"(crd2), "r"(crd3)
+      : "memory");
+#else
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+#endif
+  }
+};
+
+struct SM90_TMA_REDUCE_ADD_5D
+{
+  CUTE_HOST_DEVICE static void
+  copy(void const* const desc_ptr,
+       void const* const smem_ptr,
+       int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3, int32_t const& crd4)
+  {
+#if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+    uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(desc_ptr);
+    uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
+    asm volatile (
+      "cp.reduce.async.bulk.tensor.5d.global.shared::cta.add.bulk_group [%0, {%2, %3, %4, %5, %6}], [%1];"
+      :
+      : "l"(gmem_int_desc), "r"(smem_int_ptr),
+        "r"(crd0), "r"(crd1), "r"(crd2), "r"(crd3), "r"(crd4)
+      : "memory");
+#else
+    CUTE_INVALID_CONTROL_PATH("Trying to use tma without CUTE_ARCH_TMA_SM90_ENABLED.");
+#endif
+  }
+};
+
+struct SM90_TMA_REDUCE_ADD
+{
+  CUTE_HOST_DEVICE static void
+  copy(void const* const desc_ptr,
+       void const* const smem_ptr,
+       int32_t const& crd0)
+  {
+    return SM90_TMA_REDUCE_ADD_1D::copy(desc_ptr, smem_ptr, crd0);
+  }
+  CUTE_HOST_DEVICE static void
+  copy(void const* const desc_ptr,
+       void const* const smem_ptr,
+       int32_t const& crd0, int32_t const& crd1)
+  {
+    return SM90_TMA_REDUCE_ADD_2D::copy(desc_ptr, smem_ptr, crd0, crd1);
+  }
+  CUTE_HOST_DEVICE static void
+  copy(void const* const desc_ptr,
+       void const* const smem_ptr,
+       int32_t const& crd0, int32_t const& crd1, int32_t const& crd2)
+  {
+    return SM90_TMA_REDUCE_ADD_3D::copy(desc_ptr, smem_ptr, crd0, crd1, crd2);
+  }
+  CUTE_HOST_DEVICE static void
+  copy(void const* const desc_ptr,
+       void const* const smem_ptr,
+       int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3)
+  {
+    return SM90_TMA_REDUCE_ADD_4D::copy(desc_ptr, smem_ptr, crd0, crd1, crd2, crd3);
+  }
+  CUTE_HOST_DEVICE static void
+  copy(void const* const desc_ptr,
+       void const* const smem_ptr,
+       int32_t const& crd0, int32_t const& crd1, int32_t const& crd2, int32_t const& crd3, int32_t const& crd4)
+  {
+    return SM90_TMA_REDUCE_ADD_5D::copy(desc_ptr, smem_ptr, crd0, crd1, crd2, crd3, crd4);
+  }
+};
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
 /// BULK_COPY : Copy a bulk of memory between shared memory and global memory
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct SM90_BULK_COPY_G2S
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const gmem_ptr, uint64_t& smem_mbar,
-       void const* const smem_ptr, int32_t load_bytes)
+  copy(void const* gmem_ptr, uint64_t* mbar_ptr,
+       void      * smem_ptr, int32_t load_bytes)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
-    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(&smem_mbar);
+    uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr);
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile("cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [%0], [%1], %2, [%3];\n"
                      :
                      : "r"(smem_int_ptr), "l"(gmem_ptr), "r"(load_bytes), "r"(smem_int_mbar)
                      : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use BULK_COPY without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use BULK_COPY without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
+
+  struct PREFETCH
+  {
+    CUTE_HOST_DEVICE static void
+    copy(void const* gmem_ptr, int32_t load_bytes)
+    {
+  #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
+      asm volatile("cp.async.bulk.prefetch.L2.global [%0], %1;\n"
+                      :
+                      : "l"(gmem_ptr), "r"(load_bytes)
+                      : "memory");
+  #else
+      CUTE_INVALID_CONTROL_PATH("Trying to use BULK_COPY without CUTE_ARCH_TMA_SM90_ENABLED.");
+  #endif
+    }
+  };
 };
 
 struct SM90_BULK_COPY_S2G
 {
   CUTE_HOST_DEVICE static void
-  copy(void const* const smem_ptr,
-       void const* const gmem_ptr, int32_t store_bytes)
+  copy(void const* smem_ptr,
+       void      * gmem_ptr, int32_t store_bytes)
   {
 #if defined(CUTE_ARCH_TMA_SM90_ENABLED)
     uint32_t smem_int_ptr  = cast_smem_ptr_to_uint(smem_ptr);
     asm volatile("cp.async.bulk.global.shared::cta.bulk_group [%0], [%1], %2;\n"
                      :
                      : "l"(gmem_ptr), "r"(smem_int_ptr), "r"(store_bytes)
                      : "memory");
 #else
-    CUTE_RUNTIME_ASSERT("Trying to use BULK_COPY without CUTE_ARCH_TMA_SM90_ENABLED.");
+    CUTE_INVALID_CONTROL_PATH("Trying to use BULK_COPY without CUTE_ARCH_TMA_SM90_ENABLED.");
 #endif
   }
 };
 
 struct SM90_BULK_COPY_AUTO {};
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cute/arch/mma_sm61.hpp

```diff
@@ -54,15 +54,15 @@
   fma(int32_t& d, uint32_t const& a, uint32_t const& b, int32_t const& c)
   {
 #if defined(CUTE_ARCH_MMA_SM61_ENABLED)
     asm volatile("dp4a.s32.s32 %0, %1, %2, %3;"
                  : "=r"(d)
                  : "r"(a), "r"(b), "r"(c));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM61_DP4A without CUTE_ARCH_MMA_SM61_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM61_DP4A without CUTE_ARCH_MMA_SM61_ENABLED");
 #endif
   }
 };
 
 struct SM61_DP2A
 {
   using DRegisters = int32_t[1];
@@ -75,13 +75,13 @@
   fma(int32_t& d, uint32_t const& a, uint32_t const& b, int32_t const& c)
   {
 #if defined(CUTE_ARCH_MMA_SM61_ENABLED)
     asm volatile("dp2a.s32.s32 %0, %1, %2, %3;"
                  : "=r"(d)
                  : "r"(a), "r"(b), "r"(c));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM61_DP2A without CUTE_ARCH_MMA_SM61_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM61_DP2A without CUTE_ARCH_MMA_SM61_ENABLED");
 #endif
   }
 };
 
 } // namespace cute
```

## cutlass_library/source/include/cute/arch/mma_sm70.hpp

```diff
@@ -70,15 +70,15 @@
                  "{%6, %7},"
                  "{%8, %9, %10, %11};\n"
         : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
         :  "r"(a0),  "r"(a1),
            "r"(b0),  "r"(b1),
            "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM70_8x8x4_F16F16F16F16_TN without CUTE_ARCH_MMA_SM70_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM70_8x8x4_F16F16F16F16_TN without CUTE_ARCH_MMA_SM70_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct SM70_8x8x4_F16F16F16F16_NT
@@ -102,15 +102,15 @@
                  "{%6, %7},"
                  "{%8, %9, %10, %11};\n"
         : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
         :  "r"(a0),  "r"(a1),
            "r"(b0),  "r"(b1),
            "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM70_8x8x4_F16F16F16F16_NT without CUTE_ARCH_MMA_SM70_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM70_8x8x4_F16F16F16F16_NT without CUTE_ARCH_MMA_SM70_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct SM70_8x8x4_F16F16F16F16_NN
@@ -134,15 +134,15 @@
                  "{%6, %7},"
                  "{%8, %9, %10, %11};\n"
         : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
         :  "r"(a0),  "r"(a1),
            "r"(b0),  "r"(b1),
            "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM70_8x8x4_F16F16F16F16_NN without CUTE_ARCH_MMA_SM70_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM70_8x8x4_F16F16F16F16_NN without CUTE_ARCH_MMA_SM70_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct SM70_8x8x4_F16F16F16F16_TT
@@ -166,15 +166,15 @@
                  "{%6, %7},"
                  "{%8, %9, %10, %11};\n"
         : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
         :  "r"(a0),  "r"(a1),
            "r"(b0),  "r"(b1),
            "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM70_8x8x4_F16F16F16F16_TT without CUTE_ARCH_MMA_SM70_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM70_8x8x4_F16F16F16F16_TT without CUTE_ARCH_MMA_SM70_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 //
@@ -206,15 +206,15 @@
         : "=f"(d0), "=f"(d1), "=f"(d2), "=f"(d3),
           "=f"(d4), "=f"(d5), "=f"(d6), "=f"(d7)
         :  "r"(a0),  "r"(a1),
            "r"(b0),  "r"(b1),
            "f"(c0),  "f"(c1),  "f"(c2),  "f"(c3),
            "f"(c4),  "f"(c5),  "f"(c6),  "f"(c7));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM70_8x8x4_F32F16F16F32_TN without CUTE_ARCH_MMA_SM70_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM70_8x8x4_F32F16F16F32_TN without CUTE_ARCH_MMA_SM70_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct SM70_8x8x4_F32F16F16F32_NT
@@ -242,15 +242,15 @@
         : "=f"(d0), "=f"(d1), "=f"(d2), "=f"(d3),
           "=f"(d4), "=f"(d5), "=f"(d6), "=f"(d7)
         :  "r"(a0),  "r"(a1),
            "r"(b0),  "r"(b1),
            "f"(c0),  "f"(c1),  "f"(c2),  "f"(c3),
            "f"(c4),  "f"(c5),  "f"(c6),  "f"(c7));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM70_8x8x4_F32F16F16F32_NT without CUTE_ARCH_MMA_SM70_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM70_8x8x4_F32F16F16F32_NT without CUTE_ARCH_MMA_SM70_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct SM70_8x8x4_F32F16F16F32_NN
@@ -278,15 +278,15 @@
         : "=f"(d0), "=f"(d1), "=f"(d2), "=f"(d3),
           "=f"(d4), "=f"(d5), "=f"(d6), "=f"(d7)
         :  "r"(a0),  "r"(a1),
            "r"(b0),  "r"(b1),
            "f"(c0),  "f"(c1),  "f"(c2),  "f"(c3),
            "f"(c4),  "f"(c5),  "f"(c6),  "f"(c7));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM70_8x8x4_F32F16F16F32_NN without CUTE_ARCH_MMA_SM70_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM70_8x8x4_F32F16F16F32_NN without CUTE_ARCH_MMA_SM70_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 struct SM70_8x8x4_F32F16F16F32_TT
@@ -314,15 +314,15 @@
         : "=f"(d0), "=f"(d1), "=f"(d2), "=f"(d3),
           "=f"(d4), "=f"(d5), "=f"(d6), "=f"(d7)
         :  "r"(a0),  "r"(a1),
            "r"(b0),  "r"(b1),
            "f"(c0),  "f"(c1),  "f"(c2),  "f"(c3),
            "f"(c4),  "f"(c5),  "f"(c6),  "f"(c7));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM70_8x8x4_F32F16F16F32_TT without CUTE_ARCH_MMA_SM70_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM70_8x8x4_F32F16F16F32_TT without CUTE_ARCH_MMA_SM70_ENABLED");
 #endif
   }
 
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cute/arch/mma_sm75.hpp

```diff
@@ -70,15 +70,15 @@
                  "{%6},"
                  "{%7, %8, %9, %10};\n"
         : "=f"(d0), "=f"(d1), "=f"(d2), "=f"(d3)
         :  "r"(a0),  "r"(a1),
            "r"(b0),
            "f"(c0),  "f"(c1),  "f"(c2),  "f"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM75_16x8x8_F32F16F16F32_TN without CUTE_ARCH_MMA_SM75_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM75_16x8x8_F32F16F16F32_TN without CUTE_ARCH_MMA_SM75_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 //
@@ -106,15 +106,15 @@
                  "{%3},"
                  "{%4, %5};\n"
         : "=r"(d0), "=r"(d1)
         :  "r"(a0),
            "r"(b0),
            "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM75_8x8x16_S32S8S8S32_TN without CUTE_ARCH_MMA_SM75_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM75_8x8x16_S32S8S8S32_TN without CUTE_ARCH_MMA_SM75_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // end namespace cute
```

## cutlass_library/source/include/cute/arch/mma_sm80.hpp

```diff
@@ -29,14 +29,15 @@
  *
  **************************************************************************************************/
 
 #pragma once
 
 #include <cute/config.hpp>
 #include <cute/arch/mma.hpp>
+#include <cute/numeric/complex.hpp>
 
 // Config
 #if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800))
 #  define CUTE_ARCH_MMA_SM80_ENABLED
 
 #if (__CUDA_ARCH__ <= 900)
 #define CUTE_ARCH_MMA_B1_AND_SM80_ENABLED
@@ -76,15 +77,15 @@
       "{%4},"
       "{%5, %6};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x8_F16F16F16F16_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x8_F16F16F16F16_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x16 TN
@@ -109,15 +110,15 @@
       "{%6,  %7},"
       "{%8,  %9};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x16_F16F16F16F16_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x16_F16F16F16F16_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x8 TN
@@ -142,15 +143,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=f"(d0), "=f"(d1), "=f"(d2), "=f"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "f"(c0),  "f"(c1),  "f"(c2),  "f"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x8_F32F16F16F32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x8_F32F16F16F32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x16 TN
@@ -175,15 +176,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=f"(d0), "=f"(d1), "=f"(d2), "=f"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "f"(c0),  "f"(c1),  "f"(c2),  "f"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x16_F32F16F16F32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x16_F32F16F16F32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x8 TN
@@ -208,15 +209,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=f"(d0), "=f"(d1), "=f"(d2), "=f"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "f"(c0),  "f"(c1),  "f"(c2),  "f"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x8_F32BF16BF16F32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x8_F32BF16BF16F32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x16 TN
@@ -241,15 +242,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=f"(d0), "=f"(d1), "=f"(d2), "=f"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "f"(c0),  "f"(c1),  "f"(c2),  "f"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x16_F32BF16BF16F32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x16_F32BF16BF16F32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x4 TN
@@ -274,15 +275,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=f"(d0), "=f"(d1), "=f"(d2), "=f"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "f"(c0),  "f"(c1),  "f"(c2),  "f"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x4_F32TF32TF32F32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x4_F32TF32TF32F32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x8 TN
@@ -307,15 +308,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=f"(d0), "=f"(d1), "=f"(d2), "=f"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "f"(c0),  "f"(c1),  "f"(c2),  "f"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x8_F32TF32TF32F32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x8_F32TF32TF32F32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x4 TN
@@ -340,15 +341,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=d"(d0), "=d"(d1)
       :  "d"(a0),
          "d"(b0),
          "d"(c0),  "d"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x4_F64F64F64F64_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x4_F64F64F64F64_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 // MMA 8x8x4 TN with Planar Complex multiplication
 struct SM80_8x8x4_C64C64C64C64_TN
 {
@@ -381,22 +382,22 @@
       id0, id1,
       a0.imag(),
       b0.real(),
       c0.imag(), c1.imag());
 
     // d.real() = -a.imag() * b.imag() + d.real();
     SM80_8x8x4_F64F64F64F64_TN::fma(
-      rd0, rd1, 
+      rd0, rd1,
       -a0.imag(),
       b0.imag(),
       d0.real(), d1.real());
 
     // d.imag() =  a.real() * b.imag() + d.imag();
     SM80_8x8x4_F64F64F64F64_TN::fma(
-      id0, id1, 
+      id0, id1,
       a0.real(),
       b0.imag(),
       d0.imag(), d1.imag());
   }
 };
 
 // MMA 8x8x4 TN with Gaussian Complex multiplication:
@@ -408,23 +409,23 @@
 //  then
 //    re = t0 - t1
 //    im = t2 - t0 - t1
 struct SM80_8x8x4_GC64C64C64GC64_TN
 {
   struct GaussComplex {
     double t0, t1, t2;
-    
+
     CUTE_HOST_DEVICE //constexpr
     operator complex<double>() const { return complex<double>(t0 - t1, t2 - t0 - t1); }
-    
+
     CUTE_HOST_DEVICE friend //constexpr
     complex<double> operator*(GaussComplex const& a, complex<double> const& b) { return static_cast<complex<double>>(a) * b; }
     CUTE_HOST_DEVICE friend //constexpr
     complex<double> operator*(complex<double> const& a, GaussComplex const& b) { return b * a; }
-    
+
     CUTE_HOST_DEVICE friend //constexpr
     complex<double> operator+(GaussComplex const& a, complex<double> const& b) { return static_cast<complex<double>>(a) + b; }
     CUTE_HOST_DEVICE friend //constexpr
     complex<double> operator+(complex<double> const& a, GaussComplex const& b) { return b + a; }
   };
 
   using DRegisters = GaussComplex[2];
@@ -477,15 +478,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x16_S32S8S8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x16_S32S8S8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x16 TN
@@ -510,15 +511,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x16_S32S8S8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x16_S32S8S8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x16 TN
@@ -543,15 +544,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x16_S32S8S8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x16_S32S8S8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x16 TN
@@ -576,15 +577,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x16_S32S8S8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x16_S32S8S8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -609,15 +610,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32S8S8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32S8S8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -642,15 +643,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32S8S8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32S8S8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x16 TN
@@ -675,15 +676,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x16_S32S8U8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x16_S32S8U8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x16 TN
@@ -708,15 +709,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x16_S32S8U8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x16_S32S8U8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x16 TN
@@ -741,15 +742,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x16_S32S8U8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x16_S32S8U8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x16 TN
@@ -774,15 +775,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x16_S32S8U8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x16_S32S8U8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -807,15 +808,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32S8U8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32S8U8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -840,15 +841,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32S8U8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32S8U8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x16 TN
@@ -873,15 +874,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x16_S32U8S8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x16_S32U8S8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x16 TN
@@ -906,15 +907,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x16_S32U8S8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x16_S32U8S8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x16 TN
@@ -939,15 +940,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x16_S32U8S8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x16_S32U8S8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x16 TN
@@ -972,15 +973,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x16_S32U8S8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x16_S32U8S8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -1005,15 +1006,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32U8S8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32U8S8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -1038,15 +1039,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32U8S8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32U8S8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x16 TN
@@ -1071,15 +1072,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x16_S32U8U8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x16_S32U8U8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x16 TN
@@ -1104,15 +1105,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x16_S32U8U8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x16_S32U8U8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x16 TN
@@ -1137,15 +1138,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x16_S32U8U8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x16_S32U8U8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x16 TN
@@ -1170,15 +1171,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x16_S32U8U8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x16_S32U8U8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -1203,15 +1204,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32U8U8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32U8U8S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -1236,15 +1237,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32U8U8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32U8U8S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x32 TN
@@ -1269,15 +1270,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x32_S32S4S4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x32_S32S4S4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x32 TN
@@ -1302,15 +1303,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x32_S32S4S4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x32_S32S4S4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -1335,15 +1336,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32S4S4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32S4S4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -1368,15 +1369,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32S4S4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32S4S4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x64 TN
@@ -1401,15 +1402,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x64_S32S4S4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x64_S32S4S4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x64 TN
@@ -1434,15 +1435,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x64_S32S4S4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x64_S32S4S4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x32 TN
@@ -1467,15 +1468,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x32_S32S4U4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x32_S32S4U4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x32 TN
@@ -1500,15 +1501,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x32_S32S4U4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x32_S32S4U4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -1533,15 +1534,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32S4U4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32S4U4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -1566,15 +1567,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32S4U4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32S4U4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x64 TN
@@ -1599,15 +1600,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x64_S32S4U4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x64_S32S4U4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x64 TN
@@ -1632,15 +1633,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x64_S32S4U4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x64_S32S4U4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x32 TN
@@ -1665,15 +1666,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x32_S32U4S4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x32_S32U4S4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x32 TN
@@ -1698,15 +1699,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x32_S32U4S4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x32_S32U4S4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -1731,15 +1732,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32U4S4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32U4S4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -1764,15 +1765,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32U4S4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32U4S4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x64 TN
@@ -1797,15 +1798,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x64_S32U4S4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x64_S32U4S4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x64 TN
@@ -1830,15 +1831,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x64_S32U4S4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x64_S32U4S4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x32 TN
@@ -1863,15 +1864,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x32_S32U4U4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x32_S32U4U4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 8x8x32 TN
@@ -1896,15 +1897,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x32_S32U4U4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x32_S32U4U4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -1929,15 +1930,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32U4U4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32U4U4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x32 TN
@@ -1962,15 +1963,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x32_S32U4U4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x32_S32U4U4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x64 TN
@@ -1995,15 +1996,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x64_S32U4U4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x64_S32U4U4S32_TN without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x64 TN
@@ -2028,15 +2029,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x64_S32U4U4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x64_S32U4U4S32_TN_SATURATE without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
@@ -2063,15 +2064,15 @@
       "{%3},"
       "{%4, %5};\n"
       : "=r"(d0), "=r"(d1)
       :  "r"(a0),
          "r"(b0),
          "r"(c0),  "r"(c1));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_8x8x128_S32U1U1S32_TN_XORPOPC without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_8x8x128_S32U1U1S32_TN_XORPOPC without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x128 TN
@@ -2096,15 +2097,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),
          "r"(b0),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x128_S32U1U1S32_TN_XORPOPC without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x128_S32U1U1S32_TN_XORPOPC without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x256 TN
@@ -2129,15 +2130,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=r"(d0), "=r"(d1), "=r"(d2), "=r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "r"(b0),  "r"(b1),
          "r"(c0),  "r"(c1),  "r"(c2),  "r"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM80_16x8x256_S32U1U1S32_TN_XORPOPC without CUTE_ARCH_MMA_SM80_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM80_16x8x256_S32U1U1S32_TN_XORPOPC without CUTE_ARCH_MMA_SM80_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace cute
```

## cutlass_library/source/include/cute/arch/mma_sm90.hpp

```diff
@@ -69,15 +69,15 @@
       "{%6},"
       "{%7,  %8,  %9,  %10};\n"
       : "=d"(d0), "=d"(d1), "=d"(d2), "=d"(d3)
       :  "d"(a0),  "d"(a1),
          "d"(b0),
          "d"(c0),  "d"(c1),  "d"(c2),  "d"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_16x8x4_F64F64F64F64_TN without CUTE_ARCH_MMA_SM90_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_16x8x4_F64F64F64F64_TN without CUTE_ARCH_MMA_SM90_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x8 TN
@@ -102,15 +102,15 @@
       "{%8,  %9},"
       "{%10, %11, %12, %13};\n"
       : "=d"(d0), "=d"(d1), "=d"(d2), "=d"(d3)
       :  "d"(a0),  "d"(a1),  "d"(a2),  "d"(a3),
          "d"(b0),  "d"(b1),
          "d"(c0),  "d"(c1),  "d"(c2),  "d"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_16x8x8_F64F64F64F64_TN without CUTE_ARCH_MMA_SM90_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_16x8x8_F64F64F64F64_TN without CUTE_ARCH_MMA_SM90_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x16 TN
@@ -137,15 +137,15 @@
       "{%16, %17, %18, %19};\n"
       : "=d"(d0), "=d"(d1), "=d"(d2), "=d"(d3)
       :  "d"(a0),  "d"(a1),  "d"(a2),  "d"(a3),
          "d"(a4),  "d"(a5),  "d"(a6),  "d"(a7),
          "d"(b0),  "d"(b1),  "d"(b2),  "d"(b3),
          "d"(c0),  "d"(c1),  "d"(c2),  "d"(c3));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_16x8x16_F64F64F64F64_TN without CUTE_ARCH_MMA_SM90_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_16x8x16_F64F64F64F64_TN without CUTE_ARCH_MMA_SM90_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // MMA 16x8x4 TN
@@ -360,45 +360,193 @@
   static_assert(is_static<TileShape_MNK>::value, "TileShape_MNK must be static.");
   static_assert(rank(TileShape_MNK{}) == 3, "TileShape_MNK must be rank 3.");
   static_assert(size<0>(TileShape_MNK{}) % 64 == 0, "Tile_M must be a multiple of 64.");
   auto Tile_N = size<1>(TileShape_MNK{});
 
   // FP16 accumulator
   if constexpr (is_same_v<ElementC, half_t>) {
-    static_assert(is_same_v<ElementA, half_t>, "Element types for AB must be half if ElementC is half.");
-    static_assert(is_same_v<ElementB, half_t>, "Element types for AB must be half if ElementC is half.");
-    static_assert(size<2>(TileShape_MNK{}) % 16 == 0, "Tile_K must be a multiple of 16.");
-
-    // Dispatch against the Tile N mode size
-    if constexpr (Tile_N % 256 == 0) {
-      return SM90_64x256x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
-    }
-    else if constexpr (Tile_N % 192 == 0) {
-      return SM90_64x192x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
-    }
-    else if constexpr (Tile_N % 128 == 0) {
-      return SM90_64x128x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
-    }
-    else if constexpr (Tile_N % 96 == 0) {
-      return SM90_64x96x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
+    if constexpr (is_same_v<ElementA, half_t> && is_same_v<ElementB, half_t>) {
+      static_assert(size<2>(TileShape_MNK{}) % 16 == 0, "Tile_K must be a multiple of 16.");
+
+      // Dispatch against the Tile N mode size
+      if constexpr (Tile_N % 256 == 0) {
+        return SM90_64x256x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
+      }
+      else if constexpr (Tile_N % 192 == 0) {
+        return SM90_64x192x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
+      }
+      else if constexpr (Tile_N % 128 == 0) {
+        return SM90_64x128x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
+      }
+      else if constexpr (Tile_N % 96 == 0) {
+        return SM90_64x96x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
+      }
+      else if constexpr (Tile_N % 64 == 0) {
+        return SM90_64x64x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
+      }
+      else if constexpr (Tile_N % 32 == 0) {
+        return SM90_64x32x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
+      }
+      else if constexpr (Tile_N % 16 == 0) {
+        return SM90_64x16x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
+      }
+      else if constexpr (Tile_N % 8 == 0) {
+        return SM90_64x8x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
+      }
+      else {
+          static_assert(Tile_N % 8 == 0, "Tile_N must be a multiple of 8.");
+      }
     }
-    else if constexpr (Tile_N % 64 == 0) {
-      return SM90_64x64x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
+
+    // FP8
+    // Input A: float_e4m3_t ; Input B: float_e4m3_t
+    else if constexpr (is_same_v<ElementA, float_e4m3_t> && is_same_v<ElementB, float_e4m3_t>) {
+      static_assert(MajorA == GMMA::Major::K, "MajorA must be GMMA::Major::K for this config.");
+      static_assert(MajorB == GMMA::Major::K, "MajorB must be GMMA::Major::K for this config.");
+      static_assert(size<2>(TileShape_MNK{}) % 32 == 0, "Tile_K must be a multiple of 32.");
+
+      if constexpr (Tile_N % 256 == 0) {
+        return SM90_64x256x32_F16E4M3E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 192 == 0) {
+        return SM90_64x192x32_F16E4M3E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 128 == 0) {
+        return SM90_64x128x32_F16E4M3E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 96 == 0) {
+        return SM90_64x96x32_F16E4M3E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 64 == 0) {
+        return SM90_64x64x32_F16E4M3E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 32 == 0) {
+        return SM90_64x32x32_F16E4M3E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 16 == 0) {
+        return SM90_64x16x32_F16E4M3E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 8 == 0) {
+        return SM90_64x8x32_F16E4M3E4M3_SS_TN<Args...>{};
+      }
+      else {
+        static_assert(Tile_N % 8 == 0, "Tile_N must be a multiple of 8.");
+      }
     }
-    else if constexpr (Tile_N % 32 == 0) {
-      return SM90_64x32x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
+
+    // FP8
+    // Input A: float_e4m3_t ; Input B: float_e5m2_t
+    else if constexpr (is_same_v<ElementA, float_e4m3_t> && is_same_v<ElementB, float_e5m2_t>) {
+      static_assert(MajorA == GMMA::Major::K, "MajorA must be GMMA::Major::K for this config.");
+      static_assert(MajorB == GMMA::Major::K, "MajorB must be GMMA::Major::K for this config.");
+      static_assert(size<2>(TileShape_MNK{}) % 32 == 0, "Tile_K must be a multiple of 32.");
+
+      if constexpr (Tile_N % 256 == 0) {
+        return SM90_64x256x32_F16E4M3E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 192 == 0) {
+        return SM90_64x192x32_F16E4M3E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 128 == 0) {
+        return SM90_64x128x32_F16E4M3E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 96 == 0) {
+        return SM90_64x96x32_F16E4M3E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 64 == 0) {
+        return SM90_64x64x32_F16E4M3E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 32 == 0) {
+        return SM90_64x32x32_F16E4M3E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 16 == 0) {
+        return SM90_64x16x32_F16E4M3E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 8 == 0) {
+        return SM90_64x8x32_F16E4M3E5M2_SS_TN<Args...>{};
+      }
+      else {
+        static_assert(Tile_N % 8 == 0, "Tile_N must be a multiple of 8.");
+      }
     }
-    else if constexpr (Tile_N % 16 == 0) {
-      return SM90_64x16x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
+
+    // FP8
+    // Input A: float_e5m2_t ; Input B: float_e5m2_t
+    else if constexpr (is_same_v<ElementA, float_e5m2_t> && is_same_v<ElementB, float_e5m2_t>) {
+      static_assert(MajorA == GMMA::Major::K, "MajorA must be GMMA::Major::K for this config.");
+      static_assert(MajorB == GMMA::Major::K, "MajorB must be GMMA::Major::K for this config.");
+      static_assert(size<2>(TileShape_MNK{}) % 32 == 0, "Tile_K must be a multiple of 32.");
+
+      if constexpr (Tile_N % 256 == 0) {
+        return SM90_64x256x32_F16E5M2E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 192 == 0) {
+        return SM90_64x192x32_F16E5M2E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 128 == 0) {
+        return SM90_64x128x32_F16E5M2E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 96 == 0) {
+        return SM90_64x96x32_F16E5M2E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 64 == 0) {
+        return SM90_64x64x32_F16E5M2E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 32 == 0) {
+        return SM90_64x32x32_F16E5M2E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 16 == 0) {
+        return SM90_64x16x32_F16E5M2E5M2_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 8 == 0) {
+        return SM90_64x8x32_F16E5M2E5M2_SS_TN<Args...>{};
+      }
+      else {
+        static_assert(Tile_N % 8 == 0, "Tile_N must be a multiple of 8.");
+      }
     }
-    else if constexpr (Tile_N % 8 == 0) {
-      return SM90_64x8x16_F16F16F16_SS<MajorA, MajorB, Args...>{};
+
+    // FP8
+    // Input A: float_e5m2_t ; Input B: float_e4m3_t
+    else if constexpr (is_same_v<ElementA, float_e5m2_t> && is_same_v<ElementB, float_e4m3_t>) {
+      static_assert(MajorA == GMMA::Major::K, "MajorA must be GMMA::Major::K for this config.");
+      static_assert(MajorB == GMMA::Major::K, "MajorB must be GMMA::Major::K for this config.");
+      static_assert(size<2>(TileShape_MNK{}) % 32 == 0, "Tile_K must be a multiple of 32.");
+
+      if constexpr (Tile_N % 256 == 0) {
+        return SM90_64x256x32_F16E5M2E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 192 == 0) {
+        return SM90_64x192x32_F16E5M2E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 128 == 0) {
+        return SM90_64x128x32_F16E5M2E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 96 == 0) {
+        return SM90_64x96x32_F16E5M2E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 64 == 0) {
+        return SM90_64x64x32_F16E5M2E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 32 == 0) {
+        return SM90_64x32x32_F16E5M2E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 16 == 0) {
+        return SM90_64x16x32_F16E5M2E4M3_SS_TN<Args...>{};
+      }
+      else if constexpr (Tile_N % 8 == 0) {
+        return SM90_64x8x32_F16E5M2E4M3_SS_TN<Args...>{};
+      }
+      else {
+        static_assert(Tile_N % 8 == 0, "Tile_N must be a multiple of 8.");
+      }
     }
+
     else {
-        static_assert(Tile_N % 8 == 0, "Tile_N must be a multiple of 8.");
+      static_assert(sizeof(ElementA) == 0, "No eligible GMMA operator for request configuration.");
     }
   }
 
   // FP32 accumulator
   else if constexpr (is_same_v<ElementC, float>) {
 
     // FP16 inputs
```

## cutlass_library/source/include/cute/arch/mma_sm90_gmma.hpp

```diff
@@ -45,40 +45,40 @@
 CUTE_HOST_DEVICE
 void
 warpgroup_arrive()
 {
 #if defined(CUTE_ARCH_MMA_SM90A_ENABLED)
   asm volatile ("wgmma.fence.sync.aligned;\n" ::: "memory");
 #else
-  CUTE_RUNTIME_ASSERT("Attempting to use wgmma.fence without CUTE_ARCH_MMA_SM90A_ENABLED");
+  CUTE_INVALID_CONTROL_PATH("Attempting to use wgmma.fence without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
 }
 
 template <int N>
 CUTE_HOST_DEVICE
 void
 warpgroup_wait()
 {
   static_assert(N >= 0 && N <= 7, "WGMMA wait: N must be in range [0, 7]");
 #if defined(CUTE_ARCH_MMA_SM90A_ENABLED)
   asm volatile("wgmma.wait_group.sync.aligned %0;\n" :: "n"(N) : "memory");
 #else
-  CUTE_RUNTIME_ASSERT("Attempting to use wgmma.wait_group<N> without CUTE_ARCH_MMA_SM90A_ENABLED");
+  CUTE_INVALID_CONTROL_PATH("Attempting to use wgmma.wait_group<N> without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
 }
 
 // Marks the commit point for one or more sized batch of warpgroup MMAs.
 CUTE_HOST_DEVICE
 void
 warpgroup_commit_batch()
 {
 #if defined(CUTE_ARCH_MMA_SM90A_ENABLED)
   asm volatile("wgmma.commit_group.sync.aligned;\n" ::: "memory");
 #else
-  CUTE_RUNTIME_ASSERT("Attempting to use wgmma.commit_group without CUTE_ARCH_MMA_SM90A_ENABLED");
+  CUTE_INVALID_CONTROL_PATH("Attempting to use wgmma.commit_group without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
 }
 
 CUTE_HOST_DEVICE
 void
 warpgroup_fence_operand(uint32_t& reg) {
   // MSVC emits a build error for 'asm volatile'
@@ -152,15 +152,15 @@
       " p,  %5, %6, %7, %8;\n"
     "}\n"
       : "+r"(d0), "+r"(d1)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x16 F16+=F16*F16
@@ -198,15 +198,15 @@
       " p,   %8,  %9,  %10;\n"
     "}\n"
       : "+r"(d0), "+r"(d1)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x16 F16+=F16*F16
@@ -241,15 +241,15 @@
       " p,   %7,  %8,  %9,  %10;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x16 F16+=F16*F16
@@ -287,15 +287,15 @@
       " p,   %10, %11, %12;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x16 F16+=F16*F16
@@ -332,15 +332,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x16 F16+=F16*F16
@@ -380,15 +380,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x16 F16+=F16*F16
@@ -430,15 +430,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x16 F16+=F16*F16
@@ -483,15 +483,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x16 F16+=F16*F16
@@ -538,15 +538,15 @@
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15),
         "+r"(d16), "+r"(d17), "+r"(d18), "+r"(d19),
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x16 F16+=F16*F16
@@ -596,15 +596,15 @@
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15),
         "+r"(d16), "+r"(d17), "+r"(d18), "+r"(d19),
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x16 F16+=F16*F16
@@ -656,15 +656,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x16 F16+=F16*F16
@@ -719,15 +719,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x16 F16+=F16*F16
@@ -789,15 +789,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x16 F16+=F16*F16
@@ -862,15 +862,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x16 F16+=F16*F16
@@ -942,15 +942,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x16 F16+=F16*F16
@@ -1025,15 +1025,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x16 F32+=F16*F16
@@ -1068,15 +1068,15 @@
       " p,   %7,  %8,  %9,  %10;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x16 F32+=F16*F16
@@ -1114,15 +1114,15 @@
       " p,   %10, %11, %12;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x16 F32+=F16*F16
@@ -1159,15 +1159,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x16 F32+=F16*F16
@@ -1207,15 +1207,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x16 F32+=F16*F16
@@ -1257,15 +1257,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x16 F32+=F16*F16
@@ -1310,15 +1310,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x16 F32+=F16*F16
@@ -1370,15 +1370,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x16 F32+=F16*F16
@@ -1433,15 +1433,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x16 F32+=F16*F16
@@ -1503,15 +1503,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x16 F32+=F16*F16
@@ -1576,15 +1576,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x16 F32+=F16*F16
@@ -1656,15 +1656,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x16 F32+=F16*F16
@@ -1739,15 +1739,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x16 F32+=F16*F16
@@ -1839,15 +1839,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x16 F32+=F16*F16
@@ -1942,15 +1942,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x16 F32+=F16*F16
@@ -2062,15 +2062,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x16_F32F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x16 F32+=F16*F16
@@ -2185,15 +2185,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x16_F32F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x16 F32+=BF16*BF16
@@ -2228,15 +2228,15 @@
       " p,   %7,  %8,  %9,  %10;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x16 F32+=BF16*BF16
@@ -2274,15 +2274,15 @@
       " p,   %10, %11, %12;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x16 F32+=BF16*BF16
@@ -2319,15 +2319,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x16 F32+=BF16*BF16
@@ -2367,15 +2367,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x16 F32+=BF16*BF16
@@ -2417,15 +2417,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x16 F32+=BF16*BF16
@@ -2470,15 +2470,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x16 F32+=BF16*BF16
@@ -2530,15 +2530,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x16 F32+=BF16*BF16
@@ -2593,15 +2593,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x16 F32+=BF16*BF16
@@ -2663,15 +2663,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x16 F32+=BF16*BF16
@@ -2736,15 +2736,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x16 F32+=BF16*BF16
@@ -2816,15 +2816,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x16 F32+=BF16*BF16
@@ -2899,15 +2899,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x16 F32+=BF16*BF16
@@ -2999,15 +2999,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x16 F32+=BF16*BF16
@@ -3102,15 +3102,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x16 F32+=BF16*BF16
@@ -3222,15 +3222,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x16_F32BF16BF16_SS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x16 F32+=BF16*BF16
@@ -3345,15 +3345,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x16_F32BF16BF16_RS without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x8 TN F32+=TF32*TF32
@@ -3386,15 +3386,15 @@
       " p,   %7,  %8;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x8 TN F32+=TF32*TF32
@@ -3427,15 +3427,15 @@
       " p,   %10, %11;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x8 TN F32+=TF32*TF32
@@ -3470,15 +3470,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x8 TN F32+=TF32*TF32
@@ -3513,15 +3513,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x8 TN F32+=TF32*TF32
@@ -3561,15 +3561,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x8 TN F32+=TF32*TF32
@@ -3609,15 +3609,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x8 TN F32+=TF32*TF32
@@ -3667,15 +3667,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x8 TN F32+=TF32*TF32
@@ -3725,15 +3725,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x8 TN F32+=TF32*TF32
@@ -3793,15 +3793,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x8 TN F32+=TF32*TF32
@@ -3861,15 +3861,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x8 TN F32+=TF32*TF32
@@ -3939,15 +3939,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x8 TN F32+=TF32*TF32
@@ -4017,15 +4017,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x8 TN F32+=TF32*TF32
@@ -4115,15 +4115,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x8 TN F32+=TF32*TF32
@@ -4213,15 +4213,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x8 TN F32+=TF32*TF32
@@ -4331,15 +4331,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x8_F32TF32TF32_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x8 TN F32+=TF32*TF32
@@ -4449,15 +4449,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x8_F32TF32TF32_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=S8*S8
@@ -4486,15 +4486,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=S8*S8
@@ -4523,15 +4523,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=S8*S8
@@ -4562,15 +4562,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=S8*S8
@@ -4601,15 +4601,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=S8*S8
@@ -4645,15 +4645,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=S8*S8
@@ -4689,15 +4689,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=S8*S8
@@ -4743,15 +4743,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=S8*S8
@@ -4797,15 +4797,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=S8*S8
@@ -4861,15 +4861,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=S8*S8
@@ -4925,15 +4925,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=S8*S8
@@ -4999,15 +4999,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=S8*S8
@@ -5073,15 +5073,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=S8*S8
@@ -5167,15 +5167,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=S8*S8
@@ -5261,15 +5261,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=S8*S8
@@ -5375,15 +5375,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32S8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=S8*S8
@@ -5489,15 +5489,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32S8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=S8*S8
@@ -5526,15 +5526,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=S8*S8
@@ -5563,15 +5563,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=S8*S8
@@ -5602,15 +5602,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=S8*S8
@@ -5641,15 +5641,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=S8*S8
@@ -5685,15 +5685,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=S8*S8
@@ -5729,15 +5729,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=S8*S8
@@ -5783,15 +5783,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=S8*S8
@@ -5837,15 +5837,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=S8*S8
@@ -5901,15 +5901,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=S8*S8
@@ -5965,15 +5965,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=S8*S8
@@ -6039,15 +6039,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=S8*S8
@@ -6113,15 +6113,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=S8*S8
@@ -6207,15 +6207,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=S8*S8
@@ -6301,15 +6301,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=S8*S8
@@ -6415,15 +6415,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32S8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=S8*S8
@@ -6529,15 +6529,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32S8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=S8*U8
@@ -6566,15 +6566,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=S8*U8
@@ -6603,15 +6603,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=S8*U8
@@ -6642,15 +6642,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=S8*U8
@@ -6681,15 +6681,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=S8*U8
@@ -6725,15 +6725,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=S8*U8
@@ -6769,15 +6769,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=S8*U8
@@ -6823,15 +6823,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=S8*U8
@@ -6877,15 +6877,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=S8*U8
@@ -6941,15 +6941,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=S8*U8
@@ -7005,15 +7005,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=S8*U8
@@ -7079,15 +7079,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=S8*U8
@@ -7153,15 +7153,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=S8*U8
@@ -7247,15 +7247,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=S8*U8
@@ -7341,15 +7341,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=S8*U8
@@ -7455,15 +7455,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32S8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=S8*U8
@@ -7569,15 +7569,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32S8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=S8*U8
@@ -7606,15 +7606,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=S8*U8
@@ -7643,15 +7643,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=S8*U8
@@ -7682,15 +7682,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=S8*U8
@@ -7721,15 +7721,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=S8*U8
@@ -7765,15 +7765,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=S8*U8
@@ -7809,15 +7809,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=S8*U8
@@ -7863,15 +7863,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=S8*U8
@@ -7917,15 +7917,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=S8*U8
@@ -7981,15 +7981,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=S8*U8
@@ -8045,15 +8045,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=S8*U8
@@ -8119,15 +8119,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=S8*U8
@@ -8193,15 +8193,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=S8*U8
@@ -8287,15 +8287,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=S8*U8
@@ -8381,15 +8381,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=S8*U8
@@ -8495,15 +8495,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32S8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=S8*U8
@@ -8609,15 +8609,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32S8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=U8*S8
@@ -8646,15 +8646,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=U8*S8
@@ -8683,15 +8683,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=U8*S8
@@ -8722,15 +8722,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=U8*S8
@@ -8761,15 +8761,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=U8*S8
@@ -8805,15 +8805,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=U8*S8
@@ -8849,15 +8849,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=U8*S8
@@ -8903,15 +8903,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=U8*S8
@@ -8957,15 +8957,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=U8*S8
@@ -9021,15 +9021,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=U8*S8
@@ -9085,15 +9085,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=U8*S8
@@ -9159,15 +9159,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=U8*S8
@@ -9233,15 +9233,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=U8*S8
@@ -9327,15 +9327,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=U8*S8
@@ -9421,15 +9421,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=U8*S8
@@ -9535,15 +9535,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32U8S8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=U8*S8
@@ -9649,15 +9649,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32U8S8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=U8*S8
@@ -9686,15 +9686,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=U8*S8
@@ -9723,15 +9723,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=U8*S8
@@ -9762,15 +9762,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=U8*S8
@@ -9801,15 +9801,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=U8*S8
@@ -9845,15 +9845,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=U8*S8
@@ -9889,15 +9889,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=U8*S8
@@ -9943,15 +9943,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=U8*S8
@@ -9997,15 +9997,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=U8*S8
@@ -10061,15 +10061,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=U8*S8
@@ -10125,15 +10125,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=U8*S8
@@ -10199,15 +10199,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=U8*S8
@@ -10273,15 +10273,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=U8*S8
@@ -10367,15 +10367,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=U8*S8
@@ -10461,15 +10461,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=U8*S8
@@ -10575,15 +10575,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32U8S8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=U8*S8
@@ -10689,15 +10689,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32U8S8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=U8*U8
@@ -10726,15 +10726,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=U8*U8
@@ -10763,15 +10763,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=U8*U8
@@ -10802,15 +10802,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=U8*U8
@@ -10841,15 +10841,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=U8*U8
@@ -10885,15 +10885,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=U8*U8
@@ -10929,15 +10929,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=U8*U8
@@ -10983,15 +10983,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=U8*U8
@@ -11037,15 +11037,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=U8*U8
@@ -11101,15 +11101,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=U8*U8
@@ -11165,15 +11165,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=U8*U8
@@ -11239,15 +11239,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=U8*U8
@@ -11313,15 +11313,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=U8*U8
@@ -11407,15 +11407,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=U8*U8
@@ -11501,15 +11501,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=U8*U8
@@ -11615,15 +11615,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32U8U8_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=U8*U8
@@ -11729,15 +11729,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32U8U8_SS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=U8*U8
@@ -11766,15 +11766,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN S32+=U8*U8
@@ -11803,15 +11803,15 @@
       " p;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=U8*U8
@@ -11842,15 +11842,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN S32+=U8*U8
@@ -11881,15 +11881,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=U8*U8
@@ -11925,15 +11925,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN S32+=U8*U8
@@ -11969,15 +11969,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=U8*U8
@@ -12023,15 +12023,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN S32+=U8*U8
@@ -12077,15 +12077,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=U8*U8
@@ -12141,15 +12141,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN S32+=U8*U8
@@ -12205,15 +12205,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=U8*U8
@@ -12279,15 +12279,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN S32+=U8*U8
@@ -12353,15 +12353,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=U8*U8
@@ -12447,15 +12447,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN S32+=U8*U8
@@ -12541,15 +12541,15 @@
         "+r"(d84), "+r"(d85), "+r"(d86), "+r"(d87),
         "+r"(d88), "+r"(d89), "+r"(d90), "+r"(d91),
         "+r"(d92), "+r"(d93), "+r"(d94), "+r"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=U8*U8
@@ -12655,15 +12655,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32U8U8_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN S32+=U8*U8
@@ -12769,15 +12769,15 @@
         "+r"(d116), "+r"(d117), "+r"(d118), "+r"(d119),
         "+r"(d120), "+r"(d121), "+r"(d122), "+r"(d123),
         "+r"(d124), "+r"(d125), "+r"(d126), "+r"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_S32U8U8_RS_TN_SATURATE without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F16+=E4M3*E4M3
@@ -12810,15 +12810,15 @@
       " p,  %5, %6;\n"
     "}\n"
       : "+r"(d0), "+r"(d1)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F16+=E4M3*E4M3
@@ -12851,15 +12851,15 @@
       " p,   %8,  %9;\n"
     "}\n"
       : "+r"(d0), "+r"(d1)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F32+=E4M3*E4M3
@@ -12892,15 +12892,15 @@
       " p,   %7,  %8;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F32+=E4M3*E4M3
@@ -12933,15 +12933,15 @@
       " p,   %10, %11;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F16+=E4M3*E4M3
@@ -12974,15 +12974,15 @@
       " p,   %7,  %8;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F16+=E4M3*E4M3
@@ -13015,15 +13015,15 @@
       " p,   %10, %11;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F32+=E4M3*E4M3
@@ -13058,15 +13058,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F32+=E4M3*E4M3
@@ -13101,15 +13101,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F16+=E4M3*E4M3
@@ -13144,15 +13144,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F16+=E4M3*E4M3
@@ -13187,15 +13187,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F32+=E4M3*E4M3
@@ -13235,15 +13235,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F32+=E4M3*E4M3
@@ -13283,15 +13283,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F16+=E4M3*E4M3
@@ -13331,15 +13331,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F16+=E4M3*E4M3
@@ -13379,15 +13379,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F32+=E4M3*E4M3
@@ -13437,15 +13437,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F32+=E4M3*E4M3
@@ -13495,15 +13495,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F16+=E4M3*E4M3
@@ -13548,15 +13548,15 @@
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15),
         "+r"(d16), "+r"(d17), "+r"(d18), "+r"(d19),
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F16+=E4M3*E4M3
@@ -13601,15 +13601,15 @@
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15),
         "+r"(d16), "+r"(d17), "+r"(d18), "+r"(d19),
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F32+=E4M3*E4M3
@@ -13669,15 +13669,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F32+=E4M3*E4M3
@@ -13737,15 +13737,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F16+=E4M3*E4M3
@@ -13795,15 +13795,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F16+=E4M3*E4M3
@@ -13853,15 +13853,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F32+=E4M3*E4M3
@@ -13931,15 +13931,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F32+=E4M3*E4M3
@@ -14009,15 +14009,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F16+=E4M3*E4M3
@@ -14077,15 +14077,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F16+=E4M3*E4M3
@@ -14145,15 +14145,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F32+=E4M3*E4M3
@@ -14243,15 +14243,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F32+=E4M3*E4M3
@@ -14341,15 +14341,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F16+=E4M3*E4M3
@@ -14419,15 +14419,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F16E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F16+=E4M3*E4M3
@@ -14497,15 +14497,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F16E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F32+=E4M3*E4M3
@@ -14615,15 +14615,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F32E4M3E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F32+=E4M3*E4M3
@@ -14733,15 +14733,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F32E4M3E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F16+=E4M3*E5M2
@@ -14774,15 +14774,15 @@
       " p,  %5, %6;\n"
     "}\n"
       : "+r"(d0), "+r"(d1)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F16+=E4M3*E5M2
@@ -14815,15 +14815,15 @@
       " p,   %8,  %9;\n"
     "}\n"
       : "+r"(d0), "+r"(d1)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F32+=E4M3*E5M2
@@ -14856,15 +14856,15 @@
       " p,   %7,  %8;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F32+=E4M3*E5M2
@@ -14897,15 +14897,15 @@
       " p,   %10, %11;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F16+=E4M3*E5M2
@@ -14938,15 +14938,15 @@
       " p,   %7,  %8;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F16+=E4M3*E5M2
@@ -14979,15 +14979,15 @@
       " p,   %10, %11;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F32+=E4M3*E5M2
@@ -15022,15 +15022,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F32+=E4M3*E5M2
@@ -15065,15 +15065,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F16+=E4M3*E5M2
@@ -15108,15 +15108,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F16+=E4M3*E5M2
@@ -15151,15 +15151,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F32+=E4M3*E5M2
@@ -15199,15 +15199,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F32+=E4M3*E5M2
@@ -15247,15 +15247,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F16+=E4M3*E5M2
@@ -15295,15 +15295,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F16+=E4M3*E5M2
@@ -15343,15 +15343,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F32+=E4M3*E5M2
@@ -15401,15 +15401,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F32+=E4M3*E5M2
@@ -15459,15 +15459,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F16+=E4M3*E5M2
@@ -15512,15 +15512,15 @@
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15),
         "+r"(d16), "+r"(d17), "+r"(d18), "+r"(d19),
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F16+=E4M3*E5M2
@@ -15565,15 +15565,15 @@
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15),
         "+r"(d16), "+r"(d17), "+r"(d18), "+r"(d19),
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F32+=E4M3*E5M2
@@ -15633,15 +15633,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F32+=E4M3*E5M2
@@ -15701,15 +15701,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F16+=E4M3*E5M2
@@ -15759,15 +15759,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F16+=E4M3*E5M2
@@ -15817,15 +15817,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F32+=E4M3*E5M2
@@ -15895,15 +15895,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F32+=E4M3*E5M2
@@ -15973,15 +15973,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F16+=E4M3*E5M2
@@ -16041,15 +16041,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F16+=E4M3*E5M2
@@ -16109,15 +16109,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F32+=E4M3*E5M2
@@ -16207,15 +16207,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F32+=E4M3*E5M2
@@ -16305,15 +16305,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F16+=E4M3*E5M2
@@ -16383,15 +16383,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F16E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F16+=E4M3*E5M2
@@ -16461,15 +16461,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F16E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F32+=E4M3*E5M2
@@ -16579,15 +16579,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F32E4M3E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F32+=E4M3*E5M2
@@ -16697,15 +16697,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F32E4M3E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F16+=E5M2*E4M3
@@ -16738,15 +16738,15 @@
       " p,  %5, %6;\n"
     "}\n"
       : "+r"(d0), "+r"(d1)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F16+=E5M2*E4M3
@@ -16779,15 +16779,15 @@
       " p,   %8,  %9;\n"
     "}\n"
       : "+r"(d0), "+r"(d1)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F32+=E5M2*E4M3
@@ -16820,15 +16820,15 @@
       " p,   %7,  %8;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F32+=E5M2*E4M3
@@ -16861,15 +16861,15 @@
       " p,   %10, %11;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F16+=E5M2*E4M3
@@ -16902,15 +16902,15 @@
       " p,   %7,  %8;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F16+=E5M2*E4M3
@@ -16943,15 +16943,15 @@
       " p,   %10, %11;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F32+=E5M2*E4M3
@@ -16986,15 +16986,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F32+=E5M2*E4M3
@@ -17029,15 +17029,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F16+=E5M2*E4M3
@@ -17072,15 +17072,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F16+=E5M2*E4M3
@@ -17115,15 +17115,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F32+=E5M2*E4M3
@@ -17163,15 +17163,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F32+=E5M2*E4M3
@@ -17211,15 +17211,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F16+=E5M2*E4M3
@@ -17259,15 +17259,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F16+=E5M2*E4M3
@@ -17307,15 +17307,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F32+=E5M2*E4M3
@@ -17365,15 +17365,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F32+=E5M2*E4M3
@@ -17423,15 +17423,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F16+=E5M2*E4M3
@@ -17476,15 +17476,15 @@
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15),
         "+r"(d16), "+r"(d17), "+r"(d18), "+r"(d19),
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F16+=E5M2*E4M3
@@ -17529,15 +17529,15 @@
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15),
         "+r"(d16), "+r"(d17), "+r"(d18), "+r"(d19),
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F32+=E5M2*E4M3
@@ -17597,15 +17597,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F32+=E5M2*E4M3
@@ -17665,15 +17665,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F16+=E5M2*E4M3
@@ -17723,15 +17723,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F16+=E5M2*E4M3
@@ -17781,15 +17781,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F32+=E5M2*E4M3
@@ -17859,15 +17859,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F32+=E5M2*E4M3
@@ -17937,15 +17937,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F16+=E5M2*E4M3
@@ -18005,15 +18005,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F16+=E5M2*E4M3
@@ -18073,15 +18073,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F32+=E5M2*E4M3
@@ -18171,15 +18171,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F32+=E5M2*E4M3
@@ -18269,15 +18269,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F16+=E5M2*E4M3
@@ -18347,15 +18347,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F16E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F16+=E5M2*E4M3
@@ -18425,15 +18425,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F16E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F32+=E5M2*E4M3
@@ -18543,15 +18543,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F32E5M2E4M3_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F32+=E5M2*E4M3
@@ -18661,15 +18661,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F32E5M2E4M3_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F16+=E5M2*E5M2
@@ -18702,15 +18702,15 @@
       " p,  %5, %6;\n"
     "}\n"
       : "+r"(d0), "+r"(d1)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F16+=E5M2*E5M2
@@ -18743,15 +18743,15 @@
       " p,   %8,  %9;\n"
     "}\n"
       : "+r"(d0), "+r"(d1)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F32+=E5M2*E5M2
@@ -18784,15 +18784,15 @@
       " p,   %7,  %8;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x8x32 TN F32+=E5M2*E5M2
@@ -18825,15 +18825,15 @@
       " p,   %10, %11;\n"
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x8x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x8x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F16+=E5M2*E5M2
@@ -18866,15 +18866,15 @@
       " p,   %7,  %8;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F16+=E5M2*E5M2
@@ -18907,15 +18907,15 @@
       " p,   %10, %11;\n"
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F32+=E5M2*E5M2
@@ -18950,15 +18950,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x16x32 TN F32+=E5M2*E5M2
@@ -18993,15 +18993,15 @@
     "}\n"
       : "+f"(d0), "+f"(d1), "+f"(d2), "+f"(d3),
         "+f"(d4), "+f"(d5), "+f"(d6), "+f"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x16x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x16x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F16+=E5M2*E5M2
@@ -19036,15 +19036,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F16+=E5M2*E5M2
@@ -19079,15 +19079,15 @@
     "}\n"
       : "+r"(d0), "+r"(d1), "+r"(d2), "+r"(d3),
         "+r"(d4), "+r"(d5), "+r"(d6), "+r"(d7)
       :  "r"(a0),  "r"(a1),  "r"(a2),  "r"(a3),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F32+=E5M2*E5M2
@@ -19127,15 +19127,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x32x32 TN F32+=E5M2*E5M2
@@ -19175,15 +19175,15 @@
         "+f"(d04), "+f"(d05), "+f"(d06), "+f"(d07),
         "+f"(d08), "+f"(d09), "+f"(d10), "+f"(d11),
         "+f"(d12), "+f"(d13), "+f"(d14), "+f"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x32x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x32x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F16+=E5M2*E5M2
@@ -19223,15 +19223,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F16+=E5M2*E5M2
@@ -19271,15 +19271,15 @@
         "+r"(d04), "+r"(d05), "+r"(d06), "+r"(d07),
         "+r"(d08), "+r"(d09), "+r"(d10), "+r"(d11),
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F32+=E5M2*E5M2
@@ -19329,15 +19329,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x64x32 TN F32+=E5M2*E5M2
@@ -19387,15 +19387,15 @@
         "+f"(d20), "+f"(d21), "+f"(d22), "+f"(d23),
         "+f"(d24), "+f"(d25), "+f"(d26), "+f"(d27),
         "+f"(d28), "+f"(d29), "+f"(d30), "+f"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x64x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x64x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F16+=E5M2*E5M2
@@ -19440,15 +19440,15 @@
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15),
         "+r"(d16), "+r"(d17), "+r"(d18), "+r"(d19),
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F16+=E5M2*E5M2
@@ -19493,15 +19493,15 @@
         "+r"(d12), "+r"(d13), "+r"(d14), "+r"(d15),
         "+r"(d16), "+r"(d17), "+r"(d18), "+r"(d19),
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F32+=E5M2*E5M2
@@ -19561,15 +19561,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x96x32 TN F32+=E5M2*E5M2
@@ -19629,15 +19629,15 @@
         "+f"(d36), "+f"(d37), "+f"(d38), "+f"(d39),
         "+f"(d40), "+f"(d41), "+f"(d42), "+f"(d43),
         "+f"(d44), "+f"(d45), "+f"(d46), "+f"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x96x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x96x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F16+=E5M2*E5M2
@@ -19687,15 +19687,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F16+=E5M2*E5M2
@@ -19745,15 +19745,15 @@
         "+r"(d20), "+r"(d21), "+r"(d22), "+r"(d23),
         "+r"(d24), "+r"(d25), "+r"(d26), "+r"(d27),
         "+r"(d28), "+r"(d29), "+r"(d30), "+r"(d31)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F32+=E5M2*E5M2
@@ -19823,15 +19823,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x128x32 TN F32+=E5M2*E5M2
@@ -19901,15 +19901,15 @@
         "+f"(d52), "+f"(d53), "+f"(d54), "+f"(d55),
         "+f"(d56), "+f"(d57), "+f"(d58), "+f"(d59),
         "+f"(d60), "+f"(d61), "+f"(d62), "+f"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x128x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x128x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F16+=E5M2*E5M2
@@ -19969,15 +19969,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F16+=E5M2*E5M2
@@ -20037,15 +20037,15 @@
         "+r"(d36), "+r"(d37), "+r"(d38), "+r"(d39),
         "+r"(d40), "+r"(d41), "+r"(d42), "+r"(d43),
         "+r"(d44), "+r"(d45), "+r"(d46), "+r"(d47)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F32+=E5M2*E5M2
@@ -20135,15 +20135,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x192x32 TN F32+=E5M2*E5M2
@@ -20233,15 +20233,15 @@
         "+f"(d84), "+f"(d85), "+f"(d86), "+f"(d87),
         "+f"(d88), "+f"(d89), "+f"(d90), "+f"(d91),
         "+f"(d92), "+f"(d93), "+f"(d94), "+f"(d95)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x192x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x192x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F16+=E5M2*E5M2
@@ -20311,15 +20311,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F16E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F16+=E5M2*E5M2
@@ -20389,15 +20389,15 @@
         "+r"(d52), "+r"(d53), "+r"(d54), "+r"(d55),
         "+r"(d56), "+r"(d57), "+r"(d58), "+r"(d59),
         "+r"(d60), "+r"(d61), "+r"(d62), "+r"(d63)
       :  "r"(a00),  "r"(a01),  "r"(a02),  "r"(a03),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F16E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F32+=E5M2*E5M2
@@ -20507,15 +20507,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "l"(desc_a),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F32E5M2E5M2_SS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA 64x256x32 TN F32+=E5M2*E5M2
@@ -20625,15 +20625,15 @@
         "+f"(d116), "+f"(d117), "+f"(d118), "+f"(d119),
         "+f"(d120), "+f"(d121), "+f"(d122), "+f"(d123),
         "+f"(d124), "+f"(d125), "+f"(d126), "+f"(d127)
       :  "r"(a000),  "r"(a001),  "r"(a002),  "r"(a003),
          "l"(desc_b),
          "r"(int32_t(scale_D)), "n"(int32_t(scaleA)), "n"(int32_t(scaleB)));
 #else
-    CUTE_RUNTIME_ASSERT("Attempting to use SM90_64x256x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
+    CUTE_INVALID_CONTROL_PATH("Attempting to use SM90_64x256x32_F32E5M2E5M2_RS_TN without CUTE_ARCH_MMA_SM90A_ENABLED");
 #endif
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace cute
```

## cutlass_library/source/include/cute/arch/util.hpp

```diff
@@ -113,34 +113,70 @@
   return __nvvm_get_smem_pointer(ptr);
 
 #elif defined(__CUDA_ARCH__)
 
   uint32_t smem_ptr;
 
   asm(
-  "{ .reg .u64 smem_ptr; cvta.to.shared.u64 smem_ptr, %1; cvt.u32.u64 %0, smem_ptr; }\n" 
+  "{ .reg .u64 smem_ptr; cvta.to.shared.u64 smem_ptr, %1; cvt.u32.u64 %0, smem_ptr; }\n"
     : "=r"(smem_ptr) : "l"(ptr));
 
   return smem_ptr;
 
 #else
 
 
   (void) ptr;
   printf("ERROR: cast_smem_ptr_to_uint not supported but used.\n");
   return 0;
 
 #endif
 }
 
+namespace detail {
+
 //
-// Utility for pointer interfaces
+// Wrapper for MMAOp::fma
 //
 
-namespace detail {
+template <class MmaOp>
+struct CallFMA {
+  template <class... Args>
+  CUTE_HOST_DEVICE constexpr void
+  operator()(Args&&... args) const {
+    return MmaOp::fma(static_cast<Args&&>(args)...);
+  }
+};
+
+//
+// Wrapper for CopyOp::copy
+//
+
+template <class CopyOp>
+struct CallCOPY {
+  template <class... Args>
+  CUTE_HOST_DEVICE constexpr void
+  operator()(Args&&... args) const {
+    return CopyOp::copy(static_cast<Args&&>(args)...);
+  }
+};
+
+//
+// Utility for exploding pointers/arrays/tensors into functions
+//
+
+template <class Fn,
+          class PtrA, int... I>
+CUTE_HOST_DEVICE constexpr
+void
+explode(Fn fn,
+        PtrA&& a, int_sequence<I...>)
+{
+  return fn(a[I]...);
+}
 
 template <class Fn,
           class PtrS, int... Is,
           class PtrD, int... Id>
 CUTE_HOST_DEVICE constexpr
 void
 explode(Fn fn,
@@ -177,79 +213,86 @@
         PtrB&& b, int_sequence<Ib...>,
         PtrC&& c, int_sequence<Ic...>)
 {
   return fn(d[Id]..., a[Ia]..., b[Ib]..., c[Ic]...);
 }
 
 template <class Fn,
+          class PtrD, int... Id,
           class PtrA, int... Ia,
           class PtrB, int... Ib,
           class PtrC, int... Ic,
-          class ParamType>
+          class PtrE, int... Ie>
 CUTE_HOST_DEVICE constexpr
 void
-explode_with_d_scaling(Fn fn,
+explode(Fn fn,
+        PtrD&& d, int_sequence<Id...>,
         PtrA&& a, int_sequence<Ia...>,
         PtrB&& b, int_sequence<Ib...>,
         PtrC&& c, int_sequence<Ic...>,
-        ParamType&& p0)
+        PtrE&& e, int_sequence<Ie...>)
 {
-  return fn(a[Ia]..., b[Ib]..., c[Ic]..., p0);
+  return fn(d[Id]..., a[Ia]..., b[Ib]..., c[Ic]..., e[Ie]...);
 }
 
 template <class Fn,
-          class PtrD, int... Id,
-          class PtrA, int... Ia,
-          class PtrB, int... Ib,
-          class PtrC, int... Ic,
-          class ParamType>
+          class PtrD,   int... Id,
+          class PtrA,   int... Ia,
+          class PtrB,   int... Ib,
+          class PtrC,   int... Ic,
+          class PtrSFA, int... Isfa,
+          class PtrSFB, int... Isfb>
 CUTE_HOST_DEVICE constexpr
 void
-explode_with_d_scaling(Fn fn,
-        PtrD&& d, int_sequence<Id...>,
-        PtrA&& a, int_sequence<Ia...>,
-        PtrB&& b, int_sequence<Ib...>,
-        PtrC&& c, int_sequence<Ic...>,
-        ParamType&& p0)
+explode(Fn fn,
+        PtrD&& d,     int_sequence<Id...>,
+        PtrA&& a,     int_sequence<Ia...>,
+        PtrB&& b,     int_sequence<Ib...>,
+        PtrC&& c,     int_sequence<Ic...>,
+        PtrSFA&& sfa, int_sequence<Isfa...>,
+        PtrSFB&& sfb, int_sequence<Isfb...>)
 {
-  return fn(d[Id]..., a[Ia]..., b[Ib]..., c[Ic]..., p0);
+  return fn(d[Id]..., a[Ia]..., b[Ib]..., c[Ic]..., sfa[Isfa]..., sfb[Isfb]...);
 }
+//
+// Utility for exploding tuples into functions
+//
 
-} // end namespace detail
-
-template <int SRegCount, int DRegCount,
-          class Fn, class PtrS, class PtrD>
+template <class Fn,
+          class TupleA, int... I>
 CUTE_HOST_DEVICE constexpr
 void
-explode(Fn fn, PtrS&& s, PtrD&& d)
+explode_tuple(Fn fn,
+              TupleA&& a, int_sequence<I...>)
 {
-  return detail::explode(fn,
-                         s, make_int_sequence<SRegCount>{},
-                         d, make_int_sequence<DRegCount>{});
+  return fn(get<I>(a)...);
 }
 
-template <int ARegCount, int BRegCount, int CRegCount, 
-          class Fn, class PtrA, class PtrB, class PtrC>
+template <class Fn,
+          class TupleA, int... Ia,
+          class TupleB, int... Ib>
 CUTE_HOST_DEVICE constexpr
 void
-explode(Fn fn, PtrA&& a, PtrB&& b, PtrC&& c)
+explode_tuple(Fn fn,
+              TupleA&& a, int_sequence<Ia...>,
+              TupleB&& b, int_sequence<Ib...>)
 {
-  return detail::explode(fn,
-                         a, make_int_sequence<ARegCount>{},
-                         b, make_int_sequence<BRegCount>{},
-                         c, make_int_sequence<CRegCount>{});
+  return fn(get<Ia>(a)..., get<Ib>(b)...);
 }
 
-template <int DRegCount, int ARegCount, int BRegCount, int CRegCount,
-          class Fn, class PtrD, class PtrA, class PtrB, class PtrC>
+template <class Fn,
+          class TupleA, int... Ia,
+          class TupleB, int... Ib,
+          class TupleC, int... Ic>
 CUTE_HOST_DEVICE constexpr
 void
-explode(Fn fn, PtrD&& d, PtrA&& a, PtrB&& b, PtrC&& c)
+explode_tuple(Fn fn,
+              TupleA&& a, int_sequence<Ia...>,
+              TupleB&& b, int_sequence<Ib...>,
+              TupleC&& c, int_sequence<Ic...>)
 {
-  return detail::explode(fn,
-                         d, make_int_sequence<DRegCount>{},
-                         a, make_int_sequence<ARegCount>{},
-                         b, make_int_sequence<BRegCount>{},
-                         c, make_int_sequence<CRegCount>{});
+  return fn(get<Ia>(a)..., get<Ib>(b)..., get<Ic>(c)...);
 }
 
+} // end namespace detail
+
 } // end namespace cute
```

## cutlass_library/source/include/cute/atom/copy_atom.hpp

```diff
@@ -77,15 +77,15 @@
   static constexpr int NumValDst = size<1>(ValLayoutDst{});
 
   // Additional Trait parameters/transformations
   template <class... TraitsArgs>
   CUTE_HOST_DEVICE
   auto
   with(TraitsArgs&&... args) const {
-    auto traits = Traits::with(std::forward<TraitsArgs>(args)...);
+    auto traits = Traits::with(static_cast<TraitsArgs&&>(args)...);
     return Copy_Atom<decltype(traits), CopyInternalType>{traits};
   }
 
   //
   // Tensor call interfaces
   //
 
@@ -347,44 +347,44 @@
 
   template <class STensor>
   CUTE_HOST_DEVICE
   auto
   partition_S(STensor&& stensor) const {
     //static_assert(sizeof(typename remove_cvref_t<STensor>::value_type) == sizeof(typename TiledCopy::ValType),
     //              "Expected ValType for tiling SrcTensor.");
-    auto thr_tensor = make_tensor(std::forward<STensor>(stensor).data(), TiledCopy::tidfrg_S(stensor.layout()));
+    auto thr_tensor = make_tensor(static_cast<STensor&&>(stensor).data(), TiledCopy::tidfrg_S(stensor.layout()));
     return thr_tensor(thr_idx_, _, repeat<rank_v<STensor>>(_));
   }
 
   template <class DTensor>
   CUTE_HOST_DEVICE
   auto
   partition_D(DTensor&& dtensor) const {
     //static_assert(sizeof(typename remove_cvref_t<DTensor>::value_type) == sizeof(typename TiledCopy::ValType),
     //              "Expected ValType for tiling DstTensor.");
-    auto thr_tensor = make_tensor(std::forward<DTensor>(dtensor).data(), TiledCopy::tidfrg_D(dtensor.layout()));
+    auto thr_tensor = make_tensor(static_cast<DTensor&&>(dtensor).data(), TiledCopy::tidfrg_D(dtensor.layout()));
     return thr_tensor(thr_idx_, _, repeat<rank_v<DTensor>>(_));
   }
 
   template <class STensor>
   CUTE_HOST_DEVICE static
   auto
   retile_S(STensor&& stensor) {
     // static_assert(sizeof(typename remove_cvref_t<STensor>::value_type) == sizeof(typename TiledCopy::ValType),
     //               "Expected ValType for tiling SrcTensor.");
-    return make_tensor(std::forward<STensor>(stensor).data(), TiledCopy::retile(stensor.layout()));
+    return make_tensor(static_cast<STensor&&>(stensor).data(), TiledCopy::retile(stensor.layout()));
   }
 
   template <class DTensor>
   CUTE_HOST_DEVICE static
   auto
   retile_D(DTensor&& dtensor) {
     // static_assert(sizeof(typename remove_cvref_t<DTensor>::value_type) == sizeof(typename TiledCopy::ValType),
     //               "Expected ValType for tiling DstTensor.");
-    return make_tensor(std::forward<DTensor>(dtensor).data(), TiledCopy::retile(dtensor.layout()));
+    return make_tensor(static_cast<DTensor&&>(dtensor).data(), TiledCopy::retile(dtensor.layout()));
   }
 };
 
 
 template <class... Args,
           class LayoutCopy_TV,
           class Tiler>
@@ -595,15 +595,15 @@
 
 // The logical size of a TileCopy
 template <int... I, class... Args>
 CUTE_HOST_DEVICE constexpr
 auto
 tile_size(TiledCopy<Args...> const&)
 {
-  return size<I...>(typename TiledCopy<Args...>::TiledShape_MN{});
+  return size<I...>(typename TiledCopy<Args...>::Tiler_MN{});
 }
 
 // The number of threads involved in a TiledCopy
 template <class... Args>
 CUTE_HOST_DEVICE constexpr
 auto
 size(TiledCopy<Args...> const&)
@@ -752,14 +752,15 @@
   printf(latex_footer);
 }
 
 } // end namespace cute
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
+#include <cute/atom/copy_traits_sm50.hpp>
 #include <cute/atom/copy_traits_sm75.hpp>
 #include <cute/atom/copy_traits_sm80.hpp>
 #include <cute/atom/copy_traits_sm90.hpp>
 
 // Config
 #if (__CUDACC_VER_MAJOR__ >= 12)
 #  define CUTE_COPY_ATOM_TMA_SM90_ENABLED
```

## cutlass_library/source/include/cute/atom/copy_traits.hpp

```diff
@@ -88,29 +88,14 @@
   // Map from (dst-thr,dst-val) to bit
   using DstLayout = Layout<Shape<_1,_1>, Stride<_0,_0>>;
 
   // Reference map from (thr,val) to bit
   using RefLayout = SrcLayout;
 };
 
-namespace detail {
-
-template <class Operation,
-          class PtrS, int... Is,
-          class PtrD, int... Id>
-CUTE_HOST_DEVICE constexpr
-void
-copy_explode(PtrS&& s, int_sequence<Is...>,
-             PtrD&& d, int_sequence<Id...>)
-{
-  return Operation::copy(s[Is]..., d[Id]...);
-}
-
-} // end namespace detail
-
 //
 // Generic copy_unpack for common argument-based Copy_Traits
 //
 
 template <class CopyOp, class... Args,
           class SEngine, class SLayout,
           class DEngine, class DLayout>
@@ -135,16 +120,17 @@
   Tensor rD = recast<RegTypeDst>(dst);
 
   CUTE_STATIC_ASSERT_V(size(rS) == Int<RegNumSrc>{},
     "Copy_Traits: src failed to vectorize into registers. Layout is incompatible with this CopyOp.");
   CUTE_STATIC_ASSERT_V(size(rD) == Int<RegNumDst>{},
     "Copy_Traits: dst failed to vectorize into registers. Layout is incompatible with this CopyOp.");
 
-  detail::copy_explode<CopyOp>(rS, make_int_sequence<RegNumSrc>{},
-                               rD, make_int_sequence<RegNumDst>{});
+  detail::explode(detail::CallCOPY<CopyOp>{},
+                  rS, make_int_sequence<RegNumSrc>{},
+                  rD, make_int_sequence<RegNumDst>{});
 }
 
 //
 // Accept mutable temporaries
 //
 
 template <class CopyOp, class... Args,
```

## cutlass_library/source/include/cute/atom/copy_traits_sm90_tma.hpp

```diff
@@ -34,14 +34,16 @@
 #include <cuda.h>
 #endif
 
 #include <cute/atom/copy_traits_sm90_tma_swizzle.hpp>
 #include <cute/atom/copy_traits.hpp>
 #include <cute/atom/copy_atom.hpp>
 
+#include <cute/algorithm/prefetch.hpp>
+
 #include <cute/numeric/integral_ratio.hpp>
 
 namespace cute
 {
 
 template <class GmemTmaBasisStrides_, class TmaGmemBasis_, class TmaSwizzle_>
 struct AuxTmaParams {
@@ -49,85 +51,65 @@
   GmemStrides g_stride_;
   using TmaGmemBasis = TmaGmemBasis_;           // Layout for Tma box shape -> Gmem mode(s), always static
   static_assert(is_static<TmaGmemBasis>::value);
   using TmaSwizzle   = TmaSwizzle_;             // Tma swizzle, always Swizzle<B,M,S>
   static_assert(is_static<TmaSwizzle>::value);
 };
 
-//////////////////////////////////////////////////////////////////////////////
-///////////////////////////// TMA_LOAD ///////////////////////////////////////
-//////////////////////////////////////////////////////////////////////////////
-
-struct SM90_TMA_LOAD_OP : SM90_TMA_LOAD {};
-
-// The executable SM90_TMA_LOAD with tma_desc and tma_mbar
-template <class NumBitsPerTMA>
-struct Copy_Traits<SM90_TMA_LOAD_OP, NumBitsPerTMA>
+// Utility for unpacking TMA_LOAD arguments into a CopyOp
+template <class CopyOp>
+struct TMA_LOAD_Unpack
 {
-  using ThrID   = Layout<_1>;
-
-  // Map from (src-thr,src-val) to bit
-  using SrcLayout = Layout<Shape<_1,NumBitsPerTMA>>;
-  // Map from (dst-thr,dst-val) to bit
-  using DstLayout = Layout<Shape<_1,NumBitsPerTMA>>;
-
-  // Reference map from (thr,val) to bit
-  using RefLayout = SrcLayout;
-
-  // SM90_TMA_LOAD arguments
-  TmaDescriptor const& tma_desc_;
-  uint64_t& tma_load_mbar_;
-
-  template <class Coord, int... Is>
-  CUTE_HOST_DEVICE constexpr
-  void
-  copy_unpack_(void const* const dst_ptr,
-               Coord const& src_coord, seq<Is...>) const
+  template <class... Args,
+            class TS, class SLayout,
+            class TD, class DLayout>
+  CUTE_HOST_DEVICE friend constexpr void
+  copy_unpack(Copy_Traits<CopyOp, Args...> const& traits,
+              Tensor<TS,SLayout>           const& src,
+              Tensor<TD,DLayout>                & dst)
   {
+    auto src_coord = src.data().coord_;
+    if constexpr (detail::is_prefetch<CopyOp>) {
+      return detail::explode_tuple(detail::CallCOPY<CopyOp>{},
+                                   traits.opargs_, tuple_seq<decltype(traits.opargs_)>{},
+                                   src_coord, tuple_seq<decltype(src_coord)>{});
+    } else {
+      static_assert(is_smem<TD>::value, "SM90_TMA_LOAD requires the destination be shared memory.");
+      void* dst_ptr = cute::raw_pointer_cast(dst.data());
 #if 0
-    auto [c0,c1,c2,c3,c4] = append<5>(src_coord, 0);
-    printf("THR (%d,%d,%d) BLK (%d,%d,%d) TMACRD (%d,%d,%d,%d,%d) SMEMADDR (%p)\n",
-           threadIdx.x, threadIdx.y, threadIdx.z,
-           blockIdx.x, blockIdx.y, blockIdx.z,
-           int32_t(c0), int32_t(c1), int32_t(c2), int32_t(c3), int32_t(c4), dst_ptr);
+      auto [c0,c1,c2,c3,c4] = append<5>(src_coord, 0);
+      printf("THR (%d,%d,%d) BLK (%d,%d,%d) TMACRD (%d,%d,%d,%d,%d) SMEMADDR (%p)\n",
+            threadIdx.x, threadIdx.y, threadIdx.z,
+            blockIdx.x, blockIdx.y, blockIdx.z,
+            int32_t(c0), int32_t(c1), int32_t(c2), int32_t(c3), int32_t(c4), dst_ptr);
 #endif
-
-    SM90_TMA_LOAD::copy(&tma_desc_, tma_load_mbar_,
-                        dst_ptr, get<Is>(src_coord)...);
+      return detail::explode_tuple(detail::CallCOPY<CopyOp>{},
+                                   traits.opargs_, tuple_seq<decltype(traits.opargs_)>{},
+                                   make_tuple(dst_ptr), seq<0>{},
+                                   src_coord, tuple_seq<decltype(src_coord)>{});
+    }
   }
+};
 
-  // This is the copy_unpack dispatch for this Copy_Traits
-  // Src needs to be a gmem tensor with TmaCoordIterator .data()
-  // Dst needs to be a smem tensor
-  template <class TS, class SLayout,
-            class TD, class DLayout>
-  CUTE_HOST_DEVICE friend constexpr
-  void
-  copy_unpack(Copy_Traits        const& traits,
-              Tensor<TS,SLayout> const& src,
-              Tensor<TD,DLayout>      & dst)
-  {
-    static_assert(is_smem<TD>::value, "Expected smem dst for SM90_TMA_LOAD");
+//////////////////////////////////////////////////////////////////////////////
+///////////////////////////// TMA_LOAD ///////////////////////////////////////
+//////////////////////////////////////////////////////////////////////////////
 
-    traits.copy_unpack_(cute::raw_pointer_cast(dst.data()), src.data().coord_, tuple_seq<decltype(src.data().coord_)>{});
-  }
-};
+struct SM90_TMA_LOAD_OP : SM90_TMA_LOAD {};
 
 // The non-executable SM90_TMA_LOAD with tma_desc and no tma_mbar
 // Use .with(tma_mbar) to construct an executable version
 template <class NumBitsPerTMA, class AuxParams_>
 struct Copy_Traits<SM90_TMA_LOAD, NumBitsPerTMA, AuxParams_>
 {
-  using ThrID   = Layout<_1>;
-
+  using ThrID     = Layout<_1>;
   // Map from (src-thr,src-val) to bit
   using SrcLayout = Layout<Shape<_1,NumBitsPerTMA>>;
   // Map from (dst-thr,dst-val) to bit
   using DstLayout = Layout<Shape<_1,NumBitsPerTMA>>;
-
   // Reference map from (thr,val) to bit
   using RefLayout = SrcLayout;
 
   // SM90_TMA_LOAD arguments
   TmaDescriptor tma_desc_;
   using AuxParams = AuxParams_;
   AuxParams aux_params_;
@@ -140,23 +122,23 @@
   }
 
   // Construct an executable SM90_TMA_LOAD with tma_mbar
   CUTE_HOST_DEVICE constexpr
   Copy_Traits<SM90_TMA_LOAD_OP, NumBitsPerTMA>
   with(uint64_t& tma_mbar, [[maybe_unused]] uint16_t const& multicast_mask = 0) const {
     // We accept multicast_mask here to keep the API for both atoms consistent
-    return {tma_desc_, tma_mbar};
+    return {{}, {&tma_desc_, &tma_mbar}};
   }
 
   // Construct an executable SM90_TMA_LOAD with tma_mbar (temp. overloaded for grouped gemm/ptr array gemm)
   CUTE_HOST_DEVICE constexpr
   Copy_Traits<SM90_TMA_LOAD_OP, NumBitsPerTMA>
   with(TmaDescriptor const* new_tma_desc, uint64_t& tma_mbar, [[maybe_unused]] uint16_t const& multicast_mask = 0) const {
     // We accept multicast_mask here to keep the API for both atoms consistent
-    return {*new_tma_desc, tma_mbar};
+    return {{}, {new_tma_desc, &tma_mbar}};
   }
 
   // Generate the TMA coord tensor
   template <class GShape>
   CUTE_HOST_DEVICE constexpr
   auto
   get_tma_tensor(GShape const& g_shape) const {
@@ -169,80 +151,73 @@
             class TD, class DLayout>
   CUTE_HOST_DEVICE friend constexpr void
   copy_unpack(Copy_Traits        const& traits,
               Tensor<TS,SLayout> const& src,
               Tensor<TD,DLayout>      & dst) = delete;
 };
 
-//////////////////////////////////////////////////////////////////////////////
-///////////////////////////// TMA_LOAD_MULTICAST /////////////////////////////
-//////////////////////////////////////////////////////////////////////////////
-
-struct SM90_TMA_LOAD_MULTICAST_OP : SM90_TMA_LOAD_MULTICAST {};
-
+// The executable SM90_TMA_LOAD with tma_desc and tma_mbar
 template <class NumBitsPerTMA>
-struct Copy_Traits<SM90_TMA_LOAD_MULTICAST_OP, NumBitsPerTMA>
+struct Copy_Traits<SM90_TMA_LOAD_OP, NumBitsPerTMA>
+     : TMA_LOAD_Unpack<SM90_TMA_LOAD_OP>
 {
-  using ThrID   = Layout<_1>;
-
+  using ThrID     = Layout<_1>;
   // Map from (src-thr,src-val) to bit
   using SrcLayout = Layout<Shape<_1,NumBitsPerTMA>>;
   // Map from (dst-thr,dst-val) to bit
   using DstLayout = Layout<Shape<_1,NumBitsPerTMA>>;
-
   // Reference map from (thr,val) to bit
   using RefLayout = SrcLayout;
 
-  // SM90_TMA_LOAD_MULTICAST arguments
-  TmaDescriptor const& tma_desc_;
-  uint64_t& tma_load_mbar_;
-  uint16_t const& multicast_mask_;
-
-  template <class Coord, int... Is>
-  CUTE_HOST_DEVICE constexpr
-  void
-  copy_unpack_(void const* const dst_ptr,
-               Coord const& src_coord, seq<Is...>) const
-  {
-#if 0
-    auto [c0,c1,c2,c3,c4] = append<5>(src_coord, 0);
-    printf("THR (%d,%d,%d) BLK (%d,%d,%d) TMACRD (%d,%d,%d,%d,%d) SMEMADDR (%p)\n",
-           threadIdx.x, threadIdx.y, threadIdx.z,
-           blockIdx.x, blockIdx.y, blockIdx.z,
-           int32_t(c0), int32_t(c1), int32_t(c2), int32_t(c3), int32_t(c4), dst_ptr);
-#endif
+  // SM90_TMA_LOAD arguments
+  tuple<
+  TmaDescriptor const*,
+  uint64_t* // smem mbarrier
+  > const opargs_;
+};
 
-    SM90_TMA_LOAD_MULTICAST::copy(&tma_desc_, tma_load_mbar_, multicast_mask_,
-                                  dst_ptr, get<Is>(src_coord)...);
-  }
+// The prefetch for SM90_TMA_LOAD with tma_desc
+template <class NumBitsPerTMA, class... Args>
+struct Copy_Traits<SM90_TMA_LOAD::PREFETCH, NumBitsPerTMA, Args...>
+     : TMA_LOAD_Unpack<SM90_TMA_LOAD::PREFETCH>
+{
+  using ThrID     = Layout<_1>;
+  // Map from (src-thr,src-val) to bit
+  using SrcLayout = Layout<Shape<_1,NumBitsPerTMA>>;
+  // Map from (dst-thr,dst-val) to bit
+  using DstLayout = Layout<Shape<_1,NumBitsPerTMA>>;
+  // Reference map from (thr,val) to bit
+  using RefLayout = SrcLayout;
 
-  template <class TS, class SLayout,
-            class TD, class DLayout>
-  CUTE_HOST_DEVICE friend constexpr
-  void
-  copy_unpack(Copy_Traits        const& traits,
-              Tensor<TS,SLayout> const& src,
-              Tensor<TD,DLayout>      & dst)
-  {
-    static_assert(is_smem<TD>::value, "Expected smem dst for SM90_TMA_LOAD_MULTICAST");
+  // SM90_TMA_LOAD::PREFETCH arguments
+  tuple<TmaDescriptor const*> const opargs_;
 
-    traits.copy_unpack_(cute::raw_pointer_cast(dst.data()), src.data().coord_, tuple_seq<decltype(src.data().coord_)>{});
-  }
+  // Construct with any other Traits' TMA Desc
+  template <class... CopyArgs>
+  CUTE_HOST_DEVICE
+  Copy_Traits(Copy_Traits<CopyArgs...> const& traits)
+    : opargs_({&traits.tma_desc_}) {}
 };
 
+//////////////////////////////////////////////////////////////////////////////
+///////////////////////////// TMA_LOAD_MULTICAST /////////////////////////////
+//////////////////////////////////////////////////////////////////////////////
+
+struct SM90_TMA_LOAD_MULTICAST_OP : SM90_TMA_LOAD_MULTICAST {};
+
+// The non-executable SM90_TMA_LOAD_MULTICAST with tma_desc and no tma_mbar
+// Use .with(tma_mbar, multicast_mask) to construct an executable version
 template <class NumBitsPerTMA, class AuxParams_>
 struct Copy_Traits<SM90_TMA_LOAD_MULTICAST, NumBitsPerTMA, AuxParams_>
 {
-  using ThrID   = Layout<_1>;
-
+  using ThrID     = Layout<_1>;
   // Map from (src-thr,src-val) to bit
   using SrcLayout = Layout<Shape<_1,NumBitsPerTMA>>;
   // Map from (dst-thr,dst-val) to bit
   using DstLayout = Layout<Shape<_1,NumBitsPerTMA>>;
-
   // Reference map from (thr,val) to bit
   using RefLayout = SrcLayout;
 
   // SM90_TMA_LOAD_MULTICAST arguments
   TmaDescriptor tma_desc_;
   using AuxParams = AuxParams_;
   AuxParams aux_params_;
@@ -254,22 +229,22 @@
     return &tma_desc_;
   }
 
   // Construct an executable SM90_TMA_LOAD_MULTICAST with tma_mbar
   CUTE_HOST_DEVICE constexpr
   Copy_Traits<SM90_TMA_LOAD_MULTICAST_OP, NumBitsPerTMA>
   with(uint64_t& tma_load_mbar, uint16_t const& multicast_mask) const {
-    return {tma_desc_, tma_load_mbar, multicast_mask};
+    return {{}, {&tma_desc_, &tma_load_mbar, multicast_mask}};
   }
 
   // Construct an executable SM90_TMA_LOAD_MULTICAST_OP with tma_mbar (temp. overloaded for grouped gemm/ptr array gemm)
   CUTE_HOST_DEVICE constexpr
   Copy_Traits<SM90_TMA_LOAD_MULTICAST_OP, NumBitsPerTMA>
   with(TmaDescriptor const* new_tma_desc, uint64_t& tma_load_mbar, uint16_t const& multicast_mask) const {
-    return {*new_tma_desc, tma_load_mbar, multicast_mask};
+    return {{}, {new_tma_desc, &tma_load_mbar, multicast_mask}};
   }
 
   // Generate the TMA coord tensor
   template <class GShape>
   CUTE_HOST_DEVICE constexpr
   auto
   get_tma_tensor(GShape const& g_shape) const {
@@ -282,33 +257,117 @@
             class TD, class DLayout>
   CUTE_HOST_DEVICE friend constexpr void
   copy_unpack(Copy_Traits        const& traits,
               Tensor<TS,SLayout> const& src,
               Tensor<TD,DLayout>      & dst) = delete;
 };
 
+// The executable SM90_TMA_LOAD_MULTICAST with tma_desc and tma_mbar and multicast_mask
+template <class NumBitsPerTMA>
+struct Copy_Traits<SM90_TMA_LOAD_MULTICAST_OP, NumBitsPerTMA>
+     : TMA_LOAD_Unpack<SM90_TMA_LOAD_MULTICAST_OP>
+{
+  using ThrID     = Layout<_1>;
+  // Map from (src-thr,src-val) to bit
+  using SrcLayout = Layout<Shape<_1,NumBitsPerTMA>>;
+  // Map from (dst-thr,dst-val) to bit
+  using DstLayout = Layout<Shape<_1,NumBitsPerTMA>>;
+  // Reference map from (thr,val) to bit
+  using RefLayout = SrcLayout;
+
+  // SM90_TMA_LOAD_MULTICAST arguments
+  tuple<
+  TmaDescriptor const*,
+  uint64_t*, // smem mbarrier
+  uint16_t   // multicast mask
+  > const opargs_;
+};
+
 //////////////////////////////////////////////////////////////////////////////
 ///////////////////////////// TMA_STORE //////////////////////////////////////
 //////////////////////////////////////////////////////////////////////////////
 
 // The executable SM90_TMA_STORE with tma_desc
 template <class NumBitsPerTMA, class AuxParams_>
 struct Copy_Traits<SM90_TMA_STORE, NumBitsPerTMA, AuxParams_>
 {
+  using ThrID     = Layout<_1>;
+  // Map from (src-thr,src-val) to bit
+  using SrcLayout = Layout<Shape<_1,NumBitsPerTMA>>;
+  // Map from (dst-thr,dst-val) to bit
+  using DstLayout = Layout<Shape<_1,NumBitsPerTMA>>;
+  // Reference map from (thr,val) to bit
+  using RefLayout = SrcLayout;
+
+  // SM90_TMA_STORE arguments
+  TmaDescriptor tma_desc_;
+  using AuxParams = AuxParams_;
+  AuxParams aux_params_;
+
+  // Return TmaDescriptor/TensorMap
+  CUTE_HOST_DEVICE constexpr
+  TmaDescriptor const*
+  get_tma_descriptor() const {
+    return &tma_desc_;
+  }
+
+  // Generate the TMA coord tensor
+  template <class GShape>
+  CUTE_HOST_DEVICE constexpr
+  auto
+  get_tma_tensor(GShape const& g_shape) const {
+    static_assert(is_congruent<decltype(g_shape), decltype(aux_params_.g_stride_)>::value);
+    return make_counting_tensor(make_layout(g_shape, aux_params_.g_stride_));
+  }
+
+  template <class TS, class SLayout,
+            class TD, class DLayout>
+  CUTE_HOST_DEVICE friend constexpr void
+  copy_unpack(Copy_Traits        const& traits,
+              Tensor<TS,SLayout> const& src,
+              Tensor<TD,DLayout>      & dst)
+  {
+    static_assert(is_smem<TS>::value, "Expected smem src for SM90_TMA_STORE");
+    //static_assert(is_gmem<TD>::value, "Expected gmem dst for SM90_TMA_STORE");  // TMA spoofed src tensor
+
+    void const* const desc_ptr = &(traits.tma_desc_);
+    void const* const src_ptr  = cute::raw_pointer_cast(src.data());
+    auto dst_coord = dst.data().coord_;
+#if 0
+    auto [c0,c1,c2,c3,c4] = append<5>(dst_coord, 0);
+    printf("THR (%d,%d,%d) BLK (%d,%d,%d) TMACRD (%d,%d,%d,%d,%d) SMEMADDR (%p)\n",
+           threadIdx.x, threadIdx.y, threadIdx.z,
+           blockIdx.x, blockIdx.y, blockIdx.z,
+           int32_t(c0), int32_t(c1), int32_t(c2), int32_t(c3), int32_t(c4), src_ptr);
+#endif
+    return detail::explode_tuple(detail::CallCOPY<SM90_TMA_STORE>{},
+                                 make_tuple(desc_ptr, src_ptr), seq<0,1>{},
+                                 dst_coord, tuple_seq<decltype(dst_coord)>{});
+  }
+};
+
+//////////////////////////////////////////////////////////////////////////////
+///////////////////////////// TMA_REDUCE_ADD //////////////////////////////////////
+//////////////////////////////////////////////////////////////////////////////
+
+// The executable SM90_TMA_REDUCE_ADD with tma_desc
+template <class NumBitsPerTMA, class AuxParams_>
+struct Copy_Traits<SM90_TMA_REDUCE_ADD, NumBitsPerTMA, AuxParams_>
+{
   using ThrID   = Layout<_1>;
 
   // Map from (src-thr,src-val) to bit
   using SrcLayout = Layout<Shape<_1,NumBitsPerTMA>>;
   // Map from (dst-thr,dst-val) to bit
   using DstLayout = Layout<Shape<_1,NumBitsPerTMA>>;
 
   // Reference map from (thr,val) to bit
   using RefLayout = SrcLayout;
 
-  // SM90_TMA_STORE arguments
+  // SM90_TMA_REDUCE_ADD arguments
   TmaDescriptor tma_desc_;
   using AuxParams = AuxParams_;
   AuxParams aux_params_;
 
   // Return TmaDescriptor/TensorMap
   CUTE_HOST_DEVICE constexpr
   TmaDescriptor const*
@@ -335,31 +394,31 @@
     auto [c0,c1,c2,c3,c4] = append<5>(dst_coord, 0);
     printf("THR (%d,%d,%d) BLK (%d,%d,%d) TMACRD (%d,%d,%d,%d,%d) SMEMADDR (%p)\n",
            threadIdx.x, threadIdx.y, threadIdx.z,
            blockIdx.x, blockIdx.y, blockIdx.z,
            int32_t(c0), int32_t(c1), int32_t(c2), int32_t(c3), int32_t(c4), src_ptr);
 #endif
 
-    SM90_TMA_STORE::copy(&tma_desc_,
+    SM90_TMA_REDUCE_ADD::copy(&tma_desc_,
                          src_ptr, get<Is>(dst_coord)...);
   }
 
   // This is the copy_unpack dispatch for this Copy_Traits
   // Src needs to be a smem tensor
   // Dst needs to be a gmem tensor with TmaCoordIterator .data()
   template <class TS, class SLayout,
             class TD, class DLayout>
   CUTE_HOST_DEVICE friend constexpr
   void
   copy_unpack(Copy_Traits        const& traits,
               Tensor<TS,SLayout> const& src,
               Tensor<TD,DLayout>      & dst)
   {
-    static_assert(is_smem<TS>::value, "Expected smem src for SM90_TMA_STORE");
-    //static_assert(is_gmem<TD>::value, "Expected gmem dst for SM90_TMA_STORE");  // TMA spoofed src tensor
+    static_assert(is_smem<TS>::value, "Expected smem src for SM90_TMA_REDUCE_ADD");
+    //static_assert(is_gmem<TD>::value, "Expected gmem dst for SM90_TMA_REDUCE_ADD");  // TMA spoofed src tensor
 
     traits.copy_unpack_(cute::raw_pointer_cast(src.data()), dst.data().coord_, tuple_seq<decltype(dst.data().coord_)>{});
   }
 };
 
 //////////////////////////////////////////////////////////////////////////////
 ///////////////////////////// BULK COPY //////////////////////////////////////
@@ -379,35 +438,56 @@
   // Reference map from (thr,val) to bit
   using RefLayout = SrcLayout;
 
   // SM90_BULK_COPY_G2S arguments
   // 0: uint64_t* bulk_load_memory_barrier
   cute::tuple<OpArgs...> bulk_load_mbar_;
 
+  // Record the memory barrier for the instruction
+  CUTE_HOST_DEVICE constexpr
+  Copy_Traits<SM90_BULK_COPY_G2S, NumBitsPerTMA, uint64_t*>
+  with(uint64_t& bulk_mbar) const {
+    return {{&bulk_mbar}};
+  }
+
   template <class TS, class SLayout,
             class TD, class DLayout>
   CUTE_HOST_DEVICE friend constexpr
   void
   copy_unpack(Copy_Traits        const& traits,
               Tensor<TS,SLayout> const& src,
               Tensor<TD,DLayout>      & dst)
   {
     static_assert(is_same<cute::tuple<OpArgs...>, cute::tuple<uint64_t*>>::value,
                   "Extra arguments not set. Set .with() before use.");
     static_assert(is_gmem<TS>::value, "Expected gmem src for SM90_BULK_COPY_G2S");
     static_assert(is_smem<TD>::value, "Expected smem dst for SM90_BULK_COPY_G2S");
-    SM90_BULK_COPY_G2S::copy(raw_pointer_cast(src.data()), *get<0>(traits.bulk_load_mbar_),
+    SM90_BULK_COPY_G2S::copy(raw_pointer_cast(src.data()), get<0>(traits.bulk_load_mbar_),
                              raw_pointer_cast(dst.data()), int32_t(NumBitsPerTMA::value / 8));
   }
+};
 
-  // Record the memory barrier for the instruction
-  CUTE_HOST_DEVICE constexpr
-  Copy_Traits<SM90_BULK_COPY_G2S, NumBitsPerTMA, uint64_t*>
-  with(uint64_t& bulk_mbar) const {
-    return {{&bulk_mbar}};
+template <class NumBitsPerTMA, class... Args>
+struct Copy_Traits<SM90_BULK_COPY_G2S::PREFETCH, NumBitsPerTMA, Args...>
+     : Copy_Traits<SM90_BULK_COPY_G2S, NumBitsPerTMA>
+{
+  template <class... CopyArgs>
+  CUTE_HOST_DEVICE
+  Copy_Traits(Copy_Traits<CopyArgs...> const& traits) {}
+
+  template <class TS, class SLayout,
+            class TD, class DLayout>
+  CUTE_HOST_DEVICE friend constexpr
+  void
+  copy_unpack(Copy_Traits        const& traits,
+              Tensor<TS,SLayout> const& src,
+              Tensor<TD,DLayout>      & dst)
+  {
+    static_assert(is_gmem<TS>::value, "Expected gmem src for SM90_BULK_PREFETCH");
+    SM90_BULK_COPY_G2S::PREFETCH::copy(raw_pointer_cast(src.data()), int32_t(NumBitsPerTMA::value / 8));
   }
 };
 
 template <class NumBitsPerTMA>
 struct Copy_Traits<SM90_BULK_COPY_S2G, NumBitsPerTMA>
 {
   static_assert(int32_t(NumBitsPerTMA::value / 8) % 16 == 0,
@@ -649,25 +729,25 @@
 
 template <class GEngine, class GLayout,
           class TmaGmemBasisStride,
           class ShapeT, size_t TmaRank>
 CUTE_HOST_DEVICE constexpr
 void
 fill_tma_gmem_shape_stride(Tensor<GEngine,GLayout>   const& gtensor,           // Gmem Shapes and Strides, in units of TmaInternalType
-                           TmaGmemBasisStride        const& tma_gbasis_stride, // Map Tma mode idx -> Gmem mode(s) 
+                           TmaGmemBasisStride        const& tma_gbasis_stride, // Map Tma mode idx -> Gmem mode(s)
                            cute::array<ShapeT,   TmaRank> & gmem_prob_shape,   // Tma Shapes, uint32_t or uin64_t
                            cute::array<uint64_t, TmaRank> & gmem_prob_stride)  // Tma Strides
 {
   static_assert(is_tuple<TmaGmemBasisStride>::value);
   static_assert(is_same<uint32_t, ShapeT>::value || is_same<uint64_t, ShapeT>::value);
 
   using TmaInternalType = typename GEngine::value_type;
   constexpr int tma_rank = decltype(rank(tma_gbasis_stride))::value;
   static_assert(TmaRank >= tma_rank);
-  
+
   auto gmem_shape  =  shape(gtensor);
   auto gmem_stride = stride(gtensor);
   // Use the indirections in tma_gbasis_stride into gtensor to construct the tma gmem shapes/strides
   for_each(make_seq<tma_rank>{}, [&](auto i) {
     constexpr int tma_i_rank = decltype(rank<i>(tma_gbasis_stride))::value;
     if constexpr (tma_i_rank == 1) {
       // Trivial contribution of this gmem mode to this tma mode
@@ -699,20 +779,20 @@
 
 // Overload for an existing Copy_Traits
 template <class GEngine, class GLayout,
           class Op, class Bits, class Aux,
           class ShapeT, size_t TmaRank>
 CUTE_HOST_DEVICE constexpr
 void
-fill_tma_gmem_shape_stride(Copy_Traits<Op,Bits,Aux>  const& tma_traits,    
+fill_tma_gmem_shape_stride(Copy_Traits<Op,Bits,Aux>  const& tma_traits,
                            Tensor<GEngine,GLayout>   const& gtensor,           // Gmem Shapes and Strides, value_type = TmaInternalType
                            cute::array<ShapeT,   TmaRank> & gmem_prob_shape,   // Tma Shapes, uint32_t or uin64_t
                            cute::array<uint64_t, TmaRank> & gmem_prob_stride)  // Tma Strides
 {
-  return fill_tma_gmem_shape_stride(gtensor, stride(typename Aux::TmaGmemBasis{}), 
+  return fill_tma_gmem_shape_stride(gtensor, stride(typename Aux::TmaGmemBasis{}),
                                     gmem_prob_shape, gmem_prob_stride);
 }
 
 // Use a sidx2gmode to read through the GMEM tensor
 //   and construct a TMA Descriptor for the resulting instruction
 // At the same time, construct the Tma Tensor's Stride to generate
 //   the TMA coordinates that the instruction consumes.
@@ -820,15 +900,15 @@
   assert(smem_box_stride[4] >= (uint32_t(1)));               // Stride must be min 1
   assert(smem_box_stride[4] <= (uint32_t(8)));               // Stride must be max 2^3 = 8
 
     //
     // Construct the descriptor
     //
 
-    TmaDescriptor tma_desc = {0};
+    TmaDescriptor tma_desc{};
 
     //
     // TMA general info
     //
 
   #if (__CUDACC_VER_MAJOR__ >= 12) && !defined(__CUDACC_RTC__)
 
@@ -893,15 +973,15 @@
       if constexpr (decltype(j == Int<0>{})::value) {
         auto scale = recast_ratio * basis_get(ei, stride(gtensor));
         return E<j>{} * scale;         // Return TMA Coord basis -- with a recast scale factor
       } else
       if constexpr (decltype(rank<j>(tma_gmem_basis_stride) == Int<1>{})::value) {
         return E<j>{};                 // Return TMA Coord basis -- known scale of Int<1>{}
       } else {
-        int32_t scale = ceil_div(int32_t(di * sizeof_bits_v<TmaInternalType> / cute::max(gmem_prob_stride[j], 16)), 8);
+        int32_t scale = ceil_div(int32_t(di * sizeof_bits_v<TmaInternalType> / cute::max(gmem_prob_stride[j], uint64_t{16})), 8);
         return E<j>{} * scale;         // Return TMA Coord basis -- with a dynamic scale factor
       }
     }
   });
 
 #if 0
     print("gmem_tma_basis_stride : "); print(gmem_tma_basis_stride); print("\n");
@@ -944,15 +1024,15 @@
                                                                             smem_swizzle,
                                                                             num_multicast);
 
   //
   // Construct the Copy_Traits
   //
 
-  constexpr int num_bits_per_tma = size(tma_gbasis) * sizeof_bits<TmaInternalType>::value;
+  constexpr int num_bits_per_tma = size(tma_gbasis) * sizeof_bits_v<TmaInternalType>;
   using Traits = Copy_Traits<CopyOp, cute::C<num_bits_per_tma>, decltype(aux_params)>;
   using Atom   = Copy_Atom<Traits, typename GEngine::value_type>;
 
   Traits tma_traits{tma_desc, aux_params};
 
 #if 0
   print("num_bits_per_tma :  "); print(num_bits_per_tma); print("\n");
@@ -1101,21 +1181,30 @@
 auto
 make_tma_copy(CopyOp                  const& copy_op,
               Tensor<GEngine,GLayout> const& gtensor,
               SLayout                 const& slayout,
               CTA_Tiler               const& cta_tiler,
               Cluster_Size            const& cluster_size)
 {
+  if constexpr (cute::is_same_v<CopyOp, SM90_TMA_LOAD_IM2COL> ||
+                cute::is_same_v<CopyOp, SM90_TMA_STORE_IM2COL>) {
+    return make_im2col_tma_copy(copy_op,
+                                gtensor,
+                                slayout,
+                                cta_tiler,
+                                cluster_size);
+  } else {
     auto cta_v_tile = make_identity_layout(shape(gtensor)).compose(cta_tiler);
     auto cta_t_tile = make_layout(cluster_size);
     // Prefer TmaInternalType if specified. Fallback to GEngine::value_type
     using TmaType = conditional_t<is_same<void, TmaInternalType>::value, typename GEngine::value_type, TmaInternalType>;
     return detail::make_tma_copy_tiled<TmaType>(copy_op,
                                                 gtensor, slayout,
                                                 cta_t_tile, cta_v_tile);
+  }
 }
 
 // Explicit defaulting
 template <class CopyOp,
           class GEngine, class GLayout,
           class SLayout>
 CUTE_HOST_RTC
@@ -1175,53 +1264,72 @@
           class SEngine, class SLayout,
           class GEngine, class GLayout>
 CUTE_DEVICE
 auto
 tma_partition(Copy_Atom<Args...>      const& copy_atom,
               CtaCoord                const& cta_coord,
               Layout<TShape,TStride>  const& cta_layout,  // T: CTA coord -> logical multicast id
-              Tensor<SEngine,SLayout> const& stensor,     // SMEM Tensor (TMATile, Iter)
-              Tensor<GEngine,GLayout> const& gtensor)     // GMEM Tensor (TMATile, Iter)
+              Tensor<SEngine,SLayout> const& stensor,     // SMEM Tensor (TMATile, Rest...)
+              Tensor<GEngine,GLayout> const& gtensor)     // GMEM Tensor (TMATile, Rest...)
 {
+  CUTE_STATIC_ASSERT_V(size<0>(stensor) == size<0>(gtensor));
+
   // Invert the smem to get the largest contiguous vector in the smem layout
   Layout inv_smem_layout = right_inverse(get_nonswizzle_portion(layout<0>(stensor)));
   // Scale that up to cover all of the smem_coords
   Layout layout_v = tile_to_shape(make_layout(inv_smem_layout), size<0>(stensor));
 
   // Factor out the single-instrucion portion
   Layout tma_layout_v = make_layout(Int<Copy_Atom<Args...>::NumValSrc>{});
-  Layout layout_V = logical_divide(layout_v, tma_layout_v);
+  auto layout_V = make_tile(logical_divide(layout_v, tma_layout_v));
 
+  // Append with _ until we cover all Rest... modes
+  auto glayout_V = append<rank_v<decltype(gtensor)>>(layout_V, _);
+  auto slayout_V = append<rank_v<decltype(stensor)>>(layout_V, _);
   // Transform tile mode and coalesce
-  Tensor gtensor_v = coalesce(gtensor.compose(layout_V, _), Shape<Shape<_1,_1>,_1>{});   // ((TMA,TMA_Iter),Iter)
-  Tensor stensor_v = coalesce(stensor.compose(layout_V, _), Shape<Shape<_1,_1>,_1>{});   // ((TMA,TMA_Iter),Iter)
+  Tensor gtensor_v = coalesce(gtensor.compose(glayout_V), Shape<Shape<_1,_1>>{});    // ((TMA,TMA_Iter), Rest...)
+  Tensor stensor_v = coalesce(stensor.compose(slayout_V), Shape<Shape<_1,_1>>{});    // ((TMA,TMA_Iter), Rest...)
 
 #if 0
   if (thread0()) {
+    print("cta_coord  : "); print(cta_coord); print("\n");
+    print("cta_layout : "); print(cta_layout); print("\n");
+    print("gtensor   : "); print(gtensor); print("\n");
+    print("stensor   : "); print(stensor); print("\n");
     print("layout_V  : "); print(layout_V); print("\n");
     print("gtensor_v : "); print(gtensor_v); print("\n");
     print("stensor_v : "); print(stensor_v); print("\n");
   }
 #endif
 
-  // Restride the cta-into-tma-instr layout
-  Layout tma_layout_t  = composition(make_layout(Int<1>{}, shape_div(size(tma_layout_v), cosize(cta_layout))), cta_layout);
-  Layout tma_layout_tv = make_layout(tma_layout_t, tma_layout_v);
-
-  // Transform TMA mode
-  Tensor gtensor_tv = gtensor_v.compose(make_tile(tma_layout_tv, _), _);                 // (((Thr,Frg),TMA_Iter),Iter)
-  Tensor stensor_tv = stensor_v.compose(make_tile(tma_layout_tv, _), _);                 // (((Thr,Frg),TMA_Iter),Iter)
+  // Offset inside the TMA-mode for the multicast
+  auto multicast_offset = cta_layout(cta_coord) * (size(tma_layout_v) / cosize(cta_layout));
+  auto multicast_coord  = make_coord(make_coord(multicast_offset, Int<0>{}));
+  auto scoord = append<SLayout::rank>(multicast_coord, Int<0>{});
+  auto gcoord = append<GLayout::rank>(multicast_coord, Int<0>{});
 
-#if 0
-  if (thread0()) {
-    print("tma_layout_tv : "); print(tma_layout_tv); print("\n");
-    print("gtensor_tv : "); print(gtensor_tv); print("\n");
-    print("stensor_tv : "); print(stensor_tv); print("\n");
-  }
-#endif
+  Tensor gresult = domain_offset(gcoord, gtensor_v);
+  Tensor sresult = domain_offset(scoord, stensor_v);
+
+  return cute::make_tuple(gresult, sresult);
+}
 
-  // Slice and group Frg,TMA_Iter and return
-  auto c = make_coord(make_coord(make_coord(cta_coord, _), _), _);
-  return cute::make_tuple(group_modes<0,2>(gtensor_tv(c)), group_modes<0,2>(stensor_tv(c)));
+// TMA Multicast Masks Calculation
+template <int Mode, class CtaLayout, class CtaCoord>
+CUTE_HOST_DEVICE constexpr
+auto
+create_tma_multicast_mask(CtaLayout const& cta_layout_vmnk,
+                          CtaCoord  const& cta_coord_vmnk)
+{
+  auto cta_coord_slicer = replace<Mode>(cta_coord_vmnk, _);
+  auto [cta_layout, elected_cta] = slice_and_offset(cta_coord_slicer, cta_layout_vmnk);
+  // Get the instruction code
+  uint16_t mcast_mask = 0;
+  for (int i = 0; i < size(cta_layout); ++i) {
+    mcast_mask |= uint16_t(1) << cta_layout(i);
+  }
+  // Shift by the instruction's elected block rank (dynamic)
+  mcast_mask <<= elected_cta;
+  return mcast_mask;
 }
 
 } // end namespace cute
```

## cutlass_library/source/include/cute/atom/mma_atom.hpp

```diff
@@ -31,15 +31,14 @@
 #pragma once
 
 #include <cute/config.hpp>
 
 #include <cute/arch/mma.hpp>
 
 #include <cute/atom/mma_traits.hpp>
-
 #include <cute/tensor.hpp>
 #include <cute/util/type_traits.hpp>
 
 namespace cute {
 
 template <class... Args>
 struct MMA_Atom;
@@ -74,15 +73,15 @@
   using FrgTypeC = typename detail::FrgTypeC_or_Default<Traits>::type;
 
   // Additional Trait parameters/transformations
   template <class... TraitsArgs>
   CUTE_HOST_DEVICE
   auto
   with(TraitsArgs&&... args) const {
-    auto traits = Traits::with(std::forward<TraitsArgs>(args)...);
+    auto traits = Traits::with(static_cast<TraitsArgs&&>(args)...);
     return MMA_Atom<decltype(traits)>{traits};
   }
 
   //
   // Tensor call interfaces
   //
 
@@ -153,15 +152,15 @@
     CUTE_STATIC_ASSERT_V(rank(atensor) >= Int<3>{});  // VMK
     CUTE_STATIC_ASSERT_V(size<0>(atensor) == size<1>(LayoutA_TV{}));
 
     if constexpr (has_dereference<FrgTypeA>::value) {
       // If the intended FrgTypeA is a view (of the current tensor), forward the whole
       static_assert(is_same<ValTypeA, typename remove_cvref_t<ATensor>::value_type>::value
                       , "Expecting ValTypeA type");
-      return make_tensor<FrgTypeA>(std::forward<ATensor>(atensor));
+      return make_tensor<FrgTypeA>(static_cast<ATensor&&>(atensor));
     } else {
       // Else, the intended FrgTypeA is a value type, construct a new tensor with a fragment layout
       return make_fragment_like<FrgTypeA>(atensor);
     }
 
     CUTE_GCC_UNREACHABLE;
   }
@@ -175,15 +174,15 @@
     CUTE_STATIC_ASSERT_V(rank(btensor) >= Int<3>{});  // VNK
     CUTE_STATIC_ASSERT_V(size<0>(btensor) == size<1>(LayoutB_TV{}));
 
     if constexpr (has_dereference<FrgTypeB>::value) {
       // If the intended FrgTypeB is a view (of the current tensor), forward the whole
       static_assert(is_same<ValTypeB, typename remove_cvref_t<BTensor>::value_type>::value
                       , "Expecting ValTypeB type");
-      return make_tensor<FrgTypeB>(std::forward<BTensor>(btensor));
+      return make_tensor<FrgTypeB>(static_cast<BTensor&&>(btensor));
     } else {
       // Else, the intended FrgTypeB is a value type, construct a new tensor with a fragment layout
       return make_fragment_like<FrgTypeB>(btensor);
     }
 
     CUTE_GCC_UNREACHABLE;
   }
@@ -209,15 +208,15 @@
   using AtomThrID      = typename MMA_Atom::ThrID;
   using AtomLayoutC_TV = typename MMA_Atom::LayoutC_TV;
   using AtomLayoutA_TV = typename MMA_Atom::LayoutA_TV;
   using AtomLayoutB_TV = typename MMA_Atom::LayoutB_TV;
 
   static_assert(   rank_v<AtomLayoutMNK>  == 3,   "TiledMMA requires rank-3 AtomLayoutMNK");
   static_assert(   rank_v<PermutationMNK> == 3,   "TiledMMA requires rank-3 PermutationMNK");
-  static_assert(  is_tile<PermutationMNK>::value, "TiledMMA requires independent permutations of MNK.");
+  static_assert( is_tuple<PermutationMNK>::value, "TiledMMA requires independent permutations of MNK.");
   static_assert(is_static<PermutationMNK>::value, "TiledMMA requires static permutations of MNK.");
 
   using ThrLayoutVMNK = decltype(tiled_product(AtomThrID{}, AtomLayoutMNK{}));
   ThrLayoutVMNK thr_layout_vmnk_;
 
   CUTE_HOST_DEVICE constexpr
   TiledMMA(MMA_Atom const& mma_atom = {}, AtomLayoutMNK const& thr_layout_mnk = {})
@@ -387,15 +386,15 @@
     auto core_size = size<I>(AtomShape_MNK{}) * size<I+1>(get_thr_layout_vmnk());
     [[maybe_unused]] auto perm_size = size<I>(PermutationMNK{});
     if constexpr (is_underscore<decltype(perm_size)>::value) {
       return core_size;
     } else {
       return cute::max(core_size, perm_size);
     }
-  
+
     CUTE_GCC_UNREACHABLE;
   }
 
   CUTE_HOST_DEVICE constexpr
   auto
   get_layoutC_MN() const
   {
@@ -513,37 +512,37 @@
   ThrVMNK thr_vmnk_;
 
   template <class CTensor>
   CUTE_HOST_DEVICE constexpr
   auto
   partition_C(CTensor&& ctensor) const
   {
-    auto thr_tensor = make_tensor(std::forward<CTensor>(ctensor).data(), this->thrfrg_C(ctensor.layout()));
+    auto thr_tensor = make_tensor(static_cast<CTensor&&>(ctensor).data(), this->thrfrg_C(ctensor.layout()));
 
     auto thr_vmn = make_coord(get<0>(thr_vmnk_), make_coord(get<1>(thr_vmnk_), get<2>(thr_vmnk_)));
     return thr_tensor(thr_vmn, make_coord(_, repeat<rank<1,1>(thr_tensor)>(_)));
   }
 
   template <class ATensor>
   CUTE_HOST_DEVICE constexpr
   auto
   partition_A(ATensor&& atensor) const
   {
-    auto thr_tensor = make_tensor(std::forward<ATensor>(atensor).data(), this->thrfrg_A(atensor.layout()));
+    auto thr_tensor = make_tensor(static_cast<ATensor&&>(atensor).data(), this->thrfrg_A(atensor.layout()));
 
     auto thr_vmk = make_coord(get<0>(thr_vmnk_), make_coord(get<1>(thr_vmnk_), get<3>(thr_vmnk_)));
     return thr_tensor(thr_vmk, make_coord(_, repeat<rank<1,1>(thr_tensor)>(_)));
   }
 
   template <class BTensor>
   CUTE_HOST_DEVICE constexpr
   auto
   partition_B(BTensor&& btensor) const
   {
-    auto thr_tensor = make_tensor(std::forward<BTensor>(btensor).data(), this->thrfrg_B(btensor.layout()));
+    auto thr_tensor = make_tensor(static_cast<BTensor&&>(btensor).data(), this->thrfrg_B(btensor.layout()));
 
     auto thr_vnk = make_coord(get<0>(thr_vmnk_), make_coord(get<2>(thr_vmnk_), get<3>(thr_vmnk_)));
     return thr_tensor(thr_vnk, make_coord(_, repeat<rank<1,1>(thr_tensor)>(_)));
   }
 
   template <class CTensor>
   CUTE_HOST_DEVICE constexpr
@@ -712,14 +711,15 @@
 CUTE_HOST_DEVICE
 void
 print(MMA_Atom<MMA_Traits<Args...>> const&)
 {
   using Atom = MMA_Atom<MMA_Traits<Args...>>;
   print("MMA_Atom\n");
   print("  ThrID:      "); print(typename Atom::ThrID{});      print("\n");
+  print("  Shape_MNK:  "); print(typename Atom::Shape_MNK{});  print("\n");
   print("  LayoutA_TV: "); print(typename Atom::LayoutA_TV{}); print("\n");
   print("  LayoutB_TV: "); print(typename Atom::LayoutB_TV{}); print("\n");
   print("  LayoutC_TV: "); print(typename Atom::LayoutC_TV{}); print("\n");
 }
 
 template <class Atom, class TiledThr, class TiledPerm>
 CUTE_HOST_DEVICE
@@ -740,15 +740,23 @@
   print("ThrMMA\n");
   print("  Thr VMNK: "); print(thr_mma.thr_vmnk_); print("\n");
   print(static_cast<TiledMMA>(thr_mma));
 }
 
 template <class... Args>
 CUTE_HOST_DEVICE
-auto
+void
+print_latex(MMA_Atom<Args...> const& mma_atom)
+{
+  print_latex(make_tiled_mma(mma_atom));
+}
+
+template <class... Args>
+CUTE_HOST_DEVICE
+void
 print_latex(TiledMMA<Args...> const& mma)
 {
   auto layout_and_thrid_C = mma.get_layoutC_MN();
   auto layoutC_MN = get<0>(layout_and_thrid_C);
   auto thrID_C    = get<1>(layout_and_thrid_C);
 
   auto layout_and_thrid_A = mma.get_layoutA_MK();
@@ -760,15 +768,15 @@
   auto thrID_B    = get<1>(layout_and_thrid_B);
 
   print_latex_mma(layoutC_MN, thrID_C,
                   layoutA_MK, thrID_A,
                   layoutB_NK, thrID_B);
 }
 
-// MNK MMA Layout to console printer -- 8-value color coded by thread
+// MNK MMA Layout to console printer
 template <class LayoutC, class ThrIDC,
           class LayoutA, class ThrIDA,
           class LayoutB, class ThrIDB>
 CUTE_HOST_DEVICE
 void
 print_layout_mma(LayoutC const& C, ThrIDC const& TC,  // (m,n) -> (tid,vid)  and  tid -> thr_idx
                  LayoutA const& A, ThrIDA const& TA,  // (m,k) -> (tid,vid)  and  tid -> thr_idx
```

## cutlass_library/source/include/cute/atom/mma_traits.hpp

```diff
@@ -145,47 +145,47 @@
     // assert((void*)&C == (void*)&D);
 
     Tensor rC = recast<RegTypeC>(D);  // NOTE: D and C are same, so use mutable D
 
     //CUTE_STATIC_ASSERT_V(size(rC) == Int<RegNumC>{});
 
     if constexpr (detail::supports_output_scaling<MMATraits>::value) {
-      detail::explode_with_d_scaling(MMA_Op::fma,
-            rA, make_int_sequence<RegNumA>{},
-            rB, make_int_sequence<RegNumB>{},
-            rC, make_int_sequence<RegNumC>{},
-            traits.accumulate_);
+      detail::explode(MMA_Op::fma,
+                      rA, make_int_sequence<RegNumA>{},
+                      rB, make_int_sequence<RegNumB>{},
+                      rC, make_int_sequence<RegNumC>{},
+                      &(traits.accumulate_), seq<0>{});
     }
     else {
       detail::explode(MMA_Op::fma,
-                  rA, make_int_sequence<RegNumA>{},
-                  rB, make_int_sequence<RegNumB>{},
-                  rC, make_int_sequence<RegNumC>{});
+                      rA, make_int_sequence<RegNumA>{},
+                      rB, make_int_sequence<RegNumB>{},
+                      rC, make_int_sequence<RegNumC>{});
     }
   }
   else {
       Tensor rD = recast<RegTypeD>(D);
       Tensor rC = recast<RegTypeC>(C);
 
       CUTE_STATIC_ASSERT_V(size(rD) == Int<RegNumD>{});
       CUTE_STATIC_ASSERT_V(size(rC) == Int<RegNumC>{});
       if constexpr (detail::supports_output_scaling<MMATraits>::value) {
-        detail::explode_with_d_scaling(MMA_Op::fma,
+        detail::explode(MMA_Op::fma,
                         rD, make_int_sequence<RegNumD>{},
                         rA, make_int_sequence<RegNumA>{},
                         rB, make_int_sequence<RegNumB>{},
                         rC, make_int_sequence<RegNumC>{},
-                        traits.accumulate_);
+                        &(traits.accumulate_), seq<0>{});
       }
       else {
         detail::explode(MMA_Op::fma,
-                  rD, make_int_sequence<RegNumD>{},
-                  rA, make_int_sequence<RegNumA>{},
-                  rB, make_int_sequence<RegNumB>{},
-                  rC, make_int_sequence<RegNumC>{});
+                        rD, make_int_sequence<RegNumD>{},
+                        rA, make_int_sequence<RegNumA>{},
+                        rB, make_int_sequence<RegNumB>{},
+                        rC, make_int_sequence<RegNumC>{});
       }
   }
 }
 
 //
 // Accept mutable temporaries
 //
@@ -194,15 +194,15 @@
           class TD, class DLayout,
           class TA, class ALayout,
           class TB, class BLayout,
           class TC, class CLayout>
 CUTE_HOST_DEVICE constexpr
 void
 mma_unpack(MMA_Traits<MMA_Op, MMA_Args...> const& traits,
-           Tensor<TD, DLayout>      && D,
+           Tensor<TD, DLayout>     &&  D,
            Tensor<TA, ALayout> const&  A,
            Tensor<TB, BLayout> const&  B,
            Tensor<TC, CLayout> const&  C)
 {
   mma_unpack(traits, D, A, B, C);
 }
```

## cutlass_library/source/include/cute/atom/mma_traits_sm80.hpp

```diff
@@ -28,20 +28,16 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
 #include <cute/arch/mma_sm80.hpp>
 #include <cute/atom/mma_traits.hpp>
-
 #include <cute/layout.hpp>
-
-#include <cute/numeric/integer_subbyte.hpp>
-
-#include <cutlass/numeric_types.h>
+#include <cute/numeric/numeric_types.hpp>
 
 namespace cute
 {
 
 namespace {
 
 // (T32,V1) -> (M8,N8)
```

## cutlass_library/source/include/cute/atom/mma_traits_sm90_gmma.hpp

```diff
@@ -204,15 +204,15 @@
 
   // Layout type
   constexpr GMMA::LayoutType LAYOUT_TYPE = GMMA::layout_type(u128_tensor);
   desc.bitfield.layout_type_ = uint8_t(LAYOUT_TYPE);
 
   // Start address (4LSB not included)
   uint32_t start_address = cast_smem_ptr_to_uint(raw_pointer_cast(u128_tensor.data()));
-  desc.bitfield.start_address_ = start_address >> 4;
+  desc.bitfield.start_address_ = static_cast<uint16_t>(start_address >> 4);
 
   constexpr uint8_t base_offset = 0;
   desc.bitfield.base_offset_ = base_offset;
 
   // LayoutType meta
   constexpr int W = LAYOUT_TYPE == GMMA::LayoutType::INTERLEAVE ? 1 :
                     LAYOUT_TYPE == GMMA::LayoutType::B32        ? 2 :
```

## cutlass_library/source/include/cute/container/alignment.hpp

```diff
@@ -28,15 +28,15 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
 #include <cute/config.hpp>
 
-#include <cute/numeric/int.hpp>
+#include <cute/numeric/numeric_types.hpp>
 #include <cute/numeric/math.hpp>
 
 namespace cute
 {
 
 // Test if a pointer is aligned to N bytes
 template <int N>
```

## cutlass_library/source/include/cute/container/array.hpp

```diff
@@ -351,37 +351,37 @@
 template <class T, size_t N>
 CUTE_HOST_DEVICE constexpr
 void clear(array<T,N>& a)
 {
   a.fill(T(0));
 }
 
-template <typename T, size_t N>
+template <class T, size_t N>
 CUTE_HOST_DEVICE constexpr
 void fill(array<T,N>& a, T const& value)
 {
   a.fill(value);
 }
 
 template <class T, size_t N>
 CUTE_HOST_DEVICE constexpr
 void swap(array<T,N>& a, array<T,N>& b)
 {
   a.swap(b);
 }
 
 /// @return A cute::array of the elements of @c t in reverse order.
-template <typename T, size_t N>
-CUTE_HOST_DEVICE constexpr cute::array<T, N>
-reverse(cute::array<T, N> const& t) {
+template <class T, size_t N>
+CUTE_HOST_DEVICE constexpr
+cute::array<T,N> reverse(cute::array<T,N> const& t) 
+{
   if constexpr (N == 0u) {
     return t;
-  }
-  else {
-    cute::array<T, N> t_r{};
+  } else {
+    cute::array<T,N> t_r{};
     for (size_t k = 0; k < N; ++k) {
       t_r[k] = t[N - k - 1];
     }
     return t_r;
   }
 }
 
@@ -418,15 +418,15 @@
 }
 
 template <size_t I, class T, size_t N>
 CUTE_HOST_DEVICE constexpr
 T&& get(array<T,N>&& a)
 {
   static_assert(I < N, "Index out of range");
-  return std::move(a[I]);
+  return cute::move(a[I]);
 }
 
 } // end namespace cute
 
 namespace CUTE_STL_NAMESPACE
 {
 
@@ -438,35 +438,35 @@
 template <size_t I, class T, size_t N>
 struct tuple_element<I, cute::array<T,N>>
 {
   using type = T;
 };
 
 template <class T, size_t N>
-struct tuple_size<const cute::array<T,N>>
+struct tuple_size<cute::array<T,N> const>
     : CUTE_STL_NAMESPACE::integral_constant<size_t, N>
 {};
 
 template <size_t I, class T, size_t N>
-struct tuple_element<I, const cute::array<T,N>>
+struct tuple_element<I, cute::array<T,N> const>
 {
   using type = T;
 };
 
 } // end namespace CUTE_STL_NAMESPACE
 
 #ifdef CUTE_STL_NAMESPACE_IS_CUDA_STD
 namespace std
 {
 
 #if defined(__CUDACC_RTC__)
 template <class... _Tp>
 struct tuple_size;
 
-template<size_t _Ip, class... _Tp>
+template <size_t _Ip, class... _Tp>
 struct tuple_element;
 #endif
 
 template <class T, size_t N>
 struct tuple_size<cute::array<T,N>>
     : CUTE_STL_NAMESPACE::integral_constant<size_t, N>
 {};
@@ -474,19 +474,19 @@
 template <size_t I, class T, size_t N>
 struct tuple_element<I, cute::array<T,N>>
 {
   using type = T;
 };
 
 template <class T, size_t N>
-struct tuple_size<const cute::array<T,N>>
+struct tuple_size<cute::array<T,N> const>
     : CUTE_STL_NAMESPACE::integral_constant<size_t, N>
 {};
 
 template <size_t I, class T, size_t N>
-struct tuple_element<I, const cute::array<T,N>>
+struct tuple_element<I, cute::array<T,N> const>
 {
   using type = T;
 };
 
 } // end namespace std
 #endif // CUTE_STL_NAMESPACE_IS_CUDA_STD
```

## cutlass_library/source/include/cute/container/array_subbyte.hpp

```diff
@@ -33,37 +33,28 @@
            in a packed storage.
 */
 
 #pragma once
 
 #include <cute/config.hpp>
 
-#include <cute/numeric/int.hpp>           // sizeof_bits
+#include <cute/numeric/numeric_types.hpp>
 #include <cute/numeric/integral_constant.hpp>
 
 namespace cute
 {
-
-template <class T>
-struct is_subbyte {
-  static constexpr bool value = sizeof_bits_v<T> < 8;
-};
-
-template <class T>
-constexpr bool is_subbyte_v = is_subbyte<T>::value;
-
 //
 // Underlying subbyte storage type
 //
 template <class T>
-using subbyte_storage_type_t = conditional_t<(sizeof_bits_v<T> <=   8), uint8_t,
-                               conditional_t<(sizeof_bits_v<T> <=  16), uint16_t,
-                               conditional_t<(sizeof_bits_v<T> <=  32), uint32_t,
-                               conditional_t<(sizeof_bits_v<T> <=  64), uint64_t,
-                               conditional_t<(sizeof_bits_v<T> <= 128), uint128_t,
+using subbyte_storage_type_t = conditional_t<(cute::sizeof_bits_v<T> <=   8), uint8_t,
+                               conditional_t<(cute::sizeof_bits_v<T> <=  16), uint16_t,
+                               conditional_t<(cute::sizeof_bits_v<T> <=  32), uint32_t,
+                               conditional_t<(cute::sizeof_bits_v<T> <=  64), uint64_t,
+                               conditional_t<(cute::sizeof_bits_v<T> <= 128), uint128_t,
                                T>>>>>;
 
 template <class T> struct subbyte_iterator;
 template <class, class> struct swizzle_ptr;
 
 //
 // subbyte_reference
@@ -179,14 +170,19 @@
   }
 
   // Extract to type element_type
   CUTE_HOST_DEVICE constexpr
   operator element_type() const {
     return get();
   }
+
+  // Address
+  subbyte_iterator<T> operator&() const {
+    return {ptr_, idx_};
+  }
 };
 
 //
 // subbyte_iterator
 //   Random-access iterator over subbyte references
 //
 template <class T>
@@ -310,24 +306,24 @@
   }
 
   // Conversion to NewT_ with possible loss of subbyte index
   template <class NewT_>
   CUTE_HOST_DEVICE constexpr friend
   auto recast_ptr(subbyte_iterator const& x) {
     using NewT = conditional_t<(is_const_v<T>), NewT_ const, NewT_>;
-    if constexpr (is_subbyte<NewT>::value) {       // Making subbyte_iter, preserve the subbyte idx
+    if constexpr (cute::is_subbyte_v<NewT>) {       // Making subbyte_iter, preserve the subbyte idx
       return subbyte_iterator<NewT>(x.ptr_, x.idx_);
     } else {                                       // Not subbyte, assume/assert subbyte idx 0
       return reinterpret_cast<NewT*>(raw_pointer_cast(x));
     }
     CUTE_GCC_UNREACHABLE;
   }
 
   CUTE_HOST_DEVICE friend void print(subbyte_iterator x) {
-    printf("subptr[%db](%p.%u)", int(sizeof_bits<T>::value), x.ptr_, x.idx_);
+    printf("subptr[%db](%p.%u)", int(sizeof_bits_v<T>), x.ptr_, x.idx_);
   }
 };
 
 //
 // array_subbyte
 //   Statically sized array for non-byte-aligned data types
 //
@@ -365,16 +361,16 @@
   static constexpr size_type StorageElements = (N * sizeof_bits_v<value_type> + sizeof_bits_v<storage_type> - 1) / sizeof_bits_v<storage_type>;
 
   // Internal storage
   storage_type storage[StorageElements];
 
 public:
 
-  CUTE_HOST_DEVICE constexpr
-  array_subbyte() {}
+  constexpr
+  array_subbyte() = default;
 
   CUTE_HOST_DEVICE constexpr
   array_subbyte(array_subbyte const& x) {
     CUTE_UNROLL
     for (size_type i = 0; i < StorageElements; ++i) {
       storage[i] = x.storage[i];
     }
@@ -558,15 +554,15 @@
 }
 
 template <size_t I, class T, size_t N>
 CUTE_HOST_DEVICE constexpr
 T&& get(array_subbyte<T,N>&& a)
 {
   static_assert(I < N, "Index out of range");
-  return std::move(a[I]);
+  return cute::move(a[I]);
 }
 
 } // end namespace cute
 
 namespace CUTE_STL_NAMESPACE
 {
 
@@ -604,15 +600,15 @@
 namespace std
 {
 
 #if defined(__CUDACC_RTC__)
 template <class... _Tp>
 struct tuple_size;
 
-template<size_t _Ip, class... _Tp>
+template <size_t _Ip, class... _Tp>
 struct tuple_element;
 #endif
 
 template <class T, size_t N>
 struct tuple_size<cute::array_subbyte<T,N>>
     : CUTE_STL_NAMESPACE::integral_constant<size_t, N>
 {};
```

## cutlass_library/source/include/cute/container/bit_field.hpp

```diff
@@ -33,15 +33,15 @@
            be used in unions to bit-wise define parameters.
 */
 
 #pragma once
 
 #include <cute/config.hpp>
 
-#include <cute/numeric/int.hpp>   // uint_bit_t
+#include <cute/numeric/numeric_types.hpp>   // uint_bit_t
 
 namespace cute
 {
 
 class dummy_type {};
 
 template <uint32_t BitStart, uint32_t NumBits, class OtherValueType = dummy_type>
```

## cutlass_library/source/include/cute/container/cuda_types.hpp

```diff
@@ -92,19 +92,19 @@
 #if ! defined(_MSC_VER)
 constexpr
 #endif
 uint32_t&& get(dim3&& a)
 {
   static_assert(I < 3, "Index out of range");
   if constexpr (I == 0) {
-    return std::move(a.x);
+    return cute::move(a.x);
   } else if constexpr (I == 1) {
-    return std::move(a.y);
+    return cute::move(a.y);
   } else if constexpr (I == 2) {
-    return std::move(a.z);
+    return cute::move(a.z);
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
 // Specialize cute::tuple-traits for external types
 template <>
@@ -158,19 +158,19 @@
 
 template <size_t I>
 CUTE_HOST_DEVICE constexpr
 uint32_t&& get(uint3&& a)
 {
   static_assert(I < 3, "Index out of range");
   if constexpr (I == 0) {
-    return std::move(a.x);
+    return cute::move(a.x);
   } else if constexpr (I == 1) {
-    return std::move(a.y);
+    return cute::move(a.y);
   } else if constexpr (I == 2) {
-    return std::move(a.z);
+    return cute::move(a.z);
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
 // Specialize cute::tuple-traits for external types
 template <>
```

## cutlass_library/source/include/cute/container/tuple.hpp

```diff
@@ -122,26 +122,22 @@
 
 template <size_t N, class T>
 CUTE_HOST_DEVICE constexpr T& getv(EBO<N, T, false>& x)
 { return x.t_; }
 
 template <size_t N, class T>
 CUTE_HOST_DEVICE constexpr T&& getv(EBO<N, T, false>&& x)
-{ return static_cast<T&&>(x.t_); }
+{ return cute::move(x.t_); }
 
 template <class IdxSeq, class... T>
 struct TupleBase;
 
-// Base class of cute::tuple.
-// It inherits from EBO<i, t> for each (i, t) in (I..., T...).
-// The actual storage (for nonempty t) lives in the base classes.
-// index_sequence is a way to wrap up a sequence of zero or more
-// compile-time integer values in a single type.
-// We only ever use index_sequence<0, 1, ..., sizeof...(T)> in practice,
-// as the type alias TupleBase below indicates.
+// Base class of cute::tuple binds each element to an index
+// by inheriting from EBO<i, t> for each (i, t) in (I..., T...).
+// The storage (for nonempty t) lives in the base classes.
 template <size_t... I, class... T>
 struct TupleBase<index_sequence<I...>, T...>
     : EBO<I,T>...
 {
   CUTE_HOST_DEVICE constexpr
   TupleBase() {}
 
@@ -165,19 +161,14 @@
 //using TupleBase = detail::TupleBase<make_index_sequence<sizeof...(T)>, T...>;
 
 // This is the actual cute::tuple class.
 // The storage (if any) lives in TupleBase's EBO base classes.
 //
 // Inheriting from the above alias TupleBase
 // causes MSVC 2022 build errors when assigning one tuple to another:
-//
-// illegal member initialization:
-// 'TupleBase< /* template arguments */ >' is not a base or member
-//
-// Not using the alias or any kind of alias fixed the errors.
 // In summary: this is verbose as a work-around for MSVC build errors.
 template <class... T>
 struct tuple : detail::TupleBase<make_index_sequence<sizeof...(T)>, T...>
 {
   CUTE_HOST_DEVICE constexpr
   tuple() {}
 
@@ -361,18 +352,18 @@
 auto
 tuple_cat(T0 const& t0, T1 const& t1, T2 const& t2, T3 const& t3, T4 const& t4,
           index_sequence<I0...>, index_sequence<I1...>, index_sequence<I2...>, index_sequence<I3...>, index_sequence<I4...>)
 {
   return cute::make_tuple(get<I0>(t0)..., get<I1>(t1)..., get<I2>(t2)..., get<I3>(t3)..., get<I4>(t4)...);
 }
 
-template<class T0, class T1>
+template <class T0, class T1>
 struct tuple_cat_static;
 
-template<class... T0s, class... T1s>
+template <class... T0s, class... T1s>
 struct tuple_cat_static<tuple<T0s...>, tuple<T1s...>> {
   using type = tuple<T0s..., T1s...>;
 };
 
 } // end namespace detail
 
 CUTE_HOST_DEVICE constexpr
@@ -626,31 +617,25 @@
 
 namespace detail {
 
 template <class Tuple, size_t... Is>
 CUTE_HOST_DEVICE void print_tuple(Tuple const& t,
                                   index_sequence<Is...>, char s = '(', char e = ')')
 {
-  using eat = int[];
   using cute::print;
-  (void) eat {(print(s), 0),
-              (print(Is == 0 ? "" : ","), print(get<Is>(t)), 0)...,
-              (print(e), 0)};
+  ((void(print(Is == 0 ? s : ',')), void(print(get<Is>(t)))), ...); print(e);
 }
 
 #if !defined(__CUDACC_RTC__)
 template <class Tuple, std::size_t... Is>
 CUTE_HOST std::ostream& print_tuple_os(std::ostream& os, Tuple const& t,
                                        index_sequence<Is...>, char s = '(', char e = ')')
 {
-  using eat = int[];
-  (void) eat {(void(os << s), 0),
-              (void(os << (Is == 0 ? "" : ",") << get<Is>(t)), 0)...,
-              (void(os << e), 0)};
-  return os;
+  (void(os << (Is == 0 ? s : ',') << get<Is>(t)), ...);
+  return os << e;
 }
 #endif // !defined(__CUDACC_RTC__)
 
 } // end namespace detail
 
 template <class Tuple,
           __CUTE_REQUIRES(is_tuple<Tuple>::value)>
@@ -703,15 +688,15 @@
 namespace std
 {
 
 #if defined(__CUDACC_RTC__)
 template <class... _Tp>
 struct tuple_size;
 
-template<size_t _Ip, class... _Tp>
+template <size_t _Ip, class... _Tp>
 struct tuple_element;
 #endif
 
 template <class... T>
 struct tuple_size<cute::tuple<T...>>
     : CUTE_STL_NAMESPACE::integral_constant<size_t, sizeof...(T)>
 {};
```

## cutlass_library/source/include/cute/container/type_list.hpp

```diff
@@ -104,15 +104,15 @@
 namespace std
 {
 
 #if defined(__CUDACC_RTC__)
 template <class... _Tp>
 struct tuple_size;
 
-template<size_t _Ip, class... _Tp>
+template <size_t _Ip, class... _Tp>
 struct tuple_element;
 #endif
 
 template <class... T>
 struct tuple_size<cute::type_list<T...>>
     : CUTE_STL_NAMESPACE::integral_constant<size_t, sizeof...(T)>
 {};
```

## cutlass_library/source/include/cute/numeric/arithmetic_tuple.hpp

```diff
@@ -59,40 +59,35 @@
   ArithmeticTuple(U const&... u)
     : tuple<T...>(u...) {}
 };
 
 template <class... T>
 struct is_tuple<ArithmeticTuple<T...>> : true_type {};
 
+template <class... Ts>
+struct is_flat<ArithmeticTuple<Ts...>> : is_flat<tuple<Ts...>> {};
+
 template <class... T>
 CUTE_HOST_DEVICE constexpr
 auto
 make_arithmetic_tuple(T const&... t) {
   return ArithmeticTuple<T...>(t...);
 }
 
-template <class... T>
+template <class T>
 CUTE_HOST_DEVICE constexpr
 auto
-as_arithmetic_tuple(tuple<T...> const& t) {
-  return ArithmeticTuple<T...>(t);
-}
-
-template <class T, __CUTE_REQUIRES(is_integral<T>::value)>
-CUTE_HOST_DEVICE constexpr
-T const&
 as_arithmetic_tuple(T const& t) {
-  return t;
-}
-
-template <class... T>
-CUTE_HOST_DEVICE constexpr
-auto
-as_arithmetic_tuple(ArithmeticTuple<T...> const& t) {
-  return t;
+  if constexpr (is_tuple<T>::value) {
+    return detail::tapply(t, [](auto const& x){ return as_arithmetic_tuple(x); },
+                          [](auto const&... a){ return make_arithmetic_tuple(a...); },
+                          tuple_seq<T>{});
+  } else {
+    return t;
+  }
 }
 
 //
 // Numeric operators
 //
 
 // Addition
@@ -104,43 +99,88 @@
   return transform_apply(append<R>(t,Int<0>{}), append<R>(u,Int<0>{}), plus{}, [](auto const&... a){ return make_arithmetic_tuple(a...); });
 }
 
 template <class... T, class... U>
 CUTE_HOST_DEVICE constexpr
 auto
 operator+(ArithmeticTuple<T...> const& t, tuple<U...> const& u) {
-  constexpr int R = cute::max(int(sizeof...(T)), int(sizeof...(U)));
-  return transform_apply(append<R>(t,Int<0>{}), append<R>(u,Int<0>{}), plus{}, [](auto const&... a){ return make_arithmetic_tuple(a...); });
+  return t + ArithmeticTuple<U...>(u);
 }
 
 template <class... T, class... U>
 CUTE_HOST_DEVICE constexpr
 auto
 operator+(tuple<T...> const& t, ArithmeticTuple<U...> const& u) {
+  return ArithmeticTuple<T...>(t) + u;
+}
+
+// Subtraction
+template <class... T, class... U>
+CUTE_HOST_DEVICE constexpr
+auto
+operator-(ArithmeticTuple<T...> const& t, ArithmeticTuple<U...> const& u) {
   constexpr int R = cute::max(int(sizeof...(T)), int(sizeof...(U)));
-  return transform_apply(append<R>(t,Int<0>{}), append<R>(u,Int<0>{}), plus{}, [](auto const&... a){ return make_arithmetic_tuple(a...); });
+  return transform_apply(append<R>(t,Int<0>{}), append<R>(u,Int<0>{}), minus{}, [](auto const&... a){ return make_arithmetic_tuple(a...); });
+}
+
+template <class... T, class... U>
+CUTE_HOST_DEVICE constexpr
+auto
+operator-(ArithmeticTuple<T...> const& t, tuple<U...> const& u) {
+  return t - ArithmeticTuple<U...>(u);
+}
+
+template <class... T, class... U>
+CUTE_HOST_DEVICE constexpr
+auto
+operator-(tuple<T...> const& t, ArithmeticTuple<U...> const& u) {
+  return ArithmeticTuple<T...>(t) - u;
+}
+
+// Negation
+template <class... T>
+CUTE_HOST_DEVICE constexpr
+auto
+operator-(ArithmeticTuple<T...> const& t) {
+  return transform_apply(t, negate{}, [](auto const&... a){ return make_arithmetic_tuple(a...); });
 }
 
 //
 // Special cases
 //
 
 template <auto t, class... U>
 CUTE_HOST_DEVICE constexpr
 ArithmeticTuple<U...> const&
 operator+(C<t>, ArithmeticTuple<U...> const& u) {
-  static_assert(t == 0, "Artihmetic tuple op+ error!");
+  static_assert(t == 0, "Arithmetic tuple op+ error!");
   return u;
 }
 
 template <class... T, auto u>
 CUTE_HOST_DEVICE constexpr
 ArithmeticTuple<T...> const&
 operator+(ArithmeticTuple<T...> const& t, C<u>) {
-  static_assert(u == 0, "Artihmetic tuple op+ error!");
+  static_assert(u == 0, "Arithmetic tuple op+ error!");
+  return t;
+}
+
+template <auto t, class... U>
+CUTE_HOST_DEVICE constexpr
+ArithmeticTuple<U...> const&
+operator-(C<t>, ArithmeticTuple<U...> const& u) {
+  static_assert(t == 0, "Arithmetic tuple op- error!");
+  return -u;
+}
+
+template <class... T, auto u>
+CUTE_HOST_DEVICE constexpr
+ArithmeticTuple<T...> const&
+operator-(ArithmeticTuple<T...> const& t, C<u>) {
+  static_assert(u == 0, "Arithmetic tuple op- error!");
   return t;
 }
 
 //
 // ArithmeticTupleIterator
 //
 
@@ -237,14 +277,34 @@
     return t;
   }
   CUTE_GCC_UNREACHABLE;
 }
 
 namespace detail {
 
+template <class T, int... I>
+CUTE_HOST_DEVICE constexpr
+auto
+to_atuple_i(T const& t, seq<I...>) {
+  return make_arithmetic_tuple((void(I),Int<0>{})..., t);
+}
+
+} // end namespace detail
+
+// Turn a ScaledBases<T,N> into a rank-N+1 ArithmeticTuple
+//    with N prefix 0s:  (_0,_0,...N...,_0,T)
+template <class T, int N>
+CUTE_HOST_DEVICE constexpr
+auto
+as_arithmetic_tuple(ScaledBasis<T,N> const& t) {
+  return detail::to_atuple_i(as_arithmetic_tuple(t.value()), make_seq<N>{});
+}
+
+namespace detail {
+
 template <int... Ns>
 struct Basis;
 
 template <>
 struct Basis<> {
   using type = Int<1>;
 };
@@ -263,88 +323,22 @@
 // E<0,0> := ((_1,_0,_0,...),_0,_0,...)
 // E<0,1> := ((_0,_1,_0,...),_0,_0,...)
 // E<1,0> := (_0,(_1,_0,_0,...),_0,...)
 // E<1,1> := (_0,(_0,_1,_0,...),_0,...)
 template <int... N>
 using E = typename detail::Basis<N...>::type;
 
-namespace detail {
-
-template <class T, int... I, int... J>
-CUTE_HOST_DEVICE constexpr
-auto
-as_arithmetic_tuple(T const& t, seq<I...>, seq<J...>) {
-  return make_arithmetic_tuple((void(I),Int<0>{})..., t, (void(J),Int<0>{})...);
-}
-
-template <class... T, int... I, int... J>
-CUTE_HOST_DEVICE constexpr
-auto
-as_arithmetic_tuple(ArithmeticTuple<T...> const& t, seq<I...>, seq<J...>) {
-  return make_arithmetic_tuple(get<I>(t)..., (void(J),Int<0>{})...);
-}
-
-} // end namespace detail
-
-// Turn a ScaledBases<T,N> into a rank-M ArithmeticTuple
-//    with N prefix 0s:  (_0,_0,...N...,_0,T,_0,...,_0,_0)
-template <int M, class T, int N>
-CUTE_HOST_DEVICE constexpr
-auto
-as_arithmetic_tuple(ScaledBasis<T,N> const& t) {
-  static_assert(M > N, "Mismatched ranks");
-  return detail::as_arithmetic_tuple(t.value(), make_seq<N>{}, make_seq<M-N-1>{});
-}
-
-// Turn a ScaledBases<T,N> into a rank-N ArithmeticTuple
-//    with N prefix 0s:  (_0,_0,...N...,_0,T)
-template <class T, int N>
-CUTE_HOST_DEVICE constexpr
-auto
-as_arithmetic_tuple(ScaledBasis<T,N> const& t) {
-  return as_arithmetic_tuple<N+1>(t);
-}
-
-// Turn an ArithmeticTuple into a rank-M ArithmeticTuple
-//    with postfix 0s:  (t0,t1,t2,...,_0,...,_0,_0)
-template <int M, class... T>
-CUTE_HOST_DEVICE constexpr
-auto
-as_arithmetic_tuple(ArithmeticTuple<T...> const& t) {
-  static_assert(M >= sizeof...(T), "Mismatched ranks");
-  return detail::as_arithmetic_tuple(t, make_seq<int(sizeof...(T))>{}, make_seq<M-int(sizeof...(T))>{});
-}
-
-template <class T, int M, class U>
-CUTE_HOST_DEVICE constexpr
-auto
-safe_div(ScaledBasis<T,M> const& b, U const& u)
-{
-  auto t = safe_div(b.value(), u);
-  return ScaledBasis<decltype(t),M>{t};
-}
-
-template <class T, int M, class U>
-CUTE_HOST_DEVICE constexpr
-auto
-shape_div(ScaledBasis<T,M> const& b, U const& u)
-{
-  auto t = shape_div(b.value(), u);
-  return ScaledBasis<decltype(t),M>{t};
-}
-
 template <class Shape>
 CUTE_HOST_DEVICE constexpr
 auto
 make_basis_like(Shape const& shape)
 {
   if constexpr (is_integral<Shape>::value) {
     return Int<1>{};
-  }
-  else {
+  } else {
     // Generate bases for each rank of shape
     return transform(tuple_seq<Shape>{}, shape, [](auto I, auto si) {
       // Generate bases for each rank of si and add an i on front
       using I_type = decltype(I);
       return transform_leaf(make_basis_like(si), [](auto e) {
         // MSVC has trouble capturing variables as constexpr,
         // so that they can be used as template arguments.
@@ -356,14 +350,36 @@
       });
     });
   }
 
   CUTE_GCC_UNREACHABLE;
 }
 
+//
+// Arithmetic
+//
+
+template <class T, int M, class U>
+CUTE_HOST_DEVICE constexpr
+auto
+safe_div(ScaledBasis<T,M> const& b, U const& u)
+{
+  auto t = safe_div(b.value(), u);
+  return ScaledBasis<decltype(t),M>{t};
+}
+
+template <class T, int M, class U>
+CUTE_HOST_DEVICE constexpr
+auto
+shape_div(ScaledBasis<T,M> const& b, U const& u)
+{
+  auto t = shape_div(b.value(), u);
+  return ScaledBasis<decltype(t),M>{t};
+}
+
 // Equality
 template <class T, int N, class U, int M>
 CUTE_HOST_DEVICE constexpr
 auto
 operator==(ScaledBasis<T,N> const& t, ScaledBasis<U,M> const& u) {
   return bool_constant<M == N>{} && t.value() == u.value();
 }
@@ -380,77 +396,58 @@
 CUTE_HOST_DEVICE constexpr
 false_type
 operator==(T const&, ScaledBasis<U,M> const&) {
   return {};
 }
 
 // Abs
-template <int N, class T>
+template <class T, int N>
 CUTE_HOST_DEVICE constexpr
 auto
 abs(ScaledBasis<T,N> const& e) {
   return ScaledBasis<decltype(abs(e.value())),N>{abs(e.value())};
 }
 
 // Multiplication
-template <class A, int N, class T>
+template <class A, class T, int N>
 CUTE_HOST_DEVICE constexpr
 auto
 operator*(A const& a, ScaledBasis<T,N> const& e) {
   auto r = a * e.value();
   return ScaledBasis<decltype(r),N>{r};
 }
 
-template <int N, class T, class B>
+template <class T, int N, class B>
 CUTE_HOST_DEVICE constexpr
 auto
 operator*(ScaledBasis<T,N> const& e, B const& b) {
   auto r = e.value() * b;
   return ScaledBasis<decltype(r),N>{r};
 }
 
 // Addition
-template <int N, class T, class... U>
-CUTE_HOST_DEVICE constexpr
-auto
-operator+(ScaledBasis<T,N> const& t, ArithmeticTuple<U...> const& u) {
-  constexpr int R = cute::max(N+1, int(sizeof...(U)));
-  return as_arithmetic_tuple<R>(t) + as_arithmetic_tuple<R>(u);
-}
-
-template <class... T, int M, class U>
-CUTE_HOST_DEVICE constexpr
-auto
-operator+(ArithmeticTuple<T...> const& t, ScaledBasis<U,M> const& u) {
-  constexpr int R = cute::max(int(sizeof...(T)), M+1);
-  return as_arithmetic_tuple<R>(t) + as_arithmetic_tuple<R>(u);
-}
-
-template <int N, class T, class... U>
+template <class T, int N, class U, int M>
 CUTE_HOST_DEVICE constexpr
 auto
-operator+(ScaledBasis<T,N> const& t, tuple<U...> const& u) {
-  constexpr int R = cute::max(N+1, int(sizeof...(U)));
-  return as_arithmetic_tuple<R>(t) + as_arithmetic_tuple(u);
+operator+(ScaledBasis<T,N> const& t, ScaledBasis<U,M> const& u) {
+  return as_arithmetic_tuple(t) + as_arithmetic_tuple(u);
 }
 
-template <class... T, int M, class U>
+template <class T, int N, class... U>
 CUTE_HOST_DEVICE constexpr
 auto
-operator+(tuple<T...> const& t, ScaledBasis<U,M> const& u) {
-  constexpr int R = cute::max(int(sizeof...(T)), M+1);
-  return as_arithmetic_tuple(t) + as_arithmetic_tuple<R>(u);
+operator+(ScaledBasis<T,N> const& t, ArithmeticTuple<U...> const& u) {
+  return as_arithmetic_tuple(t) + u;
 }
 
-template <int N, class T, int M, class U>
+template <class... T, class U, int M>
 CUTE_HOST_DEVICE constexpr
 auto
-operator+(ScaledBasis<T,N> const& t, ScaledBasis<U,M> const& u) {
-  constexpr int R = cute::max(N+1,M+1);
-  return as_arithmetic_tuple<R>(t) + as_arithmetic_tuple<R>(u);
+operator+(ArithmeticTuple<T...> const& t, ScaledBasis<U,M> const& u) {
+  return t + as_arithmetic_tuple(u);
 }
 
 template <auto t, class U, int M>
 CUTE_HOST_DEVICE constexpr
 auto
 operator+(C<t>, ScaledBasis<U,M> const& u) {
   static_assert(t == 0, "ScaledBasis op+ error!");
@@ -527,15 +524,15 @@
 namespace std
 {
 
 #if defined(__CUDACC_RTC__)
 template <class... _Tp>
 struct tuple_size;
 
-template<size_t _Ip, class... _Tp>
+template <size_t _Ip, class... _Tp>
 struct tuple_element;
 #endif
 
 template <class... T>
 struct tuple_size<cute::ArithmeticTuple<T...>>
   : CUTE_STL_NAMESPACE::integral_constant<size_t, sizeof...(T)>
 {};
```

## cutlass_library/source/include/cute/numeric/complex.hpp

```diff
@@ -26,41 +26,44 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
-#include <cute/config.hpp>
-#include <cute/util/type_traits.hpp>
 #include <cutlass/complex.h>
+#include <cute/util/type_traits.hpp>
+#include <cute/numeric/numeric_types.hpp>
 
 namespace cute
 {
 
 using cutlass::complex;
 using cutlass::is_complex;
 using cutlass::RealType;
 using cutlass::real;
 using cutlass::imag;
 using cutlass::conj;
 
+template <class T>
+static constexpr auto is_complex_v = is_complex<T>::value;
+
 /// Fused multiply-add for complex numbers
 template <class T>
 CUTE_HOST_DEVICE constexpr
 void
 fma(complex<T>      & d,
     complex<T> const& a,
     complex<T> const& b,
     complex<T> const& c)
 {
-  d.real(fma( a.real(), b.real(), c.real()));
-  d.imag(fma( a.real(), b.imag(), c.imag()));
-  d.real(fma(-a.imag(), b.imag(), d.real()));
-  d.imag(fma( a.imag(), b.real(), d.imag()));
+  fma(d.real(),  a.real(), b.real(), c.real());
+  fma(d.imag(),  a.real(), b.imag(), c.imag());
+  fma(d.real(), -a.imag(), b.imag(), d.real());
+  fma(d.imag(),  a.imag(), b.real(), d.imag());
 }
 
 /// Fused multiply-add for triplets
 template <class T>
 CUTE_HOST_DEVICE constexpr
 void
 fma(complex<T> const& a,
```

## cutlass_library/source/include/cute/numeric/int.hpp

```diff
@@ -32,34 +32,33 @@
 
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/cstdint>
 #else
 #include <cstdint>
 #endif
 
-#include <cute/numeric/integer_subbyte.hpp>
-#include <cute/numeric/uint128.hpp>
+#include <cutlass/numeric_types.h>
 
 namespace cute
 {
 
 //
 // Signed integers
 //
 
-using int2_t  = cute::int2b_t;
-using int4_t  = cute::int4b_t;
-using int8_t  = CUTE_STL_NAMESPACE::int8_t;
-using int16_t = CUTE_STL_NAMESPACE::int16_t;
-using int32_t = CUTE_STL_NAMESPACE::int32_t;
-using int64_t = CUTE_STL_NAMESPACE::int64_t;
+using int2_t  = cutlass::int2b_t;
+using int4_t  = cutlass::int4b_t;
+using CUTE_STL_NAMESPACE::int8_t;
+using CUTE_STL_NAMESPACE::int16_t;
+using CUTE_STL_NAMESPACE::int32_t;
+using CUTE_STL_NAMESPACE::int64_t;
 
 template <int N> struct int_bit;
-template <> struct int_bit<  2>  { using type = cute::int2b_t; };
-template <> struct int_bit<  4>  { using type = cute::int4b_t; };
+template <> struct int_bit<  2>  { using type = cutlass::int2b_t; };
+template <> struct int_bit<  4>  { using type = cutlass::int4b_t; };
 template <> struct int_bit<  8>  { using type = int8_t;  };
 template <> struct int_bit< 16>  { using type = int16_t; };
 template <> struct int_bit< 32>  { using type = int32_t; };
 template <> struct int_bit< 64>  { using type = int64_t; };
 
 template <int N>
 using int_bit_t = typename int_bit<N>::type;
@@ -70,82 +69,36 @@
 template <int N>
 using int_byte_t = typename int_byte<N>::type;
 
 //
 // Unsigned integers
 //
 
-using uint1_t   = cute::uint1b_t;
-using uint2_t   = cute::uint2b_t;
-using uint4_t   = cute::uint4b_t;
-using uint8_t   = CUTE_STL_NAMESPACE::uint8_t;
-using uint16_t  = CUTE_STL_NAMESPACE::uint16_t;
-using uint32_t  = CUTE_STL_NAMESPACE::uint32_t;
-using uint64_t  = CUTE_STL_NAMESPACE::uint64_t;
-using uint128_t = cute::uint128_t;
+using uint1_t   = cutlass::uint1b_t;
+using uint2_t   = cutlass::uint2b_t;
+using uint4_t   = cutlass::uint4b_t;
+using CUTE_STL_NAMESPACE::uint8_t;
+using CUTE_STL_NAMESPACE::uint16_t;
+using CUTE_STL_NAMESPACE::uint32_t;
+using CUTE_STL_NAMESPACE::uint64_t;
+using cutlass::uint128_t;
 
 template <int N> struct uint_bit;
-template <> struct uint_bit<  1> { using type = cute::uint1b_t; };
-template <> struct uint_bit<  2> { using type = cute::uint2b_t; };
-template <> struct uint_bit<  4> { using type = cute::uint4b_t; };
+template <> struct uint_bit<  1> { using type = cutlass::uint1b_t; };
+template <> struct uint_bit<  2> { using type = cutlass::uint2b_t; };
+template <> struct uint_bit<  4> { using type = cutlass::uint4b_t; };
 template <> struct uint_bit<  8> { using type = uint8_t;  };
 template <> struct uint_bit< 16> { using type = uint16_t; };
 template <> struct uint_bit< 32> { using type = uint32_t; };
 template <> struct uint_bit< 64> { using type = uint64_t; };
-template <> struct uint_bit<128> { using type = cute::uint128_t; };
+template <> struct uint_bit<128> { using type = cutlass::uint128_t; };
 
 template <int N>
 using uint_bit_t = typename uint_bit<N>::type;
 
 template <int N>
 using uint_byte = uint_bit<8*N>;
 
 template <int N>
 using uint_byte_t = typename uint_byte<N>::type;
 
-//
-// sizeof_bytes
-//
-
-template <class T>
-struct sizeof_bytes {
-  static constexpr size_t value = sizeof(T);
-};
-template <class T>
-static constexpr int sizeof_bytes_v = sizeof_bytes<T>::value;
-
-//
-// sizeof_bits
-//
-
-template <class T>
-struct sizeof_bits {
-  static constexpr size_t value = sizeof(T) * 8;
-};
-
-template <class T>
-struct sizeof_bits<T const>: sizeof_bits<T> {};
-
-template <>
-struct sizeof_bits<void> {
-  static constexpr size_t value = 0;
-};
-
-template <>
-struct sizeof_bits<bool> {
-  static constexpr size_t value = 1;
-};
-
-template <int Bits, bool Signed>
-struct sizeof_bits<integer_subbyte<Bits,Signed>> {
-  static constexpr size_t value = Bits;
-};
-
-template <int Bits, bool Signed>
-struct sizeof_bits<cutlass::integer_subbyte<Bits,Signed>> {
-  static constexpr size_t value = Bits;
-};
-
-template <class T>
-static constexpr int sizeof_bits_v = sizeof_bits<T>::value;
-
 } // namespace cute
```

## cutlass_library/source/include/cute/numeric/integral_constant.hpp

```diff
@@ -439,8 +439,39 @@
 #if !defined(__CUDACC_RTC__)
 template <auto t>
 CUTE_HOST std::ostream& operator<<(std::ostream& os, C<t> const&) {
   return os << "_" << t;
 }
 #endif
 
+
+namespace detail {
+
+// parse_int_digits takes a variadic number of digits and converts them into an int
+template <class... Ts>
+constexpr uint64_t parse_int_digits(uint64_t result, int digit, Ts... digits)
+{
+  if constexpr (sizeof...(Ts) == 0) {
+    return 10 * result + digit;
+  } else {
+    return parse_int_digits(10 * result + digit, digits...);
+  }
+}
+
+} // end namespace detail
+
+
+// This user-defined literal operator allows cute::constant written as literals. For example,
+//
+//    auto var = 32_c;
+//
+//  var has type cute::constant<int,32>.
+//
+template <char... digits>
+constexpr cute::constant<int,detail::parse_int_digits(0, (digits - '0')...)> operator "" _c()
+{
+  static_assert((('0' <= digits && digits <= '9') && ...),
+                "Expected 0 <= digit <= 9 for each digit of the integer.");
+  return {};
+}
+
 } // end namespace cute
```

## cutlass_library/source/include/cute/numeric/integral_ratio.hpp

```diff
@@ -126,14 +126,18 @@
 template <auto a, auto b, auto c, auto d>
 CUTE_HOST_DEVICE constexpr
 R<a*d,b*c>
 nratio(R<a,b>, R<c,d>) {
   return {};
 }
 
+//
+// Operators
+//
+
 template <auto a, auto b, auto x, auto y>
 CUTE_HOST_DEVICE constexpr
 typename R<a*x,b*y>::type
 operator*(R<a,b>, R<x,y>) {
   return {};
 }
 
@@ -223,22 +227,22 @@
 typename R<abs(a),abs(b)>::type
 abs(R<a,b>) {
   return {};
 }
 
 template <auto a, auto b>
 CUTE_HOST_DEVICE constexpr
-auto
+int32_t
 log_2(R<a,b>) {
   static_assert(R<a,b>::num > 0);
   static_assert(R<a,b>::den > 0);
   return log_2(static_cast<uint32_t>(R<a,b>::num)) - log_2(static_cast<uint32_t>(R<a,b>::den));
 }
 
-
+// @return A non-reduced ratio cute::R of the Trait0::value / Trait1::value
 template <class Trait0, class Trait1>
 CUTE_HOST_DEVICE constexpr
 auto
 trait_ratio(Trait0, Trait1) {
   return nratio(static_value<Trait0>(), static_value<Trait1>());
 }
```

## cutlass_library/source/include/cute/numeric/math.hpp

```diff
@@ -312,15 +312,15 @@
 
 /**
  * log2 computation
  */
 
 template <class T>
 CUTE_HOST_DEVICE constexpr
-auto
+int32_t
 log_2(T x) {
   assert(x > 0);
   static_assert(is_unsigned<T>::value, "Only to be used for unsigned integral types.");
-  return bit_width(x) - 1;
+  return static_cast<int32_t>(bit_width(x)) - 1;
 }
 
 } // namespace cute
```

## cutlass_library/source/include/cute/util/type_traits.hpp

```diff
@@ -97,14 +97,17 @@
 
 using CUTE_STL_NAMESPACE::is_lvalue_reference;
 using CUTE_STL_NAMESPACE::is_lvalue_reference_v;
 
 using CUTE_STL_NAMESPACE::is_reference;
 using CUTE_STL_NAMESPACE::is_trivially_copyable;
 
+using CUTE_STL_NAMESPACE::is_convertible;
+using CUTE_STL_NAMESPACE::is_convertible_v;
+
 using CUTE_STL_NAMESPACE::is_same;
 using CUTE_STL_NAMESPACE::is_same_v;
 
 using CUTE_STL_NAMESPACE::is_arithmetic;
 using CUTE_STL_NAMESPACE::is_unsigned;
 using CUTE_STL_NAMESPACE::is_unsigned_v;
 using CUTE_STL_NAMESPACE::is_signed;
@@ -127,30 +130,29 @@
 
 using CUTE_STL_NAMESPACE::remove_pointer;
 using CUTE_STL_NAMESPACE::remove_pointer_t;
 
 // <utility>
 using CUTE_STL_NAMESPACE::declval;
 
-template< class T >
+template <class T>
 constexpr T&& forward(remove_reference_t<T>& t) noexcept
 {
   return static_cast<T&&>(t);
 }
 
-template< class T >
+template <class T>
 constexpr T&& forward(remove_reference_t<T>&& t) noexcept
 {
-  static_assert(! is_lvalue_reference_v<T>,
-    "T cannot be an lvalue reference (e.g., U&).");
+  static_assert(! is_lvalue_reference_v<T>, "T cannot be an lvalue reference (e.g., U&).");
   return static_cast<T&&>(t);
 }
 
-template< class T >
-constexpr remove_reference_t<T>&& move( T&& t ) noexcept
+template <class T>
+constexpr remove_reference_t<T>&& move(T&& t) noexcept
 {
   return static_cast<remove_reference_t<T>&&>(t);
 }
 
 // <limits>
 using CUTE_STL_NAMESPACE::numeric_limits;
 
@@ -244,8 +246,19 @@
 
 template <class F, class... Args>
 CUTE_HOST_DEVICE constexpr auto
 is_valid(F&&, Args&&...) {
   return detail::is_valid_impl<F&&, Args&&...>(int{});
 }
 
+template <bool B, template<class...> class True, template<class...> class False>
+struct conditional_template {
+  template <class... U>
+  using type = True<U...>;
+};
+
+template <template<class...> class True, template<class...> class False>
+struct conditional_template<false, True, False> {
+  template <class... U>
+  using type = False<U...>;
+};
 } // end namespace cute
```

## cutlass_library/source/include/cutlass/array.h

```diff
@@ -29,52 +29,36 @@
  *
  **************************************************************************************************/
 /*! \file
     \brief Statically sized array of elements that accommodates all CUTLASS-supported numeric types
            and is safe to use in a union.
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 #include "cutlass/cutlass.h"
 #include "cutlass/functional.h"
-#include "cutlass/numeric_size.h"
-#include "cutlass/half.h"
-#include "cutlass/integer_subbyte.h"
-#include "cutlass/tfloat32.h"
-#include "cutlass/bfloat16.h"
-#include "cutlass/half.h"
+#include "cutlass/numeric_types.h"
 namespace cutlass {
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Statically sized array for any data type
 template <
   typename T,
   int N,
   bool RegisterSized = sizeof_bits<T>::value >= 32
 >
-class Array;
+struct Array;
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Defines the size of an Array<> in bits
 template <typename T, int N, bool RegisterSized>
 struct sizeof_bits<Array<T, N, RegisterSized> > {
-  static int const value =
-    int(sizeof(typename Array<T, N, RegisterSized>::Storage)) * 8 * int(Array<T, N, RegisterSized>::kStorageElements);
+  static constexpr int value = sizeof(Array<T, N, RegisterSized>) * 8;
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Returns true if the argument is a power of 2
 CUTLASS_HOST_DEVICE
 constexpr bool ispow2(unsigned x) {
@@ -92,29 +76,28 @@
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Statically sized array for any data type
 template <
   typename T,
   int N
 >
-class Array<T, N, true> {
-public:
+struct Array<T, N, true> {
 
   /// Storage type
   using Storage = T;
 
   /// Element type
   using Element = T;
 
   /// Number of storage elements
   //static std::size_t const kStorageElements = N;
-  static size_t const kStorageElements = N;
+  static constexpr size_t kStorageElements = N;
 
   /// Number of logical elements
-  static size_t const kElements = N;
+  static constexpr size_t kElements = N;
 
   //
   // C++ standard members
   //
 
   typedef T value_type;
   typedef size_t size_type;
@@ -348,34 +331,17 @@
 
     CUTLASS_HOST_DEVICE
     bool operator!=(const_iterator const &other) const {
       return ptr_ != other.ptr_;
     }
   };
 
-private:
-
   /// Internal storage
   Storage storage[kElements];
 
-public:
-
-  #if 0
-  CUTLASS_HOST_DEVICE
-  Array() { }
-
-  CUTLASS_HOST_DEVICE
-  Array(Array const &x) {
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kElements; ++i) {
-      storage[i] = x.storage[i];
-    }
-  }
-  #endif
-
   /// Efficient clear method
   CUTLASS_HOST_DEVICE
   void clear() {
     fill(T(0));
   }
 
   CUTLASS_HOST_DEVICE
@@ -453,15 +419,15 @@
   constexpr size_type max_size() const {
     return kElements;
   }
 
   CUTLASS_HOST_DEVICE
   void fill(T const &value) {
     CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kElements; ++i) {
+    for (int i = 0; i < int(kElements); ++i) {
       storage[i] = static_cast<Storage>(value);
     }
   }
 
   CUTLASS_HOST_DEVICE
   iterator begin() {
     return iterator(storage);
@@ -532,47 +498,33 @@
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 // Factories
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Element>
 CUTLASS_HOST_DEVICE
 Array<Element, 1> make_Array(Element x) {
-  Array<Element, 1> m;
-  m[0] = x;
-  return m;
+  return {x};
 }
 
 template <typename Element>
 CUTLASS_HOST_DEVICE
 Array<Element, 2> make_Array(Element x, Element y) {
-  Array<Element, 2> m;
-  m[0] = x;
-  m[1] = y;
-  return m;
+  return {x,y};
 }
 
 template <typename Element>
 CUTLASS_HOST_DEVICE
 Array<Element, 3> make_Array(Element x, Element y, Element z) {
-  Array<Element, 3> m;
-  m[0] = x;
-  m[1] = y;
-  m[2] = z;
-  return m;
+  return {x,y,z};
 }
 
 template <typename Element>
 CUTLASS_HOST_DEVICE
 Array<Element, 4> make_Array(Element x, Element y, Element z, Element w) {
-  Array<Element, 4> m;
-  m[0] = x;
-  m[1] = y;
-  m[2] = z;
-  m[3] = w;
-  return m;
+  return {x,y,z,w};
 }
 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 // functional.h numeric specializations
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -1088,14 +1040,84 @@
       result[i] = scalar_op(scalar, b[i], c[i]);
     }
 
     return result;
   }
 };
 
+/// Fused square-and-plus
+template <typename T, int N>
+struct square_and_plus<Array<T, N>> {
+
+  CUTLASS_HOST_DEVICE
+  Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
+    multiply_add<Array<T, N>, Array<T, N>, Array<T, N>> ma_op;
+    return ma_op(rhs, rhs, lhs);
+  }
+
+  CUTLASS_HOST_DEVICE
+  Array<T, N> operator()(Array<T, N> const &lhs, T const &rhs) const {
+    plus<Array<T, N>> plus_op;
+    multiplies<T> multiplies_op;
+    return plus_op(multiplies_op(rhs, rhs), lhs);
+  }
+};
+
+/// Inverse-square-root
+template <typename T, int N>
+struct inverse_square_root<Array<T, N>> {
+  CUTLASS_HOST_DEVICE
+  Array<T, N> operator()(Array<T, N> const &a) const {
+    Array<T, N> result;
+    inverse_square_root<T> scalar_op;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < N; ++i) {
+      result[i] = scalar_op(a[i]);
+    }
+    return result;
+  }
+};
+
+template <int N>
+struct inverse_square_root<Array<half_t, N>> {
+  CUTLASS_HOST_DEVICE
+  Array<half_t, N> operator()(Array<half_t, N> const & a) const {
+    Array<half_t, N> result;
+    #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
+
+    __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
+    __half2 const *a_ptr = reinterpret_cast<__half2 const *>(&a);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < N / 2; ++i) {
+      result_ptr[i] = h2rsqrt(a_ptr[i]);
+    }
+
+    if constexpr (N % 2) {
+      __half const *a_residual_ptr = reinterpret_cast<__half const *>(&a);
+      __half d_residual = hrsqrt(a_residual_ptr[N - 1]);
+      result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
+    }
+
+    #else
+
+    inverse_square_root<half_t> scalar_op;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < N; ++i) {
+      result[i] = scalar_op(a[i]);
+    }
+
+    #endif
+
+    return result;
+  }
+};
+
 /// Fused multiply-add-relu0
 template <typename T, int N>
 struct multiply_add_relu0<Array<T, N>, Array<T, N>, Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &a, Array<T, N> const &b, Array<T, N> const &c) const {
 
@@ -2497,28 +2519,41 @@
       result_data[i] = (a_data[i] ^ b_data[i]);
     }
 
     return result;
   }
 };
 
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 // Operator overloads
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename T, int N>
 CUTLASS_HOST_DEVICE
 Array<T, N> operator+(Array<T, N> const &lhs, Array<T, N> const &rhs) {
   plus<Array<T, N>> op;
   return op(lhs, rhs);
 }
 
 template <typename T, int N>
 CUTLASS_HOST_DEVICE
+Array<T, N> operator+(T const &lhs, Array<T, N> const &rhs) {
+  plus<Array<T, N>> op;
+  return op(lhs, rhs);
+}
+
+template <typename T, int N>
+CUTLASS_HOST_DEVICE
+Array<T, N> operator+(Array<T, N> const &lhs, T const &rhs) {
+  plus<Array<T, N>> op;
+  return op(lhs, rhs);
+}
+
+template <typename T, int N>
+CUTLASS_HOST_DEVICE
 Array<T, N> operator-(Array<T, N> const &lhs, Array<T, N> const &rhs) {
   minus<Array<T, N>> op;
   return op(lhs, rhs);
 }
 
 template <typename T, int N>
 CUTLASS_HOST_DEVICE
@@ -2605,15 +2640,15 @@
 /// Aligned array type
 template <
   /// Element type
   typename T,
   /// Number of elements in the array
   int N,
   /// Alignment requirement in bytes
-  int Alignment = sizeof_bits<T>::value * N / 8
+  int Alignment = ( sizeof_bits<T>::value * N + 7 ) / 8
 >
 class alignas(Alignment) AlignedArray: public Array<T, N> {
 public:
 
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/array_planar_complex.h

```diff
@@ -47,41 +47,27 @@
 template <typename Element_, int N>
 struct ArrayPlanarComplex {
 
   /// Underlying real element
   using Element = Element_;
 
   /// Number of logical elements
-  static size_t const kElements = N;
+  static constexpr size_t kElements = N;
 
   /// Underlying Fragment of real-valued elemenets
-  using ArrayReal = Array<Element, N>;
+  using ArrayReal = cutlass::Array<Element, N>;
 
 public:
-
   /// Fragment of real-valued elements representing the real part
   ArrayReal real;
 
   /// Fragment of real-valued elements representing the imaginary part
   ArrayReal imag;
 
 public:
-
-  /// Ctor
-  CUTLASS_HOST_DEVICE
-  ArrayPlanarComplex() { }
-
-  /// Ctor
-  CUTLASS_HOST_DEVICE
-  ArrayPlanarComplex(
-    ArrayReal const &real_,
-    ArrayReal const &imag_
-  ):
-    real(real_), imag(imag_) { }
-
   /// Sets the array to zero efficiently
   CUTLASS_HOST_DEVICE
   void clear() {
     real.clear();
     imag.clear();
   }
 };
@@ -89,15 +75,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Helper to deduce template arguments
 template <typename Element, int N>
 CUTLASS_HOST_DEVICE
 ArrayPlanarComplex<Element, N> 
 make_ArrayPlanarComplex(Array<Element, N> const &real, Array<Element, N> const &imag) {
-  return ArrayPlanarComplex<Element, N>(real, imag);
+  return ArrayPlanarComplex<Element, N>{real, imag};
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/array_subbyte.h

```diff
@@ -28,23 +28,14 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Statically sized array of elements that accommodates all CUTLASS-supported numeric types
            and is safe to use in a union.
 */
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
 #include "cutlass/platform/platform.h"
 
@@ -53,18 +44,16 @@
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Statically sized array for any data type
 template <
   typename T,
   int N
 >
-class Array<T, N, false> {
-public:
-
-  static int const kSizeBits = sizeof_bits<T>::value * N;
+struct Array<T, N, false> {
+  static constexpr int kSizeBits = sizeof_bits<T>::value * N;
 
   /// Storage type
   using Storage = typename platform::conditional<
     ((kSizeBits % 32) != 0),
     typename platform::conditional<
       ((kSizeBits % 16) != 0),
       uint8_t,
@@ -73,24 +62,24 @@
     uint32_t
   >::type;
 
   /// Element type
   using Element = T;
 
   /// Number of logical elements per stored object
-  static int const kElementsPerStoredItem = int(sizeof(Storage) * 8) / sizeof_bits<T>::value;
+  static constexpr int kElementsPerStoredItem = int(sizeof(Storage) * 8) / sizeof_bits<T>::value;
 
   /// Number of storage elements
-  static size_t const kStorageElements = (N + kElementsPerStoredItem - 1) / kElementsPerStoredItem;
+  static constexpr size_t kStorageElements = (N + kElementsPerStoredItem - 1) / kElementsPerStoredItem;
 
   /// Number of logical elements
-  static size_t const kElements = N;
+  static constexpr size_t kElements = N;
 
   /// Bitmask for covering one item
-  static Storage const kMask = ((Storage(1) << sizeof_bits<T>::value) - 1);
+  static constexpr Storage kMask = ((Storage(1) << sizeof_bits<T>::value) - 1);
 
   //
   // C++ standard members with pointer types removed
   //
 
   typedef T value_type;
   typedef size_t size_type;
@@ -101,37 +90,62 @@
   //
   // References
   //
 
   /// Reference object inserts or extracts sub-byte items
   class reference {
     /// Pointer to storage element
-    Storage *ptr_;
+    Storage *ptr_{nullptr};
 
     /// Index into elements packed into Storage object
-    int idx_;
+    int idx_{0};
 
   public:
 
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    reference(): ptr_(nullptr), idx_(0) { }
+    reference() = default;
 
     /// Ctor
     CUTLASS_HOST_DEVICE
     reference(Storage *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
 
     /// Assignment
     CUTLASS_HOST_DEVICE
     reference &operator=(T x) {
+    // `*ptr_ & kUpdateMask` will read ptr_ before write to it
+    // This means code pattern like
+    //
+    // ```cpp
+    // Array<half_t, N> result;
+    // result[0] = xxx;
+    // ```
+    // 
+    // Will leads to compiler warning on use of unintialized member variable. Although we know
+    //      this read of uninitialized member variable is harmeless.
+
+#if defined(__clang__)
+#  pragma clang diagnostic push
+#  pragma clang diagnostic ignored "-Wuninitialized"
+#elif defined(__GNUC__)
+#  pragma GCC diagnostic push
+#  pragma GCC diagnostic ignored "-Wuninitialized"
+#  pragma GCC diagnostic ignored "-Wmaybe-uninitialized"
+#endif
+
       Storage item = (reinterpret_cast<Storage const &>(x) & kMask);
 
       Storage kUpdateMask = Storage(~(kMask << (idx_ * sizeof_bits<T>::value)));
+
       *ptr_ = Storage(((*ptr_ & kUpdateMask) | (item << idx_ * sizeof_bits<T>::value)));
 
+#if defined(__clang__)
+#  pragma clang diagnostic pop
+#elif defined(__GNUC__)
+#  pragma GCC diagnostic pop
+#endif
+
       return *this;
     }
 
     CUTLASS_HOST_DEVICE
     T get() const {
       Storage item = Storage((*ptr_ >> (idx_ * sizeof_bits<T>::value)) & kMask);
       return reinterpret_cast<T const &>(item);
@@ -156,24 +170,22 @@
     }
   };
 
   /// Reference object extracts sub-byte items
   class const_reference {
 
     /// Pointer to storage element
-    Storage const *ptr_;
+    Storage const *ptr_{nullptr};
 
     /// Index into elements packed into Storage object
-    int idx_;
+    int idx_{0};
 
   public:
 
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    const_reference(): ptr_(nullptr), idx_(0) { }
+    const_reference() = default;
 
     /// Ctor
     CUTLASS_HOST_DEVICE
     const_reference(Storage const *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
 
     CUTLASS_HOST_DEVICE
     const T get() const {
@@ -205,23 +217,22 @@
   // Iterators
   //
 
   /// Bidirectional iterator over elements
   class iterator {
 
     /// Pointer to storage element
-    Storage *ptr_;
+    Storage *ptr_{nullptr};
 
     /// Index into elements packed into Storage object
-    int idx_;
+    int idx_{0};
 
   public:
 
-    CUTLASS_HOST_DEVICE
-    iterator(): ptr_(nullptr), idx_(0) { }
+    iterator() = default;
 
     CUTLASS_HOST_DEVICE
     iterator(Storage *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
 
     CUTLASS_HOST_DEVICE
     iterator &operator++() {
       ++idx_;
@@ -284,23 +295,22 @@
     }
   };
 
   /// Bidirectional constant iterator over elements
   class const_iterator {
 
     /// Pointer to storage element
-    Storage const *ptr_;
+    Storage const *ptr_{nullptr};
 
     /// Index into elements packed into Storage object
-    int idx_;
+    int idx_{0};
 
   public:
 
-    CUTLASS_HOST_DEVICE
-    const_iterator(): ptr_(nullptr), idx_(0) { }
+    const_iterator() = default;
 
     CUTLASS_HOST_DEVICE
     const_iterator(Storage const *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
 
     CUTLASS_HOST_DEVICE
     iterator &operator++() {
       ++idx_;
@@ -363,66 +373,44 @@
     }
   };
 
   /// Bidirectional iterator over elements
   class reverse_iterator {
 
     /// Pointer to storage element
-    Storage *ptr_;
+    Storage *ptr_{nullptr};
 
     /// Index into elements packed into Storage object
-    int idx_;
+    int idx_{0};
 
   public:
 
-    CUTLASS_HOST_DEVICE
-    reverse_iterator(): ptr_(nullptr), idx_(0) { }
+    reverse_iterator() = default;
 
     CUTLASS_HOST_DEVICE
     reverse_iterator(Storage *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
   };
 
   /// Bidirectional constant iterator over elements
   class const_reverse_iterator {
 
     /// Pointer to storage element
-    Storage const *ptr_;
+    Storage const *ptr_{nullptr};
 
     /// Index into elements packed into Storage object
-    int idx_;
+    int idx_{0};
 
   public:
 
-    CUTLASS_HOST_DEVICE
-    const_reverse_iterator(): ptr_(nullptr), idx_(0) { }
+    const_reverse_iterator() = default;
 
     CUTLASS_HOST_DEVICE
     const_reverse_iterator(Storage const *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
   };
 
-private:
-
-  /// Internal storage
-  Storage storage[kStorageElements];
-
-public:
-
-  #if 0
-  CUTLASS_HOST_DEVICE
-  Array() { }
-
-  CUTLASS_HOST_DEVICE
-  Array(Array const &x) {
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < int(kStorageElements); ++i) {
-      storage[i] = x.storage[i];
-    }
-  }
-  #endif
-
   /// Efficient clear method
   CUTLASS_HOST_DEVICE
   void clear() {
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < int(kStorageElements); ++i) {
       storage[i] = Storage(0);
@@ -485,15 +473,14 @@
   }
 
   CUTLASS_HOST_DEVICE
   Storage const * raw_data() const {
     return storage;
   }
 
-
   CUTLASS_HOST_DEVICE
   constexpr bool empty() const {
     return !kElements;
   }
 
   CUTLASS_HOST_DEVICE
   constexpr size_type size() const {
@@ -556,18 +543,17 @@
   }
 
   CUTLASS_HOST_DEVICE
   const_reverse_iterator crend() const {
     return const_reverse_iterator(storage);
   }
 
-  //
-  // Comparison operators
-  //
-
+private:
+  /// Internal storage
+  Storage storage[kStorageElements];
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/barrier.h

```diff
@@ -272,18 +272,16 @@
   arrive_range_inc(uint32_t idx, void *lock_ptr, int thread_idx, int first_flag_idx, int count = 1, int val = 1) {
     arrive_range_inc_helper(idx, lock_ptr, thread_idx, first_flag_idx, count, val, IntegerSequence{});
   }
 
 private:
   CUTLASS_DEVICE
   static void
-  check_barrier_in_range(uint32_t idx) {
-    if (idx >= MaxNumNamedBarriers) {
-      CUTE_RUNTIME_ASSERT("Index exceeds barrier count");
-    }
+  check_barrier_in_range([[maybe_unused]] uint32_t idx) {
+    assert((idx >= MaxNumNamedBarriers) && "Index exceeds barrier count");
   }
 
   template <uint32_t... Idx>
   CUTLASS_DEVICE
   static void
   wait_lt_helper(uint32_t idx, void *lock_ptr, int thread_idx, int flag_idx, int count, cute::integer_sequence<uint32_t, Idx...>) {
     check_barrier_in_range(idx);
```

## cutlass_library/source/include/cutlass/bfloat16.h

```diff
@@ -30,24 +30,14 @@
  **************************************************************************************************/
 /*!
     \file
     \brief Defines a proxy class for storing non-standard 16-bit floating point values with
           8 bits of exponent and 7 bit of mantissa.
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #if defined(__CUDACC_RTC__)
 #include "cutlass/floating_point_nvrtc.h"
 #else
 #include <cmath>
 #include <limits>
```

## cutlass_library/source/include/cutlass/cluster_launch.hpp

```diff
@@ -34,15 +34,14 @@
 */
 
 #pragma once
 
 #include <cuda_runtime_api.h>
 #include "cutlass/cutlass.h"
 #include "cutlass/trace.h"
-
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/type_traits>
 #else
 #include <type_traits>
 #include <cstdio>
 #endif
 
@@ -94,14 +93,31 @@
 #if defined(CUTLASS_SM90_CLUSTER_LAUNCH_ENABLED)
   init(void const* kernel_function)
 #else
   init(void const* /* kernel_function */)
 #endif
   {
 #if defined(CUTLASS_SM90_CLUSTER_LAUNCH_ENABLED)
+#if defined(CUTLASS_DEBUG_TRACE_LEVEL) && (CUTLASS_DEBUG_TRACE_LEVEL > 1)
+    if (kernel_function == nullptr) {
+      CUTLASS_TRACE_HOST("kernel_function is null");
+      return Status::kInvalid;
+    }
+    CUTLASS_TRACE_HOST("Checking previous error state before calling cudaFuncSetAttribute");
+    cudaError_t prevStatus = cudaGetLastError();
+    if (prevStatus != cudaSuccess) {
+      fprintf(stderr,
+              "[ ERROR: CUDA Runtime ] %s:%d: %s\n",
+              __FILE__,
+              __LINE__,
+              cudaGetErrorString(prevStatus));
+      return Status::kInvalid;
+    }
+    CUTLASS_TRACE_HOST("Calling cudaFuncSetAttribute");
+#endif
     // This attribute was added in CUDA 11.8.
     cudaError_t status =
         cudaFuncSetAttribute(
           kernel_function, cudaFuncAttributeNonPortableClusterSizeAllowed, 1);
     Return_Status(status);
 #else
     return Status::kInvalid;
@@ -153,14 +169,15 @@
     cudaError_t status = cudaLaunchKernelExC(&launch_config, kernel, kernel_params);
     Return_Status(status);
 #else
     CUTLASS_TRACE_HOST("ClusterLauncher: CUTLASS_SM90_CLUSTER_LAUNCH_ENABLED not defined! Aborting cluster launch.");
     return Status::kInvalid;
 #endif
   }
+
 };
 
 namespace detail {
 
 template<class Arg>
 void* checked_addressof(Arg&& arg) {
   static_assert(! std::is_rvalue_reference_v<Arg> || ! std::is_const_v<Arg>, "You cannot take the address of a const rvalue reference (const T&&).");
@@ -208,37 +225,44 @@
 /// X x = get_x();
 /// Y y = get_y();
 /// Z z = get_z();
 ///
 /// void const* kernel_ptr =
 ///   const_cast<void const*>(reinterpret_cast<void*>(
 ///     &kernel<SharedMemory, X, Y, Z>));
-/// auto status = launch_on_cluster(
+/// auto status = launch_kernel_on_cluster(
 ///   {grid_dims, block_dims, cluster_dims, sizeof(SharedMemory)},
 ///   kernel_ptr, x, y, z);
 /// @endcode
 template<class ... Args>
 CUTLASS_HOST cutlass::Status
 launch_kernel_on_cluster(const ClusterLaunchParams& params,
   void const* kernel_ptr,
   Args&& ... args)
 {
   // Unfortunately, we find ourselves needing to pass in
   // the parameters as an array of raw pointers.
   if constexpr (sizeof...(Args) == 0) {
     return cutlass::ClusterLauncher::launch(
-      params.grid_dims, params.cluster_dims, params.block_dims,
-      params.smem_size_in_bytes, params.cuda_stream,
+      params.grid_dims,
+      params.cluster_dims,
+      params.block_dims,
+      params.smem_size_in_bytes,
+      params.cuda_stream,
       kernel_ptr, nullptr);
   }
   else {
     void* kernel_params[sizeof...(Args)] = {
       detail::checked_addressof(std::forward<Args>(args))...
     };
     return cutlass::ClusterLauncher::launch(
-      params.grid_dims, params.cluster_dims, params.block_dims,
-      params.smem_size_in_bytes, params.cuda_stream,
-      kernel_ptr, kernel_params);
+      params.grid_dims,
+      params.cluster_dims,
+      params.block_dims,
+      params.smem_size_in_bytes,
+      params.cuda_stream,
+      kernel_ptr,
+      kernel_params);
   }
 }
 
 }  // namespace cutlass
```

## cutlass_library/source/include/cutlass/complex.h

```diff
@@ -24,23 +24,14 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by this unit test: `cutlass_test_unit_core_cpp11`.
-*/
 
 #pragma once
 
 #include <cuComplex.h>
 
 #include <cuda_fp16.h>
 
@@ -60,17 +51,14 @@
 
 #if !defined(__CUDACC_RTC__)
 #include <iosfwd>
 #endif
 
 namespace cutlass {
 
-
-
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// Enumeraed type describing a transformation on a complex value.
 enum class ComplexTransform {
   kNone,
   kConjugate
 };
```

## cutlass_library/source/include/cutlass/coord.h

```diff
@@ -28,24 +28,14 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief A Coord is a coordinate of arbitrary rank into a tensor or matrix
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/cstdint>
 #else
 #include <stdint.h>
 #endif
```

## cutlass_library/source/include/cutlass/core_io.h

```diff
@@ -27,23 +27,14 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Helpers for printing cutlass/core objects
 */
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
 #pragma once
 
 #include <iostream>
 #include <typeinfo>
 
 #include "cutlass/array.h"
 #include "cutlass/coord.h"
```

## cutlass_library/source/include/cutlass/cuda_host_adapter.hpp

```diff
@@ -176,14 +176,46 @@
     dim3 const grid_dims,
     dim3 const cluster_dims,
     dim3 const block_dims,
     size_t const smem_size,
     cudaStream_t cuda_stream,
     void** kernel_params,
     int32_t kernel_index) const = 0;
+
+protected:
+
+  /**
+   * Fills a buffer in Global Memory with a byte sequence copied from host memory.
+   * This function can be overriden to dispatch to the appropriate cuMemsetD*Async API
+  */
+  virtual Status memsetDeviceImpl(
+    void* destination, ///< Device memory pointer to be filled
+    void const* fill_value, ///< Value to be filled in the buffer
+    size_t fill_size, ///< Size of the data type to be used for filling the buffer
+    size_t count, ///< Number of elements of size fill_size
+    cudaStream_t stream) const = 0;
+
+public:
+
+  /// Fills a buffer in Global Memory with a byte sequence copied from host memory
+  template<class FillValueType>
+  Status memsetDevice(
+    void* destination,
+    FillValueType fill_value, 
+    size_t count,
+    cudaStream_t stream) const
+  {
+    return this->memsetDeviceImpl(
+      destination,
+      &fill_value,
+      sizeof(FillValueType),
+      count,
+      stream);
+  }
+
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/cutlass.h

```diff
@@ -29,24 +29,14 @@
  *
  **************************************************************************************************/
 
 /*! \file
     \brief Basic include for CUTLASS.
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #include "cutlass/detail/helper_macros.hpp"
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
```

## cutlass_library/source/include/cutlass/fast_math.h

```diff
@@ -24,23 +24,14 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
 
 #pragma once
 
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/cstdint>
 #else
 #include <cstdint>
@@ -149,21 +140,23 @@
  * Rounding
  ******************************************************************************/
 
 /**
  * Round dividend up to the nearest multiple of divisor
  */
 template <typename dividend_t, typename divisor_t>
-CUTLASS_HOST_DEVICE dividend_t round_nearest(dividend_t dividend, divisor_t divisor) {
+CUTLASS_HOST_DEVICE
+CUTLASS_CONSTEXPR_IF_CXX17
+dividend_t round_nearest(dividend_t dividend, divisor_t divisor) {
   return ((dividend + divisor - 1) / divisor) * divisor;
 }
 
 template <typename value_t>
 CUTLASS_HOST_DEVICE
-constexpr
+CUTLASS_CONSTEXPR_IF_CXX17
 value_t abs_for_integer(value_t a) {
   return ((a > 0) ? a : -a);
 }
 /**
  * Greatest common divisor
  */
 template <typename value_t>
@@ -181,39 +174,38 @@
 /**
  * Least common multiple
  */
 template <typename value_t>
 CUTLASS_HOST_DEVICE
 CUTLASS_CONSTEXPR_IF_CXX17
 value_t lcm(value_t a, value_t b) {
-  value_t temp = gcd(a, b);
-
-  return temp ? (cutlass::abs_for_integer(a) / temp * cutlass::abs_for_integer(b)) : value_t();
+  value_t temp = cutlass::gcd(a, b);
+  return (temp != 0) ? value_t(cutlass::abs_for_integer(a) / temp * cutlass::abs_for_integer(b)) : value_t{};
 }
 
 /**
  * Greatest common divisor
  */
 template <typename value_t>
 CUTLASS_HOST_DEVICE
-constexpr
+CUTLASS_CONSTEXPR_IF_CXX17
 value_t gcd_cxx11(value_t a, value_t b) {
-  return (a == 0 || b == 0) ? cutlass::abs_for_integer(a | b) : gcd_cxx11(b, a % b);
+  return (a == 0 || b == 0) ? cutlass::abs_for_integer(a | b) : cutlass::gcd_cxx11(b, a % b);
 }
 
 /**
  * Least common multiple
  */
 template <typename value_t>
 CUTLASS_HOST_DEVICE
-constexpr
+CUTLASS_CONSTEXPR_IF_CXX17
 value_t lcm_cxx11(value_t a, value_t b) {
-  return gcd_cxx11(a, b) ? (cutlass::abs_for_integer(a) / gcd_cxx11(a, b) *
-                            cutlass::abs_for_integer(b))
-                         : value_t();
+  return cutlass::gcd_cxx11(a, b) ? (cutlass::abs_for_integer(a) / cutlass::gcd_cxx11(a, b) *
+                                    cutlass::abs_for_integer(b))
+                                  : value_t{};
 }
 
 /// Returns the smallest value in the half-open range [a, a+b) that is a multiple of b
 CUTLASS_HOST_DEVICE
 CUTLASS_CONSTEXPR_IF_CXX17
 int round_up(int a, int b) {
   return ((a + b - 1) / b) * b;
@@ -230,34 +222,39 @@
 
 /**
  * log2 computation, what's the
  * difference between the below codes and
  * log2_up/down codes?
  */
 template <typename value_t>
-CUTLASS_HOST_DEVICE value_t clz(value_t x) {
+CUTLASS_HOST_DEVICE
+CUTLASS_CONSTEXPR_IF_CXX17
+value_t clz(value_t x) {
   for (int i = 31; i >= 0; --i) {
     if ((1 << i) & x)
       return value_t(31 - i);
   }
-  return 32;
+  return value_t(32);
 }
 
 template <typename value_t>
-CUTLASS_HOST_DEVICE value_t find_log2(value_t x) {
+CUTLASS_HOST_DEVICE
+CUTLASS_CONSTEXPR_IF_CXX17
+value_t find_log2(value_t x) {
   int a = int(31 - clz(x));
   a += (x & (x - 1)) != 0;  // Round up, add 1 if not a power of 2.
   return a;
 }
 
 
 /**
  * Find divisor, using find_log2
  */
-CUTLASS_HOST_DEVICE 
+CUTLASS_HOST_DEVICE
+CUTLASS_CONSTEXPR_IF_CXX17
 void find_divisor(unsigned int& mul, unsigned int& shr, unsigned int denom) {
   if (denom == 1) {
     mul = 0;
     shr = 0;
   } else {
     unsigned int p = 31 + find_log2(denom);
     unsigned m = unsigned(((1ull << p) + unsigned(denom) - 1) / unsigned(denom));
@@ -266,15 +263,16 @@
     shr = p - 32;
   }
 }
 
 /**
  * Find quotient and remainder using device-side intrinsics
  */
-CUTLASS_HOST_DEVICE 
+CUTLASS_HOST_DEVICE
+CUTLASS_CONSTEXPR_IF_CXX17
 void fast_divmod(int& quo, int& rem, int src, int div, unsigned int mul, unsigned int shr) {
 
   #if defined(__CUDA_ARCH__)
   // Use IMUL.HI if div != 1, else simply copy the source.
   quo = (div != 1) ? __umulhi(src, mul) >> shr : src;
   #else
   quo = int((div != 1) ? int(((int64_t)src * mul) >> 32) >> shr : src);
@@ -282,14 +280,15 @@
 
   // The remainder.
   rem = src - (quo * div);
 }
 
 // For long int input
 CUTLASS_HOST_DEVICE
+CUTLASS_CONSTEXPR_IF_CXX17
 void fast_divmod(int& quo, int64_t& rem, int64_t src, int div, unsigned int mul, unsigned int shr) {
 
   #if defined(__CUDA_ARCH__)
   // Use IMUL.HI if div != 1, else simply copy the source.
   quo = (div != 1) ? __umulhi(src, mul) >> shr : src;
   #else
   quo = int((div != 1) ? ((src * mul) >> 32) >> shr : src);
```

## cutlass_library/source/include/cutlass/float8.h

```diff
@@ -29,36 +29,31 @@
  *
  **************************************************************************************************/
 /*!
     \file
     \brief Defines a class for using IEEE half-precision floating-point types in host or
       device code.
 */
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
 
 #pragma once
 
 // FP8 types are available starting CUDA 11.8+
 #if (__CUDACC_VER_MAJOR__ >= 12) || ((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 8))
 #define CUDA_FP8_ENABLED 1
 #endif
 
 #if defined(__CUDA_ARCH__)
 #  if (__CUDA_ARCH__ >= 900)
 #    if (__CUDACC_VER_MAJOR__ >= 12) || ((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 8))
 #      define CUDA_PTX_FP8_CVT_ENABLED 1
 #    endif // (__CUDACC_VER_MAJOR__ >= 12) || ((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 8))
+#  elif (__CUDA_ARCH__ == 890)
+#    if (__CUDACC_VER_MAJOR__ > 12) || ((__CUDACC_VER_MAJOR__ == 12) && (__CUDACC_VER_MINOR__ >= 1))
+#      define CUDA_PTX_FP8_CVT_ENABLED 1
+#    endif // (__CUDACC_VER_MAJOR__ > 12) || ((__CUDACC_VER_MAJOR__ == 12) && (__CUDACC_VER_MINOR__ >= 1))
 #  endif // (__CUDA_ARCH__ >= 900)
 #endif // defined(__CUDA_ARCH__)
 
 #ifdef __GNUC__
 // Ignore checks on reinterpret-casts that are being used for bitcasts.
 #pragma GCC diagnostic ignored "-Wstrict-aliasing"
 #endif
@@ -127,15 +122,15 @@
     static constexpr int FP16_MAX_EXPONENT  = 15;
     static constexpr int FP16_MIN_EXPONENT  = -14;
     static constexpr int FP16_EXPONENT_BIAS = 15;
 
     static constexpr int FP8_NUM_BITS = 8;
     static constexpr int FP8_NUM_EXPONENT_BITS = IS_E4M3 ? 4 : 5;
     static constexpr int FP8_NUM_MANTISSA_BITS = IS_E4M3 ? 3 : 2;
-    static constexpr uint8_t  FP8_NAN = 0x7f; // Also F8_INF 
+    static constexpr uint8_t  FP8_NAN = 0x7f; // Also F8_INF
     static constexpr uint8_t  FP8_INFINITY_MASK = IS_E4M3 ? 0x78 : 0x7c;
     static constexpr int FP8_MAX_EXPONENT  = IS_E4M3 ?  7 :  15;
     static constexpr int FP8_MIN_EXPONENT  = IS_E4M3 ? -6 : -14;
     static constexpr int FP8_EXPONENT_BIAS = IS_E4M3 ?  7 :  15;
 
     static constexpr uint8_t  FP8_EXPONENT_MASK = (1 << FP8_NUM_EXPONENT_BITS) - 1;
     static constexpr uint8_t  FP8_MANTISSA_MASK = (1 << FP8_NUM_MANTISSA_BITS) - 1;
@@ -1035,14 +1030,42 @@
 
 /// float_e5m2_t <= float_e4m3_t
 CUTLASS_HOST_DEVICE
 float_e5m2_t::float_e5m2_t(float_e4m3_t x) {
     storage = from_float(float_e4m3_t::to_float(x)).storage;
 }
 
+///////////////////////////////////////////////////////////////
+///
+/// Umbrella floating-point 8-bit data type : type_erased_dynamic_float8_t
+/// This umbrella datatype can be enabled when a user provides a specific
+/// datatype in runtime argument list.
+///
+/// Currently supported runtime datatypes compatible with type_erased_dynamic_float8_t:
+///   QMMAFormat::E5M2
+///   QMMAFormat::E4M3
+///
+///////////////////////////////////////////////////////////////
+
+union type_erased_dynamic_float8_t {
+  uint8_t data;
+  cutlass::float_e5m2_t e5m2;
+  cutlass::float_e4m3_t e4m3;
+  CUTLASS_HOST_DEVICE
+  explicit operator cutlass::float_e5m2_t() const {
+    return e5m2;
+  }
+
+  CUTLASS_HOST_DEVICE
+  explicit operator cutlass::float_e4m3_t() const {
+    return e4m3;
+  }
+
+};
+
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace cutlass
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 //
 // Standard Library operations and definitions
@@ -1069,32 +1092,39 @@
   static std::float_round_style const round_style = std::round_to_nearest;
   static bool const is_iec559 = false;
   static bool const is_bounded = true;
   static bool const is_modulo = false;
   static int const digits = F8Type::FP8_NUM_MANTISSA_BITS;
 
   /// Least positive value
+  CUTLASS_HOST_DEVICE
   static F8Type min() { return F8Type::bitcast(0x01); }
 
   /// Maximum finite value
+  CUTLASS_HOST_DEVICE
   static F8Type max() { return F8Type::bitcast(F8Type::FP8_MAX_FLT); }
 
   /// Returns maximum rounding error
+  CUTLASS_HOST_DEVICE
   static F8Type round_error() { return F8Type(0.5f); }
 
   /// Returns positive infinity value
+  CUTLASS_HOST_DEVICE
   static F8Type infinity() { return F8Type::bitcast(F8Type::FP8_INFINITY_MASK); }
 
   /// Returns quiet NaN value
+  CUTLASS_HOST_DEVICE
   static F8Type quiet_NaN() { return F8Type::bitcast(F8Type::FP8_NAN); }
 
   /// Returns signaling NaN value
+  CUTLASS_HOST_DEVICE
   static F8Type signaling_NaN() { return F8Type::bitcast(F8Type::FP8_NAN); }
 
   /// Returns smallest positive subnormal value
+  CUTLASS_HOST_DEVICE
   static F8Type denorm_min() { return F8Type::bitcast(0x01); }
 };
 
 /// Numeric limits for float_e4m3_t
 template <>
 struct numeric_limits<cutlass::float_e4m3_t> :
     public float8_base_numeric_limits<cutlass::float_e4m3_t> {
@@ -1146,32 +1176,39 @@
 #endif
   static bool const is_iec559 = false;
   static bool const is_bounded = true;
   static bool const is_modulo = false;
   static int const digits = F8Type::FP8_NUM_MANTISSA_BITS;
 
   /// Least positive value
+  CUTLASS_HOST_DEVICE
   static F8Type min() { return F8Type::bitcast(0x01); }
 
   /// Maximum finite value
+  CUTLASS_HOST_DEVICE
   static F8Type max() { return F8Type::bitcast(F8Type::FP8_MAX_FLT); }
 
   /// Returns maximum rounding error
+  CUTLASS_HOST_DEVICE
   static F8Type round_error() { return F8Type(0.5f); }
 
   /// Returns positive infinity value
+  CUTLASS_HOST_DEVICE
   static F8Type infinity() { return F8Type::bitcast(F8Type::FP8_INFINITY_MASK); }
 
   /// Returns quiet NaN value
+  CUTLASS_HOST_DEVICE
   static F8Type quiet_NaN() { return F8Type::bitcast(F8Type::FP8_NAN); }
 
   /// Returns signaling NaN value
+  CUTLASS_HOST_DEVICE
   static F8Type signaling_NaN() { return F8Type::bitcast(F8Type::FP8_NAN); }
 
   /// Returns smallest positive subnormal value
+  CUTLASS_HOST_DEVICE
   static F8Type denorm_min() { return F8Type::bitcast(0x01); }
 };
 
 /// std::numeric_limits
 template <class T>
 struct numeric_limits;
```

## cutlass_library/source/include/cutlass/functional.h

```diff
@@ -29,29 +29,20 @@
  *
  **************************************************************************************************/
 /*! \file
     \brief Define basic numeric operators
 
     This is inspired by the Standard Library's <functional> header.
 */
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/half.h"
-#include "cutlass/tfloat32.h"
-#include "cutlass/bfloat16.h"
+#include "cutlass/numeric_types.h"
+
+#include <cuda_runtime.h>
 
 #if defined(CUTLASS_ARCH_WMMA_ENABLED)
 #include <mma.h>
 #endif // defined(CUTLASS_ARCH_WMMA_ENABLED)
 
 #ifdef _MSC_VER
 // Provides support for alternate operators such as 'and', 'or', ...
@@ -102,15 +93,15 @@
     return lhs;
   }
 };
 
 template <typename T>
 struct scale {
   T const scaling_factor_;
-  
+
   CUTLASS_HOST_DEVICE
   scale(float scaling_factor) : scaling_factor_(scaling_factor) {
   }
 
   T operator()(T const &rhs) const {
     T result = rhs * scaling_factor_;
     return result;
@@ -214,37 +205,66 @@
     multiplies<Output> mul_op;
 
     Output y = Output(lhs) - Output(rhs);
     return mul_op(y, y);
   }
 };
 
+// Computes the reciprocal square root
+template <typename T>
+struct inverse_square_root;
+
+template <>
+struct inverse_square_root<float> {
+  CUTLASS_HOST_DEVICE
+  float operator()(float const &lhs) const {
+#if defined(__CUDA_ARCH__)
+    return rsqrtf(lhs);
+#else
+    return 1.f / std::sqrt(lhs);
+#endif
+  }
+};
+
+template <>
+struct inverse_square_root<half_t> {
+  CUTLASS_HOST_DEVICE
+  half_t operator()(half_t const &lhs) const {
+#if defined(__CUDA_ARCH__)
+    auto result = hrsqrt(reinterpret_cast<__half const &>(lhs));
+    return reinterpret_cast<half_t const &>(result);
+#else
+    return half_t(1.f / std::sqrt(half_t::convert(lhs)));
+#endif
+  }
+};
+
 /// Divides
 template <typename T>
 struct divides {
   CUTLASS_HOST_DEVICE
   T operator()(T lhs, T const &rhs) const {
     lhs /= rhs;
     return lhs;
   }
 };
 
-/// reciprocal_approximate 
+/// reciprocal_approximate
 template <typename T>
 struct reciprocal_approximate {
   CUTLASS_HOST_DEVICE
   T operator()(T lhs) const {
-    return divide(T(1), lhs);
+    return divides<T>{}(T(1), lhs);
   }
 };
 
 template <>
 struct reciprocal_approximate <float> {
   CUTLASS_HOST_DEVICE
-  float operator()(float lhs) const { 
+  float operator()(float lhs) const {
     float ret;
       ret = 1.0f / lhs;
     return ret;
   }
 };
 
 /// Negate
@@ -252,42 +272,42 @@
 struct negate {
   CUTLASS_HOST_DEVICE
   T operator()(T lhs) const {
     return -lhs;
   }
 };
 
-/// Greater equal 
+/// Greater equal
 template <typename T>
 struct greater_equal {
   CUTLASS_HOST_DEVICE
   bool operator()(T const &lhs, T const &rhs) const {
     return (lhs >= rhs);
   }
 };
 
-/// Greater  
+/// Greater
 template <typename T>
 struct greater {
   CUTLASS_HOST_DEVICE
   bool operator()(T const &lhs, T const &rhs) const {
     return (lhs > rhs);
   }
 };
 
-/// Less equal 
+/// Less equal
 template <typename T>
 struct less_equal {
   CUTLASS_HOST_DEVICE
   bool operator()(T const &lhs, T const &rhs) const {
     return (lhs <= rhs);
   }
 };
 
-/// Less  
+/// Less
 template <typename T>
 struct less {
   CUTLASS_HOST_DEVICE
   bool operator()(T const &lhs, T const &rhs) const {
     return (lhs < rhs);
   }
 };
@@ -417,14 +437,23 @@
 struct multiply_add {
   CUTLASS_HOST_DEVICE
   C operator()(A const &a, B const &b, C const &c) const {
     return C(a) * C(b) + c;
   }
 };
 
+template <typename T>
+struct square_and_plus {
+  CUTLASS_HOST_DEVICE
+  T operator()(T lhs, T const &rhs) const {
+    multiply_add<T> multiply_add_op;
+    return multiply_add_op(rhs, rhs, lhs);
+  }
+};
+
 // Fused multiply-add that takes exactly one template parameter.
 // This is useful for working around a known Clang issue,
 // where a template template parameter with one template parameter
 // does not match classes that take multiple template parameters
 // but have defaults for all but the first.
 template <typename A>
 struct homogeneous_multiply_add : public multiply_add<A, A, A>
@@ -469,31 +498,35 @@
 
 template <typename T>
 struct first {
   CUTLASS_HOST_DEVICE
   T operator()(T const & first, T const &...) const {
     return first;
   }
+  CUTLASS_HOST_DEVICE
+  T operator()(T const & first) const {
+    return first;
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename T>
 struct logical_and {
   CUTLASS_HOST_DEVICE
   T operator()(T const &a, T const &b) const {
-    return ((a && b) ? T(1) : T());
+    return ((static_cast<bool>(a) && static_cast<bool>(b)) ? T(1) : T());
   }
 };
 
 template <typename T>
 struct logical_or {
   CUTLASS_HOST_DEVICE
   T operator()(T const &a, T const &b) const {
-    return ((a || b) ? T(1) : T());
+    return ((static_cast<bool>(a) || static_cast<bool>(b)) ? T(1) : T());
   }
 };
 
 template <typename T>
 struct logical_not {
   CUTLASS_HOST_DEVICE
   T operator()(T const &a) const {
@@ -531,16 +564,14 @@
 struct bit_xor {
   CUTLASS_HOST_DEVICE
   T operator()(T const &a, T const &b) const {
     return a ^ b;
   }
 };
 
-
-
 //////////////////////////////////////////////////////////////////////////////////////////////////
 /// Atomic reductions
 
 template <typename T>
 struct atomic_add
 {
   CUTLASS_DEVICE
```

## cutlass_library/source/include/cutlass/half.h

```diff
@@ -30,24 +30,14 @@
  **************************************************************************************************/
 /*!
     \file
     \brief Defines a class for using IEEE half-precision floating-point types in host or
       device code.
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #ifndef CUTLASS_ENABLE_F16C
 #define CUTLASS_ENABLE_F16C 0
 #endif
 
 #if defined(__CUDACC_RTC__)
```

## cutlass_library/source/include/cutlass/integer_subbyte.h

```diff
@@ -30,24 +30,14 @@
  **************************************************************************************************/
 /*!
     \file
     \brief Defines a class for using integer types smaller than one byte in host or
       device code.
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/cstdint>
 #else
 #include <cstdint>
 #endif
@@ -56,32 +46,29 @@
 #include "cutlass/numeric_size.h"
 #include "cutlass/platform/platform.h"
 
 namespace cutlass {
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// 4-bit signed integer type
 template <int Bits, bool Signed = true>
 struct integer_subbyte {
+  /// Storage type
+  using Storage = uint8_t;
 
   /// Number of bits
-  static int const kBits = Bits;
-
-  /// Whether type is signed
-  static bool const kSigned = Signed;
+  static_assert(Bits <= 8*sizeof(Storage), "Require a subbyte of bits in integer_subbyte");
 
   /// External type
-  using T = typename platform::conditional<kSigned, int, unsigned>::type;
+  using xint_t = typename platform::conditional<Signed, int, unsigned>::type;
 
-  /// Storage type
-  using Storage = uint8_t;
-
-  /// Bitmask used to truncate from larger integers
-  static Storage const kMask = Storage((1 << kBits) - 1);
+  /// Bitmask for truncation from larger integers
+  static constexpr Storage bits_mask_ = Storage(Storage(-1) >> (8 - Bits));
+  /// Bitmask for the sign bit
+  static constexpr Storage sign_mask_ = Storage((Signed ? 1 : 0) << (Bits - 1));
 
   //
   // Data members
   //
 
   Storage storage;
 
@@ -89,90 +76,85 @@
   // Methods
   //
 
   /// No operation
   integer_subbyte() = default;
 
   /// Conversion from integer type
-  CUTLASS_HOST_DEVICE
+  CUTLASS_HOST_DEVICE explicit
   integer_subbyte(int value)
-      : storage(reinterpret_cast<Storage const &>(value) & kMask) {}
+      : storage(reinterpret_cast<Storage const&>(value) & bits_mask_) {}
 
-  CUTLASS_HOST_DEVICE
+  CUTLASS_HOST_DEVICE explicit
   integer_subbyte(unsigned value)
-      : storage(reinterpret_cast<Storage const &>(value) & kMask) {}
+      : storage(reinterpret_cast<Storage const&>(value) & bits_mask_) {}
 
-  CUTLASS_HOST_DEVICE
+  CUTLASS_HOST_DEVICE explicit
   integer_subbyte(double value) {
-    T tmp = static_cast<T>(value);
-    storage = Storage(reinterpret_cast<unsigned const &>(tmp) & kMask);
+    xint_t tmp = static_cast<xint_t>(value);
+    storage = reinterpret_cast<Storage const &>(tmp) & bits_mask_;
   }
 
-  ///
+  /// Convert to int or unsigned
   CUTLASS_HOST_DEVICE
-  operator T() const {
-    if (kSigned) {
-      // Sign extend
-      if (storage & Storage(1 << (kBits - 1))) {
-        return T(storage) | ~T(kMask);
-      }
+  operator xint_t() const {
+    if (sign_mask_ & storage) {  // Sign extend
+      return xint_t(storage) | ~xint_t(bits_mask_);
+    } else {
+      return xint_t(storage);
     }
-    return T(storage);
   }
 
   /// Equality
   CUTLASS_HOST_DEVICE
-  bool operator==(integer_subbyte const &rhs) const {
+  bool operator==(integer_subbyte const& rhs) const {
     return storage == rhs.storage;
   }
 
   /// Inequality
   CUTLASS_HOST_DEVICE
-  bool operator!=(integer_subbyte const &rhs) const {
+  bool operator!=(integer_subbyte const& rhs) const {
     return storage != rhs.storage;
   }
 
   /// Less than or equal
   CUTLASS_HOST_DEVICE
-  bool operator<=(integer_subbyte const &rhs) const {
-    if (kSigned) {
-      if (storage & (1 << (kBits - 1))) {
-        return !(rhs.storage < storage);
-      }
+  bool operator<=(integer_subbyte const& rhs) const {
+    if (sign_mask_ & storage) {
+      return !(rhs.storage < storage);
+    } else {
+      return storage <= rhs.storage;
     }
-    return storage <= rhs.storage;
   }
 
   /// Less than
   CUTLASS_HOST_DEVICE
-  bool operator<(integer_subbyte const &rhs) const {
-    if (kSigned) {
-      if (storage & (1 << (kBits - 1))) {
-        return !(rhs.storage <= storage);
-      }
+  bool operator<(integer_subbyte const& rhs) const {
+    if (sign_mask_ & storage) {
+      return !(rhs.storage <= storage);
+    } else {
+      return storage < rhs.storage;
     }
-    return storage < rhs.storage;
   }
 
   /// Greater than or equal
   CUTLASS_HOST_DEVICE
-  bool operator>=(integer_subbyte const &rhs) const {
+  bool operator>=(integer_subbyte const& rhs) const {
     return !(*this < rhs);
   }
 
   /// Greater than
   CUTLASS_HOST_DEVICE
-  bool operator>(integer_subbyte const &rhs) const {
+  bool operator>(integer_subbyte const& rhs) const {
     return !(*this <= rhs);
   }
 };
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
-
 /// 1-bit Unsigned integer type
 using uint1b_t = integer_subbyte<1, false>;
 
 /// 2-bit Integer type
 using int2b_t = integer_subbyte<2, true>;
 
 /// 2-bit Unsigned integer type
@@ -180,75 +162,107 @@
 
 /// 4-bit Integer type
 using int4b_t = integer_subbyte<4, true>;
 
 /// 4-bit Unsigned integer type
 using uint4b_t = integer_subbyte<4, false>;
 
+/// 1-bit binary type
+using bin1_t = bool;
+
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Defines the size of an element in bits - specialized for uint1b_t
-template <>
-struct sizeof_bits<uint1b_t> {
-  static int const value = 1;
+template <int Bits, bool Signed>
+struct sizeof_bits<integer_subbyte<Bits,Signed>> {
+  static constexpr int value = Bits;
 };
 
-/// Defines the size of an element in bits - specialized for int2b_t
+/// Defines the size of an element in bits - specialized for bin1_t
 template <>
-struct sizeof_bits<int2b_t> {
-  static int const value = 2;
+struct sizeof_bits<bin1_t> {
+  static constexpr int value = 1;
 };
 
-/// Defines the size of an element in bits - specialized for uint2b_t
+///////////////////////////////////////////////////////////////////////////////////////////////////
+
+namespace platform {
+
 template <>
-struct sizeof_bits<uint2b_t> {
-  static int const value = 2;
+struct numeric_limits<cutlass::int4b_t> {
+  CUTLASS_HOST_DEVICE static
+  cutlass::int4b_t const lowest() noexcept { return int4b_t{-8};}
+
+  CUTLASS_HOST_DEVICE static
+  cutlass::int4b_t const max() noexcept { return int4b_t{7};}
+
+  CUTLASS_HOST_DEVICE static
+  cutlass::int4b_t const min() noexcept { return lowest();}
+
+  static constexpr bool is_integer = true;
+  static constexpr bool is_signed = true;
 };
 
-/// Defines the size of an element in bits - specialized for int4b_t
 template <>
-struct sizeof_bits<int4b_t> {
-  static int const value = 4;
+struct numeric_limits<cutlass::uint4b_t> {
+  CUTLASS_HOST_DEVICE static
+ cutlass::uint4b_t const lowest() noexcept { return uint4b_t{0};}
+
+  CUTLASS_HOST_DEVICE static
+  cutlass::uint4b_t const max() noexcept { return uint4b_t{15};}
+
+  CUTLASS_HOST_DEVICE static
+  cutlass::uint4b_t const min() noexcept { return lowest();}
+
+  static constexpr bool is_integer = true;
+  static constexpr bool is_signed = false;
 };
 
-/// Defines the size of an element in bits - specialized for uint4b_t
 template <>
-struct sizeof_bits<uint4b_t> {
-  static int const value = 4;
-};
+struct numeric_limits<cutlass::uint1b_t> {
+  CUTLASS_HOST_DEVICE static
+  cutlass::uint1b_t const lowest() noexcept { return uint1b_t{0};}
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
+  CUTLASS_HOST_DEVICE static
+  cutlass::uint1b_t const max() noexcept { return uint1b_t{1};}
 
-namespace platform {
+  CUTLASS_HOST_DEVICE static
+  cutlass::uint1b_t const min() noexcept { return lowest();}
 
-template <>
-struct numeric_limits<cutlass::int4b_t> {
-  CUTLASS_HOST_DEVICE
-  static cutlass::int4b_t const lowest() noexcept { return -8;}
-  CUTLASS_HOST_DEVICE
-  static cutlass::int4b_t const max() noexcept { return 7;}
   static constexpr bool is_integer = true;
+  static constexpr bool is_signed = false;
 };
 
 template <>
-struct numeric_limits<cutlass::uint4b_t> {
-  CUTLASS_HOST_DEVICE
-  static cutlass::uint4b_t const lowest() noexcept { return 0;}
-  CUTLASS_HOST_DEVICE
-  static cutlass::uint4b_t const max() noexcept { return 15;}
+struct numeric_limits<cutlass::int2b_t> {
+  CUTLASS_HOST_DEVICE static
+  cutlass::int2b_t lowest() noexcept { return int2b_t{-2}; }
+
+  CUTLASS_HOST_DEVICE static
+  cutlass::int2b_t min() noexcept { return lowest(); }
+
+  CUTLASS_HOST_DEVICE static
+  cutlass::int2b_t max() noexcept { return int2b_t{1}; }
+
   static constexpr bool is_integer = true;
+  static constexpr bool is_signed = true;
 };
 
 template <>
-struct numeric_limits<cutlass::uint1b_t> {
-  CUTLASS_HOST_DEVICE
-  static cutlass::uint1b_t const lowest() noexcept { return 0;}
-  CUTLASS_HOST_DEVICE
-  static cutlass::uint1b_t const max() noexcept { return 1;}
+struct numeric_limits<cutlass::uint2b_t> {
+  CUTLASS_HOST_DEVICE static
+  cutlass::uint2b_t const lowest() noexcept { return uint2b_t{0}; }
+
+  CUTLASS_HOST_DEVICE static
+  cutlass::uint2b_t const min() noexcept { return lowest(); }
+
+  CUTLASS_HOST_DEVICE static
+  cutlass::uint2b_t const max() noexcept { return uint2b_t{3}; }
+
   static constexpr bool is_integer = true;
+  static constexpr bool is_signed = false;
 };
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace platform
 } // namespace cutlass
```

## cutlass_library/source/include/cutlass/kernel_hardware_info.h

```diff
@@ -26,24 +26,14 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by this unit test: `cutlass_test_unit_core_cpp11`.
-*/
-
 #if !defined(__CUDACC_RTC__)
 #include "cuda_runtime.h"
 
 #include "cutlass/trace.h"
 #endif
 
 namespace cutlass {
```

## cutlass_library/source/include/cutlass/numeric_conversion.h

```diff
@@ -29,24 +29,14 @@
  *
  **************************************************************************************************/
 /*!
     \file
     \brief Boost-like numeric conversion operator for CUTLASS numeric types
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by this unit test: `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #if !defined(__CUDACC_RTC__)
 #include <cfenv>
 #endif
 
 #include "cutlass/cutlass.h"
@@ -283,15 +273,15 @@
   }
 };
 
 #endif
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for float <= half_t
+/// Partial specialization for float <= cutlass::half_t
 template <typename T, FloatRoundStyle Round>
 struct NumericConverter<T, T, Round> {
 
   using result_type = T;
   using source_type = T;
   static FloatRoundStyle const round_style = Round;
 
@@ -305,24 +295,24 @@
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
-// Partial specializations for float <=> half_t
+// Partial specializations for float <=> cutlass::half_t
 //
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for float <= half_t
+/// Partial specialization for float <= cutlass::half_t
 template <FloatRoundStyle Round>
-struct NumericConverter<float, half_t, Round> {
+struct NumericConverter<float, cutlass::half_t, Round> {
 
   using result_type = float;
-  using source_type = half_t;
+  using source_type = cutlass::half_t;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & s) {
 
     result_type result = static_cast<float>(s);
 
@@ -333,114 +323,114 @@
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /// Specialization for round-to-nearest
 template <>
-struct NumericConverter<half_t, float, FloatRoundStyle::round_to_nearest> {
+struct NumericConverter<cutlass::half_t, float, FloatRoundStyle::round_to_nearest> {
 
-  using result_type = half_t;
+  using result_type = cutlass::half_t;
   using source_type = float;
   static FloatRoundStyle const round_style = FloatRoundStyle::round_to_nearest;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & s) {
 
-    result_type result = static_cast<half_t>(s);
+    result_type result = static_cast<cutlass::half_t>(s);
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /// Specialization for round-toward-zero
 template <>
-struct NumericConverter<half_t, float, FloatRoundStyle::round_toward_zero> {
+struct NumericConverter<cutlass::half_t, float, FloatRoundStyle::round_toward_zero> {
 
-  using result_type = half_t;
+  using result_type = cutlass::half_t;
   using source_type = float;
   static FloatRoundStyle const round_style = FloatRoundStyle::round_toward_zero;
 
   /// Round toward zero
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & flt) {
 
   #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
-    return half_t(__float2half_rz(flt));
+    return cutlass::half_t(__float2half_rz(flt));
   #else
     // software implementation rounds toward nearest even
     unsigned const& s = reinterpret_cast<unsigned const &>(flt);
     uint16_t sign = uint16_t((s >> 16) & 0x8000);
     int32_t exp = int32_t((s >> 23) & 0xff) - 127;
     int mantissa = s & 0x7fffff;
     uint16_t u = 0;
 
     if ((s & 0x7fffffff) == 0) {
       // sign-preserving zero
-      return half_t::bitcast(sign);
+      return cutlass::half_t::bitcast(sign);
     }
 
     if (exp > 15) {
       if (exp == 128 && mantissa) {
         // not a number
         u = 0x7fff;
       } else {
         // overflow to infinity
         u = sign | 0x7c00;
       }
-      return half_t::bitcast(u);
+      return cutlass::half_t::bitcast(u);
     }
 
     if (exp >= -14) {
       // normal fp32 to normal fp16
       u = uint16_t((uint32_t(exp + 15) & 0x1f) << 10);
       u = uint16_t(u | (mantissa >> 13));
     } else {
-      // normal single-precision to subnormal half_t-precision representation
+      // normal single-precision to subnormal cutlass::half_t-precision representation
       int rshift = (-14 - exp);
       if (rshift < 32) {
         mantissa |= (1 << 23);
         mantissa = (mantissa >> rshift);
         u = (uint16_t(mantissa >> 13) & 0x3ff);
       } else {
         mantissa = 0;
         u = 0;
       }
     }
 
     u |= sign;
 
-    return half_t::bitcast(u);
+    return cutlass::half_t::bitcast(u);
 
   #endif // defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
   }
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
-// Partial specializations for float <=> bfloat16_t
+// Partial specializations for float <=> cutlass::bfloat16_t
 //
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for float <= bfloat16_t
+/// Partial specialization for float <= cutlass::bfloat16_t
 template <FloatRoundStyle Round>
-struct NumericConverter<float, bfloat16_t, Round> {
+struct NumericConverter<float, cutlass::bfloat16_t, Round> {
 
   using result_type = float;
-  using source_type = bfloat16_t;
+  using source_type = cutlass::bfloat16_t;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & s) {
 
     return static_cast<float>(s);
   }
@@ -448,33 +438,33 @@
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 template <>
-struct NumericConverter<bfloat16_t, float, FloatRoundStyle::round_to_nearest> {
-  using result_type = bfloat16_t;
+struct NumericConverter<cutlass::bfloat16_t, float, FloatRoundStyle::round_to_nearest> {
+  using result_type = cutlass::bfloat16_t;
   using source_type = float;
   static FloatRoundStyle const round_style = FloatRoundStyle::round_to_nearest;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & s) {
-    return static_cast<bfloat16_t>(s);
+    return static_cast<cutlass::bfloat16_t>(s);
   }
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 template <>
-struct NumericConverter<bfloat16_t, float, FloatRoundStyle::round_half_ulp_truncate> {
-  using result_type = bfloat16_t;
+struct NumericConverter<cutlass::bfloat16_t, float, FloatRoundStyle::round_half_ulp_truncate> {
+  using result_type = cutlass::bfloat16_t;
   using source_type = float;
   static FloatRoundStyle const round_style = FloatRoundStyle::round_half_ulp_truncate;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & s) {
     uint32_t x32 = reinterpret_cast<uint32_t const &>(s);
 
@@ -485,56 +475,56 @@
     #else
     if (std::isfinite(s)) {
       x32 += 0x8000;
     }
     #endif
 
     uint16_t x16 = uint16_t((x32 >> 16) & 0xffff);
-    return bfloat16_t::bitcast(x16);
+    return cutlass::bfloat16_t::bitcast(x16);
   }
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 template <>
-struct NumericConverter<bfloat16_t, float, FloatRoundStyle::round_toward_zero> {
-  using result_type = bfloat16_t;
+struct NumericConverter<cutlass::bfloat16_t, float, FloatRoundStyle::round_toward_zero> {
+  using result_type = cutlass::bfloat16_t;
   using source_type = float;
   static FloatRoundStyle const round_style = FloatRoundStyle::round_toward_zero;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & s) {
 
     uint32_t x32 = reinterpret_cast<uint32_t const &>(s);
     uint16_t x16 = uint16_t(x32 >> 16);
 
-    return bfloat16_t::bitcast(x16);
+    return cutlass::bfloat16_t::bitcast(x16);
   }
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
-// Partial specializations for float <=> tfloat32_t
+// Partial specializations for float <=> cutlass::tfloat32_t
 //
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for float <= tfloat32_t
+/// Partial specialization for float <= cutlass::tfloat32_t
 template <FloatRoundStyle Round>
-struct NumericConverter<float, tfloat32_t, Round> {
+struct NumericConverter<float, cutlass::tfloat32_t, Round> {
 
   using result_type = float;
-  using source_type = tfloat32_t;
+  using source_type = cutlass::tfloat32_t;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & s) {
 
     return static_cast<float>(s);
   }
@@ -542,16 +532,16 @@
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 template <>
-struct NumericConverter<tfloat32_t, float, FloatRoundStyle::round_to_nearest> {
-  using result_type = tfloat32_t;
+struct NumericConverter<cutlass::tfloat32_t, float, FloatRoundStyle::round_to_nearest> {
+  using result_type = cutlass::tfloat32_t;
   using source_type = float;
   static FloatRoundStyle const round_style = FloatRoundStyle::round_to_nearest;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & s) {
 
     unsigned storage = reinterpret_cast<unsigned const &>(s);
@@ -582,45 +572,45 @@
       // storage = (storage & ~0x1fff);
     }
     else if (storage & ~0xff800000) {
       storage = 0x7fffffff;
     }
 #endif
 
-    return tfloat32_t::bitcast(storage);
+    return cutlass::tfloat32_t::bitcast(storage);
   }
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 template <>
-struct NumericConverter<tfloat32_t, float, FloatRoundStyle::round_half_ulp_truncate> {
-  using result_type = tfloat32_t;
+struct NumericConverter<cutlass::tfloat32_t, float, FloatRoundStyle::round_half_ulp_truncate> {
+  using result_type = cutlass::tfloat32_t;
   using source_type = float;
   static FloatRoundStyle const round_style = FloatRoundStyle::round_half_ulp_truncate;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & s) {
-    return tfloat32_t::round_half_ulp_truncate(s);
+    return cutlass::tfloat32_t::round_half_ulp_truncate(s);
   }
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /// This rounding operation is similar to half_ulp_truncate except it rounds denorms toward zero.
 /// It avoids predicated code, though it requires a temporary register.
 template <>
-struct NumericConverter<tfloat32_t, float, FloatRoundStyle::round_half_ulp_trunc_dntz> {
-  using result_type = tfloat32_t;
+struct NumericConverter<cutlass::tfloat32_t, float, FloatRoundStyle::round_half_ulp_trunc_dntz> {
+  using result_type = cutlass::tfloat32_t;
   using source_type = float;
   static FloatRoundStyle const round_style = FloatRoundStyle::round_half_ulp_trunc_dntz;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & s) {
 
     unsigned y = reinterpret_cast<unsigned const &>(s);
@@ -634,63 +624,63 @@
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 template <>
-struct NumericConverter<tfloat32_t, float, FloatRoundStyle::round_toward_zero> {
-  using result_type = tfloat32_t;
+struct NumericConverter<cutlass::tfloat32_t, float, FloatRoundStyle::round_toward_zero> {
+  using result_type = cutlass::tfloat32_t;
   using source_type = float;
   static FloatRoundStyle const round_style = FloatRoundStyle::round_toward_zero;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & s) {
     uint32_t x = reinterpret_cast<uint32_t const &>(s);
-    return tfloat32_t::bitcast(x & 0xffffe000);
+    return cutlass::tfloat32_t::bitcast(x & 0xffffe000);
   }
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
-// Conversion operator for float to tfloat32_t big and small values
+// Conversion operator for float to cutlass::tfloat32_t big and small values
 //
 /////////////////////////////////////////////////////////////////////////////////////////////////
 template <
   FloatRoundStyle RoundBig = FloatRoundStyle::round_toward_zero,
   FloatRoundStyle RoundSmall = FloatRoundStyle::round_half_ulp_truncate
 >
 struct NumericConverterFastF32 {
 
-  // result_type holds big tfloat32_t at idx(0) and small tfloat32_t at idx(1)
-  using result_type = Array<tfloat32_t, 2>;
+  // result_type holds big cutlass::tfloat32_t at idx(0) and small cutlass::tfloat32_t at idx(1)
+  using result_type = Array<cutlass::tfloat32_t, 2>;
 
   // source data type
   using source_type = float;
 
   // rounding styles for big and small part
   static FloatRoundStyle const kRoundBig = RoundBig;
   static FloatRoundStyle const kRoundSmall = RoundSmall;
 
   CUTLASS_HOST_DEVICE
     static result_type convert(source_type const & source) {
 
     result_type result;
-    NumericConverter<tfloat32_t, float, kRoundBig> convert_big_;
-    NumericConverter<tfloat32_t, float, kRoundSmall> convert_small_;
+    NumericConverter<cutlass::tfloat32_t, float, kRoundBig> convert_big_;
+    NumericConverter<cutlass::tfloat32_t, float, kRoundSmall> convert_small_;
 
-    // convert and fill tfloat32_t big at idx 0
+    // convert and fill cutlass::tfloat32_t big at idx 0
     result[0] = convert_big_(source);
 
-    // convert and fill tfloat32_t small at idx 1
+    // convert and fill cutlass::tfloat32_t small at idx 1
     result[1] = convert_small_(source - static_cast<float>(result[0]));
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
     result_type operator()(source_type const &s) const {
@@ -727,17 +717,17 @@
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
-// This converter is needed to enable half_t output types when using int32_t accumulators.
+// This converter is needed to enable cutlass::half_t output types when using int32_t accumulators.
 // Since floating-point types do not require a clamp, this converter simply casts from
-// the source type to half_t.
+// the source type to cutlass::half_t.
 template <
   typename S
 >
 struct NumericConverterClamp<cutlass::half_t, S> {
 
   using result_type = cutlass::half_t;
   using source_type = S;
@@ -836,64 +826,69 @@
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Array<half, 2> <= Array<float, 2>, round to nearest
 template <>
-struct NumericArrayConverter<half_t, float, 2, FloatRoundStyle::round_to_nearest> {
+struct NumericArrayConverter<cutlass::half_t, float, 2, FloatRoundStyle::round_to_nearest> {
 
-  using result_type = Array<half_t, 2>;
+  using result_type = Array<cutlass::half_t, 2>;
   using source_type = Array<float, 2>;
   static FloatRoundStyle const round_style = FloatRoundStyle::round_to_nearest;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & source) {
-
-    Array<half_t, 2> result;
-
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
+      Array<cutlass::half_t, 2> result;
       reinterpret_cast<__half2 &>(result) = __float22half2_rn(reinterpret_cast<float2 const &>(source));
+      return result;
     #else
-      NumericConverter<half_t, float, round_style> convert_;
+      NumericConverter<cutlass::half_t, float, round_style> convert_;
+      // NOTE: cutlass::Array<half, N> is NOT an aggregate type and
+      //  below `{}` does NOT conduct zero initialization. Below `{}` will 
+      //  conduct default initialization (calling default ctr). We use this syntax
+      //  to resolve compiler warning on uninitialized member variable.
+      Array<cutlass::half_t, 2> result{};
       result[0] = convert_(source[0]);
       result[1] = convert_(source[1]);
+      return result;
     #endif
-
-    return result;
   }
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
-/// Partial specialization for Array<float, 2> <= Array<half_t, 2>, round to nearest
+/// Partial specialization for Array<float, 2> <= Array<cutlass::half_t, 2>, round to nearest
 template <FloatRoundStyle Round>
-struct NumericArrayConverter<float, half_t, 2, Round> {
+struct NumericArrayConverter<float, cutlass::half_t, 2, Round> {
 
   using result_type = Array<float, 2>;
-  using source_type = Array<half_t, 2>;
+  using source_type = Array<cutlass::half_t, 2>;
   static FloatRoundStyle const round_style = FloatRoundStyle::round_to_nearest;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & source) {
 
-    Array<float, 2> result;
-
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
-      reinterpret_cast<float2 &>(result) = __half22float2(reinterpret_cast<__half2 const &>(source));
+      float2 result2 = __half22float2(reinterpret_cast<__half2 const &>(source));
+      return {
+        float{result2.x},
+        float{result2.y}
+      };
     #else
-      NumericConverter<float, half_t, round_style> convert_;
-      result[0] = convert_(source[0]);
-      result[1] = convert_(source[1]);
+      NumericConverter<float, cutlass::half_t, round_style> convert_;
+      return {
+        convert_(source[0]),
+        convert_(source[1])
+      };
     #endif
-
-    return result;
   }
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
@@ -901,29 +896,29 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Array<half> <= Array<float>
 template <
   int N,
   FloatRoundStyle Round
 >
-struct NumericArrayConverter<half_t, float, N, Round> {
+struct NumericArrayConverter<cutlass::half_t, float, N, Round> {
 
-  using result_type = Array<half_t, N>;
+  using result_type = Array<cutlass::half_t, N>;
   using source_type = Array<float, N>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & source) {
 
-    NumericArrayConverter<half_t, float, 2, Round> convert_vector_;
-    NumericConverter<half_t, float, Round> convert_element_;
+    NumericArrayConverter<cutlass::half_t, float, 2, Round> convert_vector_;
+    NumericConverter<cutlass::half_t, float, Round> convert_element_;
 
     result_type result;
 
-    Array<half_t, 2> *result_ptr = reinterpret_cast<Array<half_t, 2> *>(&result);
+    Array<cutlass::half_t, 2> *result_ptr = reinterpret_cast<Array<cutlass::half_t, 2> *>(&result);
     Array<float, 2> const *source_ptr = reinterpret_cast<Array<float, 2> const *>(&source);
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N / 2; ++i) {
       result_ptr[i] = convert_vector_(source_ptr[i]);
     }
 
@@ -942,30 +937,30 @@
 
 
 /// Partial specialization for Array<half> <= Array<float>
 template <
   int N,
   FloatRoundStyle Round
 >
-struct NumericArrayConverter<float, half_t, N, Round> {
+struct NumericArrayConverter<float, cutlass::half_t, N, Round> {
 
   using result_type = Array<float, N>;
-  using source_type = Array<half_t, N>;
+  using source_type = Array<cutlass::half_t, N>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & source) {
 
-    NumericArrayConverter<float, half_t, 2, Round> convert_vector_;
-    NumericConverter<float, half_t, Round> convert_element_;
+    NumericArrayConverter<float, cutlass::half_t, 2, Round> convert_vector_;
+    NumericConverter<float, cutlass::half_t, Round> convert_element_;
 
     result_type result;
 
     Array<float, 2> *result_ptr = reinterpret_cast<Array<float, 2> *>(&result);
-    Array<half_t, 2> const *source_ptr = reinterpret_cast<Array<half_t, 2> const *>(&source);
+    Array<cutlass::half_t, 2> const *source_ptr = reinterpret_cast<Array<cutlass::half_t, 2> const *>(&source);
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N / 2; ++i) {
       result_ptr[i] = convert_vector_(source_ptr[i]);
     }
 
     if (N % 2) {
@@ -982,19 +977,19 @@
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Array<bfloat16_t, 2> <= Array<float, 2>, round to nearest
+/// Partial specialization for Array<cutlass::bfloat16_t, 2> <= Array<float, 2>, round to nearest
 template <>
-struct NumericArrayConverter<bfloat16_t, float, 2, FloatRoundStyle::round_to_nearest> {
+struct NumericArrayConverter<cutlass::bfloat16_t, float, 2, FloatRoundStyle::round_to_nearest> {
 
-  using result_type = Array<bfloat16_t, 2>;
+  using result_type = Array<cutlass::bfloat16_t, 2>;
   using source_type = Array<float, 2>;
   static FloatRoundStyle const round_style = FloatRoundStyle::round_to_nearest;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & source) {
 
     unsigned d;
@@ -1006,34 +1001,34 @@
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
-/// Partial specialization for Array<bfloat16_t> <= Array<float>
+/// Partial specialization for Array<cutlass::bfloat16_t> <= Array<float>
 template <
   int N,
   FloatRoundStyle Round
 >
-struct NumericArrayConverter<bfloat16_t, float, N, Round> {
+struct NumericArrayConverter<cutlass::bfloat16_t, float, N, Round> {
 
-  using result_type = Array<bfloat16_t, N>;
+  using result_type = Array<cutlass::bfloat16_t, N>;
   using source_type = Array<float, N>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & source) {
 
-    NumericArrayConverter<bfloat16_t, float, 2, Round> convert_vector_;
-    NumericConverter<bfloat16_t, float, Round> convert_element_;
+    NumericArrayConverter<cutlass::bfloat16_t, float, 2, Round> convert_vector_;
+    NumericConverter<cutlass::bfloat16_t, float, Round> convert_element_;
 
     result_type result;
 
-    Array<bfloat16_t, 2> *result_ptr = reinterpret_cast<Array<bfloat16_t, 2> *>(&result);
+    Array<cutlass::bfloat16_t, 2> *result_ptr = reinterpret_cast<Array<cutlass::bfloat16_t, 2> *>(&result);
     Array<float, 2> const *source_ptr = reinterpret_cast<Array<float, 2> const *>(&source);
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N / 2; ++i) {
       result_ptr[i] = convert_vector_(source_ptr[i]);
     }
 
@@ -1313,17 +1308,17 @@
 //
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Array<float, 2> <= Array<float_e4m3_t, 2>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverter<float, float_e4m3_t, 2, Round> {
+struct NumericArrayConverter<float, cutlass::float_e4m3_t, 2, Round> {
   using result_element = float;
-  using source_element = float_e4m3_t;
+  using source_element = cutlass::float_e4m3_t;
 
   using result_type = Array<result_element, 2>;
   using source_type = Array<source_element, 2>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -1363,15 +1358,15 @@
 };
 
 /// Partial specialization for Array<float_e4m3_t, 2> <= Array<float, 2>
 template <
   FloatRoundStyle Round
 >
 struct NumericArrayConverter<float_e4m3_t, float, 2, Round> {
-  using result_element = float_e4m3_t;
+  using result_element = cutlass::float_e4m3_t;
   using source_element = float;
 
   using result_type = Array<result_element, 2>;
   using source_type = Array<source_element, 2>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
@@ -1406,17 +1401,17 @@
   }
 };
 
 /// Partial specialization for Array<float, 2> <= Array<float_e5m2_t, 2>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverter<float, float_e5m2_t, 2, Round> {
+struct NumericArrayConverter<float, cutlass::float_e5m2_t, 2, Round> {
   using result_element = float;
-  using source_element = float_e5m2_t;
+  using source_element = cutlass::float_e5m2_t;
 
   using result_type = Array<result_element, 2>;
   using source_type = Array<source_element, 2>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -1478,15 +1473,15 @@
 
     result_type result;
     NumericConverter<T, S, Round> convert_;
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < 4; ++i) {
       if (platform::is_same<Transform, cutlass::transform::thread::UnaryTransform::Identity>::value) {
         result[i] = convert_(s[i]);
-      } 
+      }
       else { // conjugate
         result[i] = conj(convert_(s[i]));
       }
     }
 
     return result;
   }
@@ -1497,17 +1492,17 @@
   }
 };
 
 /// Partial specialization for Array<float, 4> <= Array<float_e4m3_t, 4>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverterPacked4Element<float, float_e4m3_t, Round> {
+struct NumericArrayConverterPacked4Element<float, cutlass::float_e4m3_t, Round> {
   using result_element = float;
-  using source_element = float_e4m3_t;
+  using source_element = cutlass::float_e4m3_t;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -1553,15 +1548,15 @@
 };
 
 /// Partial specialization for Array<float_e4m3_t, 4> <= Array<float, 4>
 template <
   FloatRoundStyle Round
 >
 struct NumericArrayConverterPacked4Element<float_e4m3_t, float, Round> {
-  using result_element = float_e4m3_t;
+  using result_element = cutlass::float_e4m3_t;
   using source_element = float;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
@@ -1606,17 +1601,17 @@
 //
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Array<float, 4> <= Array<float_e5m2_t, 4>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverterPacked4Element<float, float_e5m2_t, Round> {
+struct NumericArrayConverterPacked4Element<float, cutlass::float_e5m2_t, Round> {
   using result_element = float;
-  using source_element = float_e5m2_t;
+  using source_element = cutlass::float_e5m2_t;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -1662,15 +1657,15 @@
 };
 
 /// Partial specialization for Array<float_e5m2_t, 4> <= Array<float, 4>
 template <
   FloatRoundStyle Round
 >
 struct NumericArrayConverterPacked4Element<float_e5m2_t, float, Round> {
-  using result_element = float_e5m2_t;
+  using result_element = cutlass::float_e5m2_t;
   using source_element = float;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
@@ -1707,25 +1702,25 @@
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
-// Partial specializations for Array<half_t, 4> <=> Array<float_e4m3_t, 4>
+// Partial specializations for Array<cutlass::half_t, 4> <=> Array<float_e4m3_t, 4>
 //
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Array<half_t, 4> <= Array<float_e4m3_t, 4>
+/// Partial specialization for Array<cutlass::half_t, 4> <= Array<float_e4m3_t, 4>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverterPacked4Element<half_t, float_e4m3_t, Round> {
-  using result_element = half_t;
-  using source_element = float_e4m3_t;
+struct NumericArrayConverterPacked4Element<cutlass::half_t, cutlass::float_e4m3_t, Round> {
+  using result_element = cutlass::half_t;
+  using source_element = cutlass::float_e4m3_t;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -1756,21 +1751,21 @@
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
-/// Partial specialization for Array<float_e4m3_t, 4> <= Array<half_t, 4>
+/// Partial specialization for Array<float_e4m3_t, 4> <= Array<cutlass::half_t, 4>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverterPacked4Element<float_e4m3_t, half_t, Round> {
-  using result_element = float_e4m3_t;
-  using source_element = half_t;
+struct NumericArrayConverterPacked4Element<float_e4m3_t, cutlass::half_t, Round> {
+  using result_element = cutlass::float_e4m3_t;
+  using source_element = cutlass::half_t;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -1807,25 +1802,25 @@
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
-// Partial specializations for Array<half_t, 4> <=> Array<float_e5m2_t, 4>
+// Partial specializations for Array<cutlass::half_t, 4> <=> Array<float_e5m2_t, 4>
 //
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Array<half_t, 4> <= Array<float_e5m2_t, 4>
+/// Partial specialization for Array<cutlass::half_t, 4> <= Array<float_e5m2_t, 4>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverterPacked4Element<half_t, float_e5m2_t, Round> {
-  using result_element = half_t;
-  using source_element = float_e5m2_t;
+struct NumericArrayConverterPacked4Element<cutlass::half_t, cutlass::float_e5m2_t, Round> {
+  using result_element = cutlass::half_t;
+  using source_element = cutlass::float_e5m2_t;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -1856,21 +1851,21 @@
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
-/// Partial specialization for Array<float_e5m2_t, 4> <= Array<half_t, 4>
+/// Partial specialization for Array<float_e5m2_t, 4> <= Array<cutlass::half_t, 4>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverterPacked4Element<float_e5m2_t, half_t, Round> {
-  using result_element = float_e5m2_t;
-  using source_element = half_t;
+struct NumericArrayConverterPacked4Element<float_e5m2_t, cutlass::half_t, Round> {
+  using result_element = cutlass::float_e5m2_t;
+  using source_element = cutlass::half_t;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -1907,25 +1902,25 @@
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
-// Partial specializations for Array<bfloat16_t, 4> <=> Array<float_e4m3_t, 4>
+// Partial specializations for Array<cutlass::bfloat16_t, 4> <=> Array<float_e4m3_t, 4>
 //
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Array<bfloat16_t, 4> <= Array<float_e4m3_t, 4>
+/// Partial specialization for Array<cutlass::bfloat16_t, 4> <= Array<float_e4m3_t, 4>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverterPacked4Element<bfloat16_t, float_e4m3_t, Round> {
-  using result_element = bfloat16_t;
-  using source_element = float_e4m3_t;
+struct NumericArrayConverterPacked4Element<cutlass::bfloat16_t, cutlass::float_e4m3_t, Round> {
+  using result_element = cutlass::bfloat16_t;
+  using source_element = cutlass::float_e4m3_t;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -1959,21 +1954,21 @@
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
-/// Partial specialization for Array<float_e4m3_t, 4> <= Array<bfloat16_t, 4>
+/// Partial specialization for Array<float_e4m3_t, 4> <= Array<cutlass::bfloat16_t, 4>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverterPacked4Element<float_e4m3_t, bfloat16_t, Round> {
-  using result_element = float_e4m3_t;
-  using source_element = bfloat16_t;
+struct NumericArrayConverterPacked4Element<float_e4m3_t, cutlass::bfloat16_t, Round> {
+  using result_element = cutlass::float_e4m3_t;
+  using source_element = cutlass::bfloat16_t;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -2007,25 +2002,25 @@
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
-// Partial specializations for Array<bfloat16_t, 4> <=> Array<float_e5m2_t, 4>
+// Partial specializations for Array<cutlass::bfloat16_t, 4> <=> Array<float_e5m2_t, 4>
 //
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Array<bfloat16_t, 4> <= Array<float_e5m2_t, 4>
+/// Partial specialization for Array<cutlass::bfloat16_t, 4> <= Array<float_e5m2_t, 4>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverterPacked4Element<bfloat16_t, float_e5m2_t, Round> {
-  using result_element = bfloat16_t;
-  using source_element = float_e5m2_t;
+struct NumericArrayConverterPacked4Element<cutlass::bfloat16_t, cutlass::float_e5m2_t, Round> {
+  using result_element = cutlass::bfloat16_t;
+  using source_element = cutlass::float_e5m2_t;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -2059,21 +2054,21 @@
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
-/// Partial specialization for Array<float_e5m2_t, 4> <= Array<bfloat16_t, 4>
+/// Partial specialization for Array<float_e5m2_t, 4> <= Array<cutlass::bfloat16_t, 4>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverterPacked4Element<float_e5m2_t, bfloat16_t, Round> {
-  using result_element = float_e5m2_t;
-  using source_element = bfloat16_t;
+struct NumericArrayConverterPacked4Element<float_e5m2_t, cutlass::bfloat16_t, Round> {
+  using result_element = cutlass::float_e5m2_t;
+  using source_element = cutlass::bfloat16_t;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -2115,17 +2110,17 @@
 //
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Array<float_e4m3_t, 4> <= Array<float_e5m2_t, 4>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverterPacked4Element<float_e4m3_t, float_e5m2_t, Round> {
-  using result_element = float_e4m3_t;
-  using source_element = float_e5m2_t;
+struct NumericArrayConverterPacked4Element<float_e4m3_t, cutlass::float_e5m2_t, Round> {
+  using result_element = cutlass::float_e4m3_t;
+  using source_element = cutlass::float_e5m2_t;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -2146,17 +2141,17 @@
   }
 };
 
 /// Partial specialization for Array<float_e5m2_t, 4> <= Array<float_e4m3_t, 4>
 template <
   FloatRoundStyle Round
 >
-struct NumericArrayConverterPacked4Element<float_e5m2_t, float_e4m3_t, Round> {
-  using result_element = float_e5m2_t;
-  using source_element = float_e4m3_t;
+struct NumericArrayConverterPacked4Element<float_e5m2_t, cutlass::float_e4m3_t, Round> {
+  using result_element = cutlass::float_e5m2_t;
+  using source_element = cutlass::float_e4m3_t;
 
   using result_type = Array<result_element, 4>;
   using source_type = Array<source_element, 4>;
   static FloatRoundStyle const round_style = Round;
 
   CUTLASS_DEVICE
   static result_type convert(source_type const & source) {
@@ -2240,25 +2235,25 @@
 
 /// Partial specialization for Array<T, N> <= Array<float_e4m3_t, N>
 template <
   typename T,
   int N,
   FloatRoundStyle Round
 >
-struct NumericArrayConverter<T, float_e4m3_t, N, Round> :
-  public PackedNumericArrayConverter<T, float_e4m3_t, N, Round> {};
+struct NumericArrayConverter<T, cutlass::float_e4m3_t, N, Round> :
+  public PackedNumericArrayConverter<T, cutlass::float_e4m3_t, N, Round> {};
 
 /// Partial specialization for Array<T, N> <= Array<float_e5m2_t, N>
 template <
   typename T,
   int N,
   FloatRoundStyle Round
 >
-struct NumericArrayConverter<T, float_e5m2_t, N, Round> :
-  public PackedNumericArrayConverter<T, float_e5m2_t, N, Round> {};
+struct NumericArrayConverter<T, cutlass::float_e5m2_t, N, Round> :
+  public PackedNumericArrayConverter<T, cutlass::float_e5m2_t, N, Round> {};
 
 /// Partial specialization for Array<float_e4m3_t, N> <= Array<S, N>
 template <
   typename S,
   int N,
   FloatRoundStyle Round
 >
@@ -2275,78 +2270,188 @@
   public PackedNumericArrayConverter<float_e5m2_t, S, N, Round> {};
 
 /// Partial specialization for Array<float_e4m3_t, N> <= Array<float_e5m2_t, N>
 template <
   int N,
   FloatRoundStyle Round
 >
-struct NumericArrayConverter<float_e4m3_t, float_e5m2_t, N, Round> :
-  public PackedNumericArrayConverter<float_e4m3_t, float_e5m2_t, N, Round> {};
+struct NumericArrayConverter<float_e4m3_t, cutlass::float_e5m2_t, N, Round> :
+  public PackedNumericArrayConverter<float_e4m3_t, cutlass::float_e5m2_t, N, Round> {};
 
 /// Partial specialization for Array<float_e5m2_t, N> <= Array<float_e4m3_t, N>
 template <
   int N,
   FloatRoundStyle Round
 >
-struct NumericArrayConverter<float_e5m2_t, float_e4m3_t, N, Round> :
-  public PackedNumericArrayConverter<float_e5m2_t, float_e4m3_t, N, Round> {};
+struct NumericArrayConverter<float_e5m2_t, cutlass::float_e4m3_t, N, Round> :
+  public PackedNumericArrayConverter<float_e5m2_t, cutlass::float_e4m3_t, N, Round> {};
 
 /// Partial specialization for Array<float_e4m3_t, N> <= Array<float_e4m3_t, N>
 template <
   int N,
   FloatRoundStyle Round
 >
-struct NumericArrayConverter<float_e4m3_t, float_e4m3_t, N, Round> :
-  public PackedNumericArrayConverter<float_e4m3_t, float_e4m3_t, N, Round> {};
+struct NumericArrayConverter<float_e4m3_t, cutlass::float_e4m3_t, N, Round> :
+  public PackedNumericArrayConverter<float_e4m3_t, cutlass::float_e4m3_t, N, Round> {};
 
 /// Partial specialization for Array<float_e5m2_t, N> <= Array<float_e5m2_t, N>
 template <
   int N,
   FloatRoundStyle Round
 >
-struct NumericArrayConverter<float_e5m2_t, float_e5m2_t, N, Round> :
-  public PackedNumericArrayConverter<float_e5m2_t, float_e5m2_t, N, Round> {};
-
-
+struct NumericArrayConverter<float_e5m2_t, cutlass::float_e5m2_t, N, Round> :
+  public PackedNumericArrayConverter<float_e5m2_t, cutlass::float_e5m2_t, N, Round> {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-
 /// Partial specialization for Array<int8_t> <= Array<float>
 /// Conversion is performed with saturation regardless of setting of
 /// the `Round` template parameter.
 template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<int8_t, float, 1, Round> {
+
+  using result_type = Array<int8_t, 1>;
+  using source_type = Array<float, 1>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_HOST_DEVICE
+  static result_type convert(source_type const & source) {
+    // Convert to int to int8_t
+    NumericConverter<int8_t, float, Round> destination_converter;
+    result_type result;
+    result[0] = destination_converter(source[0]);
+    return result;
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) const {
+    return convert(s);
+  }
+};
+
+// To convert a FP32 to Int that has less than 32 bits, we need to convert it to int32 first.
+template <
+  typename T,
   int N,
   FloatRoundStyle Round
 >
-struct NumericArrayConverter<int8_t, float, N, Round> {
+struct NumericArrayFP32ToIntConverter {
 
-  using result_type = Array<int8_t, N>;
+  using result_type = Array<T, N>;
   using source_type = Array<float, N>;
   static FloatRoundStyle const round_style = Round;
 
+  static_assert(platform::numeric_limits<T>::is_integer, "the dest type has to be int.");
+
   CUTLASS_HOST_DEVICE
   static result_type convert(source_type const & source) {
     // Convert float to int
     Array<int32_t, N> temporary;
 
-    NumericArrayConverter<int, float, N, Round> compute_converter;
+    NumericArrayConverter<int32_t, float, N, Round> compute_converter;
     temporary = compute_converter(source);
 
     // Convert to int to int8_t
-    NumericArrayConverter<int8_t, int32_t, N, Round> destination_converter;
+    NumericArrayConverter<T, int32_t, N, Round> destination_converter;
     return destination_converter(temporary);
   }
 
   CUTLASS_HOST_DEVICE
   result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
+
+template <
+  int N,
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<int8_t, float, N, Round> {
+
+  using result_type = Array<int8_t, N>;
+  using source_type = Array<float, N>;
+
+  CUTLASS_HOST_DEVICE
+  static result_type convert(source_type const & source) {
+    NumericArrayFP32ToIntConverter<int8_t, N, Round> converter;
+    return converter(source);
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) const {
+    return convert(s);
+  }
+};
+
+template <
+  int N,
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<uint8_t, float, N, Round> {
+
+  using result_type = Array<uint8_t, N>;
+  using source_type = Array<float, N>;
+
+  CUTLASS_HOST_DEVICE
+  static result_type convert(source_type const & source) {
+    NumericArrayFP32ToIntConverter<uint8_t, N, Round> converter;
+    return converter(source);
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) const {
+    return convert(s);
+  }
+};
+
+template <
+  int N,
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<int4b_t, float, N, Round> {
+
+  using result_type = Array<int4b_t, N>;
+  using source_type = Array<float, N>;
+
+  CUTLASS_HOST_DEVICE
+  static result_type convert(source_type const & source) {
+    NumericArrayFP32ToIntConverter<int4b_t, N, Round> converter;
+    return converter(source);
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) const {
+    return convert(s);
+  }
+};
+
+template <
+  int N,
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<uint4b_t, float, N, Round> {
+
+  using result_type = Array<uint4b_t, N>;
+  using source_type = Array<float, N>;
+
+  CUTLASS_HOST_DEVICE
+  static result_type convert(source_type const & source) {
+    NumericArrayFP32ToIntConverter<uint4b_t, N, Round> converter;
+    return converter(source);
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) const {
+    return convert(s);
+  }
+};
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 750) && \
     ((__CUDACC_VER_MAJOR__ > 10) ||                     \
      ((__CUDACC_VER_MAJOR__ >= 10) && (__CUDACC_VER_MINOR__ >= 2)))
 
 /// Partial specialization for Array<int4b_t, 8> <= Array<int, 8>
@@ -2504,15 +2609,15 @@
     */
   class VectorizedConverter {
   private:
     // Base case to handle remainder elements as scalars.
     template <int Offset, size_t ParentWidth, typename ArrayConverter>
     CUTLASS_DEVICE
     static void convert_helper(
-      typename ArrayConverter::result_type& result, 
+      typename ArrayConverter::result_type& result,
       typename ArrayConverter::source_type const& source) {
 
       using ElementRes = typename ArrayConverter::result_type::Element;
       using ElementSrc = typename ArrayConverter::source_type::Element;
       // If no more converters, handle the remaining elements as scalars.
       constexpr int total_elements = ArrayConverter::result_type::kElements;
       constexpr int remainder = total_elements - Offset;
@@ -2526,22 +2631,22 @@
     }
 
     template <int Offset, size_t ParentWidth, typename ArrayConverter, typename ResultVectorArray, typename SourceVectorArray, typename... OtherVectorArrays>
     CUTLASS_DEVICE
     static void convert_helper(typename ArrayConverter::result_type& result, typename ArrayConverter::source_type const& source) {
       static_assert(sizeof...(OtherVectorArrays) % 2 == 0, "Vector converters must come in {dst, src} pairs");
       static_assert(ResultVectorArray::kElements == SourceVectorArray::kElements, "Vector converters must have the same vector width");
-      static_assert(cutlass::platform::is_same<typename ArrayConverter::result_type::Element, typename ResultVectorArray::Element>::value, 
+      static_assert(cutlass::platform::is_same<typename ArrayConverter::result_type::Element, typename ResultVectorArray::Element>::value,
         "ResultVectorArray must have the same type ArrayConverter::result_type");
-      static_assert(cutlass::platform::is_same<typename ArrayConverter::source_type::Element, typename SourceVectorArray::Element>::value, 
+      static_assert(cutlass::platform::is_same<typename ArrayConverter::source_type::Element, typename SourceVectorArray::Element>::value,
         "SourceVectorArray must have the same type ArrayConverter::result_type");
       static_assert(Offset >= 0 && Offset <= ArrayConverter::result_type::kElements, "Offset must be between 0 and N");
 
       static_assert(ParentWidth == 0 || ParentWidth > ResultVectorArray::kElements, "Vector arrays must be given in decreasing order of width");
-      
+
       constexpr int vector_width = ResultVectorArray::kElements;
       static_assert(ispow2(vector_width), "Vector width must be a power of 2");
 
       using ElementRes = typename ArrayConverter::result_type::Element;
       using ElementSrc = typename ArrayConverter::source_type::Element;
 
       constexpr int vector_bits_res = vector_width * cutlass::sizeof_bits<ElementRes>::value;
@@ -2565,16 +2670,16 @@
       constexpr int new_offset = Offset + vector_width * groups_of_vec;
       // Recurse to handle other vector converters, or the scalar base case.
       convert_helper<new_offset, ResultVectorArray::kElements, ArrayConverter, OtherVectorArrays...>(result, source);
     }
 
   public:
     /*
-        A method to convert vectors of elements using the packed_convert method of the converter. 
-        
+        A method to convert vectors of elements using the packed_convert method of the converter.
+
         Converters using this class must implement packed convert and support 1 or more vector conversions.
       */
     template <typename ArrayConverter, typename ResultVectorArray, typename SourceVectorArray, typename... OtherVectorArrays>
     CUTLASS_DEVICE
     static void convert(typename ArrayConverter::result_type& result, typename ArrayConverter::source_type const& source) {
       convert_helper<0, 0, ArrayConverter, ResultVectorArray, SourceVectorArray, OtherVectorArrays...>(result, source);
     }
@@ -2647,15 +2752,15 @@
 
     const int iters = PackedSrcType::kElements / 4;
     #pragma unroll
     for (int ii = 0; ii < iters; ++ii, lut_idx >>=16, sign >>=16) {
       uint32_t final_prmt_idx = final_prmt_base | sign;
 
       // This uses a look up table to convert packed int4s to packed fp8s, using the int4 value
-      // as the index to prmt. 
+      // as the index to prmt.
       // It first select both the positive and negative candidates, then uses the sign bit to
       // select the correct candidate.
       asm volatile(
           "{\n"
           "  .reg .b32 pos_f8s, neg_f8s;\n"
           "  prmt.b32 pos_f8s, %1, %2, %5;\n"
           "  prmt.b32 neg_f8s, %3, %4, %5;\n"
@@ -2671,24 +2776,24 @@
   friend class detail::VectorizedConverter;
 
 public:
   CUTLASS_DEVICE
   static result_type convert(source_type const &source) {
     result_type result;
     using ConverterType = NumericArrayConverter<typename result_type::Element, typename source_type::Element, N, Round>;
-    detail::VectorizedConverter::convert<ConverterType, 
-                                         result_type_packed_8, source_type_packed_8, 
+    detail::VectorizedConverter::convert<ConverterType,
+                                         result_type_packed_8, source_type_packed_8,
                                          result_type_packed_4, source_type_packed_4>(result, source);
 
     return result;
   }
 
 
   CUTLASS_DEVICE
-  result_type operator()(source_type const &s) const { 
+  result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /// Partial specialization for Array<float, N> <= Array<cutlass::int4b_t, N>
 template <FloatRoundStyle Round, int N>
 struct NumericArrayConverter<float, cutlass::int4b_t, N, Round> {
@@ -2767,15 +2872,15 @@
     static_assert((platform::is_same<PackedSrcType, source_type_packed_2>::value &&
                    platform::is_same<PackedResultType, result_type_packed_2>::value) ||
                   (platform::is_same<PackedSrcType, source_type_packed_4>::value &&
                    platform::is_same<PackedResultType, result_type_packed_4>::value) ||
                   (platform::is_same<PackedSrcType, source_type_packed_8>::value &&
                    platform::is_same<PackedResultType, result_type_packed_8>::value),
                   "Invalid PackedSrcType/PackedResultType must be 1, 2, 4 or 8 to use private convert dispatch.");
-    
+
     // Hold output FP16s in reg. We need 1 reg for every 2 elements
     PackedResultType r;
 
     // View the input as reg
     uint32_t src_reg = to_reg(source);
     constexpr int total_elements = PackedResultType::kElements == 8 ? 4 : PackedResultType::kElements;
     packed_convert_vec<0, total_elements>(r, src_reg);
@@ -2784,31 +2889,31 @@
     if (PackedResultType::kElements == 8) {
       uint32_t src_reg_shifted = src_reg >> 16;
       packed_convert_vec<4, 4>(r, src_reg_shifted);
     }
     return r;
   }
 
-  friend class detail::VectorizedConverter; 
+  friend class detail::VectorizedConverter;
 
 public:
   CUTLASS_DEVICE
   static result_type convert(source_type const &source) {
     result_type result;
     using ConverterType = NumericArrayConverter<typename result_type::Element, typename source_type::Element, N, Round>;
-    detail::VectorizedConverter::convert<ConverterType, 
-                                         result_type_packed_8, source_type_packed_8, 
-                                         result_type_packed_4, source_type_packed_4, 
+    detail::VectorizedConverter::convert<ConverterType,
+                                         result_type_packed_8, source_type_packed_8,
+                                         result_type_packed_4, source_type_packed_4,
                                          result_type_packed_2, source_type_packed_2>(result, source);
 
     return result;
   }
 
   CUTLASS_DEVICE
-  result_type operator()(source_type const &s) const { 
+  result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /// Partial specialization for Array<float, N> <= Array<int8_t, N>
 template <FloatRoundStyle Round, int N>
 struct NumericArrayConverter<float, int8_t, N, Round> {
@@ -2840,15 +2945,15 @@
   static PackedResultType packed_convert(PackedSrcType const &source) {
 
     static_assert((platform::is_same<PackedSrcType, source_type_packed_2>::value &&
                    platform::is_same<PackedResultType, result_type_packed_2>::value) ||
                   (platform::is_same<PackedSrcType, source_type_packed_4>::value &&
                    platform::is_same<PackedResultType, result_type_packed_4>::value),
                   "Invalid PackedSrcType/PackedResultType must be 2 or 4 to use private convert dispatch.");
-    
+
     PackedResultType r;
     // View the input as reg
     uint32_t src_reg = to_reg(source);
     static constexpr int fp32_base = 0x4B400000;
     uint32_t const prmt_indices[4] = {0x8880, 0x9991, 0xAAA2, 0xBBB3};
 
     int* result_as_int = reinterpret_cast<int*>(&r);
@@ -2871,23 +2976,23 @@
 
 public:
   CUTLASS_DEVICE
   static result_type convert(source_type const &source) {
     result_type result;
 
     using ConverterType = NumericArrayConverter<typename result_type::Element, typename source_type::Element, N, Round>;
-    detail::VectorizedConverter::convert<ConverterType, 
-                                         result_type_packed_4, source_type_packed_4, 
+    detail::VectorizedConverter::convert<ConverterType,
+                                         result_type_packed_4, source_type_packed_4,
                                          result_type_packed_2, source_type_packed_2>(result, source);
 
     return result;
   }
 
   CUTLASS_DEVICE
-  result_type operator()(source_type const &s) const { 
+  result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /// Partial specialization for Array<float, N> <= Array<uint8_t, N>
 template <FloatRoundStyle Round, int N>
 struct NumericArrayConverter<float, uint8_t, N, Round> {
@@ -2919,20 +3024,20 @@
   static PackedResultType packed_convert(PackedSrcType const &source) {
 
     static_assert((platform::is_same<PackedSrcType, source_type_packed_2>::value &&
                    platform::is_same<PackedResultType, result_type_packed_2>::value) ||
                   (platform::is_same<PackedSrcType, source_type_packed_4>::value &&
                    platform::is_same<PackedResultType, result_type_packed_4>::value),
                   "Invalid PackedSrcType/PackedResultType must be 2 or 4 to use private convert dispatch.");
-    
+
     PackedResultType r;
     // View the input as reg
     uint32_t src_reg = to_reg(source);
 
-    // __byte_perm simulates the add.u32 0x4B000000 to every u8 element of u8x4 source and stores 
+    // __byte_perm simulates the add.u32 0x4B000000 to every u8 element of u8x4 source and stores
     // the result in r (without introducing extra cvt.u32.u8 instruction)
     uint32_t const prmt_indices[4] = {0x7650, 0x7651, 0x7652, 0x7653};
     uint32_t* result_as_int = reinterpret_cast<uint32_t*>(&r);
     for (int ii = 0; ii < PackedResultType::kElements; ++ii) {
       result_as_int[ii] = __byte_perm(src_reg, 0x4B000000, prmt_indices[ii]);
       // Subtract the magic number 0x4B000000 from tmp in floating-point arithmetic to obtain final result
       r[ii] -= 8388608.f;
@@ -2944,23 +3049,23 @@
   friend class detail::VectorizedConverter;
 
 public:
   CUTLASS_DEVICE
   static result_type convert(source_type const &source) {
     result_type result;
     using ConverterType = NumericArrayConverter<typename result_type::Element, typename source_type::Element, N, Round>;
-    detail::VectorizedConverter::convert<ConverterType, 
-                                         result_type_packed_4, source_type_packed_4, 
+    detail::VectorizedConverter::convert<ConverterType,
+                                         result_type_packed_4, source_type_packed_4,
                                          result_type_packed_2, source_type_packed_2>(result, source);
 
     return result;
   }
 
   CUTLASS_DEVICE
-  result_type operator()(source_type const &s) const { 
+  result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// Partial specialization for Array<cutlass::half_t, N> <= Array<cutlass::int4b_t, N>
 template <FloatRoundStyle Round, int N>
@@ -3006,18 +3111,18 @@
     static_assert((platform::is_same<PackedSrcType, source_type_packed_2>::value &&
                    platform::is_same<PackedResultType, result_type_packed_2>::value) ||
                   (platform::is_same<PackedSrcType, source_type_packed_4>::value &&
                    platform::is_same<PackedResultType, result_type_packed_4>::value) ||
                   (platform::is_same<PackedSrcType, source_type_packed_8>::value &&
                    platform::is_same<PackedResultType, result_type_packed_8>::value),
                   "Invalid PackedSrcType/PackedResultType must be 2, 4 or 8 to use private convert dispatch.");
-    
+
     // Hold output FP16s in reg. We need 1 reg for every 2 elements
     using RegArray = cutlass::AlignedArray<uint32_t, PackedResultType::kElements / 2, sizeof(PackedResultType)>;
-    RegArray r; 
+    RegArray r;
 
     // View the input as reg
     uint32_t src_reg = to_reg(source);
 
     // Below constructs the following temporary:
     // fp16s_01 = {0x00, i4_01, 0x00, i4_01}
     // fp16s_23 = {0x00, i4_23, 0x00, i4_23}
@@ -3030,15 +3135,15 @@
     CUTLASS_PRAGMA_UNROLL
     for (int ii = 0; ii < RegArray::kElements; ++ii) {
       asm volatile(
           "{\n"
           "  prmt.b32 %0, %1, %2, %3;\n"
           "}\n"
           : "=r"(r[ii])
-          : "r"(src_reg), "n"(0), "r"(prmt_indices[ii]));     
+          : "r"(src_reg), "n"(0), "r"(prmt_indices[ii]));
     }
 
     // The below XOR does the following:
     // 1) Sets the exponent bits of the FP16 to the correct value for the FP16 magic_num. We will be constructing
     //    1024 + x + 8 OR 1024 + 16 * (x + 8), then using hfma to subtract 1032 from that
     // 2) Adds 8 to the int4 value that we will process in the FP16 (for uint4, we can simply avoid this step)
     // The AND does the following:
@@ -3053,15 +3158,15 @@
     CUTLASS_PRAGMA_UNROLL
     for (int ii = 0; ii < RegArray::kElements; ++ii) {
       asm volatile(
           "{\n"
           "  lop3.b32 %0, %0, %1, %2, %3;\n"
           "}\n"
           : "+r"(r[ii])
-          : "n"(and_mask), "n"(xor_mask), "n"(immLut));     
+          : "n"(and_mask), "n"(xor_mask), "n"(immLut));
     }
 
     // We will issue 2 hfmas that do the following:
     // For the high FP16:
     //  Divide by 16 {packed as a operand} to get:
     //    64 + (x + 8)
     //    x + 72
@@ -3083,31 +3188,31 @@
     for (int ii = 0; ii < RegArray::kElements; ++ii) {
       half2& fp16x2_val = reinterpret_cast<__half2&>(r[ii]);
       fp16x2_val = __hfma2(hfma_scale, fp16x2_val, hfma_bias);
     }
     return reinterpret_cast<PackedResultType&>(r);
   }
 
-  friend class detail::VectorizedConverter; 
+  friend class detail::VectorizedConverter;
 
 public:
   CUTLASS_DEVICE
   static result_type convert(source_type const &source) {
     result_type result;
     using ConverterType = NumericArrayConverter<typename result_type::Element, typename source_type::Element, N, Round>;
-    detail::VectorizedConverter::convert<ConverterType, 
-                                         result_type_packed_8, source_type_packed_8, 
-                                         result_type_packed_4, source_type_packed_4, 
+    detail::VectorizedConverter::convert<ConverterType,
+                                         result_type_packed_8, source_type_packed_8,
+                                         result_type_packed_4, source_type_packed_4,
                                          result_type_packed_2, source_type_packed_2>(result, source);
 
     return result;
   }
 
   CUTLASS_DEVICE
-  result_type operator()(source_type const &s) const { 
+  result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /// Partial specialization for Array<cutlass::half_t, N> <= Array<int8_t, N>
 template <FloatRoundStyle Round, int N>
 struct NumericArrayConverter<cutlass::half_t, int8_t, N, Round> {
@@ -3141,18 +3246,18 @@
   static PackedResultType packed_convert(PackedSrcType const &source) {
 
     static_assert((platform::is_same<PackedSrcType, source_type_packed_2>::value &&
                    platform::is_same<PackedResultType, result_type_packed_2>::value) ||
                   (platform::is_same<PackedSrcType, source_type_packed_4>::value &&
                    platform::is_same<PackedResultType, result_type_packed_4>::value),
                   "Invalid PackedSrcType/PackedResultType must be 2 or 4 to use private convert dispatch.");
-    
+
     // Hold output FP16s in reg. We need 1 reg for every 2 elements
     using RegArray = cutlass::AlignedArray<uint32_t, PackedResultType::kElements / 2, sizeof(PackedResultType)>;
-    RegArray r; 
+    RegArray r;
 
     #if 0 // Scalar conversion (Please keep this code for reference for vectorized version below)
     auto result = reinterpret_cast<PackedResultType&>(r);
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < PackedResultType::kElements; ++i) {
       int16_t tmp = source[i] + 26112 /* 0x6600 */;
       result[i] = reinterpret_cast<cutlass::half_t const &>(tmp) - 1536.0_hf;
@@ -3172,26 +3277,26 @@
     for (int ii = 0; ii < RegArray::kElements; ++ii) {
       asm volatile("prmt.b32 %0,%1,%1,%2;\n" : "=r"(r[ii]) : "r"(src_reg), "r"(prmt_indices[ii]));
     }
 
     // In the absense of add.s16x2 instruction, use bit-wise operation to execute signed addition with magic numbers to achieve
     // the same result as add.s16x2 instruction.
     // (See https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#logic-and-shift-instructions-lop3)
-    // For a logical operation F(a, b, c) the value of kImmLut can be computed by applying the same operation to 
+    // For a logical operation F(a, b, c) the value of kImmLut can be computed by applying the same operation to
     // three predefined constant values as follows:
     //                                        ta = 0xF0;
     //                                        tb = 0xCC;
     //                                        tc = 0xAA;
     //                                   kImmLut = F(ta, tb, tc);
-    // If we want F = ((a & b) ^ c) then set kImmLut = (0xF0 & 0xCC) ^ 0xAA 
-    static constexpr uint32_t kImmLut = (0xF0 & 0xCC) ^ 0xAA; 
+    // If we want F = ((a & b) ^ c) then set kImmLut = (0xF0 & 0xCC) ^ 0xAA
+    static constexpr uint32_t kImmLut = (0xF0 & 0xCC) ^ 0xAA;
 
     for (int ii = 0; ii < RegArray::kElements; ++ii) {
       // The bit-wise operation executed below is `r[ii] = (r[ii] & 0x03FF03FF) ^ 0x66006600;`
-      asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n" : 
+      asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n" :
                                 "=r"(r[ii]) : "r"(r[ii]), "n"(0x03FF03FF), "n"(0x66006600), "n"(kImmLut));
     }
 
     static constexpr uint32_t bias_rep = 0x66006600;
     const half2& bias = reinterpret_cast<const half2&>(bias_rep);
     CUTLASS_PRAGMA_UNROLL
     for (int ii = 0; ii < RegArray::kElements; ++ii) {
@@ -3205,22 +3310,22 @@
 
 public:
   CUTLASS_DEVICE
   static result_type convert(source_type const &source) {
     result_type result;
 
     using ConverterType = NumericArrayConverter<typename result_type::Element, typename source_type::Element, N, Round>;
-    detail::VectorizedConverter::convert<ConverterType, 
-                                         result_type_packed_4, source_type_packed_4, 
+    detail::VectorizedConverter::convert<ConverterType,
+                                         result_type_packed_4, source_type_packed_4,
                                          result_type_packed_2, source_type_packed_2>(result, source);
     return result;
   }
 
   CUTLASS_DEVICE
-  result_type operator()(source_type const &s) const { 
+  result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /// Partial specialization for Array<cutlass::half_t, N> <= Array<uint8_t, N>
 template <FloatRoundStyle Round, int N>
 struct NumericArrayConverter<cutlass::half_t, uint8_t, N, Round> {
@@ -3252,19 +3357,19 @@
   static PackedResultType packed_convert(PackedSrcType const &source) {
 
     static_assert((platform::is_same<PackedSrcType, source_type_packed_2>::value &&
                    platform::is_same<PackedResultType, result_type_packed_2>::value) ||
                   (platform::is_same<PackedSrcType, source_type_packed_4>::value &&
                    platform::is_same<PackedResultType, result_type_packed_4>::value),
                   "Invalid PackedSrcType/PackedResultType must be 2 or 4 to use private convert dispatch.");
-    
+
     // Hold output FP16s in reg. We need 1 reg for every 2 elements
     using RegArray = cutlass::AlignedArray<uint32_t, PackedResultType::kElements / 2, sizeof(PackedResultType)>;
-    RegArray r; 
-  
+    RegArray r;
+
     // View the input as reg
     uint32_t src_reg = to_reg(source);
     uint32_t const prmt_indices[2] = {0x5150, 0x5352};
     static constexpr uint32_t start_byte_for_fp16 = 0x64646464;
 
     for (int ii = 0; ii < RegArray::kElements; ++ii) {
       asm volatile("prmt.b32 %0,%1,%2,%3;\n" : "=r"(r[ii]) : "r"(src_reg), "n"(start_byte_for_fp16), "r"(prmt_indices[ii]));
@@ -3285,23 +3390,23 @@
 
 public:
   CUTLASS_DEVICE
   static result_type convert(source_type const &source) {
     result_type result;
 
     using ConverterType = NumericArrayConverter<typename result_type::Element, typename source_type::Element, N, Round>;
-    detail::VectorizedConverter::convert<ConverterType, 
-                                         result_type_packed_4, source_type_packed_4, 
+    detail::VectorizedConverter::convert<ConverterType,
+                                         result_type_packed_4, source_type_packed_4,
                                          result_type_packed_2, source_type_packed_2>(result, source);
 
     return result;
   }
 
   CUTLASS_DEVICE
-  result_type operator()(source_type const &s) const { 
+  result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// Partial specialization for Array<cutlass::bfloat16_t, N> <= Array<cutlass::int4b_t, N>
@@ -3348,18 +3453,18 @@
     static_assert((platform::is_same<PackedSrcType, source_type_packed_2>::value &&
                    platform::is_same<PackedResultType, result_type_packed_2>::value) ||
                   (platform::is_same<PackedSrcType, source_type_packed_4>::value &&
                    platform::is_same<PackedResultType, result_type_packed_4>::value) ||
                   (platform::is_same<PackedSrcType, source_type_packed_8>::value &&
                    platform::is_same<PackedResultType, result_type_packed_8>::value),
                   "Invalid PackedSrcType/PackedResultType must be 2, 4 or 8 to use private convert dispatch.");
-    
+
     // Hold output FP16s in reg. We need 1 reg for every 2 elements
     using RegArray = cutlass::AlignedArray<uint32_t, PackedResultType::kElements / 2, sizeof(PackedResultType)>;
-    RegArray r; 
+    RegArray r;
 
     // View the input as reg
     uint32_t src_reg = to_reg(source);
     uint32_t src_reg_shifted = src_reg >> 4;
 
     // Below constructs the following temporary:
     uint32_t const prmt_indices[4] = {0xF4F0, 0xF5F1, 0xF6F2, 0xF7F3};
@@ -3367,15 +3472,15 @@
     CUTLASS_PRAGMA_UNROLL
     for (int ii = 0; ii < RegArray::kElements; ++ii) {
       asm volatile(
           "{\n"
           "  prmt.b32 %0, %1, %2, %3;\n"
           "}\n"
           : "=r"(r[ii])
-          : "r"(src_reg), "r"(src_reg_shifted), "r"(prmt_indices[ii]));     
+          : "r"(src_reg), "r"(src_reg_shifted), "r"(prmt_indices[ii]));
     }
 
     // The below XOR does the following:
     // 1) Sets the exponent bits of the FP16 to the correct value for the FP16 magic_num. We will be constructing
     //    128 + (x + 8) and subtracting 136 to get x
     static constexpr uint32_t xor_mask = 0x43084308;
     static constexpr uint32_t and_mask = 0x000F000F;
@@ -3386,51 +3491,51 @@
     CUTLASS_PRAGMA_UNROLL
     for (int ii = 0; ii < RegArray::kElements; ++ii) {
       asm volatile(
           "{\n"
           "  lop3.b32 %0, %0, %1, %2, %3;\n"
           "}\n"
           : "+r"(r[ii])
-          : "n"(and_mask), "n"(xor_mask), "n"(immLut));     
+          : "n"(and_mask), "n"(xor_mask), "n"(immLut));
     }
 
     // We will issue 2 bfmas that do the following:
     // high BF16:
     // hi_bf16 - 136, lo_bf16 - 136
 
     // This is the BF16 {136, 136} represented as an integer.
     static constexpr uint32_t bias_rep = 0x43084308;
     const __nv_bfloat162& bias = reinterpret_cast<const __nv_bfloat162&>(bias_rep);
-    
+
     CUTLASS_PRAGMA_UNROLL
     for (int ii = 0; ii < RegArray::kElements; ++ii) {
       __nv_bfloat162& bf16x2_val = reinterpret_cast<__nv_bfloat162&>(r[ii]);
       bf16x2_val = __hsub2(bf16x2_val, bias);
     }
 
     return reinterpret_cast<PackedResultType&>(r);
   }
 
-  friend class detail::VectorizedConverter; 
+  friend class detail::VectorizedConverter;
 
 public:
   CUTLASS_DEVICE
   static result_type convert(source_type const &source) {
     result_type result;
     using ConverterType = NumericArrayConverter<typename result_type::Element, typename source_type::Element, N, Round>;
-    detail::VectorizedConverter::convert<ConverterType, 
-                                         result_type_packed_8, source_type_packed_8, 
-                                         result_type_packed_4, source_type_packed_4, 
+    detail::VectorizedConverter::convert<ConverterType,
+                                         result_type_packed_8, source_type_packed_8,
+                                         result_type_packed_4, source_type_packed_4,
                                          result_type_packed_2, source_type_packed_2>(result, source);
 
     return result;
   }
 
   CUTLASS_DEVICE
-  result_type operator()(source_type const &s) const { 
+  result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /// Partial specialization for Array<cutlass::bfloat16_t, N> <= Array<int8_t, N>
 template <FloatRoundStyle Round, int N>
 struct NumericArrayConverter<cutlass::bfloat16_t, int8_t, N, Round> {
@@ -3462,38 +3567,38 @@
   static PackedResultType packed_convert(PackedSrcType const &source) {
 
     static_assert((platform::is_same<PackedSrcType, source_type_packed_2>::value &&
                    platform::is_same<PackedResultType, result_type_packed_2>::value) ||
                   (platform::is_same<PackedSrcType, source_type_packed_4>::value &&
                    platform::is_same<PackedResultType, result_type_packed_4>::value),
                   "Invalid PackedSrcType/PackedResultType must be 2 or 4 to use private convert dispatch.");
-    
+
     NumericArrayConverter<float, int8_t, PackedResultType::kElements, Round> convert_int8_to_f32;
     Array<float, PackedResultType::kElements> tmp = convert_int8_to_f32(source);
     NumericArrayConverter<cutlass::bfloat16_t, float, PackedResultType::kElements, Round> convert_f32_to_bf16;
     return convert_f32_to_bf16(tmp);
   }
 
   friend class detail::VectorizedConverter;
 
 public:
   CUTLASS_DEVICE
   static result_type convert(source_type const &source) {
     result_type result;
 
     using ConverterType = NumericArrayConverter<typename result_type::Element, typename source_type::Element, N, Round>;
-    detail::VectorizedConverter::convert<ConverterType, 
-                                         result_type_packed_4, source_type_packed_4, 
+    detail::VectorizedConverter::convert<ConverterType,
+                                         result_type_packed_4, source_type_packed_4,
                                          result_type_packed_2, source_type_packed_2>(result, source);
 
     return result;
   }
 
   CUTLASS_DEVICE
-  result_type operator()(source_type const &s) const { 
+  result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 /// Partial specialization for Array<cutlass::bfloat16_t, N> <= Array<uint8_t, N>
 template <FloatRoundStyle Round, int N>
 struct NumericArrayConverter<cutlass::bfloat16_t, uint8_t, N, Round> {
@@ -3525,37 +3630,37 @@
   static PackedResultType packed_convert(PackedSrcType const &source) {
 
     static_assert((platform::is_same<PackedSrcType, source_type_packed_2>::value &&
                    platform::is_same<PackedResultType, result_type_packed_2>::value) ||
                   (platform::is_same<PackedSrcType, source_type_packed_4>::value &&
                    platform::is_same<PackedResultType, result_type_packed_4>::value),
                   "Invalid PackedSrcType/PackedResultType must be 2 or 4 to use private convert dispatch.");
-    
+
     NumericArrayConverter<float, uint8_t, PackedResultType::kElements, Round> convert_uint8_to_f32;
     Array<float, PackedResultType::kElements> tmp = convert_uint8_to_f32(source);
     NumericArrayConverter<cutlass::bfloat16_t, float, PackedResultType::kElements, Round> convert_f32_to_bf16_;
     return convert_f32_to_bf16_(tmp);
   }
 
   friend class detail::VectorizedConverter;
 
 public:
   CUTLASS_DEVICE
   static result_type convert(source_type const &source) {
     result_type result;
     using ConverterType = NumericArrayConverter<typename result_type::Element, typename source_type::Element, N, Round>;
-    detail::VectorizedConverter::convert<ConverterType, 
-                                         result_type_packed_4, source_type_packed_4, 
+    detail::VectorizedConverter::convert<ConverterType,
+                                         result_type_packed_4, source_type_packed_4,
                                          result_type_packed_2, source_type_packed_2>(result, source);
 
     return result;
   }
 
   CUTLASS_DEVICE
-  result_type operator()(source_type const &s) const { 
+  result_type operator()(source_type const &s) const {
     return convert(s);
   }
 };
 
 #endif // defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -3673,15 +3778,15 @@
 struct PreferredRoundingMode {
   static FloatRoundStyle const kRound = FloatRoundStyle::round_to_nearest;
 };
 
 #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 900
 /// Defines preferred rounding mode for a pair of types
 template <>
-struct PreferredRoundingMode<tfloat32_t, float> {
+struct PreferredRoundingMode<cutlass::tfloat32_t, float> {
   static FloatRoundStyle const kRound = FloatRoundStyle::round_half_ulp_truncate;
 };
 #endif
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Packs predicates into an array.
@@ -3701,15 +3806,15 @@
     uint8_t *bytes = reinterpret_cast<uint8_t *>(packed.data());
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       int word_idx = (i / kWordSize);
       int bit_idx = (i % kWordSize);
 
-      uint8_t mask = ((predicates[i] ? 1u : 0u) << bit_idx);
+      uint8_t mask = static_cast<uint8_t>((predicates[i] ? 1u : 0u) << bit_idx);
       bytes[word_idx] = (bytes[word_idx] | mask);
     }
     return packed;
   }
 };
 
 /// Packs predicates into an array
```

## cutlass_library/source/include/cutlass/numeric_size.h

```diff
@@ -29,65 +29,54 @@
  *
  **************************************************************************************************/
 /*!
     \file
     \brief Top-level include for all CUTLASS numeric types.
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #include "cutlass/cutlass.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Defines the size of an element in bits
 template <typename T>
 struct sizeof_bits {
-  static int const value = int(sizeof(T) * 8);
+  static constexpr int value = int(sizeof(T) * 8);
 };
 
 template <typename T>
 struct sizeof_bits<T const>: sizeof_bits<T> {};
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-//
-// Definitions for 1-bit binary and 4-bit integer types
-//
-
-/// 1-bit binary type
-using bin1_t = bool;
-
-/// Defines the size of an element in bits - specialized for bin1_t
 template <>
-struct sizeof_bits<bin1_t> {
-  static int const value = 1;
+struct sizeof_bits<void> {
+  static constexpr int value = 0;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Returns the number of bytes required to hold a specified number of bits
 CUTLASS_HOST_DEVICE
-constexpr int
+CUTLASS_CONSTEXPR_IF_CXX17
+int
 bits_to_bytes(int bits) {
   return (bits + 7) / 8;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+template <class T>
+struct is_subbyte {
+  static constexpr bool value = sizeof_bits<T>::value < 8;
+};
+
+template <class T>
+struct is_subbyte<T const> : is_subbyte<T> {};
+
 }  // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/numeric_types.h

```diff
@@ -28,26 +28,18 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! 
     \file
     \brief Top-level include for all CUTLASS numeric types.
 */
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
 #pragma once
 
 #include "cutlass/cutlass.h"
+#include "cutlass/platform/platform.h"
 #include "cutlass/numeric_size.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -83,14 +75,14 @@
 } // namespace detail
 
 }  // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #include "cutlass/integer_subbyte.h"
-
 #include "cutlass/half.h"
 #include "cutlass/bfloat16.h"
 #include "cutlass/tfloat32.h"
 #include "cutlass/float8.h"
+#include "cutlass/uint128.h"
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/uint128.h

```diff
@@ -28,23 +28,14 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! 
   \file
   \brief Defines an unsigned 128b integer with several operators to support 64-bit integer division.
 */
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
 #pragma once
 
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/cstdint>
 #else
 #include <cstdint>
 #include <cstdlib>
@@ -57,167 +48,171 @@
 
 /// Optionally enable GCC's built-in type
 #if (defined(__x86_64) || defined (__aarch64__)) && !(defined(__CUDA_ARCH__) && ((__CUDACC_VER_MAJOR__ <= 10) || ((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ <= 4)))) && defined(__GNUC__)
 #define CUTLASS_UINT128_NATIVE
 #elif defined(_MSC_VER) && defined(_M_AMD64) && !(defined(__CUDA_ARCH__) && ((__CUDACC_VER_MAJOR__ <= 10) || ((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ <= 4))))
 #define CUTLASS_INT128_ARITHMETIC
 #include <intrin.h>
-#if _MSC_VER >= 1920
+#if _MSC_VER >= 1920 && !defined(__CUDA_ARCH__)
 #define CUTLASS_INT128_ARITHMETIC_DIV
 #include <immintrin.h>
 #endif
 #endif
 
 namespace cutlass {
 
 ///! Unsigned 128b integer type
-struct uint128_t {
-
+struct alignas(16) uint128_t
+{
   /// Size of one part of the uint's storage in bits
-  static constexpr int kPartSize = sizeof_bits<uint64_t>::value;
+  static constexpr int storage_bits_ = 64;
 
-  struct hilo {
+  struct hilo
+  {
     uint64_t lo;
     uint64_t hi;
-
-    hilo() = default;
-
-    CUTLASS_HOST_DEVICE hilo(uint64_t lo_, uint64_t hi_):lo(lo_), hi(hi_) {}
   };
 
   // Use a union to store either low and high parts or, if present, a built-in 128b integer type.
   union {
     struct hilo hilo_;
 
-    #if defined(CUTLASS_UINT128_NATIVE)
+#if defined(CUTLASS_UINT128_NATIVE)
     unsigned __int128 native;
-    #endif // defined(CUTLASS_UINT128_NATIVE)
+#endif // defined(CUTLASS_UINT128_NATIVE)
   };
 
   //
   // Methods
   //
 
   /// Default ctor
-  uint128_t() = default;
+  CUTLASS_HOST_DEVICE
+  uint128_t() : hilo_{0, 0} {}
 
   /// Constructor from uint64
   CUTLASS_HOST_DEVICE
-  uint128_t(uint64_t lo_): hilo_(lo_, 0) { }
+  uint128_t(uint64_t lo_) : hilo_{lo_, 0} {}
 
   /// Constructor from two 64b unsigned integers
   CUTLASS_HOST_DEVICE
-  uint128_t(uint64_t lo_, uint64_t hi_): hilo_(lo_, hi_) {
-
-  }
+  uint128_t(uint64_t lo_, uint64_t hi_) : hilo_{lo_, hi_} {}
 
   /// Optional constructor from native value
-  #if defined(CUTLASS_UINT128_NATIVE)
-  uint128_t(unsigned __int128 value): native(value) { }
-  #endif
+#if defined(CUTLASS_UINT128_NATIVE)
+  uint128_t(unsigned __int128 value) : native(value) { }
+#endif
 
   /// Lossily cast to uint64
   CUTLASS_HOST_DEVICE
-  explicit operator uint64_t() const {
+  explicit operator uint64_t() const
+  {
     return hilo_.lo;
   }
 
   CUTLASS_HOST_DEVICE
-  static void exception() {
+  static void exception()
+  {
 #if defined(__CUDA_ARCH__)
   asm volatile ("  brkpt;\n");
 #else
   // throw std::runtime_error("Not yet implemented.");
   abort();
 #endif
   }
 
   /// Add
   CUTLASS_HOST_DEVICE
-  uint128_t operator+(uint128_t const &rhs) const {
-    uint128_t y;
+  uint128_t operator+(uint128_t const& rhs) const
+  {
+    uint128_t y{};
 #if defined(CUTLASS_UINT128_NATIVE)
     y.native = native + rhs.native;
 #else
     y.hilo_.lo = hilo_.lo + rhs.hilo_.lo;
-    y.hilo_.hi = hilo_.hi + rhs.hilo_.hi + (!y.hilo_.lo && (rhs.hilo_.lo));
+    y.hilo_.hi = hilo_.hi + rhs.hilo_.hi + (y.hilo_.lo < hilo_.lo);
 #endif
     return y;
   }
 
   /// Subtract
   CUTLASS_HOST_DEVICE
-  uint128_t operator-(uint128_t const &rhs) const {
-    uint128_t y;
+  uint128_t operator-(uint128_t const& rhs) const
+  {
+    uint128_t y{};
 #if defined(CUTLASS_UINT128_NATIVE)
     y.native = native - rhs.native;
 #else
     y.hilo_.lo = hilo_.lo - rhs.hilo_.lo;
     y.hilo_.hi = hilo_.hi - rhs.hilo_.hi - (rhs.hilo_.lo && y.hilo_.lo > hilo_.lo);
 #endif
     return y;
   }
 
   /// Multiply by unsigned 64b integer yielding 128b integer
   CUTLASS_HOST_DEVICE
-  uint128_t operator*(uint64_t const &rhs) const {
+  uint128_t operator*(uint64_t const& rhs) const
+  {
     uint128_t y{};
 #if defined(CUTLASS_UINT128_NATIVE)
     y.native = native * rhs;
 #elif defined(CUTLASS_INT128_ARITHMETIC)
     // Multiply by the low part
     y.hilo_.lo = _umul128(hilo_.lo, rhs, &y.hilo_.hi);
 
     // Add the high part and ignore the overflow
-    uint64_t overflow;
+    uint64_t overflow{0};
     y.hilo_.hi += _umul128(hilo_.hi, rhs, &overflow);
 #else
     CUTLASS_UNUSED(rhs);
     exception();
 #endif
     return y;
   }
 
   /// Divide 128b operation by 64b operation yielding a 64b quotient
   CUTLASS_HOST_DEVICE
-  uint64_t operator/(uint64_t const &divisor) const {
-    uint64_t quotient = 0;
+  uint64_t operator/(uint64_t const& divisor) const
+  {
+    uint64_t quotient{0};
 #if defined(CUTLASS_UINT128_NATIVE)
     quotient = uint64_t(native / divisor);
 #elif defined(CUTLASS_INT128_ARITHMETIC_DIV)
     // implemented using MSVC's arithmetic intrinsics
-    uint64_t remainder = 0;
+    uint64_t remainder{0};
     quotient = _udiv128(hilo_.hi, hilo_.lo, divisor, &remainder);
 #else
     CUTLASS_UNUSED(divisor);
     exception();
 #endif
     return quotient;
   }
 
   /// Divide 128b operation by 64b operation yielding a 64b quotient
   CUTLASS_HOST_DEVICE
-  uint64_t operator%(uint64_t const &divisor) const {
-    uint64_t remainder = 0;
+  uint64_t operator%(uint64_t const& divisor) const
+  {
+    uint64_t remainder{0};
 #if defined(CUTLASS_UINT128_NATIVE)
     remainder = uint64_t(native % divisor);
 #elif defined(CUTLASS_INT128_ARITHMETIC_DIV)
     // implemented using MSVC's arithmetic intrinsics
     (void)_udiv128(hilo_.hi, hilo_.lo, divisor, &remainder);
 #else
     CUTLASS_UNUSED(divisor);
     exception();
 #endif
     return remainder;
   }
 
   /// Computes the quotient and remainder in a single method.
   CUTLASS_HOST_DEVICE
-  uint64_t divmod(uint64_t &remainder, uint64_t divisor) const {
-    uint64_t quotient = 0;
+  uint64_t divmod(uint64_t &remainder, uint64_t divisor) const
+  {
+    uint64_t quotient{0};
 #if defined(CUTLASS_UINT128_NATIVE)
     quotient = uint64_t(native / divisor);
     remainder = uint64_t(native % divisor);
 #elif defined(CUTLASS_INT128_ARITHMETIC_DIV)
     // implemented using MSVC's arithmetic intrinsics
     quotient = _udiv128(hilo_.hi, hilo_.lo, divisor, &remainder);
 #else
@@ -226,41 +221,43 @@
     exception();
 #endif
     return quotient;
   }
 
   /// Left-shifts a 128b unsigned integer
   CUTLASS_HOST_DEVICE
-  uint128_t operator<<(int sh) const {
+  uint128_t operator<<(int sh) const
+  {
     if (sh == 0) {
       return *this;
     }
-    else if (sh >= kPartSize) {
-      return uint128_t(0, hilo_.lo << (sh - kPartSize));
+    else if (sh >= storage_bits_) {
+      return uint128_t(0, hilo_.lo << (sh - storage_bits_));
     }
     else {
       return uint128_t(
         (hilo_.lo << sh),
-        (hilo_.hi << sh) | uint64_t(hilo_.lo >> (kPartSize - sh))
+        (hilo_.hi << sh) | uint64_t(hilo_.lo >> (storage_bits_ - sh))
       );
     }
   }
 
   /// Right-shifts a 128b unsigned integer
   CUTLASS_HOST_DEVICE
-  uint128_t operator>>(int sh) const {
+  uint128_t operator>>(int sh) const
+  {
     if (sh == 0) {
       return *this;
     }
-    else if (sh >= kPartSize) {
-      return uint128_t((hilo_.hi >> (sh - kPartSize)), 0);
+    else if (sh >= storage_bits_) {
+      return uint128_t((hilo_.hi >> (sh - storage_bits_)), 0);
     }
     else {
       return uint128_t(
-        (hilo_.lo >> sh) | (hilo_.hi << (kPartSize - sh)),
+        (hilo_.lo >> sh) | (hilo_.hi << (storage_bits_ - sh)),
         (hilo_.hi >> sh)
       );
     }
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/version.h

```diff
@@ -31,16 +31,16 @@
 
 #pragma once
 
 #include <cstdint>
 #include <string>
 
 #define CUTLASS_MAJOR 3
-#define CUTLASS_MINOR 4
-#define CUTLASS_PATCH 1
+#define CUTLASS_MINOR 5
+#define CUTLASS_PATCH 0
 
 #ifdef CUTLASS_VERSIONS_GENERATED
 #include "cutlass/version_extended.h"
 #else
 #define CUTLASS_BUILD 0
 #define CUTLASS_REVISION ""
 #endif
```

## cutlass_library/source/include/cutlass/workspace.h

```diff
@@ -28,35 +28,25 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Utilities for initializing workspaces
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by this unit test: `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #if !defined(__CUDACC_RTC__)
 #include "cuda.h"
 #include "cuda_runtime.h"
 
 #include "cutlass/trace.h"
 #endif
 
 #include "cutlass.h"
-
+#include "cutlass/cuda_host_adapter.hpp"
 namespace cutlass {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 static constexpr int MinWorkspaceAlignment = 16;
 
 #if !defined(__CUDACC_RTC__)
@@ -80,24 +70,44 @@
   return Status::kSuccess;
 }
 #endif
 
 #if !defined(__CUDACC_RTC__)
 template <typename T>
 Status
-fill_workspace(void* workspace, T fill_value, size_t fill_count, cudaStream_t stream = nullptr) {
+fill_workspace(void* workspace, T fill_value, size_t fill_count, cudaStream_t stream = nullptr, CudaHostAdapter *cuda_adapter = nullptr) {
   static_assert(sizeof(T) == 4 || sizeof(T) == 2 || sizeof(T) == 1, "Unsupported fill type");
   if (fill_count > 0) {
     if (workspace == nullptr) {
       CUTLASS_TRACE_HOST("  error: device workspace must not be null");
       return Status::kErrorWorkspaceNull;
     }
 
     CUTLASS_TRACE_HOST("  filling workspace");
     CUdeviceptr d_workspace = reinterpret_cast<CUdeviceptr>(workspace);
+
+#if defined(CUTLASS_ENABLE_CUDA_HOST_ADAPTER) && CUTLASS_ENABLE_CUDA_HOST_ADAPTER
+
+    //
+    // Use the cuda host adapter
+    //
+    CUTLASS_ASSERT(cuda_adapter);
+    if (cuda_adapter) {
+      Status status = Status::kErrorInternal;
+
+      status = cuda_adapter->memsetDevice(workspace, fill_value, fill_count, stream);
+
+      if (status!=Status::kSuccess) {
+        return Status::kErrorInternal;
+      }
+    }
+    else {
+      return Status::kErrorInternal;
+    }
+#else
     CUresult result = CUDA_SUCCESS;
     if (sizeof(T) == 4) {
       result = cuMemsetD32Async(d_workspace, reinterpret_cast<uint32_t&>(fill_value), fill_count, stream);
     }
     else if (sizeof(T) == 2) {
       result = cuMemsetD16Async(d_workspace, reinterpret_cast<uint16_t&>(fill_value), fill_count, stream);
     }
@@ -112,14 +122,15 @@
         CUTLASS_TRACE_HOST("  cuMemsetD" << sizeof(T) * 8 << "Async() returned error " << *error_string_ptr);
       }
       else {
         CUTLASS_TRACE_HOST("  cuMemsetD" << sizeof(T) * 8 << "Async() returned unrecognized error");
       }
       return Status::kErrorInternal;
     }
+#endif
   }
 
   return Status::kSuccess;
 }
 #endif
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/arch/arch.h

```diff
@@ -82,14 +82,17 @@
 };
 struct Sm80 {
   static int const kMinComputeCapability = 80; 
 };
 struct Sm86 {
   static int const kMinComputeCapability = 86;
 };
+struct Sm89 {
+  static int const kMinComputeCapability = 89;
+};
 struct Sm90 {
   static int const kMinComputeCapability = 90; 
 };
 
 /// Triggers a breakpoint on the device
 CUTLASS_DEVICE
 void device_breakpoint() {
```

## cutlass_library/source/include/cutlass/arch/barrier.h

```diff
@@ -64,14 +64,15 @@
   // Data Members:
 
   // Range = [1 , NUM_THREADS_PER_CTA]
   // Range % warp-size (i.e 32) == 0
   uint32_t const num_threads_;
 
   // Range : [0, 15]
+  // Note that should be set to the final barrier ID, including ReserveNamedBarrierCount should be considered
   uint32_t const id_;
 
  public:
 
   // Constructor for CUTLASS developers:
   // effective barrier ID starts from 0
   CUTLASS_DEVICE
@@ -84,20 +85,22 @@
   NamedBarrier(uint32_t num_threads, uint32_t id = 0)
       : num_threads_(num_threads), id_(id + ReservedNamedBarrierCount) {
     CUTLASS_ASSERT(id + ReservedNamedBarrierCount <= HardwareMaxNumNamedBarriers && "Effective barrier_id should not exceed 16.");
   }
 
   CUTLASS_DEVICE
   void arrive_and_wait() const {
-    NamedBarrier::arrive_and_wait(num_threads_, id_);
+    // Note: The value of id_ is already the final barrier id (set correctly in the constructor).
+    NamedBarrier::arrive_and_wait_internal(num_threads_, id_);
   }
 
   CUTLASS_DEVICE
   void arrive() const {
-    NamedBarrier::arrive(num_threads_, id_);
+    // Note: The value of id_ is already the final barrier id (set correctly in the constructor).
+    NamedBarrier::arrive_internal(num_threads_, id_);
   }
 
   CUTLASS_DEVICE
   void sync() const {
     NamedBarrier::arrive_and_wait();
   }
 
@@ -380,16 +383,16 @@
   CUTLASS_DEVICE
   void arrive_and_expect_tx(uint32_t transaction_bytes) const {
     ClusterTransactionBarrier::arrive_and_expect_tx(&this->barrier_, transaction_bytes);
   }
 
   // Performs an arrive operation + expected transaction bytes increment
   CUTLASS_DEVICE
-  void arrive_and_expect_tx(uint32_t transaction_bytes, uint32_t cta_id) const {
-    ClusterTransactionBarrier::arrive_and_expect_tx(&this->barrier_, transaction_bytes , cta_id, true);
+  void arrive_and_expect_tx(uint32_t transaction_bytes, uint32_t cta_id, uint32_t pred = 1u) const {
+    ClusterTransactionBarrier::arrive_and_expect_tx(&this->barrier_, transaction_bytes , cta_id, pred);
   }
 
   // Performs an expected transaction bytes increment without doing an arrive operation
   CUTLASS_DEVICE
   void expect_transaction(uint32_t transaction_bytes) const {
     ClusterTransactionBarrier::expect_transaction(&this->barrier_, transaction_bytes);
   }
```

## cutlass_library/source/include/cutlass/arch/memory.h

```diff
@@ -32,14 +32,15 @@
     \brief Architecture-specific operators on memory
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/arch/cache_operation.h"
+#include "cutlass/platform/platform.h"
 
 namespace cutlass {
 namespace arch {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
```

## cutlass_library/source/include/cutlass/arch/mma.h

```diff
@@ -82,20 +82,17 @@
 
 /// Tag indicating the input is converted to 2 (big and small) TF32 components
 //  Perform 3xTF32 or 4xTF32 for every complex<F32> output element
 struct OpMultiplyAddComplexFastF32 {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Helper for determining whether staged accumulation should be used for a given operator
-template <typename Operator>
-struct UseStagedAccumulation {
-  static bool const value = platform::is_same<Operator, OpMultiplyAddFastF32>::value ||
-                            platform::is_same<Operator, OpMultiplyAddComplexFastF32>::value;
-};
+/// Tag indicating that staged accumulation is not to be used. This is valid only for SM89
+/// FP8 kernels.
+struct OpMultiplyAddFastAccum;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the complex multiply-add operation
 struct OpMultiplyAddComplex {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -246,9 +243,27 @@
 #include "cutlass/arch/mma_sm50.h"
 #include "cutlass/arch/mma_sm60.h"
 #include "cutlass/arch/mma_sm61.h"
 #include "cutlass/arch/mma_sm70.h"
 #include "cutlass/arch/mma_sm75.h"
 #include "cutlass/arch/mma_sm80.h"
 #include "cutlass/arch/mma_sparse_sm80.h"
+#include "cutlass/arch/mma_sm89.h"
+#include "cutlass/arch/mma_sparse_sm89.h"
 #include "cutlass/arch/mma_sm90.h"
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
+namespace cutlass {
+namespace arch {
+namespace detail {
+/// Helper for determining whether staged accumulation should be used for a given operator
+template <typename Operator>
+struct UseStagedAccumulation {
+  static bool const value = platform::is_same<typename Operator::MathOperator, OpMultiplyAddFastF32>::value ||
+                            platform::is_same<typename Operator::MathOperator, OpMultiplyAddComplexFastF32>::value ||
+                            is_sm89_staged_policy_v<Operator>;
+};
+} // namespace detail
+} // namespace arch
+} // namespace cutlass
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/conv/conv2d_problem_size.h

```diff
@@ -31,32 +31,23 @@
 /*! \file
     \brief This file contains definitions and utility functions for describing convolution problem sizes.
 
   Conv2dProblem desciption:
     activation (NHWC), 
     filter (KRSC), 
     output (NPQK), 
-    pading (pad_h, pad_w), 
+    pading (pad_h, pad_w),
     stride (stride_h, stride_w),
     dilation (dilation_h, dilation_w).
     
   Free functions to map:
     Map tensor extents (Conv2d -> ImplicitGemm)      : implicit_gemm_tensor_[a|b|c]_extent(ConvolutionOperator)
     Map tensor sizes (Conv2d -> ImplicitGemm)        : implicit_gemm_tensor_[a|b|c]_size(ConvolutionOperator)
     Map tensor problem sizes (Conv2d -> ImplicitGemm): implicit_gemm_problem_size(ConvolutionOperator)
 */
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/tensor_coord.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm_enumerated_types.h"
@@ -105,15 +96,15 @@
     int Q,
     int K,
     int R,
     int S,
     Mode mode
   ): 
     N(N), H(H), W(W), C(C), P(P), Q(Q), K(K), R(R), S(S),
-    pad_h(R / 2), pad_w(S / 2), stride_h(1), stride_w(1), dilation_h(1), dilation_w(1), 
+    pad_h(R / 2), pad_w(S / 2), stride_h(1), stride_w(1), dilation_h(1), dilation_w(1),
     mode(mode), split_k_slices(1), groups (1) { }
   
   /// Constructor
   CUTLASS_HOST_DEVICE
   Conv2dProblemSize(
     int N,
     int H,
@@ -129,17 +120,17 @@
     int stride_h,
     int stride_w,
     int dilation_h,
     int dilation_w,
     Mode mode,
     int split_k_slices = 1,
     int groups = 1
-  ): 
+  ):
     N(N), H(H), W(W), C(C), P(P), Q(Q), K(K), R(R), S(S),
-    pad_h(pad_h), pad_w(pad_w), stride_h(stride_h), stride_w(stride_w), 
+    pad_h(pad_h), pad_w(pad_w), stride_h(stride_h), stride_w(stride_w),
     dilation_h(dilation_h), dilation_w(dilation_w), 
     mode(mode), split_k_slices(split_k_slices), groups (groups) { }
 
   /// Constructs convolution problem size from cutlass Tensor4DCoord and MatrixCoord 
   // set user-defined output size and sets P and Q (include all data members in ctor)
   CUTLASS_HOST_DEVICE
   Conv2dProblemSize(
@@ -152,41 +143,41 @@
     cutlass::conv::Mode mode = cutlass::conv::Mode::kCrossCorrelation,
     int split_k_slices = 1,
     int groups = 1
   ):
     N(input_size.n()), H(input_size.h()), W(input_size.w()), C(input_size.c()),
     P(output_size.h()), Q(output_size.w()),
     K(filter_size.n()), R(filter_size.h()), S(filter_size.w()),
-    pad_h(padding[0]), pad_w(padding[2]), 
-    stride_h(stride.row()), stride_w(stride.column()), 
+    pad_h(padding[0]), pad_w(padding[2]),
+    stride_h(stride.row()), stride_w(stride.column()),
     dilation_h(dilation.row()), dilation_w(dilation.column()),
     mode(mode), split_k_slices(split_k_slices), groups(groups) {}
 
   /// Constructs convolution problem size from cutlass Tensor4DCoord and MatrixCoord 
   // computes output size and sets P and Q (skip output from ctor arguments)
   CUTLASS_HOST_DEVICE  
   Conv2dProblemSize(
     cutlass::Tensor4DCoord input_size,   // NHWC
     cutlass::Tensor4DCoord filter_size,  // KRSC
-    cutlass::Tensor4DCoord padding,      // pad_h, _, pad_w, _
+    cutlass::Tensor4DCoord padding,      // pad_h, upper_pad_h, pad_w, upper_pad_w
     cutlass::MatrixCoord stride,         // stride_h, stride_w
     cutlass::MatrixCoord dilation,       // dilation_h, dilation_w
     cutlass::conv::Mode mode = cutlass::conv::Mode::kCrossCorrelation,
     int split_k_slices = 1,
     int groups = 1
   ):
     N(input_size.n()), H(input_size.h()), W(input_size.w()), C(input_size.c()),
     K(filter_size.n()), R(filter_size.h()), S(filter_size.w()),
     pad_h(padding[0]), pad_w(padding[2]),
-    stride_h(stride.row()), stride_w(stride.column()), 
+    stride_h(stride.row()), stride_w(stride.column()),
     dilation_h(dilation.row()), dilation_w(dilation.column()),
     mode(mode), split_k_slices(split_k_slices), groups(groups) {
       // set output P and Q
-      P = ((H + pad_h * 2 - R * dilation_h) / stride_h) + 1;
-      Q = ((W + pad_w * 2 - S * dilation_w) / stride_w) + 1;
+      P = ((H + pad_h + padding[1] - R * dilation_h) / stride_h) + 1;
+      Q = ((W + pad_w + padding[3] - S * dilation_w) / stride_w) + 1;
     }
 
   /// Constructs convolution problem size from cutlass Tensor4DCoord and MatrixCoord 
   // set user-defined output size and sets P and Q (skip padding, striding, and dilation)
   CUTLASS_HOST_DEVICE
   Conv2dProblemSize(
     cutlass::Tensor4DCoord input_size,    // NHWC
@@ -195,15 +186,15 @@
     cutlass::conv::Mode mode = cutlass::conv::Mode::kCrossCorrelation,
     int split_k_slices = 1,
     int groups = 1
   ):
     N(input_size.n()), H(input_size.h()), W(input_size.w()), C(input_size.c()),
     P(output_size.h()), Q(output_size.w()),
     K(filter_size.n()), R(filter_size.h()), S(filter_size.w()),
-    pad_h(R / 2), pad_w(S / 2), stride_h(1), stride_w(1), 
+    pad_h(R / 2), pad_w(S / 2), stride_h(1), stride_w(1),
     dilation_h(1), dilation_w(1),
     mode(mode), split_k_slices(split_k_slices), groups(groups) {}
 
   // Reset covolution mode in the problem
   CUTLASS_HOST_DEVICE
   Conv2dProblemSize reset_mode(cutlass::conv::Mode mode_) {
     Conv2dProblemSize tmp(*this);
@@ -243,17 +234,18 @@
   cutlass::Tensor4DCoord activation_extent() const {
 
     return cutlass::Tensor4DCoord ({N, H, W, C});
   }
 
   /// Returns filter extent as Tensor4DCoord
   CUTLASS_HOST_DEVICE
-  cutlass::Tensor4DCoord filter_extent() const {
+  cutlass::Tensor4DCoord filter_extent(bool is_deconv = false) const {
 
-    return cutlass::Tensor4DCoord ({K, R, S, C / groups});
+    return is_deconv ? cutlass::Tensor4DCoord ({C, R, S, K / groups})
+        : cutlass::Tensor4DCoord ({K, R, S, C / groups});
   }
 
   /// Returns output extent as Tensor4DCoord
   CUTLASS_HOST_DEVICE
   cutlass::Tensor4DCoord output_extent() const {
 
     return cutlass::Tensor4DCoord ({N, P, Q, K});
@@ -336,14 +328,15 @@
   switch (conv_operator) {
   case Operator::kFprop:
     return gemm::GemmCoord(
       problem_size.N * problem_size.P * problem_size.Q,
       problem_size.K,
       problem_size.R * problem_size.S * problem_size.C / problem_size.groups
     );
+  case Operator::kDeconv:
   case Operator::kDgrad:
     return gemm::GemmCoord(
       problem_size.N * problem_size.H * problem_size.W,
       problem_size.C,
       problem_size.R * problem_size.S * problem_size.K
     );
   case Operator::kWgrad:
@@ -400,14 +393,15 @@
 
       switch (conv_operator) {
       case Operator::kFprop:
         elements_per_split_k_slice = (problem_size.C + problem_size.split_k_slices - 1) / problem_size.split_k_slices;
         iterations = problem_size.R * problem_size.S * ((elements_per_split_k_slice + threadblock_K - 1) / threadblock_K);
         break;
 
+      case Operator::kDeconv:
       case Operator::kDgrad:
         elements_per_split_k_slice = (problem_size.K + problem_size.split_k_slices - 1) / problem_size.split_k_slices;
         iterations = problem_size.R * problem_size.S * ((elements_per_split_k_slice + threadblock_K - 1) / threadblock_K);
         break;
 
       case Operator::kWgrad:
         elements_per_split_k_slice = (problem_size.N * problem_size.P * problem_size.Q + problem_size.split_k_slices - 1) / problem_size.split_k_slices;
@@ -501,14 +495,15 @@
   int iterations = 0; //0 means not applicable
   if (algorithm == IteratorAlgorithm::kAnalytic || algorithm == IteratorAlgorithm::kOptimized) {
     switch (conv_operator) {
       case Operator::kFprop:
         iterations = problem_size.R * problem_size.S;
         break;
 
+      case Operator::kDeconv:
       case Operator::kDgrad:
         iterations = problem_size.R * problem_size.S;
         break;
 
       default:
         break;
     }
@@ -522,84 +517,90 @@
 /// Returns ImplicitGemm tensor A extent as Tensor4DCoord
 CUTLASS_HOST_DEVICE
 cutlass::Tensor4DCoord implicit_gemm_tensor_a_extent(
   Operator conv_operator,
   Conv2dProblemSize const &problem_size) {
   switch (conv_operator) {
     case cutlass::conv::Operator::kFprop: return problem_size.activation_extent();
+    case cutlass::conv::Operator::kDeconv:
     case cutlass::conv::Operator::kDgrad: return problem_size.output_extent();
     case cutlass::conv::Operator::kWgrad: return problem_size.output_extent();
     default : break;
   }
   return cutlass::Tensor4DCoord();
 }
 
 /// Returns ImplicitGemm tensor B extent as Tensor4DCoord
 CUTLASS_HOST_DEVICE
 cutlass::Tensor4DCoord implicit_gemm_tensor_b_extent(
   Operator conv_operator,
   Conv2dProblemSize const &problem_size) {
   switch (conv_operator) {
     case cutlass::conv::Operator::kFprop: return problem_size.filter_extent();
+    case cutlass::conv::Operator::kDeconv: return problem_size.filter_extent(true);
     case cutlass::conv::Operator::kDgrad: return problem_size.filter_extent();
     case cutlass::conv::Operator::kWgrad: return problem_size.activation_extent();
     default : break;
   }
   return cutlass::Tensor4DCoord();
 }
 
 /// Returns ImplicitGemm tensor C extent as Tensor4DCoord
 CUTLASS_HOST_DEVICE
 cutlass::Tensor4DCoord implicit_gemm_tensor_c_extent(
   Operator conv_operator,
   Conv2dProblemSize const &problem_size) {
   switch (conv_operator) {
     case cutlass::conv::Operator::kFprop: return problem_size.output_extent();
+    case cutlass::conv::Operator::kDeconv:
     case cutlass::conv::Operator::kDgrad: return problem_size.activation_extent();
     case cutlass::conv::Operator::kWgrad: return problem_size.filter_extent();
     default : break;
   }
   return cutlass::Tensor4DCoord();
 }
 
 /// Returns ImplicitGemm tensor A size in number of elements
 CUTLASS_HOST_DEVICE
 int64_t implicit_gemm_tensor_a_size(
   Operator conv_operator,
   Conv2dProblemSize const &problem_size) {
   switch (conv_operator) {
     case cutlass::conv::Operator::kFprop: return problem_size.activation_size();
+    case cutlass::conv::Operator::kDeconv:
     case cutlass::conv::Operator::kDgrad: return problem_size.output_size();
     case cutlass::conv::Operator::kWgrad: return problem_size.output_size();
     default : break;
   }
   return 0;
 }
 
 /// Returns ImplicitGemm tensor B size in number of elements
 CUTLASS_HOST_DEVICE
 int64_t implicit_gemm_tensor_b_size(
   Operator conv_operator,
   Conv2dProblemSize const &problem_size) {
   switch (conv_operator) {
     case cutlass::conv::Operator::kFprop: return problem_size.filter_size();
+    case cutlass::conv::Operator::kDeconv:
     case cutlass::conv::Operator::kDgrad: return problem_size.filter_size();
     case cutlass::conv::Operator::kWgrad: return problem_size.activation_size();
     default : break;
   }
   return 0;
 }
 
 /// Returns ImplicitGemm tensor C size in number of elements
 CUTLASS_HOST_DEVICE
 int64_t implicit_gemm_tensor_c_size(
   Operator conv_operator,
   Conv2dProblemSize const &problem_size) {
   switch (conv_operator) {
     case cutlass::conv::Operator::kFprop: return problem_size.output_size();
+    case cutlass::conv::Operator::kDeconv:
     case cutlass::conv::Operator::kDgrad: return problem_size.activation_size();
     case cutlass::conv::Operator::kWgrad: return problem_size.filter_size();
     default : break;
   }
   return 0;
 }
```

## cutlass_library/source/include/cutlass/conv/conv3d_problem_size.h

```diff
@@ -40,23 +40,14 @@
     dilation (dilation_d, dilation_h, dilation_w).
   
   Free functions to map:
     Map tensor extents (Conv3d -> ImplicitGemm)      : implicit_gemm_tensor_[a|b|c]_extent(ConvolutionOperator)
     Map tensor sizes (Conv3d -> ImplicitGemm)        : implicit_gemm_tensor_[a|b|c]_size(ConvolutionOperator)
     Map tensor problem sizes (Conv3d -> ImplicitGemm): implicit_gemm_problem_size(ConvolutionOperator)  
 */
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
 
 #pragma once
 
 #include "cutlass/conv/convolution.h"
 #include "cutlass/conv/conv2d_problem_size.h"
 
 namespace cutlass {
@@ -87,15 +78,15 @@
   // Methods
   //
 public:
   CUTLASS_HOST_DEVICE
   Conv3dProblemSize(): 
     Conv2dProblemSize(),
     D(0), T(0), Z(0), 
-    pad_d(0), 
+    pad_d(0),
     stride_d(1), 
     dilation_d(1) { }
  
   /// Constructor for default padding, stride, dilation, and split-K
   CUTLASS_HOST_DEVICE
   Conv3dProblemSize(
     int N,
@@ -201,14 +192,42 @@
     D(input_size.d()), T(filter_size.d()),
     pad_d(padding[0]), stride_d(stride[0]), dilation_d(dilation[0])
     {
       // set output Z
       Z = ((D + pad_d * 2 - T * dilation_d) / stride_d) + 1;
     }
 
+  /// Constructs convolution problem size from cutlass Tensor5DCoord, Coord3D
+  // *computes* output size and sets Z, P and Q (include all data members in ctor)
+  CUTLASS_HOST_DEVICE
+  Conv3dProblemSize(
+    cutlass::Tensor5DCoord input_size,    // NDHWC
+    cutlass::Tensor5DCoord filter_size,   // KTRSC
+    CUTLASS_STL_NAMESPACE::tuple<Coord3D, Coord3D> padding, // Coord3D {pad_d, pad_h, pad_w} & Coord3D {far pad_d, pad_h, pad_w} to calculate o/p/q
+    Coord3D stride,                       // stride_d, stride_h, stride_w
+    Coord3D dilation,                     // dilation_d, dilation_h, dilation_w
+    cutlass::conv::Mode mode = cutlass::conv::Mode::kCrossCorrelation,
+    int split_k_slices = 1,
+    int groups = 1
+  ):
+    Conv2dProblemSize(
+      {input_size.n(), input_size.h(), input_size.w(), input_size.c()},
+      {filter_size.n(), filter_size.h(), filter_size.w(), filter_size.c()},
+      {CUTLASS_STL_NAMESPACE::get<0>(padding)[1], CUTLASS_STL_NAMESPACE::get<1>(padding)[1],
+       CUTLASS_STL_NAMESPACE::get<0>(padding)[2], CUTLASS_STL_NAMESPACE::get<1>(padding)[2]},
+      {stride[1], stride[2]},
+      {dilation[1], dilation[2]},
+      mode, split_k_slices, groups),
+    D(input_size.d()), T(filter_size.d()),
+    pad_d(CUTLASS_STL_NAMESPACE::get<0>(padding)[0]), stride_d(stride[0]), dilation_d(dilation[0])
+    {
+      // set output Z
+      Z = ((D + pad_d + CUTLASS_STL_NAMESPACE::get<1>(padding)[0] - T * dilation_d) / stride_d) + 1;
+    }
+
   /// Equality operator (ignores mode and split_k_slice)
   CUTLASS_HOST_DEVICE
   bool operator==(Conv3dProblemSize const &conv) const {
     return (
       (N == conv.N) && (D == conv.D) && (H == conv.H) && (W == conv.W) && (C == conv.C) &&
       (K == conv.K) && (T == conv.T) && (R == conv.R) && (S == conv.S) &&
       (Z == conv.Z) &&(P == conv.P) && (Q == conv.Q) &&
@@ -245,17 +264,18 @@
   cutlass::Tensor5DCoord activation_extent() const {
 
     return cutlass::Tensor5DCoord ({N, D, H, W, C});
   }
 
   /// Returns filter extent as Tensor5DCoord
   CUTLASS_HOST_DEVICE
-  cutlass::Tensor5DCoord filter_extent() const {
+  cutlass::Tensor5DCoord filter_extent(bool is_deconv = false) const {
 
-    return cutlass::Tensor5DCoord ({K, T, R, S, C});
+    return is_deconv ? cutlass::Tensor5DCoord ({C, T, R, S, K})
+        : cutlass::Tensor5DCoord ({K, T, R, S, C});
   }
 
   /// Returns output extent as Tensor5DCoord
   CUTLASS_HOST_DEVICE
   cutlass::Tensor5DCoord output_extent() const {
 
     return cutlass::Tensor5DCoord ({N, Z, P, Q, K});
@@ -278,15 +298,15 @@
   /// Returns output size in number of elements
   CUTLASS_HOST_DEVICE
   int64_t output_size() const {
 
     return (N * Z * P * Q * K);
   }
 
-  /// Returns output extent as Tensor5DCoord
+  /// Returns padding as Coord3D
   CUTLASS_HOST_DEVICE
   Coord3D padding() const {
 
     return Coord3D ({pad_d, pad_h, pad_w});
   }
 
   /// Returns stride as MatrixCoord
@@ -319,14 +339,15 @@
   switch (conv_operator) {
   case Operator::kFprop:
     return gemm::GemmCoord(
       problem_size.N * problem_size.Z * problem_size.P * problem_size.Q,
       problem_size.K,
       problem_size.T * problem_size.R * problem_size.S * problem_size.C
     );
+  case Operator::kDeconv:
   case Operator::kDgrad:
     return gemm::GemmCoord(
       problem_size.N * problem_size.D * problem_size.H * problem_size.W,
       problem_size.C,
       problem_size.T * problem_size.R * problem_size.S * problem_size.K
     );
   case Operator::kWgrad:
@@ -355,15 +376,16 @@
   int elements_per_split_k_slice = 0;
   if (group_mode == GroupMode::kNone) {
     switch (conv_operator) {
       case Operator::kFprop:
         elements_per_split_k_slice = (problem_size.C + problem_size.split_k_slices - 1) / problem_size.split_k_slices;
         iterations = problem_size.T * problem_size.R * problem_size.S * ((elements_per_split_k_slice + threadblock_K - 1) / threadblock_K);
         break;
-    
+
+      case Operator::kDeconv:
       case Operator::kDgrad:
         elements_per_split_k_slice =  (problem_size.K + problem_size.split_k_slices - 1) / problem_size.split_k_slices;
         iterations = problem_size.T * problem_size.R * problem_size.S * ((elements_per_split_k_slice + threadblock_K - 1) / threadblock_K);
         break;
     
       case Operator::kWgrad:
         elements_per_split_k_slice = (problem_size.N * problem_size.Z * problem_size.P * problem_size.Q + problem_size.split_k_slices - 1) / problem_size.split_k_slices;
@@ -398,84 +420,90 @@
 /// Returns ImplicitGemm tensor A extent as Tensor5DCoord
 CUTLASS_HOST_DEVICE
 cutlass::Tensor5DCoord implicit_gemm_tensor_a_extent(
   Operator conv_operator,
   Conv3dProblemSize const &problem_size) {
   switch (conv_operator) {
     case cutlass::conv::Operator::kFprop: return problem_size.activation_extent();
+    case cutlass::conv::Operator::kDeconv:
     case cutlass::conv::Operator::kDgrad: return problem_size.output_extent();
     case cutlass::conv::Operator::kWgrad: return problem_size.output_extent();
     default : break;
   }
   return cutlass::Tensor5DCoord();
 }
 
 /// Returns ImplicitGemm tensor B extent as Tensor5DCoord
 CUTLASS_HOST_DEVICE
 cutlass::Tensor5DCoord implicit_gemm_tensor_b_extent(
   Operator conv_operator,
   Conv3dProblemSize const &problem_size) {
   switch (conv_operator) {
     case cutlass::conv::Operator::kFprop: return problem_size.filter_extent();
+    case cutlass::conv::Operator::kDeconv: return problem_size.filter_extent(true);
     case cutlass::conv::Operator::kDgrad: return problem_size.filter_extent();
     case cutlass::conv::Operator::kWgrad: return problem_size.activation_extent();
     default : break;
   }
   return cutlass::Tensor5DCoord();
 }
 
 /// Returns ImplicitGemm tensor C extent as Tensor5DCoord
 CUTLASS_HOST_DEVICE
 cutlass::Tensor5DCoord implicit_gemm_tensor_c_extent(
   Operator conv_operator,
   Conv3dProblemSize const &problem_size) {
   switch (conv_operator) {
     case cutlass::conv::Operator::kFprop: return problem_size.output_extent();
+    case cutlass::conv::Operator::kDeconv:
     case cutlass::conv::Operator::kDgrad: return problem_size.activation_extent();
     case cutlass::conv::Operator::kWgrad: return problem_size.filter_extent();
     default : break;
   }
   return cutlass::Tensor5DCoord();
 }
 
 /// Returns ImplicitGemm tensor A size in number of elements
 CUTLASS_HOST_DEVICE
 int64_t implicit_gemm_tensor_a_size(
   Operator conv_operator,
   Conv3dProblemSize const &problem_size) {
   switch (conv_operator) {
     case cutlass::conv::Operator::kFprop: return problem_size.activation_size();
+    case cutlass::conv::Operator::kDeconv:
     case cutlass::conv::Operator::kDgrad: return problem_size.output_size();
     case cutlass::conv::Operator::kWgrad: return problem_size.output_size();
     default : break;
   }
   return 0;
 }
 
 /// Returns ImplicitGemm tensor B size in number of elements
 CUTLASS_HOST_DEVICE
 int64_t implicit_gemm_tensor_b_size(
   Operator conv_operator,
   Conv3dProblemSize const &problem_size) {
   switch (conv_operator) {
     case cutlass::conv::Operator::kFprop: return problem_size.filter_size();
+    case cutlass::conv::Operator::kDeconv:
     case cutlass::conv::Operator::kDgrad: return problem_size.filter_size();
     case cutlass::conv::Operator::kWgrad: return problem_size.activation_size();
     default : break;
   }
   return 0;
 }
 
 /// Returns ImplicitGemm tensor C size in number of elements
 CUTLASS_HOST_DEVICE
 int64_t implicit_gemm_tensor_c_size(
   Operator conv_operator,
   Conv3dProblemSize const &problem_size) {
   switch (conv_operator) {
     case cutlass::conv::Operator::kFprop: return problem_size.output_size();
+    case cutlass::conv::Operator::kDeconv:
     case cutlass::conv::Operator::kDgrad: return problem_size.activation_size();
     case cutlass::conv::Operator::kWgrad: return problem_size.filter_size();
     default : break;
   }
   return 0;
 }
```

## cutlass_library/source/include/cutlass/conv/convolution.h

```diff
@@ -66,24 +66,14 @@
 as operating on "A, B, Output."  Instead, use the mapping functions below,
 and adhere to using either A, B, C or Activation, Filter, Output.
 
 Map elements' data types (ImplicitGemm -> Conv): GemmToConvElementMap
 Map elements' data types (Conv -> ImplicitGemm): ConvToGemmElementMap
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/layout/tensor.h"
 #include "cutlass/tensor_coord.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm_enumerated_types.h"
@@ -94,15 +84,16 @@
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Convolutional operator
 enum class Operator {
   kFprop,
   kDgrad,
-  kWgrad
+  kWgrad,
+  kDeconv
 };
 
 /// Distinguishes convolution from cross correlation
 enum class Mode {
   kCrossCorrelation,
   kConvolution
 };
@@ -167,13 +158,37 @@
   /// Returns a Coord object
   CUTLASS_HOST_DEVICE
   static Coord<4> toCoord() {
     return make_Coord(kN, kH, kW, kC);
   }
 };
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Shape of a conv2d stride, which controls how the filter convolves around the input volume
+template <
+  /// Stride in horizontal direction
+  int u = 1,
+  /// Stride in vertical direction
+  int v = 1
+>
+struct Stride2D {
+  static int const kU = u;
+  static int const kV = v;
+
+  //
+  // Static member functions
+  //
+
+  /// Returns a Coord object
+  CUTLASS_HOST_DEVICE
+  static Coord<2> toCoord() {
+    return make_Coord(kU, kV);
+  }
+};
+
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace conv
 } // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/conv/device/implicit_gemm_convolution.h

```diff
@@ -35,14 +35,15 @@
 #pragma once
 
 #include <limits>
 
 #include "cutlass/cutlass.h"
 #include "cutlass/device_kernel.h"
 #include "cutlass/conv/convolution.h"
+#include "cutlass/cuda_host_adapter.hpp"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace conv {
 namespace device {
 
@@ -76,14 +77,16 @@
   using MathOperator = typename UnderlyingKernel::MathOperator; 
 
   static cutlass::conv::Operator const kConvolutionalOperator = UnderlyingKernel::kConvolutionalOperator;
   static cutlass::conv::IteratorAlgorithm const kIteratorAlgorithm = UnderlyingKernel::kIteratorAlgorithm;
   static cutlass::conv::StrideSupport const kStrideSupport = UnderlyingKernel::kStrideSupport;
   static cutlass::conv::GroupMode const kGroupMode = UnderlyingKernel::kGroupMode;
 
+  static bool const kEnableCudaHostAdapter = CUTLASS_ENABLE_CUDA_HOST_ADAPTER;
+
   static int const kWarpCount = 
     (ThreadblockShape::kM / WarpShape::kM) * 
     (ThreadblockShape::kN / WarpShape::kN) *
     (ThreadblockShape::kK / WarpShape::kK);
 
   /// Argument structure
   using Arguments = typename UnderlyingKernel::Arguments;
@@ -146,32 +149,32 @@
       }
     }
 
     static int const kAlignmentC = UnderlyingKernel::Epilogue::OutputTileIterator::kElementsPerAccess;
     if (kConvolutionalOperator == conv::Operator::kFprop) {
       if (args.problem_size.K % kAlignmentC)
         return Status::kErrorMisalignedOperand;
-    } else if (kConvolutionalOperator == conv::Operator::kDgrad) {
+    } else if (kConvolutionalOperator == conv::Operator::kDgrad || kConvolutionalOperator == conv::Operator::kDeconv) {
        if (args.problem_size.C % kAlignmentC)
         return Status::kErrorMisalignedOperand;
     } else if (kConvolutionalOperator == conv::Operator::kWgrad) {
        if (args.problem_size.C % kAlignmentC)
         return Status::kErrorMisalignedOperand;
     }
 
-    // check for unsupported problem sizes for strided dgrad implementation
-    if (kConvolutionalOperator == conv::Operator::kDgrad && 
+    // check for unsupported problem sizes for strided dgrad / deconv implementation
+    if ((kConvolutionalOperator == conv::Operator::kDgrad || kConvolutionalOperator == conv::Operator::kDeconv) &&
       kStrideSupport == conv::StrideSupport::kStrided) {
 
-      // split-k (serial or parallel) is not supported for strided dgrad
+      // split-k (serial or parallel) is not supported for strided dgrad / deconv
       if(args.problem_size.split_k_slices > 1) {
         return Status::kErrorNotSupported;
       }
-      
-      // dilation > {1x1} is not supported for strided dgrad
+
+      // dilation > {1x1} is not supported for strided dgrad / deconv
       if(args.problem_size.dilation_h > 1 || args.problem_size.dilation_w > 1) {
         return Status::kErrorNotSupported;
       }
     }
 
     // Determine grid shape
     ThreadblockSwizzle threadblock_swizzle;
@@ -226,15 +229,16 @@
     return workspace_bytes;
   }
 
   /// Initializes GEMM state from arguments.
   Status initialize(
     Arguments const &args, 
     void *workspace = nullptr, 
-    cudaStream_t stream = nullptr) {
+    cudaStream_t stream = nullptr,
+    CudaHostAdapter *cuda_adapter = nullptr) {
    
     if (args.problem_size.split_k_slices > 1) {
 
       if (!workspace) {
         return Status::kErrorWorkspaceNull;
       }
 
@@ -246,24 +250,30 @@
     }
 
     // initialize the params structure from the arguments
     params_ = typename UnderlyingKernel::Params(
     	args,
     	static_cast<int *>(workspace)
     );
-    
-    int smem_size = int(sizeof(typename UnderlyingKernel::SharedStorage));
 
-    if (smem_size >= (48 << 10)) {
-      cudaError_t result = cudaFuncSetAttribute(cutlass::Kernel<UnderlyingKernel>,
-                                    cudaFuncAttributeMaxDynamicSharedMemorySize,
-                                    smem_size);
-
-      if (result != cudaSuccess) {
-        return Status::kErrorInternal;
+    if constexpr (kEnableCudaHostAdapter) {
+      CUTLASS_ASSERT(cuda_adapter);
+      return Status::kSuccess;
+    }
+    else {
+      int smem_size = int(sizeof(typename UnderlyingKernel::SharedStorage));
+  
+      if (smem_size >= (48 << 10)) {
+        cudaError_t result = cudaFuncSetAttribute(cutlass::Kernel<UnderlyingKernel>,
+                                      cudaFuncAttributeMaxDynamicSharedMemorySize,
+                                      smem_size);
+  
+        if (result != cudaSuccess) {
+          return Status::kErrorInternal;
+        }
       }
     }
     
     return Status::kSuccess;
   }
 
   /// Initializes GEMM state from arguments.
@@ -277,46 +287,70 @@
     params_.output_op = args.output_op;
     params_.semaphore = static_cast<int *>(workspace);
 
     return Status::kSuccess;
   }
 
   /// Runs the kernel using initialized state.
-  Status run(cudaStream_t stream = nullptr) {
+  Status run(cudaStream_t stream = nullptr, CudaHostAdapter *cuda_adapter = nullptr) {
 
 
     ThreadblockSwizzle threadblock_swizzle;
 
     dim3 grid = threadblock_swizzle.get_grid_shape(params_.grid_tiled_shape);
     dim3 block(32 * kWarpCount, 1, 1);
 
     int smem_size = int(sizeof(typename UnderlyingKernel::SharedStorage));
+    cutlass::Status launch_result = cutlass::Status::kSuccess ;
 
-    cutlass::Kernel<UnderlyingKernel><<<grid, block, smem_size, stream>>>(params_);
+    if constexpr (kEnableCudaHostAdapter) {
+        //
+        // Use the cuda host adapter
+        //
+        CUTLASS_ASSERT(cuda_adapter);
+        if (cuda_adapter) {
+
+          void* kernel_params[] = {&params_};
+          launch_result = cuda_adapter->launch(
+              grid, dim3(1,1,1), block, smem_size, stream, kernel_params, 0
+              );
+        }
+        else {
+          launch_result = Status::kErrorInternal;
+        }
+    }
+    else {
+      cutlass::Kernel<UnderlyingKernel><<<grid, block, smem_size, stream>>>(params_);      
+    }
 
     cudaError_t result = cudaGetLastError();
-
-    return result == cudaSuccess ? Status::kSuccess : Status::kErrorInternal;
+    if (cudaSuccess == result && Status::kSuccess == launch_result) {
+      return Status::kSuccess;
+    }
+    else {
+      CUTLASS_TRACE_HOST("  Kernel launch failed. Reason: " << result);
+      return Status::kErrorInternal;
+    }
   }
 
   /// Runs the kernel using initialized state.
-  Status operator()(cudaStream_t stream = nullptr) {
-    return run(stream);
+  Status operator()(cudaStream_t stream = nullptr, CudaHostAdapter *cuda_adapter = nullptr) {
+    return run(stream, cuda_adapter);
   }
 
   /// Runs the kernel using initialized state.
   Status operator()(
     Arguments const &args, 
     void *workspace = nullptr, 
-    cudaStream_t stream = nullptr) {
+    cudaStream_t stream = nullptr, CudaHostAdapter *cuda_adapter = nullptr) {
     
-    Status status = initialize(args, workspace, stream);
+    Status status = initialize(args, workspace, stream, cuda_adapter);
     
     if (status == Status::kSuccess) {
-      status = run(stream);
+      status = run(stream, cuda_adapter);
     }
 
     return status;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/conv/kernel/default_conv2d.h

```diff
@@ -102,14 +102,57 @@
     PartitionsK,
     OutputOp,
     OutputOp::kCount
   >::Epilogue;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+template <
+  typename ArchTag,
+  typename Shape,
+  typename WarpMmaSimt,
+  typename ElementOutput,
+  typename ElementTensor,
+  typename ElementVector,
+  typename OutputOp,
+  int ElementsPerAccess
+>
+struct DefaultConvEpilogueWithBroadcastSimt {
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueWithBroadcastSimt<
+    Shape,
+    WarpMmaSimt,
+    ElementOutput,
+    ElementTensor,
+    ElementVector,
+    OutputOp,
+    ElementsPerAccess
+  >::Epilogue;
+};
+
+template <
+  typename ArchTag,
+  typename Shape,
+  typename WarpMmaSimt,
+  typename ElementOutput,
+  typename ElementTensor,
+  typename ElementVector,
+  typename OutputOp,
+  int ElementsPerAccess
+>
+struct DefaultConvEpilogueWithBroadcastSimtStridedDgrad {
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueWithBroadcastSimtStridedDgrad<
+    Shape,
+    WarpMmaSimt,
+    ElementOutput,
+    ElementTensor,
+    ElementVector,
+    OutputOp,
+    ElementsPerAccess
+  >::Epilogue;
+};
 
 template <
   typename ArchTag,
   typename Shape,
   typename WarpMmaTensorOp,
   int PartitionsK,
   typename ElementOutput,
```

## cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop.h

```diff
@@ -72,15 +72,15 @@
   typename WarpShape,
   typename InstructionShape,
   typename EpilogueOutputOp,
   typename ThreadblockSwizzle,
   int Stages,
   typename MathOperatorTag,
   conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kOptimized,
-  conv::StrideSupport StrideSupport = StrideSupport::kStrided,
+  conv::StrideSupport StrideSupport = StrideSupport::kUnity,
   /// Access granularity of A matrix in units of elements
   int AlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value,
   /// Access granularity of B matrix in units of elements
   int AlignmentB = 128 / cutlass::sizeof_bits<ElementB>::value
 > struct DefaultConv2dFprop;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -323,15 +323,14 @@
     Mma,
     Epilogue,
     ThreadblockSwizzle,
     conv::Operator::kFprop
   >;
 };
 
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Defines a kernel for Conv2dFprop specialization for Analytic IteratorAlgorithm and two stage
 /// pipeline.
 template <
   typename ElementA,
   typename LayoutA,
@@ -1163,15 +1162,19 @@
 
   // Define the epilogue
   using Epilogue = typename epilogue::threadblock::DefaultEpilogueTensorOp<
     ThreadblockShape,
     WarpMmaTensorOp,
     kPartitionsK,
     EpilogueOutputOp,
-    EpilogueOutputOp::kCount
+    EpilogueOutputOp::kCount,
+    false,
+    layout::NoPermute,
+    StrideSupport,
+    4
   >::Epilogue;
 
   // Define the kernel
   using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
     Mma,
     Epilogue,
     ThreadblockSwizzle,
@@ -1624,15 +1627,19 @@
   >;
 
   // Define the epilogue
   using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
     ThreadblockShape,
     WarpMmaSimtOp,
     EpilogueOutputOp,
-    EpilogueOutputOp::kCount
+    EpilogueOutputOp::kCount,
+    false,
+    layout::NoPermute,
+    StrideSupport,
+    4
   >::Epilogue;
 
   // Define the kernel
   using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
     Mma,
     Epilogue,
     ThreadblockSwizzle,
@@ -1737,25 +1744,28 @@
   >;
 
   // Define the epilogue
   using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
     ThreadblockShape,
     WarpMmaSimtOp,
     EpilogueOutputOp,
-    EpilogueOutputOp::kCount
+    EpilogueOutputOp::kCount,
+    false,
+    layout::NoPermute,
+    StrideSupport,
+    4
   >::Epilogue;
 
   // Define the kernel
   using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
     Mma,
     Epilogue,
     ThreadblockSwizzle,
     conv::Operator::kFprop
   >;
-
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Defines a kernel for Conv2dFprop specialization for Analytic IteratorAlgorithm, 
 /// 2 stage pipeline, and FFMA-based mainloop for SM50
 template <
@@ -1849,15 +1859,19 @@
   >;
 
   // Define the epilogue
   using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
     ThreadblockShape,
     WarpMmaSimtOp,
     EpilogueOutputOp,
-    EpilogueOutputOp::kCount
+    EpilogueOutputOp::kCount,
+    false,
+    layout::NoPermute,
+    StrideSupport,
+    4
   >::Epilogue;
 
   // Define the kernel
   using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
     Mma,
     Epilogue,
     ThreadblockSwizzle,
@@ -1963,15 +1977,19 @@
   >;
 
   // Define the epilogue
   using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
     ThreadblockShape,
     WarpMmaSimtOp,
     EpilogueOutputOp,
-    EpilogueOutputOp::kCount
+    EpilogueOutputOp::kCount,
+    false,
+    layout::NoPermute,
+    StrideSupport,
+    4
   >::Epilogue;
 
   // Define the kernel
   using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
     Mma,
     Epilogue,
     ThreadblockSwizzle,
```

## cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h

```diff
@@ -72,15 +72,15 @@
   typename WarpShape,
   typename InstructionShape,
   typename EpilogueOutputOp,
   typename ThreadblockSwizzle,
   int Stages,
   typename MathOperatorTag,
   conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kOptimized,
-  conv::StrideSupport StrideSupport = StrideSupport::kStrided
+  conv::StrideSupport StrideSupport = StrideSupport::kUnity
 > struct DefaultConv2dFpropFusion;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //                         OpClassTensorOp convolutions 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Defines a kernel for Conv2dFprop specialization for Analytic IteratorAlgorithm and multistage
```

## cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h

```diff
@@ -27,15 +27,15 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
   \brief 
-    Defines a GEMM with Reduction based on an existing UniversalGemm kernel.
+    Defines a GEMM with Broadcast based on an existing UniversalGemm kernel.
 
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 
@@ -67,15 +67,15 @@
   typename WarpShape,
   typename InstructionShape,
   typename EpilogueOutputOp,
   typename ThreadblockSwizzle,
   int Stages,
   typename MathOperatorTag,
   conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kOptimized,
-  conv::StrideSupport StrideSupport = StrideSupport::kStrided,
+  conv::StrideSupport StrideSupport = StrideSupport::kUnity,
   /// Access granularity of A matrix in units of elements
   int AlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value,
   /// Access granularity of B matrix in units of elements
   int AlignmentB = 128 / cutlass::sizeof_bits<ElementB>::value
 >
 struct DefaultConv2dFpropWithBroadcast {
 
@@ -108,14 +108,105 @@
     ElementC,
     typename EpilogueOutputOp::ElementT,
     typename EpilogueOutputOp::ElementVector,
     EpilogueOutputOp,
     ImplicitGemmBase::Epilogue::kElementsPerAccess
   >::Epilogue;
 
+  // Define the kernel
+  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolutionWithFusedEpilogue<
+    typename ImplicitGemmBase::Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    conv::Operator::kFprop
+  >;
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+//                            OpClassSimt convolutions
+/////////////////////////////////////////////////////////////////////////////////////////////////
+/// Defines a kernel for Conv2dFprop specialization for Analytic IteratorAlgorithm,
+/// multi-stage pipeline, and FFMA-based mainloop for SM80
+
+template <
+  typename ElementA,
+  typename LayoutA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ElementAccumulator,
+  typename ArchTag,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename EpilogueOutputOp,
+  typename ThreadblockSwizzle,
+  int Stages,
+  typename MathOperatorTag,
+  conv::IteratorAlgorithm IteratorAlgorithm,
+  conv::StrideSupport StrideSupport,
+  int AlignmentA,
+  int AlignmentB
+>
+struct DefaultConv2dFpropWithBroadcast <
+  ElementA,
+  LayoutA,
+  ElementB,
+  LayoutB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  arch::OpClassSimt,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  Stages,
+  MathOperatorTag,
+  IteratorAlgorithm,
+  StrideSupport,
+  AlignmentA,
+  AlignmentB
+> {
+
+  using ImplicitGemmBase = typename DefaultConv2dFprop<
+    ElementA, LayoutA,
+    ElementB, LayoutB,
+    ElementC, LayoutC,
+    ElementAccumulator,
+    arch::OpClassSimt,
+    ArchTag,
+    ThreadblockShape,
+    WarpShape,
+    InstructionShape,
+    EpilogueOutputOp,
+    ThreadblockSwizzle,
+    Stages,
+    MathOperatorTag,
+    IteratorAlgorithm,
+    StrideSupport,
+    AlignmentA,
+    AlignmentB
+  >::Kernel;
+
+  // Define epilogue
+  using Epilogue = typename cutlass::conv::kernel::detail::DefaultConvEpilogueWithBroadcastSimt<
+    ArchTag,
+    typename ImplicitGemmBase::Epilogue::Shape,
+    typename ImplicitGemmBase::Epilogue::WarpMmaOperator,
+    ElementC,
+    typename EpilogueOutputOp::ElementT,
+    typename EpilogueOutputOp::ElementVector,
+    EpilogueOutputOp,
+    ImplicitGemmBase::Epilogue::kElementsPerAccess
+  >::Epilogue;
+
   // Define the kernel
   using Kernel = cutlass::conv::kernel::ImplicitGemmConvolutionWithFusedEpilogue<
     typename ImplicitGemmBase::Mma,
     Epilogue,
     ThreadblockSwizzle,
     conv::Operator::kFprop
   >;
```

## cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h

```diff
@@ -68,15 +68,15 @@
   typename InstructionShape,
   typename EpilogueOutputOp,
   typename EpilogueReductionOp,
   typename ThreadblockSwizzle,
   int Stages,
   typename MathOperatorTag,
   conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kOptimized,
-  conv::StrideSupport StrideSupport = StrideSupport::kStrided,
+  conv::StrideSupport StrideSupport = StrideSupport::kUnity,
   /// Access granularity of A matrix in units of elements
   int AlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value,
   /// Access granularity of B matrix in units of elements
   int AlignmentB = 128 / cutlass::sizeof_bits<ElementB>::value
 >
 struct DefaultConv2dFpropWithReduction {
```

## cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_group_fprop.h

```diff
@@ -53,15 +53,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace conv {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-/// Defines a kernel for Conv2dGroupFpro
+/// Defines a kernel for Conv2dGroupFprop
 template <
   typename ElementA,
   typename LayoutA,
   typename ElementB,
   typename LayoutB,
   typename ElementC,
   typename LayoutC,
@@ -73,15 +73,15 @@
   typename InstructionShape,
   typename EpilogueOutputOp,
   typename ThreadblockSwizzle,
   int Stages,
   typename MathOperatorTag,
   conv::GroupMode GroupMode,
   conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kOptimized,
-  conv::StrideSupport StrideSupport = StrideSupport::kStrided,
+  conv::StrideSupport StrideSupport = StrideSupport::kUnity,
   /// Access granularity of A matrix in units of elements
   int AlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value,
   /// Access granularity of B matrix in units of elements
   int AlignmentB = 128 / cutlass::sizeof_bits<ElementB>::value
 > struct DefaultConv2dGroupFprop;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -131,19 +131,19 @@
   GroupMode,
   IteratorAlgorithm::kAnalytic,
   StrideSupport,
   AlignmentA,
   AlignmentB
 > {
 
-  static_assert(std::is_same<LayoutA, cutlass::layout::TensorNHWC>::value,
+  static_assert(platform::is_same<LayoutA, cutlass::layout::TensorNHWC>::value,
     "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutB, cutlass::layout::TensorNHWC>::value,
+  static_assert(platform::is_same<LayoutB, cutlass::layout::TensorNHWC>::value,
     "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutC, cutlass::layout::TensorNHWC>::value,
+  static_assert(platform::is_same<LayoutC, cutlass::layout::TensorNHWC>::value,
     "Current group conv only support NHWC layout");
 
   // Define the core components from GEMM
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
       ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
       ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
       Stages, MathOperatorTag>;
@@ -265,19 +265,19 @@
   GroupMode,
   IteratorAlgorithm::kAnalytic,
   StrideSupport,
   AlignmentA,
   AlignmentB
 > {
 
-  static_assert(std::is_same<LayoutA, cutlass::layout::TensorNHWC>::value,
+  static_assert(platform::is_same<LayoutA, cutlass::layout::TensorNHWC>::value,
     "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutB, cutlass::layout::TensorNHWC>::value,
+  static_assert(platform::is_same<LayoutB, cutlass::layout::TensorNHWC>::value,
     "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutC, cutlass::layout::TensorNHWC>::value,
+  static_assert(platform::is_same<LayoutC, cutlass::layout::TensorNHWC>::value,
     "Current group conv only support NHWC layout");
 
   // Define the core components from GEMM
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
       ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
       ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
       2, MathOperatorTag>;
@@ -396,19 +396,19 @@
   GroupMode::kSingleGroup,
   IteratorAlgorithm::kOptimized,
   StrideSupport,
   AlignmentA,
   AlignmentB
 > {
 
-  static_assert(std::is_same<LayoutA, cutlass::layout::TensorNHWC>::value,
+  static_assert(platform::is_same<LayoutA, cutlass::layout::TensorNHWC>::value,
     "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutB, cutlass::layout::TensorNHWC>::value,
+  static_assert(platform::is_same<LayoutB, cutlass::layout::TensorNHWC>::value,
     "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutC, cutlass::layout::TensorNHWC>::value,
+  static_assert(platform::is_same<LayoutC, cutlass::layout::TensorNHWC>::value,
     "Current group conv only support NHWC layout");
 
   // Define the core components from GEMM
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
       ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
       ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
       Stages, MathOperatorTag>;
@@ -526,19 +526,19 @@
   GroupMode::kSingleGroup,
   IteratorAlgorithm::kOptimized,
   StrideSupport,
   AlignmentA,
   AlignmentB
 > {
 
-  static_assert(std::is_same<LayoutA, cutlass::layout::TensorNHWC>::value,
+  static_assert(platform::is_same<LayoutA, cutlass::layout::TensorNHWC>::value,
     "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutB, cutlass::layout::TensorNHWC>::value,
+  static_assert(platform::is_same<LayoutB, cutlass::layout::TensorNHWC>::value,
     "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutC, cutlass::layout::TensorNHWC>::value,
+  static_assert(platform::is_same<LayoutC, cutlass::layout::TensorNHWC>::value,
     "Current group conv only support NHWC layout");
 
   // Define the core components from GEMM
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
     ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
     ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
     2, MathOperatorTag>;
```

## cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_dgrad.h

```diff
@@ -290,14 +290,447 @@
     conv::Operator::kDgrad,
     Conv3dProblemSize
   >;
 };
 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+//                            OpClassSimt convolutions 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <
+  typename ElementA,
+  typename LayoutA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ElementAccumulator,
+  typename ArchTag,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename EpilogueOutputOp,
+  typename ThreadblockSwizzle,
+  int Stages,
+  typename MathOperatorTag
+>
+struct DefaultConv3dDgrad <
+  ElementA,
+  LayoutA,
+  ElementB,
+  LayoutB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  arch::OpClassSimt,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  Stages,
+  MathOperatorTag,
+  IteratorAlgorithm::kAnalytic,
+  conv::StrideSupport::kStrided
+> {
+
+  // Define the core components from GEMM
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
+      ElementB, layout::RowMajor, ElementAccumulator, layout::RowMajor, arch::OpClassSimt,
+      Stages, MathOperatorTag>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using IteratorA =
+    cutlass::conv::threadblock::Conv3dDgradOutputGradientTileAccessIteratorAnalytic<
+      cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+      ElementA,
+      ThreadMapA,
+      conv::StrideSupport::kStrided
+    >;
+
+  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using IteratorB =
+    cutlass::conv::threadblock::Conv3dDgradFilterTileAccessIteratorAnalytic<
+      cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+      ElementB,
+      ThreadMapB
+    >;
+  
+  using SmemIteratorB = typename MmaCore::SmemIteratorB;
+
+  // Warp-level GEMM components
+  using WarpMmaSimtOp = typename MmaCore::MmaWarpSimt;
+  using MmaPolicy = typename MmaCore::MmaPolicy;
+
+  // Define the Mma
+  using Mma = threadblock::ImplicitGemmMultistage<
+    ThreadblockShape,
+    IteratorA,
+    SmemIteratorA,
+    arch::CacheOperation::Always,
+    IteratorB,
+    SmemIteratorB,
+    arch::CacheOperation::Always,
+    MmaPolicy,
+    Stages 
+  >;
+
+  // Define the epilogue
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
+    ThreadblockShape,
+    WarpMmaSimtOp,
+    EpilogueOutputOp,
+    EpilogueOutputOp::kCount
+  >::Epilogue;
+
+  // Define the kernel
+  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
+    Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    conv::Operator::kDgrad,
+    Conv3dProblemSize
+  >;
+
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Defines a kernel for Conv3dDgrad specialization for Optimized IteratorAlgorithm, 
+/// multi-stage pipeline, and FFMA-based mainloop for SM80
+
+template <
+  typename ElementA,
+  typename LayoutA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ElementAccumulator,
+  typename ArchTag,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename EpilogueOutputOp,
+  typename ThreadblockSwizzle,
+  int Stages,
+  typename MathOperatorTag
+>
+struct DefaultConv3dDgrad <
+  ElementA,
+  LayoutA,
+  ElementB,
+  LayoutB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  arch::OpClassSimt,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  Stages,
+  MathOperatorTag,
+  IteratorAlgorithm::kOptimized,
+  StrideSupport::kUnity
+> {
+
+  // Define the core components from GEMM
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
+      ElementB, layout::RowMajor, ElementAccumulator, layout::RowMajor, arch::OpClassSimt,
+      Stages, MathOperatorTag>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using IteratorA =
+    cutlass::conv::threadblock::Conv3dDgradOutputGradientTileAccessIteratorOptimized<
+      cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+      ElementA,
+      ThreadMapA,
+      StrideSupport::kUnity
+    >;
+
+  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using IteratorB =
+    cutlass::conv::threadblock::Conv3dDgradFilterTileAccessIteratorOptimized<
+      cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+      ElementB,
+      ThreadMapB
+      // ThreadMapB,
+      // StrideSupport::kUnity
+    >;
+  
+  using SmemIteratorB = typename MmaCore::SmemIteratorB;
+
+  // Warp-level GEMM components
+  using WarpMmaSimtOp = typename MmaCore::MmaWarpSimt;
+  using MmaPolicy = typename MmaCore::MmaPolicy;
+
+  // Define the Mma
+  using Mma = threadblock::ImplicitGemmMultistage<
+    ThreadblockShape,
+    IteratorA,
+    SmemIteratorA,
+    arch::CacheOperation::Always,
+    IteratorB,
+    SmemIteratorB,
+    arch::CacheOperation::Always,
+    MmaPolicy,
+    Stages 
+  >;
+
+  // Define the epilogue
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
+    ThreadblockShape,
+    WarpMmaSimtOp,
+    EpilogueOutputOp,
+    EpilogueOutputOp::kCount
+  >::Epilogue;
+
+  // Define the kernel
+  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
+    Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    conv::Operator::kDgrad,
+    Conv3dProblemSize
+  >;
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <
+  typename ElementA,
+  typename LayoutA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ElementAccumulator,
+  typename ArchTag,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename EpilogueOutputOp,
+  typename ThreadblockSwizzle,
+  typename MathOperatorTag
+>
+struct DefaultConv3dDgrad <
+  ElementA,
+  LayoutA,
+  ElementB,
+  LayoutB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  arch::OpClassSimt,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  2,
+  MathOperatorTag,
+  IteratorAlgorithm::kAnalytic,
+  conv::StrideSupport::kStrided
+> {
+
+  // Define the core components from GEMM
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
+      ElementB, layout::RowMajor, ElementAccumulator, layout::RowMajor, arch::OpClassSimt,
+      2, MathOperatorTag>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using IteratorA =
+    // cutlass::conv::threadblock::TileIteratorStridedDgrad<
+      cutlass::conv::threadblock::Conv3dDgradOutputGradientTileAccessIteratorAnalytic<
+        cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+        ElementA,
+        ThreadMapA,
+        conv::StrideSupport::kStrided
+      // >
+    >;
+
+  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using IteratorB =
+    // cutlass::conv::threadblock::TileIteratorStridedDgrad<
+      cutlass::conv::threadblock::Conv3dDgradFilterTileAccessIteratorAnalytic<
+        cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+        ElementB,
+        ThreadMapB
+      // >
+    >;
+  
+  using SmemIteratorB = typename MmaCore::SmemIteratorB;
+
+  // Warp-level GEMM components
+  using WarpMmaSimtOp = typename MmaCore::MmaWarpSimt;
+  using MmaPolicy = typename MmaCore::MmaPolicy;
+
+  // Define the Mma
+  using Mma = threadblock::ImplicitGemmPipelined<
+    ThreadblockShape,
+    IteratorA,
+    SmemIteratorA,
+    IteratorB,
+    SmemIteratorB,
+    ElementC,
+    LayoutC,
+    MmaPolicy
+  >;
+
+  // Define the epilogue
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
+    ThreadblockShape,
+    WarpMmaSimtOp,
+    EpilogueOutputOp,
+    EpilogueOutputOp::kCount
+  >::Epilogue;
+
+  // Define the kernel
+  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
+    Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    conv::Operator::kDgrad,
+    Conv3dProblemSize
+  >;
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Defines a kernel for Conv3dDgrad specialization for Optimized IteratorAlgorithm, 
+/// 2 stage pipeline, and FFMA-based mainloop for SM50
+template <
+  typename ElementA,
+  typename LayoutA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ElementAccumulator,
+  typename ArchTag,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename EpilogueOutputOp,
+  typename ThreadblockSwizzle,
+  typename MathOperatorTag
+>
+struct DefaultConv3dDgrad <
+  ElementA,
+  LayoutA,
+  ElementB,
+  LayoutB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  arch::OpClassSimt,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  2,
+  MathOperatorTag,
+  IteratorAlgorithm::kOptimized,
+  StrideSupport::kUnity
+> {
+
+  // Define the core components from GEMM
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
+      ElementB, layout::RowMajor, ElementAccumulator, layout::RowMajor, arch::OpClassSimt,
+      2, MathOperatorTag>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using IteratorA =
+    // cutlass::conv::threadblock::TileIterator<
+      cutlass::conv::threadblock::Conv3dDgradOutputGradientTileAccessIteratorOptimized<
+        cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+        ElementA,
+        ThreadMapA,
+        StrideSupport::kUnity
+      // >
+    >;
+
+  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using IteratorB =
+    // cutlass::conv::threadblock::TileIterator<
+      cutlass::conv::threadblock::Conv3dDgradFilterTileAccessIteratorOptimized<
+        cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+        ElementB,
+        ThreadMapB
+        // ThreadMapB,
+        // StrideSupport::kUnity
+      // >
+    >;
+  
+  using SmemIteratorB = typename MmaCore::SmemIteratorB;
+
+  // Warp-level GEMM components
+  using WarpMmaSimtOp = typename MmaCore::MmaWarpSimt;
+  using MmaPolicy = typename MmaCore::MmaPolicy;
+
+  // Define the Mma
+  using Mma = threadblock::ImplicitGemmPipelined<
+    ThreadblockShape,
+    IteratorA,
+    SmemIteratorA,
+    IteratorB,
+    SmemIteratorB,
+    ElementC,
+    LayoutC,
+    MmaPolicy
+  >;
+
+  // Define the epilogue
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
+    ThreadblockShape,
+    WarpMmaSimtOp,
+    EpilogueOutputOp,
+    EpilogueOutputOp::kCount
+  >::Epilogue;
+
+  // Define the kernel
+  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
+    Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    conv::Operator::kDgrad,
+    Conv3dProblemSize
+  >;
+
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace kernel
 } // namespace conv
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop.h

```diff
@@ -50,15 +50,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace conv {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-/// Defines a kernel for Conv2dFprop
+/// Defines a kernel for Conv3dFprop
 template <
   typename ElementA,
   typename LayoutA,
   typename ElementB,
   typename LayoutB,
   typename ElementC,
   typename LayoutC,
@@ -69,15 +69,15 @@
   typename WarpShape,
   typename InstructionShape,
   typename EpilogueOutputOp,
   typename ThreadblockSwizzle,
   int Stages,
   typename MathOperatorTag,
   conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kOptimized,
-  conv::StrideSupport StrideSupport = StrideSupport::kStrided
+  conv::StrideSupport StrideSupport = StrideSupport::kUnity
 > struct DefaultConv3dFprop;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Defines a kernel for Conv3dFprop specialization for Analytic Iterator Algorithm
 /// and 2 stage pipeline.
 template <
@@ -90,15 +90,16 @@
   typename ElementAccumulator,
   typename ArchTag,
   typename ThreadblockShape,
   typename WarpShape,
   typename InstructionShape,
   typename EpilogueOutputOp,
   typename ThreadblockSwizzle,
-  typename MathOperatorTag
+  typename MathOperatorTag,
+  conv::StrideSupport StrideSupport
 >
 struct DefaultConv3dFprop <
   ElementA,
   LayoutA,
   ElementB,
   LayoutB,
   ElementC,
@@ -109,15 +110,16 @@
   ThreadblockShape,
   WarpShape,
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   2,
   MathOperatorTag,
-  IteratorAlgorithm::kAnalytic
+  IteratorAlgorithm::kAnalytic,
+  StrideSupport
 > {
 
   // Define the core components from GEMM
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
       ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
       ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
       2, MathOperatorTag>;
@@ -181,15 +183,15 @@
     conv::Operator::kFprop,
     Conv3dProblemSize
   >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Defines a kernel for Conv2dFprop specialization for Analytic IteratorAlgorithm and multistage
+/// Defines a kernel for Conv3dFprop specialization for Analytic IteratorAlgorithm and multistage
 // pipeline.
 template <
   typename ElementA,
   typename LayoutA,
   typename ElementB,
   typename LayoutB,
   typename ElementC,
@@ -198,15 +200,16 @@
   typename ArchTag,
   typename ThreadblockShape,
   typename WarpShape,
   typename InstructionShape,
   typename EpilogueOutputOp,
   typename ThreadblockSwizzle,
   int Stages,
-  typename MathOperatorTag
+  typename MathOperatorTag,
+  conv::StrideSupport StrideSupport
 >
 struct DefaultConv3dFprop <
   ElementA,
   LayoutA,
   ElementB,
   LayoutB,
   ElementC,
@@ -217,15 +220,16 @@
   ThreadblockShape,
   WarpShape,
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
   MathOperatorTag,
-  IteratorAlgorithm::kAnalytic
+  IteratorAlgorithm::kAnalytic,
+  StrideSupport
 > {
 
   // Define the core components from GEMM
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
       ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
       ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
       Stages, MathOperatorTag>;
@@ -302,15 +306,16 @@
   typename ElementAccumulator,
   typename ArchTag,
   typename ThreadblockShape,
   typename WarpShape,
   typename InstructionShape,
   typename EpilogueOutputOp,
   typename ThreadblockSwizzle,
-  typename MathOperatorTag
+  typename MathOperatorTag,
+  conv::StrideSupport StrideSupport
 >
 struct DefaultConv3dFprop <
   ElementA,
   LayoutA,
   ElementB,
   LayoutB,
   ElementC,
@@ -321,15 +326,16 @@
   ThreadblockShape,
   WarpShape,
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   2,
   MathOperatorTag,
-  IteratorAlgorithm::kOptimized
+  IteratorAlgorithm::kOptimized,
+  StrideSupport
 > {
 
   // Define the core components from GEMM
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
       ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
       ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
       2, MathOperatorTag>;
@@ -412,15 +418,16 @@
   typename ArchTag,
   typename ThreadblockShape,
   typename WarpShape,
   typename InstructionShape,
   typename EpilogueOutputOp,
   typename ThreadblockSwizzle,
   int Stages,
-  typename MathOperatorTag
+  typename MathOperatorTag,
+  conv::StrideSupport StrideSupport
 >
 struct DefaultConv3dFprop <
   ElementA,
   LayoutA,
   ElementB,
   LayoutB,
   ElementC,
@@ -431,15 +438,16 @@
   ThreadblockShape,
   WarpShape,
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
   MathOperatorTag,
-  IteratorAlgorithm::kOptimized
+  IteratorAlgorithm::kOptimized,
+  StrideSupport
 > {
 
   // Define the core components from GEMM
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
       ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
       ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
       Stages, MathOperatorTag>;
@@ -488,15 +496,473 @@
 
   // Define the epilogue
   using Epilogue = typename epilogue::threadblock::DefaultEpilogueTensorOp<
     ThreadblockShape,
     WarpMmaTensorOp,
     1,
     EpilogueOutputOp,
-    EpilogueOutputOp::kCount
+    EpilogueOutputOp::kCount,
+    false,
+    layout::NoPermute,
+    StrideSupport,
+    5
+  >::Epilogue;
+
+  // Define the kernel
+  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
+    Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    conv::Operator::kFprop,
+    Conv3dProblemSize
+  >;
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+//                            OpClassSimt convolutions
+/////////////////////////////////////////////////////////////////////////////////////////////////
+/// Defines a kernel for Conv3dFprop specialization for Analytic IteratorAlgorithm, 
+/// multi-stage pipeline, and FFMA-based mainloop for SM80
+
+template <
+  typename ElementA,
+  typename LayoutA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ElementAccumulator,
+  typename ArchTag,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename EpilogueOutputOp,
+  typename ThreadblockSwizzle,
+  int Stages,
+  typename MathOperatorTag,
+  conv::StrideSupport StrideSupport
+>
+struct DefaultConv3dFprop <
+  ElementA,
+  LayoutA,
+  ElementB,
+  LayoutB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  arch::OpClassSimt,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  Stages,
+  MathOperatorTag,
+  IteratorAlgorithm::kAnalytic,
+  StrideSupport
+> {
+
+  // Define the core components from GEMM
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
+      ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassSimt,
+      Stages, MathOperatorTag>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using IteratorA =
+    cutlass::conv::threadblock::Conv3dFpropActivationTileAccessIteratorAnalytic<
+      cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+      ElementA,
+      ThreadMapA
+    >;
+
+  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using IteratorB =
+    cutlass::conv::threadblock::Conv3dFpropFilterTileAccessIteratorAnalytic<
+      cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+      ElementB,
+      ThreadMapB
+    >;
+  
+  using SmemIteratorB = typename MmaCore::SmemIteratorB;
+
+  // Warp-level GEMM components
+  using WarpMmaSimtOp = typename MmaCore::MmaWarpSimt;
+  using MmaPolicy = typename MmaCore::MmaPolicy;
+
+  // Define the Mma
+  using Mma = threadblock::ImplicitGemmMultistage<
+    ThreadblockShape,
+    IteratorA,
+    SmemIteratorA,
+    arch::CacheOperation::Always,
+    IteratorB,
+    SmemIteratorB,
+    arch::CacheOperation::Always,
+    MmaPolicy,
+    Stages 
+  >;
+
+  // Define the epilogue
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
+    ThreadblockShape,
+    WarpMmaSimtOp,
+    EpilogueOutputOp,
+    EpilogueOutputOp::kCount,
+    false,
+    layout::NoPermute,
+    StrideSupport,
+    5
+  >::Epilogue;
+
+  // Define the kernel
+  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
+    Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    conv::Operator::kFprop,
+    Conv3dProblemSize
+  >;
+
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Defines a kernel for Conv3dFprop specialization for Optimized IteratorAlgorithm, 
+/// multi-stage pipeline, and FFMA-based mainloop for SM80
+
+template <
+  typename ElementA,
+  typename LayoutA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ElementAccumulator,
+  typename ArchTag,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename EpilogueOutputOp,
+  typename ThreadblockSwizzle,
+  int Stages,
+  typename MathOperatorTag,
+  conv::StrideSupport StrideSupport
+>
+struct DefaultConv3dFprop <
+  ElementA,
+  LayoutA,
+  ElementB,
+  LayoutB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  arch::OpClassSimt,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  Stages,
+  MathOperatorTag,
+  IteratorAlgorithm::kOptimized,
+  StrideSupport
+> {
+
+  // Define the core components from GEMM
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
+      ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassSimt,
+      Stages, MathOperatorTag>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using IteratorA =
+    cutlass::conv::threadblock::Conv3dFpropActivationTileAccessIteratorOptimized<
+      cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+      ElementA,
+      LayoutA,
+      ThreadMapA
+    >;
+
+  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using IteratorB =
+    cutlass::conv::threadblock::Conv3dFpropFilterTileAccessIteratorOptimized<
+      cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+      ElementB,
+      LayoutB,
+      ThreadMapB
+    >;
+  
+  using SmemIteratorB = typename MmaCore::SmemIteratorB;
+
+  // Warp-level GEMM components
+  using WarpMmaSimtOp = typename MmaCore::MmaWarpSimt;
+  using MmaPolicy = typename MmaCore::MmaPolicy;
+
+  // Define the Mma
+  using Mma = threadblock::ImplicitGemmMultistage<
+    ThreadblockShape,
+    IteratorA,
+    SmemIteratorA,
+    arch::CacheOperation::Always,
+    IteratorB,
+    SmemIteratorB,
+    arch::CacheOperation::Always,
+    MmaPolicy,
+    Stages 
+  >;
+
+  // Define the epilogue
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
+    ThreadblockShape,
+    WarpMmaSimtOp,
+    EpilogueOutputOp,
+    EpilogueOutputOp::kCount,
+    false,
+    layout::NoPermute,
+    StrideSupport,
+    5
+  >::Epilogue;
+
+  // Define the kernel
+  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
+    Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    conv::Operator::kFprop,
+    Conv3dProblemSize
+  >;
+
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Defines a kernel for Conv3dFprop specialization for Analytic IteratorAlgorithm, 
+/// 2 stage pipeline, and FFMA-based mainloop for SM50
+template <
+  typename ElementA,
+  typename LayoutA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ElementAccumulator,
+  typename ArchTag,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename EpilogueOutputOp,
+  typename ThreadblockSwizzle,
+  typename MathOperatorTag,
+  conv::StrideSupport StrideSupport
+>
+struct DefaultConv3dFprop <
+  ElementA,
+  LayoutA,
+  ElementB,
+  LayoutB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  arch::OpClassSimt,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  2,
+  MathOperatorTag,
+  IteratorAlgorithm::kAnalytic,
+  StrideSupport
+> {
+
+  // Define the core components from GEMM
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
+      ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassSimt,
+      2, MathOperatorTag>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using IteratorA =
+    cutlass::conv::threadblock::TileIterator<
+      cutlass::conv::threadblock::Conv3dFpropActivationTileAccessIteratorAnalytic<
+        cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+        ElementA,
+        ThreadMapA
+      >
+    >;
+
+  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using IteratorB =
+    cutlass::conv::threadblock::TileIterator<
+      cutlass::conv::threadblock::Conv3dFpropFilterTileAccessIteratorAnalytic<
+        cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+        ElementB,
+        ThreadMapB
+      >
+    >;
+  
+  using SmemIteratorB = typename MmaCore::SmemIteratorB;
+
+  // Warp-level GEMM components
+  using WarpMmaSimtOp = typename MmaCore::MmaWarpSimt;
+  using MmaPolicy = typename MmaCore::MmaPolicy;
+
+  // Define the Mma
+  using Mma = threadblock::ImplicitGemmPipelined<
+    ThreadblockShape,
+    IteratorA,
+    SmemIteratorA,
+    IteratorB,
+    SmemIteratorB,
+    ElementC,
+    LayoutC,
+    MmaPolicy
+  >;
+
+  // Define the epilogue
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
+    ThreadblockShape,
+    WarpMmaSimtOp,
+    EpilogueOutputOp,
+    EpilogueOutputOp::kCount,
+    false,
+    layout::NoPermute,
+    StrideSupport,
+    5
+  >::Epilogue;
+
+  // Define the kernel
+  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
+    Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    conv::Operator::kFprop,
+    Conv3dProblemSize
+  >;
+
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Defines a kernel for Conv3dFprop specialization for Optimized IteratorAlgorithm, 
+/// 2 stage pipeline, and FFMA-based mainloop for SM50
+template <
+  typename ElementA,
+  typename LayoutA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ElementAccumulator,
+  typename ArchTag,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename EpilogueOutputOp,
+  typename ThreadblockSwizzle,
+  typename MathOperatorTag,
+  conv::StrideSupport StrideSupport
+>
+struct DefaultConv3dFprop <
+  ElementA,
+  LayoutA,
+  ElementB,
+  LayoutB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  arch::OpClassSimt,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  2,
+  MathOperatorTag,
+  IteratorAlgorithm::kOptimized,
+  StrideSupport
+> {
+
+  // Define the core components from GEMM
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
+      ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassSimt,
+      2, MathOperatorTag>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using IteratorA =
+    cutlass::conv::threadblock::TileIterator<
+      cutlass::conv::threadblock::Conv3dFpropActivationTileAccessIteratorOptimized<
+        cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+        ElementA,
+        LayoutA,
+        ThreadMapA
+      >
+    >;
+
+  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using IteratorB =
+    cutlass::conv::threadblock::TileIterator<
+      cutlass::conv::threadblock::Conv3dFpropFilterTileAccessIteratorOptimized<
+        cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+        ElementB,
+        LayoutB,
+        ThreadMapB
+      >
+    >;
+  
+  using SmemIteratorB = typename MmaCore::SmemIteratorB;
+
+  // Warp-level GEMM components
+  using WarpMmaSimtOp = typename MmaCore::MmaWarpSimt;
+  using MmaPolicy = typename MmaCore::MmaPolicy;
+
+  // Define the Mma
+  using Mma = threadblock::ImplicitGemmPipelined<
+    ThreadblockShape,
+    IteratorA,
+    SmemIteratorA,
+    IteratorB,
+    SmemIteratorB,
+    ElementC,
+    LayoutC,
+    MmaPolicy
+  >;
+
+  // Define the epilogue
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
+    ThreadblockShape,
+    WarpMmaSimtOp,
+    EpilogueOutputOp,
+    EpilogueOutputOp::kCount,
+    false,
+    layout::NoPermute,
+    StrideSupport,
+    5
   >::Epilogue;
 
   // Define the kernel
   using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
     Mma,
     Epilogue,
     ThreadblockSwizzle,
```

## cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h

```diff
@@ -73,15 +73,15 @@
   typename WarpShape,
   typename InstructionShape,
   typename EpilogueOutputOp,
   typename ThreadblockSwizzle,
   int Stages,
   typename MathOperatorTag,
   conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kOptimized,
-  conv::StrideSupport StrideSupport = StrideSupport::kStrided
+  conv::StrideSupport StrideSupport = StrideSupport::kUnity
 > struct DefaultConv3dFpropFusion;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //                         OpClassTensorOp convolutions 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Defines a kernel for Conv3dFprop specialzation for Analytic IteratorAlgorithm and multistage
```

## cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_wgrad.h

```diff
@@ -49,15 +49,15 @@
 
 namespace cutlass {
 namespace conv {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Defines a kernel for Conv2dWgrad
+/// Defines a kernel for Conv3dWgrad
 template <
   typename ElementA,
   typename LayoutA,
   typename ElementB,
   typename LayoutB,
   typename ElementC,
   typename LayoutC,
@@ -496,14 +496,441 @@
     Mma,
     Epilogue,
     ThreadblockSwizzle,
     conv::Operator::kWgrad,
     Conv3dProblemSize
   >;
 };
+
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+//                         OpClassSimt convolutions
+/////////////////////////////////////////////////////////////////////////////////////////////////
+/// Defines a kernel for Conv3dWgrad specialization for Analytic IteratorAlgorithm, 
+/// multi-stage pipeline, and FFMA-based mainloop for SM80
+
+template <
+  typename ElementA,
+  typename LayoutA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ElementAccumulator,
+  typename ArchTag,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename EpilogueOutputOp,
+  typename ThreadblockSwizzle,
+  int Stages,
+  typename MathOperatorTag
+>
+struct DefaultConv3dWgrad <
+  ElementA,
+  LayoutA,
+  ElementB,
+  LayoutB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  arch::OpClassSimt,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  Stages,
+  MathOperatorTag,
+  IteratorAlgorithm::kAnalytic
+> {
+
+  // Define the core components from GEMM
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::ColumnMajor,
+      ElementB, layout::RowMajor, ElementAccumulator, layout::RowMajor, arch::OpClassSimt,
+      Stages, MathOperatorTag>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using IteratorA =
+    cutlass::conv::threadblock::Conv3dWgradOutputGradientTileAccessIteratorAnalytic<
+      cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+      ElementA,
+      ThreadMapA
+    >;
+
+  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using IteratorB =
+    cutlass::conv::threadblock::Conv3dWgradActivationTileAccessIteratorAnalytic<
+      cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+      ElementB,
+      ThreadMapB
+    >;
+  
+  using SmemIteratorB = typename MmaCore::SmemIteratorB;
+
+  // Warp-level GEMM components
+  using WarpMmaSimtOp = typename MmaCore::MmaWarpSimt;
+  using MmaPolicy = typename MmaCore::MmaPolicy;
+
+  // Define the Mma
+  using Mma = threadblock::ImplicitGemmMultistage<
+    ThreadblockShape,
+    IteratorA,
+    SmemIteratorA,
+    arch::CacheOperation::Always,
+    IteratorB,
+    SmemIteratorB,
+    arch::CacheOperation::Always,
+    MmaPolicy,
+    Stages 
+  >;
+
+  // Define the epilogue
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
+    ThreadblockShape,
+    WarpMmaSimtOp,
+    EpilogueOutputOp,
+    EpilogueOutputOp::kCount
+  >::Epilogue;
+
+  // Define the kernel
+  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
+    Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    conv::Operator::kWgrad,
+    Conv3dProblemSize
+  >;
+
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Defines a kernel for Conv3dWgrad specialization for Optimized IteratorAlgorithm, 
+/// multi-stage pipeline, and FFMA-based mainloop for SM80
+
+template <
+  typename ElementA,
+  typename LayoutA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ElementAccumulator,
+  typename ArchTag,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename EpilogueOutputOp,
+  typename ThreadblockSwizzle,
+  int Stages,
+  typename MathOperatorTag
+>
+struct DefaultConv3dWgrad <
+  ElementA,
+  LayoutA,
+  ElementB,
+  LayoutB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  arch::OpClassSimt,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  Stages,
+  MathOperatorTag,
+  IteratorAlgorithm::kOptimized
+> {
+
+  // Define the core components from GEMM
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::ColumnMajor,
+      ElementB, layout::RowMajor, ElementAccumulator, layout::RowMajor, arch::OpClassSimt,
+      Stages, MathOperatorTag>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using IteratorA =
+    cutlass::conv::threadblock::Conv3dWgradOutputGradientTileAccessIteratorOptimized<
+      cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+      ElementA,
+      ThreadMapA
+    >;
+
+  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using IteratorB =
+    cutlass::conv::threadblock::Conv3dWgradActivationTileAccessIteratorOptimized<
+      cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+      ElementB,
+      ThreadMapB
+    >;
+  
+  using SmemIteratorB = typename MmaCore::SmemIteratorB;
+
+  // Warp-level GEMM components
+  using WarpMmaSimtOp = typename MmaCore::MmaWarpSimt;
+  using MmaPolicy = typename MmaCore::MmaPolicy;
+
+  // Define the Mma
+  using Mma = threadblock::ImplicitGemmMultistage<
+    ThreadblockShape,
+    IteratorA,
+    SmemIteratorA,
+    arch::CacheOperation::Always,
+    IteratorB,
+    SmemIteratorB,
+    arch::CacheOperation::Always,
+    MmaPolicy,
+    Stages 
+  >;
+
+  // Define the epilogue
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
+    ThreadblockShape,
+    WarpMmaSimtOp,
+    EpilogueOutputOp,
+    EpilogueOutputOp::kCount
+  >::Epilogue;
+
+  // Define the kernel
+  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
+    Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    conv::Operator::kWgrad,
+    Conv3dProblemSize
+  >;
+
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Defines a kernel for Conv3dWgrad specialization for Analytic IteratorAlgorithm, 
+/// 2 stage pipeline, and FFMA-based mainloop for SM50
+template <
+  typename ElementA,
+  typename LayoutA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ElementAccumulator,
+  typename ArchTag,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename EpilogueOutputOp,
+  typename ThreadblockSwizzle,
+  typename MathOperatorTag
+>
+struct DefaultConv3dWgrad <
+  ElementA,
+  LayoutA,
+  ElementB,
+  LayoutB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  arch::OpClassSimt,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  2,
+  MathOperatorTag,
+  IteratorAlgorithm::kAnalytic
+> {
+
+  // Define the core components from GEMM
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::ColumnMajor,
+      ElementB, layout::RowMajor, ElementAccumulator, layout::RowMajor, arch::OpClassSimt,
+      2, MathOperatorTag>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using IteratorA =
+    cutlass::conv::threadblock::TileIterator<
+      cutlass::conv::threadblock::Conv3dWgradOutputGradientTileAccessIteratorAnalytic<
+        cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+        ElementA,
+        ThreadMapA
+      >
+    >;
+
+  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using IteratorB =
+    cutlass::conv::threadblock::TileIterator<
+      cutlass::conv::threadblock::Conv3dWgradActivationTileAccessIteratorAnalytic<
+        cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+        ElementB,
+        ThreadMapB
+      >
+    >;
+  
+  using SmemIteratorB = typename MmaCore::SmemIteratorB;
+
+  // Warp-level GEMM components
+  using WarpMmaSimtOp = typename MmaCore::MmaWarpSimt;
+  using MmaPolicy = typename MmaCore::MmaPolicy;
+
+  // Define the Mma
+  using Mma = threadblock::ImplicitGemmPipelined<
+    ThreadblockShape,
+    IteratorA,
+    SmemIteratorA,
+    IteratorB,
+    SmemIteratorB,
+    ElementC,
+    LayoutC,
+    MmaPolicy
+  >;
+
+  // Define the epilogue
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
+    ThreadblockShape,
+    WarpMmaSimtOp,
+    EpilogueOutputOp,
+    EpilogueOutputOp::kCount
+  >::Epilogue;
+
+  // Define the kernel
+  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
+    Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    conv::Operator::kWgrad,
+    Conv3dProblemSize
+  >;
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Defines a kernel for Conv3dWgrad specialization for Optimized IteratorAlgorithm, 
+/// 2 stage pipeline, and FFMA-based mainloop for SM50
+template <
+  typename ElementA,
+  typename LayoutA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ElementAccumulator,
+  typename ArchTag,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename EpilogueOutputOp,
+  typename ThreadblockSwizzle,
+  typename MathOperatorTag
+>
+struct DefaultConv3dWgrad <
+  ElementA,
+  LayoutA,
+  ElementB,
+  LayoutB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  arch::OpClassSimt,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  2,
+  MathOperatorTag,
+  IteratorAlgorithm::kOptimized
+> {
+
+  // Define the core components from GEMM
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::ColumnMajor,
+      ElementB, layout::RowMajor, ElementAccumulator, layout::RowMajor, arch::OpClassSimt,
+      2, MathOperatorTag>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using IteratorA =
+    cutlass::conv::threadblock::TileIterator<
+      cutlass::conv::threadblock::Conv3dWgradOutputGradientTileAccessIteratorOptimized<
+        cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+        ElementA,
+        ThreadMapA
+      >
+    >;
+
+  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using IteratorB =
+    cutlass::conv::threadblock::TileIterator<
+      cutlass::conv::threadblock::Conv3dWgradActivationTileAccessIteratorOptimized<
+        cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+        ElementB,
+        ThreadMapB
+      >
+    >;
+  
+  using SmemIteratorB = typename MmaCore::SmemIteratorB;
+
+  // Warp-level GEMM components
+  using WarpMmaSimtOp = typename MmaCore::MmaWarpSimt;
+  using MmaPolicy = typename MmaCore::MmaPolicy;
+
+  // Define the Mma
+  using Mma = threadblock::ImplicitGemmPipelined<
+    ThreadblockShape,
+    IteratorA,
+    SmemIteratorA,
+    IteratorB,
+    SmemIteratorB,
+    ElementC,
+    LayoutC,
+    MmaPolicy
+  >;
+
+  // Define the epilogue
+  using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
+    ThreadblockShape,
+    WarpMmaSimtOp,
+    EpilogueOutputOp,
+    EpilogueOutputOp::kCount
+  >::Epilogue;
+
+  // Define the kernel
+  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
+    Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    conv::Operator::kWgrad,
+    Conv3dProblemSize
+  >;
+
+};
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace kernel
 } // namespace conv
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/conv/kernel/default_depthwise_fprop.h

```diff
@@ -76,15 +76,15 @@
   typename WarpShape,
   typename InstructionShape,
   typename EpilogueOutputOp,
   typename ThreadblockSwizzle,
   int Stages,
   typename MathOperatorTag,
   conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kAnalytic,
-  conv::StrideSupport StrideSupport = StrideSupport::kStrided,
+  conv::StrideSupport StrideSupport = StrideSupport::kUnity,
   /// Access granularity of A matrix in units of elements
   int AlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value,
   /// Access granularity of B matrix in units of elements
   int AlignmentB = cutlass::sizeof_bits<ElementB>::value / cutlass::sizeof_bits<ElementB>::value
 > struct DefaultDepthwiseFprop;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -105,15 +105,15 @@
   typename WarpShape,
   typename InstructionShape,
   typename EpilogueOutputOp,
   typename ThreadblockSwizzle,
   int Stages,
   typename MathOperatorTag,
   conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kAnalytic,
-  conv::StrideSupport StrideSupport = StrideSupport::kStrided,
+  conv::StrideSupport StrideSupport = StrideSupport::kUnity,
   // MatrixShape<Height, Width>
   typename StrideShape = cutlass::MatrixShape<-1, -1>,
   // MatrixShape< Height, Width> 
   typename DilationShape =  cutlass::MatrixShape<-1, -1>, 
   /// Access granularity of A matrix in units of elements
   int AlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value,
   /// Access granularity of B matrix in units of elements
```

## cutlass_library/source/include/cutlass/conv/kernel/direct_convolution.h

```diff
@@ -151,15 +151,15 @@
         problem_size,
         {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
         args.problem_size.split_k_slices);
 
     swizzle_log_tile = threadblock_swizzle.get_log_tile(grid_tiled_shape);
 
     // Dynamic SMEM usage because stride and dilation are runtime params.
-    smem_size_ = (iterator_A.activation_size * kStages + iterator_B.filter_size);
+    smem_size_ = (max(iterator_A.activation_size, int(sizeof(typename Epilogue::SharedStorage))) * kStages + iterator_B.filter_size);
   }
 
   CUTLASS_HOST_DEVICE
   int get_smem_size() {
     // Dynamic Smem Size
     return smem_size_;
   }
```

## cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution.h

```diff
@@ -57,15 +57,15 @@
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   typename Mma_,                                  ///! Threadblock-scoped matrix multiply-accumulate 
   typename Epilogue_,                             ///! Epilogue
   typename ThreadblockSwizzle_,                   ///! Threadblock swizzling function
-  conv::Operator ConvOperator,                    ///! Convolutional operator (Fprop, Dgrad, Wgrad)
+  conv::Operator ConvOperator,                    ///! Convolutional operator (Fprop, Dgrad, Wgrad, Deconv)
   typename ConvProblemSize_ = Conv2dProblemSize,  ///! Convolutional operator on 2D or 3D problem
   conv::GroupMode GroupMode_ = conv::GroupMode::kNone    ///! Group mode
 >
 struct ImplicitGemmConvolution {
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
@@ -229,17 +229,17 @@
     ):
       problem_size(args.problem_size),
       implicit_gemm_problem_size(cutlass::conv::implicit_gemm_problem_size(kConvolutionalOperator, args.problem_size)),
       iterator_A(Mma::IteratorA::getParams(args.problem_size, args.ref_A.layout())),
       ptr_A(args.ref_A.data()),
       iterator_B(args.problem_size, args.ref_B.layout()),
       ptr_B(args.ref_B.data()),
-      iterator_C(ConvOutputIteratorParameter::layout(args.ref_C)),
+      iterator_C(ConvOutputIteratorParameter::layout(args.ref_C), args.problem_size),
       ptr_C(args.ref_C.data()),
-      iterator_D(ConvOutputIteratorParameter::layout(args.ref_D)),
+      iterator_D(ConvOutputIteratorParameter::layout(args.ref_D), args.problem_size),
       ptr_D(args.ref_D.data()),
       output_op(args.output_op),
       semaphore(semaphore),
       split_k_mode(args.split_k_mode)
     {
       gemm_k_iterations = implicit_gemm_k_iterations(
         kConvolutionalOperator,
@@ -393,15 +393,14 @@
       params.iterator_C,
       params.ptr_C,
       ConvOutputIteratorParameter::extent(params.problem_size),
       thread_idx,
       threadblock_offset
     );
 
-
     // Construct the epilogue
     Epilogue epilogue(
       shared_storage.epilogue, 
       thread_idx, 
       warp_idx, 
       lane_idx);
```

## cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h

```diff
@@ -198,40 +198,38 @@
 
     }
 
   };
 
   /// Parameters structure
   struct Params {
-    ConvProblemSize problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    gemm::GemmCoord implicit_gemm_problem_size;
-    int swizzle_log_tile;
-    int gemm_k_iterations;
-    typename Mma::IteratorA::Params iterator_A;
-    typename Mma::IteratorA::Element const *ptr_A;
-    typename Mma::IteratorB::Params iterator_B;
-    typename Mma::IteratorB::Element const *ptr_B;
-    typename Mma::IteratorScaleBias::Params iterator_scale_bias;
-    typename Mma::IteratorScaleBias::Element const *ptr_scale;
-    typename Mma::IteratorScaleBias::Element const *ptr_bias;
-    typename Epilogue::OutputTileIterator::Params iterator_C;
-    typename Epilogue::OutputTileIterator::Element *ptr_C;
-    typename Epilogue::OutputTileIterator::Params iterator_D;
-    typename Epilogue::OutputTileIterator::Element *ptr_D;
-    typename EpilogueOutputOp::Params output_op;
-    int *semaphore;
-    SplitKMode split_k_mode;
+    ConvProblemSize problem_size{};
+    cutlass::gemm::GemmCoord grid_tiled_shape{};
+    gemm::GemmCoord implicit_gemm_problem_size{};
+    int swizzle_log_tile{0};
+    int gemm_k_iterations{0};
+    typename Mma::IteratorA::Params iterator_A{};
+    typename Mma::IteratorA::Element const *ptr_A = nullptr;
+    typename Mma::IteratorB::Params iterator_B{};
+    typename Mma::IteratorB::Element const *ptr_B = nullptr;
+    typename Mma::IteratorScaleBias::Params iterator_scale_bias{};
+    typename Mma::IteratorScaleBias::Element const *ptr_scale = nullptr;
+    typename Mma::IteratorScaleBias::Element const *ptr_bias = nullptr;
+    typename Epilogue::OutputTileIterator::Params iterator_C {};
+    typename Epilogue::OutputTileIterator::Element *ptr_C = nullptr;
+    typename Epilogue::OutputTileIterator::Params iterator_D {};
+    typename Epilogue::OutputTileIterator::Element *ptr_D = nullptr;
+    typename EpilogueOutputOp::Params output_op {};
+    int *semaphore = nullptr;
+    SplitKMode split_k_mode {};
 
     //
     // Methods
     //
-
-    CUTLASS_HOST_DEVICE
-    Params(): swizzle_log_tile(0), gemm_k_iterations(0) { }
+    Params() = default;
 
     /// 
     CUTLASS_HOST_DEVICE
     Params(
       Arguments const &args,
       int *semaphore = nullptr
     ):
```

## cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h

```diff
@@ -154,29 +154,28 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    ConvProblemSize problem_size;
-    TensorRefA ref_A;
-    TensorRefB ref_B;
-    TensorRefC ref_C;
-    TensorRefC ref_D;
-    typename EpilogueOutputOp::Params output_op;
-    SplitKMode split_k_mode;
+    ConvProblemSize problem_size{};
+    TensorRefA ref_A{};
+    TensorRefB ref_B{};
+    TensorRefC ref_C{};
+    TensorRefC ref_D{};
+    typename EpilogueOutputOp::Params output_op{};
+    SplitKMode split_k_mode{};
 
     //
     // Methods
     //
 
     /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Arguments() { }
+    Arguments() = default;
    
     CUTLASS_HOST_DEVICE 
     Arguments(
       ConvProblemSize const & problem_size
     ):
       problem_size(problem_size) { }
 
@@ -201,38 +200,36 @@
 
     }
 
   };
 
   /// Parameters structure
   struct Params {
-    ConvProblemSize problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
-    FastDivmod stride_h_divmod;
-    FastDivmod stride_w_divmod;
-    int gemm_k_iterations;
-    typename Mma::IteratorA::Params iterator_A;
-    typename Mma::IteratorA::Element const *ptr_A;
-    typename Mma::IteratorB::Params iterator_B;
-    typename Mma::IteratorB::Element const *ptr_B;
-    typename Epilogue::OutputTileIterator::Params iterator_C;
-    typename Epilogue::OutputTileIterator::Element *ptr_C;
-    typename Epilogue::OutputTileIterator::Params iterator_D;
-    typename Epilogue::OutputTileIterator::Element *ptr_D;
-    typename EpilogueOutputOp::Params output_op;
-    int *semaphore;
-    SplitKMode split_k_mode;
+    ConvProblemSize problem_size{};
+    cutlass::gemm::GemmCoord grid_tiled_shape{};
+    int swizzle_log_tile{0};
+    FastDivmod stride_h_divmod{};
+    FastDivmod stride_w_divmod{};
+    int gemm_k_iterations{0};
+    typename Mma::IteratorA::Params iterator_A{};
+    typename Mma::IteratorA::Element const *ptr_A = nullptr;
+    typename Mma::IteratorB::Params iterator_B{};
+    typename Mma::IteratorB::Element const *ptr_B = nullptr;
+    typename Epilogue::OutputTileIterator::Params iterator_C{};
+    typename Epilogue::OutputTileIterator::Element *ptr_C = nullptr;
+    typename Epilogue::OutputTileIterator::Params iterator_D{};
+    typename Epilogue::OutputTileIterator::Element *ptr_D = nullptr;
+    typename EpilogueOutputOp::Params output_op {};
+    int *semaphore = nullptr;
+    SplitKMode split_k_mode {};
 
     //
     // Methods
     //
-
-    CUTLASS_HOST_DEVICE
-    Params(): gemm_k_iterations(0) { }
+    Params() = default;
 
     /// 
     CUTLASS_HOST_DEVICE
     Params(
       Arguments const &args,
       int *semaphore = nullptr
     ):
```

## cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h

```diff
@@ -57,15 +57,15 @@
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   typename Mma_,                                  ///! Threadblock-scoped matrix multiply-accumulate 
   typename Epilogue_,                             ///! Epilogue
   typename ThreadblockSwizzle_,                   ///! Threadblock swizzling function
-  conv::Operator ConvOperator,                    ///! Convolutional operator (Fprop, Dgrad, Wgrad)
+  conv::Operator ConvOperator,                    ///! Convolutional operator (Fprop, Dgrad, Wgrad, Deconv)
   typename ConvProblemSize_ = Conv2dProblemSize   ///! Convolutional operator on 2D or 3D problem
 >
 struct ImplicitGemmConvolutionWithFusedEpilogue {
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
   using EpilogueOutputOp = typename Epilogue::OutputOp;
```

## cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h

```diff
@@ -63,15 +63,16 @@
 
 template <
   typename Shape_,
   typename Element_,
   typename Layout_,
   typename ThreadMap_,
   typename AccessType_ = cutlass::AlignedArray<Element_, ThreadMap_::kElementsPerAccess>,
-  conv::GroupMode GroupMode_ = conv::GroupMode::kNone
+  conv::GroupMode GroupMode_ = conv::GroupMode::kNone,
+  bool IsDeconv_ = false
 >
 class Conv2dFpropFilterTileAccessIteratorAnalytic {
 public:
   
   //
   // Types
   //
@@ -81,25 +82,26 @@
   using Layout = Layout_;
   using ThreadMap = ThreadMap_;
   using AccessType = AccessType_;
   using TensorRef = cutlass::TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
+  static bool const IsDeconv = IsDeconv_;
   static IteratorAlgorithm const kIteratorAlgorithm = conv::IteratorAlgorithm::kAnalytic;
   static StrideSupport const kStrideSupport = conv::StrideSupport::kStrided;
   static int const kConvDim = 2;
   using ConvProblemSize = typename conv::Conv2dProblemSize;
   static conv::GroupMode const kGroupMode = GroupMode_;
  
   static int const kAccessesPerVector = ThreadMap::kElementsPerAccess / AccessType::kElements;
   
   static_assert(!(ThreadMap::kElementsPerAccess % AccessType::kElements), 
     "Vectors implied by the thread map must be divisible by the access type.");
- 
+
   //
   // Simplifying assertions
   //
   static_assert(ThreadMap::Iterations::kContiguous == 1,
     "Require Iterations::kContiguous == 1");
 
   //
@@ -148,30 +150,33 @@
     filter_s_(0),
     filter_c_(0) {
 
     layout::PitchLinearCoord thread_coord = ThreadMap::initial_offset(thread_idx);
 
     filter_c_ = threadblock_offset.row() + thread_coord.contiguous();
 
+    auto input_channels = (IsDeconv ? problem_size_.K : problem_size_.C);
+    auto output_channels = (IsDeconv ? problem_size_.C : problem_size_.K);
+
     if (kGroupMode != conv::GroupMode::kNone) {
       filter_c_init_ = filter_c_;
       if (kGroupMode == conv::GroupMode::kDepthwise){
         channels_per_group_ = 1;
         crs_per_group_ = problem_size_.S * problem_size_.R;
       } else {
-        channels_per_group_ = problem_size_.C / problem_size_.groups;
+        channels_per_group_ = input_channels / problem_size_.groups;
         crs_per_group_ = problem_size_.S * problem_size_.R * ((channels_per_group_ + Shape::kRow - 1) / Shape::kRow);
       }
     }
 
     CUTLASS_PRAGMA_UNROLL
     for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
       offset_k_[s] = threadblock_offset.column() + thread_coord.strided() + s * ThreadMap::Delta::kStrided;
       if (kGroupMode != conv::GroupMode::kNone && kGroupMode != conv::GroupMode::kDepthwise) {
-        group_idx_offset_k_[s] = (thread_coord.strided() + s * ThreadMap::Delta::kStrided) / (problem_size_.K / problem_size_.groups);
+        group_idx_offset_k_[s] = (thread_coord.strided() + s * ThreadMap::Delta::kStrided) / (output_channels / problem_size_.groups);
       }
     }
 
     set_iteration_index(0);
   }
 
   /// Overrides the internal iteration index
@@ -237,20 +242,23 @@
 
   /// Returns true if the current coordinate is within the activations tensor W
   CUTLASS_HOST_DEVICE
   bool valid() const {
 
     TensorCoord coord = at();
 
+    auto input_channels = (IsDeconv ? problem_size_.K : problem_size_.C);
+    auto output_channels = (IsDeconv ? problem_size_.C : problem_size_.K);
+
     if (kGroupMode == conv::GroupMode::kNone) {
-      return coord.n() < problem_size_.K && coord.c() < problem_size_.C;
+      return coord.n() < output_channels && coord.c() < input_channels;
     } else if (kGroupMode == conv::GroupMode::kDepthwise) {
-      return coord.n() < problem_size_.K && coord.c() < 1; // channels_per_group_ is always equal to ONE.
+      return coord.n() < output_channels && coord.c() < 1; // channels_per_group_ is always equal to ONE.
     } else {
-      return coord.n() < problem_size_.K && coord.c() < channels_per_group_ &&
+      return coord.n() < output_channels && coord.c() < channels_per_group_ &&
              group_idx_offset_c_ == group_idx_offset_k_[iteration_strided_];
     }
   }
 
   /// Returns a pointer to the vector starting at the current coordinate
   CUTLASS_HOST_DEVICE
   AccessType const *get() const {
@@ -285,27 +293,30 @@
     return *this;
   }
 
   /// Determines whether the Implicit GEMM can execute the given problem.
   CUTLASS_HOST_DEVICE
   static Status can_implement(Conv2dProblemSize const &problem_size) {
 
+    auto input_channels = (IsDeconv ? problem_size.K : problem_size.C);
+    auto output_channels = (IsDeconv ? problem_size.C : problem_size.K);
+
     // check alignment constraint on iterator's contiguous dimension
-    if ((problem_size.C / problem_size.groups) % AccessType::kElements) {
+    if ((input_channels / problem_size.groups) % AccessType::kElements) {
       return Status::kErrorInvalidProblem;
     }
 
     if (platform::is_same<Layout, layout::TensorCxRSKx<32>>::value) {
-      if (problem_size.K % 32) {
+      if (output_channels % 32) {
         return Status::kErrorInvalidProblem;
       }
     }
 
     if (platform::is_same<Layout, layout::TensorCxRSKx<64>>::value) {
-      if (problem_size.K % 64) {
+      if (output_channels % 64) {
         return Status::kErrorInvalidProblem;
       }
     }
 
     return Status::kSuccess;
   }
 };
```

## cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h

```diff
@@ -63,15 +63,16 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   typename Shape_,
   typename Element_,
   typename Layout_,
   typename ThreadMap_,
-  typename AccessType_ = cutlass::AlignedArray<Element_, ThreadMap_::kElementsPerAccess>
+  typename AccessType_ = cutlass::AlignedArray<Element_, ThreadMap_::kElementsPerAccess>,
+  bool IsDeconv_ = false
 >
 class Conv2dFpropFilterTileAccessIteratorOptimized{
 public:
   
   //
   // Types
   //
@@ -81,14 +82,15 @@
   using Layout = Layout_;
   using ThreadMap = ThreadMap_;
   using AccessType = AccessType_;
   using TensorRef = cutlass::TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
+  static bool const IsDeconv = IsDeconv_;
   static IteratorAlgorithm const kIteratorAlgorithm = conv::IteratorAlgorithm::kOptimized;
   static StrideSupport const kStrideSupport = conv::StrideSupport::kStrided;
   static int const kConvDim = 2;
   using ConvProblemSize = typename conv::Conv2dProblemSize;
  
   static int const kAccessesPerVector = ThreadMap::kElementsPerAccess / AccessType::kElements;
   
@@ -172,19 +174,19 @@
     filter_rs_(0),
     filter_c_(0) {
 
     layout::PitchLinearCoord thread_coord = ThreadMap::initial_offset(thread_idx);
 
     filter_c_ = threadblock_offset.row() + thread_coord.contiguous();
     Index column = threadblock_offset.column() + thread_coord.strided();
-    channels_per_group_ = problem_size_.C / problem_size_.groups;
+    channels_per_group_ = (IsDeconv ? problem_size_.K : problem_size_.C) / problem_size_.groups;
 
     CUTLASS_PRAGMA_UNROLL
     for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
-      uint32_t pred = ((column + s * ThreadMap::Delta::kStrided < problem_size_.K) ? 1u : 0);
+      uint32_t pred = ((column + s * ThreadMap::Delta::kStrided < (IsDeconv ? problem_size_.C : problem_size_.K)) ? 1u : 0);
 
       CUTLASS_PRAGMA_UNROLL
       for (int v_idx = 0; v_idx < kAccessesPerVector; ++v_idx) {
         predicates_[v_idx] |= (pred << s);
       }
     }
 
@@ -283,27 +285,30 @@
     return *this;
   }
 
   /// Determines whether the Implicit GEMM can execute the given problem.
   CUTLASS_HOST_DEVICE
   static Status can_implement(Conv2dProblemSize const &problem_size) {
 
+    auto input_channels = (IsDeconv ? problem_size.K : problem_size.C);
+    auto output_channels = (IsDeconv ? problem_size.C : problem_size.K);
+
     // check alignment constraint on iterator's contiguous dimension
-    if ((problem_size.C / problem_size.groups) % AccessType::kElements) {
+    if ((input_channels / problem_size.groups) % AccessType::kElements) {
       return Status::kErrorInvalidProblem;
     }
 
     if (platform::is_same<Layout, layout::TensorCxRSKx<32>>::value) {
-      if (problem_size.K % 32) {
+      if (output_channels % 32) {
         return Status::kErrorInvalidProblem;
       }
     }
 
     if (platform::is_same<Layout, layout::TensorCxRSKx<64>>::value) {
-      if (problem_size.K % 64) {
+      if (output_channels % 64) {
         return Status::kErrorInvalidProblem;
       }
     }
 
     return Status::kSuccess;
   }
 };
```

## cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h

```diff
@@ -60,15 +60,16 @@
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   typename Shape_,
   typename Element_,
-  typename ThreadMap_
+  typename ThreadMap_,
+  bool IsDeconv_ = false
 >
 class Conv3dFpropFilterTileAccessIteratorAnalytic {
 public:
   
   //
   // Types
   //
@@ -78,14 +79,15 @@
   using Layout = layout::TensorNDHWC;
   using ThreadMap = ThreadMap_;
   using AccessType = AlignedArray<Element, ThreadMap::kElementsPerAccess>;
   using TensorRef = cutlass::TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
+  static bool const IsDeconv = IsDeconv_;
   static IteratorAlgorithm const kIteratorAlgorithm = conv::IteratorAlgorithm::kAnalytic;
   static StrideSupport const kStrideSupport = conv::StrideSupport::kStrided;
   static int const kConvDim = 3;
   using ConvProblemSize = typename conv::Conv3dProblemSize;
   static int const kAccessesPerVector = 1;
   
   //
@@ -194,16 +196,19 @@
 
   /// Returns true if the current coordinate is within the activations tensor W
   CUTLASS_HOST_DEVICE
   bool valid() const {
 
     TensorCoord coord = at();
 
-    return coord.n() < problem_size_.K &&
-      coord.c() < problem_size_.C;
+    auto input_channels = (IsDeconv ? problem_size_.K : problem_size_.C);
+    auto output_channels = (IsDeconv ? problem_size_.C : problem_size_.K);
+
+    return coord.n() < output_channels &&
+      coord.c() < input_channels;
   }
 
   /// Returns a pointer to the vector starting at the current coordinate
   CUTLASS_HOST_DEVICE
   AccessType const *get() const {
 
     TensorCoord coord = at();
@@ -230,16 +235,18 @@
     return *this;
   }
 
   /// Determines whether the Implicit GEMM can execute the given problem.
   CUTLASS_HOST_DEVICE
   static Status can_implement(ConvProblemSize const &problem_size) {
 
+    auto input_channels = (IsDeconv ? problem_size.K : problem_size.C);
+    auto output_channels = (IsDeconv ? problem_size.C : problem_size.K);
     // check alignment constraint on iterator's contiguous dimension
-    if (problem_size.K % (128/sizeof_bits<Element>::value)) {
+    if (input_channels % (128/sizeof_bits<Element>::value)) {
       return Status::kErrorInvalidProblem;
     }
     return Status::kSuccess;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h

```diff
@@ -62,15 +62,16 @@
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   typename Shape_,
   typename Element_,
   typename Layout_,
-  typename ThreadMap_
+  typename ThreadMap_,
+  bool IsDeconv_ = false
 >
 class Conv3dFpropFilterTileAccessIteratorOptimized{
 public:
   
   //
   // Types
   //
@@ -80,14 +81,15 @@
   using Layout = Layout_;
   using ThreadMap = ThreadMap_;
   using AccessType = AlignedArray<Element, ThreadMap::kElementsPerAccess>;
   using TensorRef = cutlass::TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
+  static bool const IsDeconv = IsDeconv_;
   static IteratorAlgorithm const kIteratorAlgorithm = conv::IteratorAlgorithm::kOptimized;
   static StrideSupport const kStrideSupport = conv::StrideSupport::kStrided;
   static int const kConvDim = 3;
   using ConvProblemSize = typename conv::Conv3dProblemSize;
   static int const kAccessesPerVector = 1;
   
   //
@@ -168,19 +170,19 @@
     layout::PitchLinearCoord thread_coord = ThreadMap::initial_offset(thread_idx);
 
     filter_c_ = threadblock_offset.row() + thread_coord.contiguous();
     Index column = threadblock_offset.column() + thread_coord.strided();
 
     CUTLASS_PRAGMA_UNROLL
     for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
-      uint32_t pred = ((column + s * ThreadMap::Delta::kStrided < problem_size_.K) ? 1u : 0);
+      uint32_t pred = ((column + s * ThreadMap::Delta::kStrided < (IsDeconv ? problem_size_.C : problem_size_.K)) ? 1u : 0);
       predicates_ |= (pred << s);
     }
 
-    if (filter_c_ >= problem_size.C) {
+    if (filter_c_ >= (IsDeconv ? problem_size_.K : problem_size_.C)) {
       predicates_ = 0u;
     }
 
     pointer_ += (
       params_.layout({filter_c_, column}) 
     ) * sizeof_bits<Element>::value / 8;
 
@@ -210,15 +212,15 @@
     if (filter_trs_ == params_.TRS) {
 
       filter_trs_ = 0;
       next = params_.inc_next_c;
       filter_c_ += params_.filter_c_delta;
     }
       
-    if (filter_c_ >= problem_size_.C) {
+    if (filter_c_ >= (IsDeconv ? problem_size_.K : problem_size_.C)) {
       predicates_ = 0;
     }
       
     pointer_ += next;
   }
 
   /// Returns true if the current coordinate is within the filter tensor W
@@ -255,16 +257,18 @@
     return *this;
   }
 
   /// Determines whether the Implicit GEMM can execute the given problem.
   CUTLASS_HOST_DEVICE
   static Status can_implement(Conv3dProblemSize const &problem_size) {
 
+    auto input_channels = (IsDeconv ? problem_size.K : problem_size.C);
+
     // check alignment constraint on iterator's contiguous dimension
-    if (problem_size.C % (128/sizeof_bits<Element>::value)) {
+    if (input_channels % (128/sizeof_bits<Element>::value)) {
       return Status::kErrorInvalidProblem;
     }
 
     return Status::kSuccess;
   }
 };
```

## cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_multistage.h

```diff
@@ -135,15 +135,15 @@
     static int const kAccessesPerGroupB =
         (AsyncCopyIterationsPerStageB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
 
     // Optional staged-accumulation (e.g., tf32x3 kernels) for improved numerical
     // accuracy, where each mainloop iteration first accumulates into a temporary
     // set of freshly-cleared accumulators, which are subsequently added to the
     // final accumulator set.
-    static bool const kStagedAccumulation = arch::UseStagedAccumulation<typename Operator::MathOperator>::value;
+    static bool const kStagedAccumulation = arch::detail::UseStagedAccumulation<Operator>::value;
   };
 
  private:
 
   using WarpLoadedFragmentA = typename Operator::FragmentA;
   using WarpLoadedFragmentB = typename Operator::FragmentB;
   using WarpTransformedFragmentA = typename Operator::TransformedFragmentA;
```

## cutlass_library/source/include/cutlass/detail/layout.hpp

```diff
@@ -28,16 +28,18 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
 #include "cutlass/layout/matrix.h"
 #include "cutlass/layout/tensor.h"
+#include "cutlass/numeric_types.h"
 
 #include "cute/layout.hpp"
+#include "cute/util/type_traits.hpp"
 #include "cute/arch/copy_sm90_tma.hpp"
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass::detail {
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 // For each cutlass::layout, provides its corresponding cute stride types, 64b by default
@@ -81,47 +83,101 @@
 };
 
 // For each cutlass::layout *, provides its corresponding cute stride types, 64b by default
 // Used by pointer array and grouped gemm
 // Maps to modes [M, K, L]
 template <>
 struct TagToStrideA<layout::RowMajor *> {
-  using UnderlyingType = cute::Stride<int64_t, cute::Int<1>, int64_t>;
+  using UnderlyingType = cute::Stride<int64_t, cute::Int<1>, cute::Int<0>>;
   using type = UnderlyingType*;
   using tag = layout::RowMajor;
 };
 
 // Maps to modes [M, K, L]
 template <>
 struct TagToStrideA<layout::ColumnMajor *> {
-  using UnderlyingType = cute::Stride<cute::Int<1>, int64_t, int64_t>;
+  using UnderlyingType = cute::Stride<cute::Int<1>, int64_t, cute::Int<0>>;
   using type = UnderlyingType*;
   using tag = layout::ColumnMajor;
 };
 
 // Maps to modes [N, K, L]
 template <>
 struct TagToStrideB<layout::RowMajor *> {
-  using UnderlyingType = cute::Stride<cute::Int<1>, int64_t, int64_t>;
+  using UnderlyingType = cute::Stride<cute::Int<1>, int64_t, cute::Int<0>>;
   using type = UnderlyingType*;
   using tag = layout::RowMajor;
 };
 
 // Maps to modes [N, K, L]
 template <>
 struct TagToStrideB<layout::ColumnMajor *> {
-  using UnderlyingType = cute::Stride<int64_t, cute::Int<1>, int64_t>;
+  using UnderlyingType = cute::Stride<int64_t, cute::Int<1>, cute::Int<0>>;
   using type = UnderlyingType*;
   using tag = layout::ColumnMajor;
 };
 
 // Maps to modes [M, N, L]
 template <class LayoutTag>
 struct TagToStrideC : TagToStrideA<LayoutTag> { };
 
+// Conv: Maps to modes ((P,N), C, _0) for compatiblity with GEMM epilogues expecting a batch mode stride
+template <>
+struct TagToStrideC<cutlass::layout::TensorNWC> {
+  using type = cute::Stride<cute::Stride<int64_t, int64_t>, cute::Int<1>, cute::Int<0>>;
+};
+
+// Conv: Maps to modes (PN, C, _0) for compatiblity with GEMM epilogues expecting a batch mode stride
+template <>
+struct TagToStrideC<cutlass::layout::TensorLinearizedNWC> {
+  using type = cute::Stride<int64_t, cute::Int<1>, cute::Int<0>>;
+};
+
+// Conv: Maps to modes ((P,Q,N), C, _0) for compatiblity with GEMM epilogues expecting a batch mode stride
+template <>
+struct TagToStrideC<cutlass::layout::TensorNHWC> {
+  using type = cute::Stride<cute::Stride<int64_t, int64_t, int64_t>, cute::Int<1>, cute::Int<0>>;
+};
+
+// Conv: Maps to modes (PQN, C, _0) for compatiblity with GEMM epilogues expecting a batch mode stride
+template <>
+struct TagToStrideC<cutlass::layout::TensorLinearizedNHWC> {
+  using type = cute::Stride<int64_t, cute::Int<1>, cute::Int<0>>;
+};
+
+// Conv: Maps to modes ((P,Q,Z,N), C, _0) for compatiblity with GEMM epilogues expecting a batch mode stride
+template <>
+struct TagToStrideC<cutlass::layout::TensorNDHWC> {
+  using type = cute::Stride<cute::Stride<int64_t, int64_t, int64_t, int64_t>, cute::Int<1>, cute::Int<0>>;
+};
+
+// Conv: Maps to modes (PQZN, C, _0) for compatiblity with GEMM epilogues expecting a batch mode stride
+template <>
+struct TagToStrideC<cutlass::layout::TensorLinearizedNDHWC> {
+  using type = cute::Stride<int64_t, cute::Int<1>, cute::Int<0>>;
+};
+
+// Conv: Maps to modes (K, (C,S), _0) for compatiblity with GEMM epilogues expecting a batch mode stride
+template <>
+struct TagToStrideC<cutlass::layout::TensorKCS> {
+  using type = cute::Stride<int64_t, cute::Stride<cute::Int<1>, int64_t>, cute::Int<0>>;
+};
+
+// Conv: Maps to modes (K, (C,S,R), _0) for compatiblity with GEMM epilogues expecting a batch mode stride
+template <>
+struct TagToStrideC<cutlass::layout::TensorKCSR> {
+  using type = cute::Stride<int64_t, cute::Stride<cute::Int<1>, int64_t, int64_t>, cute::Int<0>>;
+};
+
+// Conv: Maps to modes (K, (C,S,R,T), _0) for compatiblity with GEMM epilogues expecting a batch mode stride
+template <>
+struct TagToStrideC<cutlass::layout::TensorKCSRT> {
+  using type = cute::Stride<int64_t, cute::Stride<cute::Int<1>, int64_t, int64_t, int64_t>, cute::Int<0>>;
+};
+
 // Convenience aliases
 template<class LayoutTag>
 using TagToStrideA_t = typename TagToStrideA<LayoutTag>::type;
 
 template<class LayoutTag>
 using TagToStrideB_t = typename TagToStrideB<LayoutTag>::type;
 
@@ -225,16 +281,23 @@
                   ) {
       return true;
     }
   }
   return false;
 }
 
+template <class X, class = void>
+struct RawDtype { using type = X; };
+
+template <class X>
+struct RawDtype<X,cute::void_t<typename X::raw_type>> { using type = typename X::raw_type; };
+
+
 // Inspects a TiledCopy and returns its alignment in terms of element count
-template <class GmemTiledCopy, class Element>
+template <class GmemTiledCopy, class Element, class ElementMma = Element>
 constexpr int
 get_alignment_count_from_gmem_tiled_copy() {
 
   if constexpr (cute::is_void_v<GmemTiledCopy>) {
     return 1;
   }
```

## cutlass_library/source/include/cutlass/epilogue/dispatch_policy.hpp

```diff
@@ -128,21 +128,23 @@
 //
 //////////////////////////////////////////////////////////////////////////////
 
 template<
   int StagesC_,
   int StagesD_,
   int FragmentSize_,
-  bool ReuseSmemC_
+  bool ReuseSmemC_,
+  bool DelayTmaStore_
 >
 struct Sm90TmaWarpSpecialized {
   constexpr static int StagesC = StagesC_;
   constexpr static int StagesD = StagesD_;
   constexpr static int FragmentSize = FragmentSize_;
   constexpr static bool ReuseSmemC = ReuseSmemC_;
+  constexpr static bool DelayTmaStore = DelayTmaStore_;
 };
 
 
 // DEPRECATED policies, will be removed in next release
 template<
   int StagesC_,
   int StagesD_,
```

## cutlass_library/source/include/cutlass/epilogue/collective/collective_builder.hpp

```diff
@@ -57,15 +57,15 @@
   class ElementCompute,
   class ElementC,
   class GmemLayoutTagC,
   int AlignmentC,
   class ElementD,
   class GmemLayoutTagD,
   int AlignmentD,
-  class Schedule,
+  class EpilogueScheduleType,
   class FusionOpOrCallbacks = cutlass::epilogue::fusion::LinearCombination<ElementD,ElementCompute,ElementC,ElementCompute>,
   class Enable = void
 >
 struct CollectiveBuilder {
   static_assert(cutlass::detail::dependent_false<ArchTag>,
       "Could not build a collective epilogue for given parameters.");
 };
@@ -96,15 +96,15 @@
 >
 struct CallbacksBuilder<
   DispatchPolicy,
   FusionCallbacks,
   TileShape_MNK,
   EpilogueTile_MN,
   ElementAccumulator,
-  enable_if_t<not is_base_of_v<fusion::FusionOperation, FusionCallbacks>>
+  cute::enable_if_t<not is_base_of_v<fusion::FusionOperation, FusionCallbacks>>
 > {
   using Callbacks = FusionCallbacks;
 };
 
 } // namespace detail
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/epilogue/collective/default_epilogue.hpp

```diff
@@ -35,15 +35,16 @@
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/dispatch_policy.hpp"
 #include "cutlass/epilogue/collective/detail.hpp"
 
 #include "cute/tensor.hpp"
-#include "cute/numeric/int.hpp"
+#include "cute/numeric/numeric_types.hpp"
+#include "cutlass/cuda_host_adapter.hpp"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace collective {
 
@@ -60,15 +61,15 @@
 class DefaultEpilogue {
 public:
   //
   // Type Aliases
   //
   using EpilogueSchedule = EpilogueSchedule_;
   using DispatchPolicy = EpilogueSchedule_;
-  
+
   // derived types of output thread level operator
   using ThreadEpilogueOp = ThreadEpilogueOp_;
   using ElementOutput = typename ThreadEpilogueOp::ElementOutput;
   using ElementAccumulator = typename ThreadEpilogueOp::ElementAccumulator;
   using ElementCompute = typename ThreadEpilogueOp::ElementCompute;
   using ElementScalar = ElementCompute;
   using ElementC = typename ThreadEpilogueOp::ElementC;
@@ -83,14 +84,16 @@
   using AlignmentType = typename cute::uint_bit<sizeof_bits<ElementOutput>::value * kOutputAlignment>::type;
 
   static_assert(cute::rank(StrideC{}) == 3, "StrideCD must be rank-3: [M, N, L]");
   static_assert(cute::rank(StrideD{}) == 3, "StrideCD must be rank-3: [M, N, L]");
 
   struct SharedStorage { };
 
+  using TensorStorage = SharedStorage;
+
   // Host side epilogue arguments
   struct Arguments {
     typename ThreadEpilogueOp::Params thread{};
     ElementC const* ptr_C = nullptr;
     StrideC dC{};
     ElementD* ptr_D = nullptr;
     StrideD dD{};
@@ -116,15 +119,16 @@
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     return 0;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return cutlass::Status::kSuccess;
   }
 
   template<class ProblemShape>
   CUTLASS_HOST_DEVICE static bool
   can_implement(
       [[maybe_unused]] ProblemShape const& problem_shape,
```

## cutlass_library/source/include/cutlass/epilogue/collective/default_epilogue_array.hpp

```diff
@@ -35,17 +35,18 @@
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/dispatch_policy.hpp"
 #include "cutlass/epilogue/collective/detail.hpp"
 
 #include "cute/tensor.hpp"
-#include "cute/numeric/int.hpp"
+#include "cute/numeric/numeric_types.hpp"
 #include "cutlass/trace.h"
 
+#include "cutlass/cuda_host_adapter.hpp"
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace collective {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -119,15 +120,16 @@
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     return 0;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return cutlass::Status::kSuccess;
   }
 
   template<class ProblemShape>
   CUTLASS_HOST_DEVICE static bool
   can_implement(
       [[maybe_unused]] ProblemShape const& problem_shape,
```

## cutlass_library/source/include/cutlass/epilogue/collective/detail.hpp

```diff
@@ -34,15 +34,15 @@
 #include "cutlass/cutlass.h"
 #include "cutlass/pipeline/pipeline.hpp"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/dispatch_policy.hpp"
 #include "cutlass/epilogue/dispatch_policy.hpp"
 
 #include "cute/tensor.hpp"
-#include "cute/numeric/int.hpp"
+#include "cute/numeric/numeric_types.hpp"
 #include "cute/util/type_traits.hpp"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace collective {
@@ -59,27 +59,41 @@
 
 template <class Stride>
 constexpr bool
 is_n_major() {
   return cutlass::gemm::detail::is_major<1,Stride>();
 }
 
+template <class Stride>
+constexpr bool
+is_im2col() {
+  return cute::is_same_v<Stride, cutlass::detail::TagToStrideC_t<cutlass::layout::TensorNWC>>
+      || cute::is_same_v<Stride, cutlass::detail::TagToStrideC_t<cutlass::layout::TensorNHWC>>
+      || cute::is_same_v<Stride, cutlass::detail::TagToStrideC_t<cutlass::layout::TensorNDHWC>>;
+}
+
 using cutlass::atomic_maximum;
 
 template <class T>
 static constexpr int elements_per_access_v = cutlass::sizeof_bits<uint32_t>::value / cutlass::sizeof_bits<T>::value;
 
 template <class EpilogueSchedule>
 static constexpr bool sm90_is_cooperative_v =
   cute::is_base_of_v<cutlass::epilogue::TmaWarpSpecializedCooperative, EpilogueSchedule>;
 
 template <class EpilogueSchedule>
 static constexpr bool sm90_is_warp_specialized_v =
   cute::is_base_of_v<cutlass::epilogue::TmaWarpSpecialized, EpilogueSchedule>;
 
+template <class GmemLayoutTag>
+static constexpr bool is_im2col_mode =
+  cute::is_same_v<GmemLayoutTag, cutlass::layout::TensorNWC> ||
+  cute::is_same_v<GmemLayoutTag, cutlass::layout::TensorNHWC> ||
+  cute::is_same_v<GmemLayoutTag, cutlass::layout::TensorNDHWC>;
+
 template <class T>
 struct EmptyStorage {
   CUTLASS_HOST_DEVICE
   T* data() { return nullptr; }
 };
 
 template<class EpilogueSchedule, class Stride>
@@ -101,14 +115,36 @@
 
 template <typename ThreadEpilogueOp>
 struct IsThreadEpilogueOpWithBias <ThreadEpilogueOp, cute::void_t<typename ThreadEpilogueOp::ElementBias>> { 
   static constexpr bool value = true; 
   using type = typename ThreadEpilogueOp::ElementBias; 
 };
 
+template <typename ThreadEpilogueOp, typename = void>
+struct IsThreadEpilogueOpWithPerChannelScaling {
+  static constexpr bool value = false;
+};
+
+template <typename ThreadEpilogueOp>
+struct IsThreadEpilogueOpWithPerChannelScaling <ThreadEpilogueOp, cute::enable_if_t<ThreadEpilogueOp::IsPerChannelScalingSupported>> {
+  static constexpr bool value = true;
+};
+
+template <typename ThreadEpilogueOp, typename = void>
+struct IsThreadEpilogueOpWithActivation {
+  static constexpr bool value = false;
+  using type = void;
+};
+
+template <typename ThreadEpilogueOp>
+struct IsThreadEpilogueOpWithActivation <ThreadEpilogueOp, cute::enable_if_t<ThreadEpilogueOp::IsEltActSupported>> {
+  static constexpr bool value = true;
+  using type = typename ThreadEpilogueOp::ActivationFn;
+};
+
 // Wrapper class to use operator-style epilogues in sm90 TMA warp-specialized kernels
 template <class EpilogueOp>
 class Sm90TmaWarpSpecializedAdapter : public EpilogueOp {
 public:
   using GmemTiledCopyC = void;
   using GmemTiledCopyD = void;
```

## cutlass_library/source/include/cutlass/epilogue/collective/epilogue_tensor_broadcast.hpp

```diff
@@ -51,14 +51,15 @@
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/epilogue/collective/detail.hpp"
 
 #include "cute/tensor.hpp"
+#include "cutlass/cuda_host_adapter.hpp"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace collective {
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -138,15 +139,16 @@
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     return 0;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return cutlass::Status::kSuccess;
   }
 
   template <class ProblemShape>
   CUTLASS_HOST_DEVICE static bool
   can_implement(
       [[maybe_unused]] ProblemShape const& problem_shape,
```

## cutlass_library/source/include/cutlass/epilogue/collective/sm70_epilogue_vectorized.hpp

```diff
@@ -125,15 +125,16 @@
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     return 0;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return cutlass::Status::kSuccess;
   }
 
   template <class ProblemShape>
   CUTLASS_HOST_DEVICE static bool
   can_implement(
       [[maybe_unused]] ProblemShape const& problem_shape,
```

## cutlass_library/source/include/cutlass/epilogue/collective/sm90_epilogue_tma_warpspecialized.hpp

```diff
@@ -41,28 +41,30 @@
 #include "cutlass/epilogue/thread/scale_type.h"
 #include "cutlass/epilogue/fusion/callbacks.hpp"
 #include "cutlass/epilogue/fusion/sm90_callbacks_tma_warpspecialized.hpp"
 #include "cutlass/detail/layout.hpp"
 #include "cutlass/trace.h"
 
 #include "cute/tensor.hpp"
+#include "cutlass/cuda_host_adapter.hpp"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace collective {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   int StagesC_,
   int StagesD_,
   int FragmentSize_,
   bool ReuseSmemC_,
+  bool DelayTmaStore_,
   class CtaTileMNK_,   //     (CTA_M,CTA_N,CTA_K)
   class EpilogueTile_, // (EPI_TILE_M,EPI_TILE_N)
   class ElementC_,
   class StrideC_,
   class ElementD_,
   class StrideD_,
   class FusionCallbacks_,
@@ -70,15 +72,15 @@
   class SmemLayoutAtomC_,
   class CopyOpS2R_,
   class CopyOpS2G_,
   class SmemLayoutAtomD_,
   class CopyOpR2S_
 >
 class CollectiveEpilogue<
-    Sm90TmaWarpSpecialized<StagesC_,StagesD_,FragmentSize_,ReuseSmemC_>,
+    Sm90TmaWarpSpecialized<StagesC_,StagesD_,FragmentSize_,ReuseSmemC_,DelayTmaStore_>,
     CtaTileMNK_,
     EpilogueTile_,
     ElementC_,
     StrideC_,
     ElementD_,
     StrideD_,
     FusionCallbacks_,
@@ -89,15 +91,15 @@
     SmemLayoutAtomD_,
     CopyOpR2S_
 > {
 public:
   //
   // Type Aliases
   //
-  using DispatchPolicy = Sm90TmaWarpSpecialized<StagesC_,StagesD_,FragmentSize_,ReuseSmemC_>;
+  using DispatchPolicy = Sm90TmaWarpSpecialized<StagesC_,StagesD_,FragmentSize_,ReuseSmemC_,DelayTmaStore_>;
   using CtaTileMNK = CtaTileMNK_;
   using EpilogueTile = EpilogueTile_;
   using FusionCallbacks = FusionCallbacks_;
   using ElementC = ElementC_;
   using StrideC = StrideC_;
   using ElementD = ElementD_;
   using StrideD = StrideD_;
@@ -125,18 +127,22 @@
   constexpr static bool is_destination_supported = not cute::is_void_v<ElementD>;
   using SmemElementD = cute::conditional_t<not is_destination_supported,fusion::get_element_aux_t<FusionCallbacks>, ElementD>;
   static_assert(not cute::is_void_v<SmemElementD>, "SmemElementD is void");
   using SmemElementC = cute::conditional_t<not is_source_supported,SmemElementD,ElementC>; // prevents void ref breakages
   constexpr static int StagesC = StagesC_;
   constexpr static int StagesD = StagesD_;
   constexpr static bool ReuseSmemC = ReuseSmemC_ and is_destination_supported;
+  constexpr static bool DelayTmaStore = DelayTmaStore_;
 
   constexpr static bool is_m_major_C = detail::is_m_major<StrideC>();
   constexpr static bool is_m_major_D = detail::is_m_major<StrideD>();
 
+  constexpr static bool is_im2col_C = cute::is_same_v<CopyOpG2S, SM90_TMA_LOAD_IM2COL>;
+  constexpr static bool is_im2col_D = cute::is_same_v<CopyOpS2G, SM90_TMA_STORE_IM2COL>;
+
   using SmemLayoutC = decltype(tile_to_shape(
       SmemLayoutAtomC{},
       make_shape(size<0>(EpilogueTile{}), size<1>(EpilogueTile{}), Int<StagesC>{}),
       cute::conditional_t<is_m_major_C, Step<_2,_1,_3>, Step<_1,_2,_3>>{} ));
   using SmemLayoutD = decltype(tile_to_shape(
       SmemLayoutAtomD{},
       make_shape(size<0>(EpilogueTile{}), size<1>(EpilogueTile{}), Int<ReuseSmemC ? StagesC : StagesD>{}),
@@ -209,20 +215,24 @@
 
   // Device side epilogue params
   struct Params {
     using TMA_C = decltype(make_tma_copy(
         CopyOpG2S{},
         make_tensor(make_gmem_ptr(static_cast<SmemElementC const*>(nullptr)),
             repeat_like(StrideC{}, int32_t(0)), StrideC{}),
-        SmemLayoutC{}(_,_,0)));
+        take<0,2>(SmemLayoutC{}),
+        EpilogueTile{},
+        _1{}));
     using TMA_D = decltype(make_tma_copy(
         CopyOpS2G{},
         make_tensor(make_gmem_ptr(static_cast<SmemElementD const*>(nullptr)),
             repeat_like(StrideD{}, int32_t(0)), StrideD{}),
-        SmemLayoutD{}(_,_,0)));
+        take<0,2>(SmemLayoutD{}),
+        EpilogueTile{},
+        _1{}));
 
     typename FusionCallbacks::Params thread{};
     TMA_C tma_load_c;
     TMA_D tma_store_d;
   };
 
   //
@@ -234,31 +244,28 @@
   to_underlying_arguments(
       ProblemShape const& problem_shape,
       Arguments const& args,
       [[maybe_unused]] void* workspace) {
     // Optionally append 1s until problem shape is rank-4 in case its is only rank-3 (MNK)
     auto problem_shape_MNKL = append<4>(problem_shape, 1);
     auto [M, N, K, L] = problem_shape_MNKL;
-    auto M_C =
-        size(M)
-      ;
-    auto M_D =
-        size(M)
-      ;
+    // For fprop/dgrad kernel, problem shape M is multimodal which should be linearized under tiled mode
+    auto M_C = conditional_return<is_im2col_C>(M, size(M));
+    auto M_D = conditional_return<is_im2col_D>(M, size(M));
 
-    typename Params::TMA_C tma_load_c;
+    typename Params::TMA_C tma_load_c = {};
     if constexpr (is_source_supported) {
       Tensor tensor_c = make_tensor(make_gmem_ptr(args.ptr_C), make_layout(make_shape(M_C,N,L), args.dC));
-      tma_load_c = make_tma_copy(CopyOpG2S{}, tensor_c, SmemLayoutC{}(_,_,0));
+      tma_load_c = make_tma_copy(CopyOpG2S{}, tensor_c, take<0,2>(SmemLayoutC{}), EpilogueTile{}, _1{});
     }
 
     typename Params::TMA_D tma_store_d;
     if constexpr (is_destination_supported) {
       Tensor tensor_d = make_tensor(make_gmem_ptr(args.ptr_D), make_layout(make_shape(M_D,N,L), args.dD));
-      tma_store_d = make_tma_copy(CopyOpS2G{}, tensor_d, SmemLayoutD{}(_,_,0));
+      tma_store_d = make_tma_copy(CopyOpS2G{}, tensor_d, take<0,2>(SmemLayoutD{}), EpilogueTile{}, _1{});
     }
 
     return {
       FusionCallbacks::to_underlying_arguments(problem_shape, args.thread, workspace),
       tma_load_c,
       tma_store_d
     };
@@ -268,16 +275,17 @@
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     return FusionCallbacks::get_workspace_size(problem_shape, args.thread);
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
-    return FusionCallbacks::initialize_workspace(problem_shape, args.thread, workspace, stream);
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream, 
+    CudaHostAdapter* cuda_adapter = nullptr) {
+    return FusionCallbacks::initialize_workspace(problem_shape, args.thread, workspace, stream, cuda_adapter);
   }
 
   template <class ProblemShape>
   CUTLASS_HOST_DEVICE static bool
   can_implement(
       ProblemShape const& problem_shape,
       [[maybe_unused]] Arguments const& args) {
@@ -304,18 +312,15 @@
   }
 
   template<class TileShapeMNK>
   CUTLASS_HOST_DEVICE
   static constexpr int
   get_load_pipe_increment(TileShapeMNK tile_shape_MNK) {
     // Compute number of epilogue subtiles
-    constexpr int epi_m = size<0>(tile_shape_MNK) / size<0>(EpilogueTile{});
-    constexpr int epi_n = size<1>(tile_shape_MNK) / size<1>(EpilogueTile{});
-
-    return epi_m * epi_n;
+    return size<1>(zipped_divide(make_layout(take<0,2>(tile_shape_MNK)), EpilogueTile{}));
   }
 
   template<class TileShapeMNK>
   CUTLASS_HOST_DEVICE
   static constexpr int
   get_store_pipe_increment(TileShapeMNK tile_shape_MNK) {
     return get_load_pipe_increment(tile_shape_MNK);
@@ -362,26 +367,22 @@
       int subtile_idx=-1) {
     using namespace cute;
 
     // Indexing variables
     auto [M, N, K, L] = problem_shape_mnkl;
     auto [m_coord, n_coord, k_coord, l_coord] = tile_coord_mnkl;
 
-    auto coord_shape =
-        make_coord(m_coord, n_coord, l_coord)
-      ;
+    // The tma tensor C under im2col mode only has two modes (M, N) which
+    // should be local tiled with only (m_coord, n_coord).
+    auto coord_shape = conditional_return<is_im2col_C>(
+      make_coord(m_coord, n_coord),
+      make_coord(m_coord, n_coord, l_coord));
 
     // Tile residue
-    auto m_max_coord = unwrap(cute::transform(make_seq<rank<0>(tile_shape_MNK)>{}, [&](auto i) {
-      return get<0,i>(problem_shape_mnkl) - get<0,i>(tile_shape_MNK) * get<0,i>(tile_coord_mnkl);
-    }));
-    auto n_max_coord = unwrap(cute::transform(make_seq<rank<1>(tile_shape_MNK)>{}, [&](auto i) {
-      return get<1,i>(problem_shape_mnkl) - get<1,i>(tile_shape_MNK) * get<1,i>(tile_coord_mnkl);
-    }));
-    auto residue_mn = make_coord(m_max_coord, n_max_coord);
+    auto residue_mn = make_coord(M,N);
 
     // Represent the full source tensor, slice to get the tile this CTA is currently responsible for
     Tensor mC_mn = params.tma_load_c.get_tma_tensor(make_shape(M,N,L));                                //       (M,N,L)
     Tensor mC = coalesce(mC_mn, take<0,2>(CtaTileMNK{}));
     Tensor gC = local_tile(mC, take<0,2>(CtaTileMNK{}), coord_shape);                                  // (CTA_M,CTA_N)
 
     // Apply epilogue subtile, get matching smem tensor
@@ -507,17 +508,19 @@
     auto [M, N, K, L] = problem_shape_mnkl;
     auto [m_coord, n_coord, k_coord, l_coord] = tile_coord_mnkl;
     auto mma_tile_m = tile_size<0>(tiled_mma);
     auto mma_tile_n = tile_size<1>(tiled_mma);
     auto epi_tile_m = size<0>(EpilogueTile{});
     auto epi_tile_n = size<1>(EpilogueTile{});
 
-    auto coord_shape =
-        make_coord(m_coord, n_coord, l_coord)
-      ;
+    // The tma tensor D under im2col mode only has two modes (M, N) which
+    // should be local tiled with only (m_coord, n_coord).
+    auto coord_shape = conditional_return<is_im2col_D>( 
+        make_coord(m_coord, n_coord),
+        make_coord(m_coord, n_coord, l_coord));
 
     // Represent the full output tensor, slice to get the tile this CTA is responsible for
     Tensor mD_mn = params.tma_store_d.get_tma_tensor(make_shape(M,N,L));                               //       (M,N,L)
     Tensor mD = coalesce(mD_mn, take<0,2>(CtaTileMNK{}));
     Tensor gD = local_tile(mD, take<0,2>(CtaTileMNK{}), coord_shape);                                  // (CTA_M,CTA_N)
 
     // Apply epilogue subtiling
@@ -568,37 +571,30 @@
     ThrCopy thread_s2r = tiled_s2r.get_slice(thread_idx);
     Tensor tSR_sC        = thread_s2r.partition_S(sC_epi);                                  // (S2R,S2R_M,S2R_N,PIPE_C)
     Layout tSR_rC_layout = thread_s2r.retile_D(tRS_rD).layout();                            // (S2R,S2R_M,S2R_N)
 
     // Allocate C registers
     // If C smem load is a non-vectorized dst(i) = src(i) then we can allocate C registers directly in the compute type
     // to eliminate some redundant pack+unpack instruction sequences for sub-word types
-    constexpr bool IsDirectS2R = cute::is_same_v<CopyOpS2R,DefaultCopy>
+    constexpr bool IsDirectS2R = cute::is_same_v<CopyOpS2R, AutoVectorizingCopyWithAssumedAlignment<128>>
                                 && decltype(max_common_vector(tSR_rC_layout, tSR_sC.layout()))::value <= 1;
     using RegisterElementC = cute::conditional_t<IsDirectS2R, ElementCompute, SmemElementC>;
     Tensor tRS_rC = make_tensor<RegisterElementC>(tRS_rD_layout);                                  // (R2S,R2S_M,R2S_N)
     Tensor tSR_rC = thread_s2r.retile_D(tRS_rC);                                                   // (S2R,S2R_M,S2R_N)
 
     // thread(b)lock-partition for (s)mem to (g)mem copy (bSG_)
     ThrCopy thrblk_s2g = params.tma_store_d.get_slice(Int<0>{});
     Tensor bSG_sD = thrblk_s2g.partition_S(sD_epi);                                    // (S2G,S2G_M,S2G_N,PIPE_D)
     Tensor bSG_gD = thrblk_s2g.partition_D(gD_epi);                                    // (S2G,S2G_M,S2G_N,EPI_M,EPI_N)
 
-    // Coordinate tensors and residue for tile quantization
-    auto m_max_coord = unwrap(cute::transform(make_seq<rank<0>(CtaTileMNK{})>{}, [&](auto i) {
-      auto c_m = get<0,i>(problem_shape_mnkl) - get<0,i>(CtaTileMNK{}) * get<0,i>(tile_coord_mnkl);
-      return cute::max(0, c_m);
-    }));
-    auto n_max_coord = unwrap(cute::transform(make_seq<rank<1>(CtaTileMNK{})>{}, [&](auto i) {
-      auto c_n = get<1,i>(problem_shape_mnkl) - get<1,i>(CtaTileMNK{}) * get<1,i>(tile_coord_mnkl);
-      return cute::max(0, c_n);
-    }));
-    auto residue_mn = make_coord(m_max_coord, n_max_coord);
-    Tensor cD = make_identity_tensor(take<0,2>(CtaTileMNK{}));
+    // OOB predication for tile quantization "residue"
+    Tensor mD_crd = make_identity_tensor(make_shape(M,N));
+    Tensor cD = local_tile(mD_crd, take<0,2>(CtaTileMNK{}), make_coord(m_coord, n_coord));
     Tensor tRS_cD = thread_r2s.partition_S(flat_divide(cD, EpilogueTile{}));
+    auto residue_mn = make_coord(M,N);
 
     CUTE_STATIC_ASSERT(mma_tile_m == epi_tile_m, "EPI_TILE_M must equal MMA_TILE_M");
     CUTE_STATIC_ASSERT(mma_tile_n % epi_tile_n == 0, "EPI_TILE_N must divide MMA_TILE_N");
 
     // Get the fusion callbacks for the consumer store warps
     constexpr bool RefSrc = true; // Register tensors reference R2S copy src layout
     auto cst_args = cutlass::epilogue::fusion::detail::ConsumerStoreArgs{
@@ -630,26 +626,73 @@
     // store_pipe_producer_state tracks the acquire and load_pipe_consumer_state tracks the release, in circular buffer fashion.
     LoadPipelineState load_wait_state = load_pipe_consumer_state;
     if constexpr (ReuseSmemC) {
       load_wait_state = store_pipe_producer_state;
       load_wait_state.phase_ ^= 1;
     }
 
+    // We can delay issue of TMA store by one iteration to achieve better interleaving of non-TMA instructions
+    // Sync requirements of smem reuse may preclude this optimization
+    // Delayed stores cause delayed stage releases which causes deadlock when StagesC == StagesD
+    int epi_m_prev = 0, epi_n_prev = 0;
+    static_assert(not (DelayTmaStore and ReuseSmemC and StagesC == StagesD), "This TMA epilogue configuration will deadlock");
+
+    // The TMA store sequence for one subtile iteration
+    auto tma_store_fn = [&] (int epi_m, int epi_n) {
+      // Write the tile from smem to gmem with TMA
+      cutlass::arch::fence_view_async_shared(); // ensure smem writes are visible to TMA
+      synchronize(); // ensure all threads have issued their async fence
+      if constexpr (is_destination_supported) {
+        if (issue_tma_store) {
+          copy(params.tma_store_d, bSG_sD(_,_,_,store_pipe_producer_state.index()), bSG_gD(_,_,_,epi_m,epi_n));
+        }
+      }
+
+      // Post async fence, pre TMA commit callback entry point
+      cst_callbacks.tma_store(epi_m, epi_n, store_pipe_producer_state.count(), issue_tma_store);
+
+      // Commit the TMA stores for this stage
+      if (issue_tma_store) {
+        store_pipeline.producer_commit(store_pipe_producer_state);
+      }
+      ++store_pipe_producer_state;
+      ++issued_stores;
+
+      // Wait for the next smem buffer to be available
+      if (issue_tma_store) {
+        store_pipeline.producer_acquire(store_pipe_producer_state);
+      }
+      synchronize();
+
+      if constexpr (ReuseSmemC) {
+        // producer_acquire returns when at most StagesD-1 committed stores are pending
+        bool store_finished = issued_stores > StorePipeline::UnacquiredStages;
+        // Let dma warp know earliest smem buffer is consumed and empty after StagesD producer commits
+        if (store_finished) {
+          if (is_producer_load_needed) {
+            load_pipeline.consumer_release(load_pipe_consumer_state);
+          }
+          ++load_pipe_consumer_state;
+        }
+      }
+    };
+
     //
     // BEGIN EPILOGUE
     //
 
     // Pre-loop fusion callback entry point
     cst_callbacks.begin();
 
     // For each output tile
     CUTLASS_PRAGMA_UNROLL
     for (int epi_n = 0; epi_n < size<3>(gD_epi); ++epi_n) {
       CUTLASS_PRAGMA_UNROLL
       for (int epi_m = 0; epi_m < size<2>(gD_epi); ++epi_m) {
+        bool is_first_iteration = epi_m == 0 && epi_n == 0;
         bool is_last_iteration = epi_m == size<2>(gD_epi)-1 && epi_n == size<3>(gD_epi)-1;
 
         if (subtile_idx != -1 && (epi_n * static_cast<int>(size<2>(gD_epi)) + epi_m) != subtile_idx) {
           continue;
         }
         // The current tile in accumulator
         int mma_m = epi_m;
@@ -676,86 +719,57 @@
             load_pipeline.consumer_release(load_pipe_consumer_state);
             ++load_pipe_consumer_state;
           }
           ++load_wait_state;
         }
 
         // Vectorized fragment loop with visitor callback entry point
-        int r2s_v = epi_n * size(tRS_rD_frg);
+        int epi_n_in_mma = epi_n % (mma_tile_n / epi_tile_n);
+        int r2s_v = epi_n_in_mma * size(tRS_rD_frg);
         CUTLASS_PRAGMA_UNROLL
         for (int epi_v = 0; epi_v < size(tRS_rD_frg); ++epi_v) {
           tRS_rD_frg(epi_v) = cst_callbacks.visit(tRS_rAcc_frg_mn(r2s_v + epi_v), epi_v, epi_m, epi_n);
         }
 
-        // Copy tile from register to smem
-        if constexpr (is_destination_supported) {
-          copy(tiled_r2s, tRS_rD, tRS_sD(_,_,_,store_pipe_producer_state.index()));
-        }
-
-        // Post visit, pre async fence callback entry point
-        constexpr bool issue_smem_store = true; // No smem store predication
-        cst_callbacks.postvisit(epi_m, epi_n, store_pipe_producer_state.count(), issue_smem_store);
-
-        // Write the tile from smem to gmem with TMA
-        cutlass::arch::fence_view_async_shared(); // ensure smem writes are visible to TMA
-        synchronize(); // ensure all threads have issued their async fence
-        if constexpr (is_destination_supported) {
-          if (issue_tma_store) {
-            copy(params.tma_store_d, bSG_sD(_,_,_,store_pipe_producer_state.index()), bSG_gD(_,_,_,epi_m,epi_n));
+        // The latest we can delay the TMA store is right before the smem store of the next iteration
+        // since the current TMA store needs to be committed before we can acquire the next smem buffer
+        if constexpr (DelayTmaStore) {
+          // Issue TMA stores for the previous subtile
+          if (not is_first_iteration and subtile_idx == -1) {
+            tma_store_fn(epi_m_prev, epi_n_prev);
           }
+          epi_m_prev = epi_m;
+          epi_n_prev = epi_n;
         }
 
-        // Post async fence, pre TMA commit callback entry point
-        cst_callbacks.step(epi_m, epi_n, store_pipe_producer_state.count(), issue_tma_store);
+        // Smem reduction callback entry point using current store buffer for workspace
+        cst_callbacks.reduce(sD_epi(_,_,store_pipe_producer_state.index()),
+                              synchronize, epi_m, epi_n, is_last_iteration);
 
-        // Commit the TMA stores for this stage
-        if (issue_tma_store) {
-          store_pipeline.producer_commit(store_pipe_producer_state);
-        }
-        ++store_pipe_producer_state;
-        ++issued_stores;
-
-        // Wait for the next smem buffer to be available
-        if (issue_tma_store) {
-          store_pipeline.producer_acquire(store_pipe_producer_state);
+        // Copy tile from register to smem
+        if constexpr (is_destination_supported) {
+          copy(tiled_r2s, tRS_rD, tRS_sD(_,_,_,store_pipe_producer_state.index()));
         }
-        synchronize();
 
-        if constexpr (ReuseSmemC) {
-          // producer_acquire returns when at most StagesD-1 committed stores are pending
-          bool store_finished = issued_stores > StorePipeline::UnacquiredStages;
-
-          // Free an smem buffer for reduction if necessary
-          if (cst_callbacks.is_reduction_buffer_needed(epi_m, epi_n, is_last_iteration) && not store_finished) {
-            if (issue_tma_store) {
-              store_pipeline.producer_tail(store_pipe_producer_state); // wait for all TMA stores to finish
-            }
-            synchronize();
-          }
+        // Post reduction, pre TMA store callback entry point
+        constexpr bool issue_smem_store = true; // No smem store predication
+        cst_callbacks.postreduce(epi_m, epi_n, store_pipe_producer_state.count(), issue_smem_store);
 
-          // Smem reduction callback entry point using least recently acquired load buffer for workspace
-          cst_callbacks.reduce(sC_epi(_,_,load_pipe_consumer_state.index()),
-                               synchronize, epi_m, epi_n, is_last_iteration);
-
-          // Let dma warp know earliest smem buffer is consumed and empty after StagesD producer commits
-          if (store_finished) {
-            if (is_producer_load_needed) {
-              load_pipeline.consumer_release(load_pipe_consumer_state);
-            }
-            ++load_pipe_consumer_state;
-          }
-        }
-        else {
-          // Smem reduction callback entry point using most recently acquired store buffer for workspace
-          cst_callbacks.reduce(sD_epi(_,_,store_pipe_producer_state.index()),
-                               synchronize, epi_m, epi_n, is_last_iteration);
+        if constexpr (not DelayTmaStore) {
+          // Issue TMA stores for this subtile
+          tma_store_fn(epi_m, epi_n);
         }
       } // for epi_m
     } // for epi_n
 
+    if constexpr (DelayTmaStore) {
+      // Issue TMA stores for the last subtile
+      tma_store_fn(epi_m_prev, epi_n_prev);
+    }
+
     // Post-loop fusion callback entry point
     cst_callbacks.end();
 
     return cute::make_tuple(load_pipe_consumer_state, store_pipe_producer_state);
   }
 
   CUTLASS_DEVICE auto
```

## cutlass_library/source/include/cutlass/epilogue/collective/sm90_epilogue_tma_warpspecialized_bias_elementwise.hpp

```diff
@@ -61,15 +61,15 @@
   class CopyOpS2R_,
   class CopyOpS2G_,
   class SmemLayoutAtomD_,
   class CopyOpR2S_
 >
 class Sm90EpilogueTmaWarpSpecializedBiasElementwise
   : public CollectiveEpilogue<
-      Sm90TmaWarpSpecialized<StagesC_, StagesD_, FragmentSize_, false>,
+      Sm90TmaWarpSpecialized<StagesC_, StagesD_, FragmentSize_, false, false>,
       BlockTileShape_,
       EpilogueTileShape_,
       ElementC_,
       StrideC_,
       ElementD_,
       StrideD_,
       FusionCallbacks_,
@@ -79,15 +79,15 @@
       CopyOpS2G_,
       SmemLayoutAtomD_,
       CopyOpR2S_
 > {
 private:
   using Impl =
     CollectiveEpilogue<
-      Sm90TmaWarpSpecialized<StagesC_, StagesD_, FragmentSize_, false>,
+      Sm90TmaWarpSpecialized<StagesC_, StagesD_, FragmentSize_, false, false>,
       BlockTileShape_,
       EpilogueTileShape_,
       ElementC_,
       StrideC_,
       ElementD_,
       StrideD_,
       FusionCallbacks_,
@@ -107,25 +107,25 @@
   // Constructor inheritance
   using Impl::Impl;
 
   // Host side epilogue arguments
   struct [[deprecated("use Sm90TmaWarpSpecialized Arguments instead")]]
   Arguments {
     struct ThreadArgs {
-      ElementCompute alpha;
-      ElementCompute beta;
-      ElementCompute const *alpha_ptr;
-      ElementCompute const *beta_ptr;
+      ElementCompute alpha{1};
+      ElementCompute beta{0};
+      ElementCompute const *alpha_ptr{nullptr};
+      ElementCompute const *beta_ptr{nullptr};
     } thread;
-    ElementC_ const* ptr_C;
-    StrideC_ dC;
-    ElementD_* ptr_D;
-    StrideD_ dD;
-    ElementBias const* ptr_Bias = nullptr;
-    ElementT* ptr_T = nullptr;
+    ElementC_ const* ptr_C{nullptr};
+    StrideC_ dC{};
+    ElementD_* ptr_D{nullptr};
+    StrideD_ dD{};
+    ElementBias const* ptr_Bias{nullptr};
+    ElementT* ptr_T{nullptr};
 
     CUTLASS_HOST_DEVICE
     operator typename Impl::Arguments() const {
       typename Impl::Arguments arguments;
       arguments.thread.alpha = thread.alpha;
       arguments.thread.beta = thread.beta;
       arguments.thread.alpha_ptr = thread.alpha_ptr;
```

## cutlass_library/source/include/cutlass/epilogue/collective/builders/sm90_builder.inl

```diff
@@ -63,19 +63,22 @@
 template<class TileShapeMNK, class EpilogueTileMN, class ElementC, class ElementD, class Schedule>
 constexpr auto
 sm90_get_tma_dispatch_policy() {
   using namespace cute;
 
   constexpr int EpiTiles = size(shape_div(take<0,2>(TileShapeMNK{}), EpilogueTileMN{}));
   constexpr int FragmentSize = size(EpilogueTileMN{}) / (detail::sm90_is_cooperative_v<Schedule> ? 256 : 128);
-  constexpr int ReuseSmemC = (sizeof_bits_v<ElementC> == sizeof_bits_v<ElementD>) && (sizeof_bits_v<ElementD> > 8);
-  constexpr int StagesD = 2;
-  constexpr int StagesC = ReuseSmemC ? cute::max(EpiTiles, StagesD + 1) : EpiTiles;
+  // 8b residuals load fast and consume little smem, so the perf cost of waiting on stores to finish outweighs the cost of extra allocation
+  constexpr bool ReuseSmem = (sizeof_bits_v<ElementC> == sizeof_bits_v<ElementD>) && (sizeof_bits_v<ElementD> > 8);
+  constexpr bool DelayTmaStore = is_void_v<ElementC>; // TMA store delay performs worse with residual loads
+  constexpr int StagesD = cute::min(EpiTiles, 2);
+  constexpr int StagesC = ReuseSmem ? cute::max(cute::min(EpiTiles, 4), StagesD+1)
+                                    : cute::min(EpiTiles, 4);
 
-  return Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>{};
+  return Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmem, DelayTmaStore>{};
 }
 
 // Returns the smem layout atom to be used for C or D matrix
 template<class GmemStrideType, class Element, class EpilogueTile_MN>
 constexpr auto
 sm90_get_epilogue_smem_swizzle_layout_atom() {
   using namespace cute;
@@ -98,37 +101,50 @@
 }
 
 // Attempts to compute a reasonable epilogue tile based on block tile shape or allows the user to provide one.
 template <class ElementD, class EpilogueTileType, class Schedule, class TileShape_MNK>
 constexpr auto
 sm90_compute_tile_shape_or_override() {
   if constexpr (cute::is_same_v<EpilogueTileType, EpilogueTileAuto>) {
-
-    if constexpr (detail::sm90_is_cooperative_v<Schedule>) {
-      using N_tile = decltype(cute::min(_32{}, get<1>(TileShape_MNK{})));
-      if constexpr (size<0>(TileShape_MNK{}) >= 128) {
-        return Shape<_128, N_tile>{};
-      }
-      else {
-        return Shape<_64, N_tile>{};
+    auto epi_tile = [&] () {
+      if constexpr (detail::sm90_is_cooperative_v<Schedule>) {
+        auto tile_m = cute::min(_128{}, size<0>(TileShape_MNK{}));
+        auto tile_n = cute::min(_32{}, size<1>(TileShape_MNK{}));
+        return make_shape(tile_m, tile_n);
       }
-    }
-    else if constexpr (detail::sm90_is_warp_specialized_v<Schedule>) {
-      if constexpr (sizeof_bits_v<ElementD> == 8) {
-        using N_tile = decltype(cute::min(_64{}, get<1>(TileShape_MNK{})));
-        return Shape<_64, N_tile>{};
+      else if constexpr (detail::sm90_is_warp_specialized_v<Schedule>) {
+        constexpr int N_perf = sizeof_bits_v<ElementD> == 8 ? 64 : 32;
+        auto tile_m = cute::min(_64{}, size<0>(TileShape_MNK{}));
+        auto tile_n = cute::min(Int<N_perf>{}, size<1>(TileShape_MNK{}));
+        return make_shape(tile_m, tile_n);
       }
       else {
-        using N_tile = decltype(cute::min(_32{}, get<1>(TileShape_MNK{})));
-        return Shape<_64,N_tile>{};
+        static_assert(cutlass::detail::dependent_false<Schedule>, "Unsupported schedule.");
       }
-    }
-    else {
-      static_assert(cutlass::detail::dependent_false<Schedule>, "Unsupported schedule.");
-    }
+    }();
+
+    return cute::transform(epi_tile, seq<0,1>{},
+      [] (auto epi_tiler, auto I) {
+        auto cta_tiler = make_layout(get<I>(TileShape_MNK{}));
+        // This is a multimodal CTA tiler, transform before returning
+        if constexpr (depth(cta_tiler) > 0) {
+          // This is an implicit multimodal tiler, match profile and return
+          if constexpr (tuple_size_v<decltype(shape(cta_tiler))> == 1) {
+            return make_tile(epi_tiler);
+          }
+          // This is an explicit multimodal tiler, compose out epi tiler
+          else {
+            return composition(cta_tiler, epi_tiler);
+          }
+        }
+        // This is a flat CTA tiler, no need for transformation
+        else {
+          return epi_tiler;
+        }
+      });
   }
   else if constexpr (cute::is_tuple<EpilogueTileType>::value) {
     EpilogueTileType epi_tile;
     constexpr int M = size<0>(shape(epi_tile));
     constexpr int N = size<1>(shape(epi_tile));
 
     static_assert(!is_layout<EpilogueTileType>::value, "EpilogueTile must be a cute::Tile or cute::Shape");
@@ -174,75 +190,77 @@
     return SM75_U16x8_LDSM_T{};
   }
   else if constexpr (cute::is_same_v<SmemStoreOp, SM90_U32x4_STSM_N>) {
     return SM75_U32x4_LDSM_N{};
   }
   else {
     // auto-vectorizing load
-    return AutoVectorizingCopyWithAssumedAlignment{};
+    return AutoVectorizingCopyWithAssumedAlignment<128>{};
   }
 }
 
 // callbacks builder with TMA aux out
 template <
   int StagesC,
   int StagesD,
   int FragmentSize,
   bool ReuseSmemC,
+  bool DelayTmaStore,
   class FusionOp,
   class TileShape_MNK,
   class EpilogueTile_MN,
   class ElementAccumulator
 >
 struct CallbacksBuilder<
-  Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+  Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
   FusionOp,
   TileShape_MNK,
   EpilogueTile_MN,
   ElementAccumulator,
-  enable_if_t<(FusionOp::IsAuxOutSupported ^ FusionOp::IsAuxInSupported) // only one aux tensor
-              && not is_subbyte_v<typename FusionOp::ElementAux>>
+  cute::enable_if_t<(FusionOp::IsAuxOutSupported ^ FusionOp::IsAuxInSupported) // only one aux tensor
+              && not cute::is_subbyte_v<typename FusionOp::ElementAux>>
 > {
   using GmemStrideTypeAux = gemm::TagToStrideC_t<typename FusionOp::GmemLayoutTagAux>;
   using SmemLayoutAtomAux = decltype(detail::sm90_get_epilogue_smem_swizzle_layout_atom<
     GmemStrideTypeAux, typename FusionOp::ElementAux, EpilogueTile_MN>());
   using CopyOpR2S = decltype(detail::sm90_get_smem_store_op_for_accumulator<
     GmemStrideTypeAux, typename FusionOp::ElementAux>());
   using CopyOpS2R = decltype(detail::sm90_get_smem_load_op_for_source<
     GmemStrideTypeAux, typename FusionOp::ElementAux>());
-  using SmemCopyOpAux = conditional_t<FusionOp::IsAuxOutSupported, CopyOpR2S, CopyOpS2R>;
+  using SmemCopyOpAux = cute::conditional_t<FusionOp::IsAuxOutSupported, CopyOpR2S, CopyOpS2R>;
 
   using Callbacks = fusion::FusionCallbacks<
-    Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+    Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
     FusionOp, TileShape_MNK, EpilogueTile_MN,
     SmemLayoutAtomAux, SmemCopyOpAux
   >;
 };
 
 template <
   int StagesC,
   int StagesD,
   int FragmentSize,
   bool ReuseSmemC,
+  bool DelayTmaStore,
   class FusionOp,
   class TileShape_MNK,
   class EpilogueTile_MN,
   class ElementAccumulator
 >
 struct CallbacksBuilder<
-  Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+  Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
   FusionOp,
   TileShape_MNK,
   EpilogueTile_MN,
   ElementAccumulator,
-  enable_if_t<(FusionOp::IsAuxOutSupported ^ FusionOp::IsAuxInSupported) // only one aux tensor
+  cute::enable_if_t<(FusionOp::IsAuxOutSupported ^ FusionOp::IsAuxInSupported) // only one aux tensor
               && sizeof_bits_v<typename FusionOp::ElementAux> == 1>
 > {
   using Callbacks = fusion::FusionCallbacks<
-    Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+    Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
     FusionOp, TileShape_MNK, EpilogueTile_MN,
     Layout<_1,_0>, DefaultCopy // aux bit tensor doesn't use smem
   >;
 };
 
 // Helper for building TMA warp-specialized collective epilogues, specialized by
 // the fusion operation performed and the dispatch policy to use.
@@ -268,20 +286,22 @@
   // Passing void C disables source load + smem allocation
   using ElementC = cute::conditional_t<cute::is_void_v<ElementC_>,ElementD,ElementC_>; // prevents void ref breakages
   using GmemLayoutTagC = cute::conditional_t<cute::is_void_v<ElementC_>,GmemLayoutTagD,GmemLayoutTagC_>;
 
   using GmemStrideTypeC = cutlass::detail::TagToStrideC_t<GmemLayoutTagC>;
   using GmemStrideTypeD = cutlass::detail::TagToStrideC_t<GmemLayoutTagD>;
 
-  using CopyOpS2G =
+  using CopyOpS2G = cute::conditional_t<detail::is_im2col_mode<GmemLayoutTagD>,
+      SM90_TMA_STORE_IM2COL,
       SM90_TMA_STORE
-    ;
-  using CopyOpG2S =
+    >;
+  using CopyOpG2S = cute::conditional_t<detail::is_im2col_mode<GmemLayoutTagC>,
+      SM90_TMA_LOAD_IM2COL,
       SM90_TMA_LOAD
-    ;
+    >;
 
   // TMA builder allows for passing callbacks directly, which is either a fusion::FusionCallbacks
   // instance or a direct visitor implementation, e.g. fusion::Sm90LinearCombination
   using FusionCallbacks = 
     typename CallbacksBuilder<
       DispatchPolicy,
       FusionOpOrCallbacks,
@@ -693,15 +713,16 @@
   // defined as decltype of a detail::sm90_get_tma_dispatch_policy call.
   // Instead, we paste in the contents of that function.  A natural refactoring
   // would be to create a type alias in the detail namespace.
   using DispatchPolicy = Sm90TmaWarpSpecialized<
     /* StagesC = */ size(shape_div(take<0, 2>(TileShape_MNK{}), EpilogueTile_MN{})),
     /* StagesD = */ 2,
     /* FragmentSize = */ size(EpilogueTile_MN{}) / (detail::sm90_is_cooperative_v<Schedule> ? 256 : 128),
-    /* ReuseSmemC = */ sizeof_bits_v<ElementC_> == sizeof_bits_v<ElementD>
+    /* ReuseSmemC = */ sizeof_bits_v<ElementC_> == sizeof_bits_v<ElementD>,
+    false
   >;
 
   using GmemStrideTypeAux = gemm::TagToStrideC_t<GmemLayoutTagD>;
   using SmemLayoutAtomAux = decltype(detail::sm90_get_epilogue_smem_swizzle_layout_atom<
     GmemStrideTypeAux, typename Schedule::ElementT, EpilogueTile_MN>());
   using SmemCopyOpAux = decltype(detail::sm90_get_smem_store_op_for_accumulator<
     GmemStrideTypeAux, typename Schedule::ElementT>());
```

## cutlass_library/source/include/cutlass/epilogue/fusion/operations.hpp

```diff
@@ -118,14 +118,15 @@
 >
 struct LinCombEltAct
     : LinearCombination<ElementOutput_, ElementCompute_, ElementSource_, ElementScalar_, RoundStyle_> {
   using ActivationFn = ActivationFn_<ElementCompute_>;
   static constexpr bool IsEltActSupported = true;
 };
 
+
 // D = alpha * acc + beta * C + per-row bias
 template<
   class ElementOutput_,
   class ElementCompute_,
   class ElementBias_ = ElementOutput_,
   class ElementSource_ = ElementOutput_,
   class ElementScalar_ = ElementCompute_,
```

## cutlass_library/source/include/cutlass/epilogue/fusion/sm90_callbacks_tma_warpspecialized.hpp

```diff
@@ -57,23 +57,24 @@
 
 // D = alpha * acc
 template <
   int StagesC,
   int StagesD,
   int FragmentSize,
   bool ReuseSmemC,
+  bool DelayTmaStore,
   class ElementOutput,
   class ElementCompute,
   class ElementScalar,
   FloatRoundStyle RoundStyle,
   class CtaTileShapeMNK,
   class EpilogueTile
 >
 struct FusionCallbacks<
-    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
     fusion::ScaledAcc<ElementOutput, ElementCompute, ElementScalar, RoundStyle>,
     CtaTileShapeMNK,
     EpilogueTile
 > : Sm90EVT<Sm90Compute<multiplies, ElementOutput, ElementCompute, RoundStyle>,
       Sm90ScalarBroadcast<ElementScalar>,
       Sm90AccFetch
     > {
@@ -128,24 +129,25 @@
   >;
 
 template <
   int StagesC,
   int StagesD,
   int FragmentSize,
   bool ReuseSmemC,
+  bool DelayTmaStore,
   class ElementOutput,
   class ElementCompute,
   class ElementSource,
   class ElementScalar,
   FloatRoundStyle RoundStyle,
   class CtaTileShapeMNK,
   class EpilogueTile
 >
 struct FusionCallbacks<
-    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
     fusion::LinearCombination<ElementOutput, ElementCompute, ElementSource, ElementScalar, RoundStyle>,
     CtaTileShapeMNK,
     EpilogueTile
 > : Sm90LinearCombination<typename cutlass::detail::get_unpacked_element_type<ElementOutput>::type, ElementCompute, ElementSource, ElementScalar, RoundStyle> {
 
   using Impl = Sm90LinearCombination<typename cutlass::detail::get_unpacked_element_type<ElementOutput>::type, ElementCompute, ElementSource, ElementScalar, RoundStyle>;
   using Operation = fusion::LinearCombination<ElementOutput, ElementCompute, ElementSource, ElementScalar, RoundStyle>;
@@ -192,25 +194,26 @@
   >;
 
 template <
   int StagesC,
   int StagesD,
   int FragmentSize,
   bool ReuseSmemC,
+  bool DelayTmaStore,
   template <class> class ActivationFn,
   class ElementOutput,
   class ElementCompute,
   class ElementSource,
   class ElementScalar,
   FloatRoundStyle RoundStyle,
   class CtaTileShapeMNK,
   class EpilogueTile
 >
 struct FusionCallbacks<
-    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
     fusion::LinCombEltAct<ActivationFn, ElementOutput, ElementCompute, ElementSource, ElementScalar, RoundStyle>,
     CtaTileShapeMNK,
     EpilogueTile
 > : Sm90LinCombEltAct<ActivationFn, ElementOutput, ElementCompute, ElementSource, ElementScalar, RoundStyle> {
 
   using Impl = Sm90LinCombEltAct<ActivationFn, typename cutlass::detail::get_unpacked_element_type<ElementOutput>::type, ElementCompute, ElementSource, ElementScalar, RoundStyle>;
   using Operation = fusion::LinCombEltAct<ActivationFn, ElementOutput, ElementCompute, ElementSource, ElementScalar, RoundStyle>;
@@ -271,26 +274,27 @@
   >;
 
 template <
   int StagesC,
   int StagesD,
   int FragmentSize,
   bool ReuseSmemC,
+  bool DelayTmaStore,
   class ElementOutput,
   class ElementCompute,
   class ElementBias,
   class ElementSource,
   class ElementScalar,
   int AlignmentBias,
   FloatRoundStyle RoundStyle,
   class CtaTileShapeMNK,
   class EpilogueTile
 >
 struct FusionCallbacks<
-    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
     fusion::LinCombPerRowBias<ElementOutput, ElementCompute, ElementBias, ElementSource, ElementScalar, AlignmentBias, RoundStyle>,
     CtaTileShapeMNK,
     EpilogueTile
 > : Sm90LinCombPerRowBias<
       CtaTileShapeMNK, ElementOutput, ElementCompute, ElementBias, ElementSource, ElementScalar, AlignmentBias, RoundStyle> {
   using Impl = Sm90LinCombPerRowBias<
     CtaTileShapeMNK, ElementOutput, ElementCompute, ElementBias, ElementSource, ElementScalar, AlignmentBias, RoundStyle>;
@@ -347,27 +351,28 @@
   >;
 
 template <
   int StagesC,
   int StagesD,
   int FragmentSize,
   bool ReuseSmemC,
+  bool DelayTmaStore,
   template <class> class ActivationFn,
   class ElementOutput,
   class ElementCompute,
   class ElementBias,
   class ElementSource,
   class ElementScalar,
   int AlignmentBias,
   FloatRoundStyle RoundStyle,
   class CtaTileShapeMNK,
   class EpilogueTile
 >
 struct FusionCallbacks<
-    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
     fusion::LinCombPerRowBiasEltAct<
       ActivationFn, ElementOutput, ElementCompute, ElementBias, ElementSource, ElementScalar, AlignmentBias, RoundStyle
     >,
     CtaTileShapeMNK,
     EpilogueTile
 > : Sm90LinCombPerRowBiasEltAct<
       CtaTileShapeMNK, ActivationFn, ElementOutput, ElementCompute, ElementBias, ElementSource, ElementScalar, AlignmentBias, RoundStyle
@@ -448,14 +453,15 @@
   >;
 
 template <
   int StagesC,
   int StagesD,
   int FragmentSize,
   bool ReuseSmemC,
+  bool DelayTmaStore,
   class GmemLayoutTagAux,
   template <class> class ActivationFn,
   class ElementOutput,
   class ElementCompute,
   class ElementAux,
   class ElementBias,
   class ElementSource,
@@ -465,15 +471,15 @@
   FloatRoundStyle RoundStyle,
   class CtaTileShapeMNK,
   class EpilogueTile,
   class SmemLayoutAtom,
   class CopyOpR2S
 >
 struct FusionCallbacks<
-    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
     fusion::LinCombPerRowBiasEltActAux<
       GmemLayoutTagAux, ActivationFn, ElementOutput, ElementCompute,
       ElementAux, ElementBias, ElementSource, ElementScalar, AlignmentAux, AlignmentBias, RoundStyle
     >,
     CtaTileShapeMNK,
     EpilogueTile,
     SmemLayoutAtom,
@@ -549,18 +555,18 @@
   class ElementScalar = ElementCompute,
   int AlignmentBias = 128 / sizeof_bits_v<ElementBias>,
   int AlignmentScalar = 128 / sizeof_bits_v<ElementScalar>,
   FloatRoundStyle RoundStyle = FloatRoundStyle::round_to_nearest
 >
 using Sm90PerRowLinCombPerRowBias =
   Sm90EVT<Sm90Compute<homogeneous_multiply_add, ElementOutput, ElementCompute, RoundStyle>, // beta * C + (alpha * acc + bias)
-    Sm90ColBroadcast<0, CtaTileShapeMNK, ElementScalar, Stride<_1,_0,_0>, AlignmentScalar>, // beta
+    Sm90ColBroadcast<0, CtaTileShapeMNK, ElementScalar, Stride<_1,_0,int>, AlignmentScalar>, // beta
     Sm90SrcFetch<ElementSource>, // C
     Sm90EVT<Sm90Compute<homogeneous_multiply_add, ElementCompute, ElementCompute, RoundStyle>, // alpha * acc + bias
-      Sm90ColBroadcast<0, CtaTileShapeMNK, ElementScalar, Stride<_1,_0,_0>, AlignmentScalar>, // alpha
+      Sm90ColBroadcast<0, CtaTileShapeMNK, ElementScalar, Stride<_1,_0,int>, AlignmentScalar>, // alpha
       Sm90AccFetch, // acc
       Sm90ColBroadcast<0, CtaTileShapeMNK, ElementBias, Stride<_1,_0,int>, AlignmentBias> // bias
     >
   >;
 
 // D = activation(per-row alpha * acc + per-row beta * C + per-row bias)
 template<
@@ -582,28 +588,29 @@
   >;
 
 template <
   int StagesC,
   int StagesD,
   int FragmentSize,
   bool ReuseSmemC,
+  bool DelayTmaStore,
   template <class> class ActivationFn,
   class ElementOutput,
   class ElementCompute,
   class ElementBias,
   class ElementSource,
   class ElementScalar,
   int AlignmentBias,
   int AlignmentScalar,
   FloatRoundStyle RoundStyle,
   class CtaTileShapeMNK,
   class EpilogueTile
 >
 struct FusionCallbacks<
-    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
     fusion::PerRowLinCombPerRowBiasEltAct<
       ActivationFn, ElementOutput, ElementCompute, ElementBias, ElementSource, ElementScalar, AlignmentBias, AlignmentScalar, RoundStyle
     >,
     CtaTileShapeMNK,
     EpilogueTile
 > : Sm90PerRowLinCombPerRowBiasEltAct<
       CtaTileShapeMNK, ActivationFn, ElementOutput, ElementCompute, ElementBias, ElementSource, ElementScalar, AlignmentBias, AlignmentScalar, RoundStyle
@@ -615,38 +622,42 @@
     >;
   using Operation =
     fusion::PerRowLinCombPerRowBiasEltAct<
       ActivationFn, ElementOutput, ElementCompute, ElementBias, ElementSource, ElementScalar, AlignmentBias, AlignmentScalar, RoundStyle
     >;
 
   struct Arguments {
+    using StrideAlpha = Stride<_1,_0,int>;
+    using StrideBeta  = Stride<_1,_0,int>;
     ElementScalar alpha = ElementScalar(1);
     ElementScalar beta = ElementScalar(0);
     ElementScalar const* alpha_ptr = nullptr;
     ElementScalar const* beta_ptr = nullptr;
+    StrideAlpha dAlpha = {};
+    StrideBeta  dBeta  = {};
 
     using StrideBias = Stride<_1,_0,int>;
     ElementBias const* bias_ptr = nullptr;
     StrideBias dBias = {};
 
     using ActivationArguments = typename Sm90Compute<ActivationFn, ElementOutput, ElementCompute, RoundStyle>::Arguments;
     ActivationArguments activation = ActivationArguments();
 
     operator typename Impl::Arguments() const {
       return
         {    // unary op : activation(beta * C + (alpha * acc + bias))
           {    // ternary op : beta * C + (alpha * acc + bias)
-            {beta_ptr, beta}, // leaf args : beta
-            {},               // leaf args : C
-            {                 // ternary op : alpha * acc + bias
-              {alpha_ptr, alpha},   // leaf args : alpha
-              {},                   // leaf args : acc
+            {beta_ptr, beta, dBeta}, // leaf args : beta
+            {},                      // leaf args : C
+            {                        // ternary op : alpha * acc + bias
+              {alpha_ptr, alpha, dAlpha}, // leaf args : alpha
+              {},                         // leaf args : acc
               {bias_ptr, ElementBias(0), dBias}, // leaf args : bias
-              {}              // ternary args : multiply_add
-            },                // end ternary op
+              {}                     // ternary args : multiply_add
+            },                       // end ternary op
             {} // ternary args : multiply_add
           },   // end ternary op
           activation // unary args : activation
         };   // end unary op
     }
   };
 
@@ -654,14 +665,17 @@
   using Impl::Impl;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace detail {
 
+template <typename T>
+constexpr bool is_fp8_v = cute::is_same_v<T,float_e4m3_t> || cute::is_same_v<T,float_e5m2_t>;
+
 // We only apply the scaling factor if output is fp8
 template <typename ElementOutput>
 struct ScaleOutOp { template <typename T> using Op = cutlass::first<T>; };
 template <>
 struct ScaleOutOp<float_e4m3_t> { template <typename T> using Op = cutlass::multiplies<T>; };
 template <>
 struct ScaleOutOp<float_e5m2_t> { template <typename T> using Op = cutlass::multiplies<T>; };
@@ -719,27 +733,28 @@
   >;
 
 template <
   int StagesC,
   int StagesD,
   int FragmentSize,
   bool ReuseSmemC,
+  bool DelayTmaStore,
   template <class> class ActivationFn,
   class ElementOutput,
   class ElementCompute,
   class ElementBias,
   class ElementSource,
   class ElementScalar,
   int AlignmentBias,
   FloatRoundStyle RoundStyle,
   class CtaTileShapeMNK,
   class EpilogueTile
 >
 struct FusionCallbacks<
-    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
     fusion::ScaledLinCombPerRowBiasEltAct<
       ActivationFn, ElementOutput, ElementCompute, ElementBias, ElementSource, ElementScalar, AlignmentBias, RoundStyle
     >,
     CtaTileShapeMNK,
     EpilogueTile
 > : Sm90ScaledLinCombPerRowBiasEltAct<
       CtaTileShapeMNK, ActivationFn, ElementOutput, ElementCompute, ElementBias, ElementSource, ElementScalar, AlignmentBias, RoundStyle
@@ -818,14 +833,16 @@
 // else
 //   D = activation(Z)
 // if Aux is fp8 
 //   amax_aux = max(abs(elements in Z))
 //   Aux = scale_aux * Z
 // else
 //   Aux = Z
+
+// fp8 aux specialization
 template<
   class CtaTileShapeMNK,
   class EpilogueTile,
   int StagesD,
   class StrideAux,
   class SmemLayoutAtom,
   class CopyOpR2S,
@@ -837,43 +854,111 @@
   class ElementBias = ElementOutput,
   class ElementSource = ElementOutput,
   class ElementScalar = ElementCompute,
   int AlignmentAux = 128 / sizeof_bits_v<ElementAux>,
   int AlignmentBias = 128 / sizeof_bits_v<ElementBias>,
   FloatRoundStyle RoundStyle = FloatRoundStyle::round_to_nearest
 >
-using Sm90ScaledLinCombPerRowBiasEltActAmaxAux =
+using Sm90ScaledLinCombPerRowBiasEltActAmaxAuxFp8 =
   Sm90SplitTreeVisitor<
     // Z = scale_a * scale_b * alpha * acc + scale_c * beta * C + per-row bias
     Sm90ScaledLinCombPerRowBias<CtaTileShapeMNK, ElementCompute, ElementCompute, ElementBias, ElementSource, ElementScalar, AlignmentBias, RoundStyle>,
     // D = activation(Z) * scale_d, amax_d = max(abs(elements in D))
     Sm90EVT<Sm90Compute<detail::ScaleOutOp<ElementOutput>::template Op, ElementOutput, ElementCompute, RoundStyle>, // activation(Z) * scale_d
       Sm90EVT<Sm90ScalarReduction<detail::amax, atomic_maximum, ElementAmax, ElementCompute, RoundStyle>, // amax_d
         Sm90EVT<Sm90Compute<ActivationFn, ElementCompute, ElementCompute, RoundStyle>, // activation(Z)
           Sm90SplitTreeFetch // Z
         >
       >,
       Sm90ScalarBroadcast<ElementScalar> // scale_d
     >,
     // Aux = Z * scale_aux, amax_aux = max(abs(elements in Aux))
     Sm90EVT<Sm90AuxStore<StagesD, EpilogueTile, ElementAux, RoundStyle, StrideAux, SmemLayoutAtom, CopyOpR2S, AlignmentAux>, // store(Aux)
-      Sm90EVT<Sm90Compute<detail::ScaleOutOp<ElementAux>::template Op, ElementCompute, ElementCompute, RoundStyle>, // Z * scale_aux
+      Sm90EVT<Sm90Compute<cutlass::multiplies, ElementCompute, ElementCompute, RoundStyle>, // Z * scale_aux
         Sm90EVT<Sm90ScalarReduction<detail::amax, atomic_maximum, ElementAmax, ElementCompute, RoundStyle>, // amax_aux
           Sm90SplitTreeFetch // Z
         >,
         Sm90ScalarBroadcast<ElementScalar> // scale_aux
       >
     >
   >;
 
+// non-fp8 aux specialization
+// lets us use some EVT specializations such as relu + uint1b_t aux
+template<
+  class CtaTileShapeMNK,
+  class EpilogueTile,
+  int StagesD,
+  class StrideAux,
+  class SmemLayoutAtom,
+  class CopyOpR2S,
+  template <class> class ActivationFn,
+  class ElementOutput,
+  class ElementCompute,
+  class ElementAux = ElementOutput,
+  class ElementAmax = ElementCompute,
+  class ElementBias = ElementOutput,
+  class ElementSource = ElementOutput,
+  class ElementScalar = ElementCompute,
+  int AlignmentAux = 128 / sizeof_bits_v<ElementAux>,
+  int AlignmentBias = 128 / sizeof_bits_v<ElementBias>,
+  FloatRoundStyle RoundStyle = FloatRoundStyle::round_to_nearest
+>
+using Sm90ScaledLinCombPerRowBiasEltActAmaxAuxNotFp8 =
+  // D = activation(Z) * scale_d, amax_d = max(abs(elements in D))
+  Sm90EVT<Sm90Compute<detail::ScaleOutOp<ElementOutput>::template Op, ElementOutput, ElementCompute, RoundStyle>, // activation(Z) * scale_d
+    Sm90EVT<Sm90ScalarReduction<detail::amax, atomic_maximum, ElementAmax, ElementCompute, RoundStyle>, // amax_d
+      Sm90EVT<Sm90Compute<ActivationFn, ElementCompute, ElementCompute, RoundStyle>, // activation(Z)
+        Sm90EVT<Sm90AuxStore<StagesD, EpilogueTile, ElementAux, RoundStyle, StrideAux, SmemLayoutAtom, CopyOpR2S, AlignmentAux>, // Aux = Z
+          // Z = scale_a * scale_b * alpha * acc + scale_c * beta * C + per-row bias
+          Sm90ScaledLinCombPerRowBias<CtaTileShapeMNK, ElementCompute, ElementCompute, ElementBias, ElementSource, ElementScalar, AlignmentBias, RoundStyle>,
+        >
+      >
+    >,
+    Sm90ScalarBroadcast<ElementScalar> // scale_d
+  >;
+
+// dispatcher
+template<
+  class CtaTileShapeMNK,
+  class EpilogueTile,
+  int StagesD,
+  class StrideAux,
+  class SmemLayoutAtom,
+  class CopyOpR2S,
+  template <class> class ActivationFn,
+  class ElementOutput,
+  class ElementCompute,
+  class ElementAux = ElementOutput,
+  class ElementAmax = ElementCompute,
+  class ElementBias = ElementOutput,
+  class ElementSource = ElementOutput,
+  class ElementScalar = ElementCompute,
+  int AlignmentAux = 128 / sizeof_bits_v<ElementAux>,
+  int AlignmentBias = 128 / sizeof_bits_v<ElementBias>,
+  FloatRoundStyle RoundStyle = FloatRoundStyle::round_to_nearest
+>
+using Sm90ScaledLinCombPerRowBiasEltActAmaxAux = conditional_t<detail::is_fp8_v<ElementAux>,
+  Sm90ScaledLinCombPerRowBiasEltActAmaxAuxFp8<
+    CtaTileShapeMNK, EpilogueTile, StagesD, StrideAux, SmemLayoutAtom, CopyOpR2S, ActivationFn,
+    ElementOutput, ElementCompute, ElementAux, ElementAmax, ElementBias, ElementSource, ElementScalar,AlignmentAux, AlignmentBias, RoundStyle
+  >,
+  Sm90ScaledLinCombPerRowBiasEltActAmaxAuxNotFp8<
+    CtaTileShapeMNK, EpilogueTile, StagesD, StrideAux, SmemLayoutAtom, CopyOpR2S, ActivationFn,
+    ElementOutput, ElementCompute, ElementAux, ElementAmax, ElementBias, ElementSource, ElementScalar, AlignmentAux, AlignmentBias, RoundStyle
+  >
+>;
+
+
 template <
   int StagesC,
   int StagesD,
   int FragmentSize,
   bool ReuseSmemC,
+  bool DelayTmaStore,
   class GmemLayoutTagAux,
   template <class> class ActivationFn,
   class ElementOutput,
   class ElementCompute,
   class ElementAux,
   class ElementAmax,
   class ElementBias,
@@ -884,15 +969,15 @@
   FloatRoundStyle RoundStyle,
   class CtaTileShapeMNK,
   class EpilogueTile,
   class SmemLayoutAtom,
   class CopyOpR2S
 >
 struct FusionCallbacks<
-    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
     fusion::ScaledLinCombPerRowBiasEltActAmaxAux<
       GmemLayoutTagAux, ActivationFn, ElementOutput, ElementCompute,
       ElementAux, ElementAmax, ElementBias, ElementSource, ElementScalar, AlignmentAux, AlignmentBias, RoundStyle
     >,
     CtaTileShapeMNK,
     EpilogueTile,
     SmemLayoutAtom,
@@ -944,78 +1029,109 @@
     ElementAmax* amax_aux_ptr = nullptr;
 
     using StrideAux = cutlass::gemm::TagToStrideC_t<GmemLayoutTagAux>;
     ElementAux* aux_ptr = nullptr;
     StrideAux dAux = {};
 
     operator typename Impl::Arguments() const {
-      typename Impl::Arguments args;
-      // always use structured binding to unpack DAG args since it may or may not be a tuple
-      auto& [Z_args, aux_args, D_args] = args;
-
-      Z_args =
-        {    // ternary op : (scale_c * beta) * C + ((scale_a * scale_b * alpha) * acc + bias)
-          {{scale_c, beta},
-           {scale_c_ptr, beta_ptr}
-           },  // leaf args : (scale_c * beta)
-          {},  // leaf args : C
-          {    // ternary op : (scale_a * scale_b * alpha) * acc + bias
-            {{scale_a, scale_b, alpha}, 
-             {scale_a_ptr, scale_b_ptr, alpha_ptr}
-             },                   // leaf args : (scale_a * scale_b * alpha)
-            {},                   // leaf args : acc
-            {bias_ptr, ElementBias(0), dBias}, // leaf args : bias
-            {} // ternary args : multiply_add
-          },   // end ternary op
-          {} // ternary args : multiply_add
-        };   // end ternary op
-
       // Only compute amax_d if D is fp8
       ElementAmax* amax_D_ptr_ = nullptr;
-      if constexpr (cute::is_same_v<ElementOutput, float_e4m3_t> ||
-                    cute::is_same_v<ElementOutput, float_e5m2_t>) {
+      if constexpr (detail::is_fp8_v<ElementOutput>) {
         amax_D_ptr_ = amax_D_ptr;
       }
-      D_args =
-        {    // binary op : activation(Z) * scale_d or activation(Z)
-          {    // unary op : reduce(activation(Z))
-            {             // unary op : activation(Z)
-              {},             // leaf args : Z
-              activation      // unary args : activation
-            },                // end unary op
-            {amax_D_ptr_} // unary args : reduce
-          },              // end unary op
-          {{scale_d},
-           {scale_d_ptr}
-           },  // leaf args : scale_d
-          {} // binary args : multiplies or first
-        };   // end binary op
 
-      // Only compute amax_aux if aux is fp8
-      ElementAmax* amax_aux_ptr_ = nullptr;
-      if constexpr (cute::is_same_v<ElementAux, float_e4m3_t> ||
-                    cute::is_same_v<ElementAux, float_e5m2_t>) {
-        amax_aux_ptr_ = amax_aux_ptr;
-      }
-      aux_args =
-        {    // unary op : store(Aux)
-          {    // binary op : Z * scale_d or Z
-            {    // unary op : reduce(Z)
-              {},             // leaf args : Z
-              {amax_aux_ptr_} // unary args : reduce
-            },   // end unary op
-            {{scale_aux},
-             {scale_aux_ptr}
+      // Aux is fp8 -> DAG arguments
+      if constexpr (detail::is_fp8_v<ElementAux>) {
+        typename Impl::Arguments args;
+        // always use structured binding to unpack DAG args since it may or may not be a tuple
+        auto& [Z_args, aux_args, D_args] = args;
+
+        Z_args =
+          {    // ternary op : (scale_c * beta) * C + ((scale_a * scale_b * alpha) * acc + bias)
+            {{scale_c, beta},
+             {scale_c_ptr, beta_ptr}
+             },  // leaf args : (scale_c * beta)
+            {},  // leaf args : C
+            {    // ternary op : (scale_a * scale_b * alpha) * acc + bias
+              {{scale_a, scale_b, alpha}, 
+               {scale_a_ptr, scale_b_ptr, alpha_ptr}
+               },                   // leaf args : (scale_a * scale_b * alpha)
+              {},                   // leaf args : acc
+              {bias_ptr, ElementBias(0), dBias}, // leaf args : bias
+              {} // ternary args : multiply_add
+            },   // end ternary op
+            {} // ternary args : multiply_add
+          };   // end ternary op
+
+        D_args =
+          {    // binary op : activation(Z) * scale_d or activation(Z)
+            {    // unary op : reduce(activation(Z))
+              {             // unary op : activation(Z)
+                {},             // leaf args : Z
+                activation      // unary args : activation
+              },                // end unary op
+              {amax_D_ptr_} // unary args : reduce
+            },              // end unary op
+            {{scale_d},
+             {scale_d_ptr}
              },  // leaf args : scale_d
             {} // binary args : multiplies or first
-          },   // end binary op
-          {aux_ptr, dAux} // unary args : store
-        };   // end unary op
+          };   // end binary op
+
+        aux_args =
+          {    // unary op : store(Aux)
+            {    // binary op : Z * scale_d or Z
+              {    // unary op : reduce(Z)
+                {},            // leaf args : Z
+                {amax_aux_ptr} // unary args : reduce
+              },   // end unary op
+              {{scale_aux},
+               {scale_aux_ptr}
+               },  // leaf args : scale_d
+              {} // binary args : multiplies
+            },   // end binary op
+            {aux_ptr, dAux} // unary args : store
+          };   // end unary op
 
-      return args;
+        return args;
+      }
+
+      // Aux is not fp8 -> Tree arguments
+      else {
+        return
+          {  // binary op : activation(Z) * scale_d or activation(Z)
+            {  // unary op : reduce(activation(Z))
+              {  // unary op : activation(Z)
+                {  // unary op : store(Z)
+                  {  // ternary op : (scale_c * beta) * C + ((scale_a * scale_b * alpha) * acc + bias)
+                    {{scale_c, beta},
+                     {scale_c_ptr, beta_ptr}
+                    },                // leaf args : (scale_c * beta)
+                    {},               // leaf args : C
+                    {                 // ternary op : (scale_a * scale_b * alpha) * acc + bias
+                      {{scale_a, scale_b, alpha}, 
+                       {scale_a_ptr, scale_b_ptr, alpha_ptr}
+                      },                // leaf args : (scale_a * scale_b * alpha)
+                      {},               // leaf args : acc
+                      {bias_ptr, ElementBias(0), dBias
+                      },                // leaf args : bias
+                      {}              // ternary args : multiply_add
+                    },                // end ternary op
+                    {}              // ternary args : multiply_add
+                  },                // end ternary op
+                  {aux_ptr, dAux} // unary args : store
+                },                // end unary op
+                activation     // unary args : activation
+              },               // end unary op
+              {amax_D_ptr_} // unary args : reduce
+            },              // end unary op
+            {{scale_d},{scale_d_ptr}}, // leaf args : scale_d
+            {} // binary args : multiplies or first
+          };   // end binary op
+      }
     }
   };
 
   // Ctor inheritance
   using Impl::Impl;
 };
 
@@ -1044,14 +1160,15 @@
   >;
 
 template <
   int StagesC,
   int StagesD,
   int FragmentSize,
   bool ReuseSmemC,
+  bool DelayTmaStore,
   class GmemLayoutTagAux,
   template <class> class ActivationFn,
   class ElementOutput,
   class ElementCompute,
   class ElementAux,
   class ElementSource,
   class ElementScalar,
@@ -1059,15 +1176,15 @@
   FloatRoundStyle RoundStyle,
   class CtaTileShapeMNK,
   class EpilogueTile,
   class SmemLayoutAtom,
   class CopyOpS2R
 >
 struct FusionCallbacks<
-    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
     fusion::LinCombDeEltAct<
       GmemLayoutTagAux, ActivationFn, ElementOutput, ElementCompute,
       ElementAux, ElementSource, ElementScalar, AlignmentAux, RoundStyle
     >,
     CtaTileShapeMNK,
     EpilogueTile,
     SmemLayoutAtom,
@@ -1142,26 +1259,27 @@
   class ElementScalar = ElementCompute,
   int AlignmentAux = 128 / sizeof_bits_v<ElementAux>,
   int AlignmentBias = 128 / sizeof_bits_v<ElementBias>,
   FloatRoundStyle RoundStyle = FloatRoundStyle::round_to_nearest
 >
 using Sm90LinCombDeEltActDePerRowBias =
   Sm90EVT<Sm90Compute<cutlass::epilogue::thread::Identity, ElementOutput, ElementCompute, RoundStyle>, // Identity for final conversion
-    Sm90EVT<Sm90ColReduction<plus, plus, 0, CtaTileShapeMNK,
+    Sm90EVT<Sm90ColReduction<plus, plus, plus, 0, CtaTileShapeMNK,
                              ElementBias, ElementCompute, RoundStyle, Stride<_1,_0,int>, AlignmentBias>,
       Sm90LinCombDeEltAct<CtaTileShapeMNK, EpilogueTile, Stages, StrideAux, SmemLayoutAtom, CopyOpS2R, ActivationFn,
                           ElementCompute, ElementCompute, ElementAux, ElementSource, ElementScalar, AlignmentAux, RoundStyle>
     >
   >;
 
 template <
   int StagesC,
   int StagesD,
   int FragmentSize,
   bool ReuseSmemC,
+  bool DelayTmaStore,
   class GmemLayoutTagAux,
   template <class> class ActivationFn,
   class ElementOutput,
   class ElementCompute,
   class ElementAux,
   class ElementBias,
   class ElementSource,
@@ -1171,15 +1289,15 @@
   FloatRoundStyle RoundStyle,
   class CtaTileShapeMNK,
   class EpilogueTile,
   class SmemLayoutAtom,
   class CopyOpS2R
 >
 struct FusionCallbacks<
-    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC>,
+    epilogue::Sm90TmaWarpSpecialized<StagesC, StagesD, FragmentSize, ReuseSmemC, DelayTmaStore>,
     fusion::LinCombDeEltActDePerRowBias<
       GmemLayoutTagAux, ActivationFn, ElementOutput, ElementCompute,
       ElementAux, ElementBias, ElementSource, ElementScalar, AlignmentAux, AlignmentBias, RoundStyle
     >,
     CtaTileShapeMNK,
     EpilogueTile,
     SmemLayoutAtom,
```

## cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_compute_tma_warpspecialized.hpp

```diff
@@ -107,27 +107,28 @@
 
   using Arguments = typename ComputeArguments<ComputeFn<ElementCompute>>::type;
 
   using Params = Arguments;
 
   template <class ProblemShape>
   static constexpr Params
-  to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
+  to_underlying_arguments(ProblemShape const&, Arguments const& args, void*) {
     return args;
   }
 
   template <class ProblemShape>
   static size_t
-  get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
+  get_workspace_size(ProblemShape const&, Arguments const&) {
     return 0;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream, 
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return cutlass::Status::kSuccess;
   }
 
   CUTLASS_DEVICE bool
   is_producer_load_needed() const {
     return false;
   }
@@ -173,15 +174,15 @@
         },
         [&] (auto&&... cvt_frg_inputs) {
           using ComputeOutput = ComputeFn<Array<ElementCompute, FragmentSize>>;
           using ConvertOutput = NumericArrayConverter<ElementOutput, ElementCompute, FragmentSize, RoundStyle>;
           ComputeOutput compute_output{};
           ConvertOutput convert_output{};
 
-          if constexpr (is_same_v<Arguments, EmptyArguments>) {
+          if constexpr (cute::is_same_v<Arguments, EmptyArguments>) {
             return convert_output(compute_output(cvt_frg_inputs...));
           }
           else {
             return convert_output(compute_output(cvt_frg_inputs..., params));
           }
         }
       );
@@ -207,36 +208,34 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // beta * C + Z
 template <
   class ElementOutput,
   class ElementCompute,
   FloatRoundStyle RoundStyle,
-  class ElementScalar,
-  class StrideScalar,
-  int ScalarCount,
-  template <class> class ScalarReduceFn,
-  class ElementSource,
-  class InputAddOp // Z
+  class InputScaleOp,  // beta
+  class ElementSource, // C
+  class InputAddOp     // Z
 >
 struct Sm90TreeVisitor<
-  Sm90Compute<homogeneous_multiply_add, ElementOutput, ElementCompute, RoundStyle>,
-  Sm90ScalarBroadcast<ElementScalar, StrideScalar, ScalarCount, ScalarReduceFn>,
+  Sm90Compute<homogeneous_multiply_add, ElementOutput, ElementCompute, RoundStyle,
+              cute::void_t<decltype(declval<InputScaleOp>().is_zero())>>,
+  InputScaleOp,
   Sm90SrcFetch<ElementSource>,
   InputAddOp
 > : Sm90VisitorImpl<
-      Sm90ScalarBroadcast<ElementScalar, StrideScalar, ScalarCount, ScalarReduceFn>,
+      InputScaleOp,
       Sm90SrcFetch<ElementSource>,
       InputAddOp,
       Sm90Compute<homogeneous_multiply_add, ElementOutput, ElementCompute, RoundStyle>
     >
 {
   using Impl =
     Sm90VisitorImpl<
-      Sm90ScalarBroadcast<ElementScalar, StrideScalar, ScalarCount, ScalarReduceFn>,
+      InputScaleOp,
       Sm90SrcFetch<ElementSource>,
       InputAddOp,
       Sm90Compute<homogeneous_multiply_add, ElementOutput, ElementCompute, RoundStyle>
     >;
   using Params = typename Impl::Params;
   using SharedStorage = typename Impl::SharedStorage;
 
@@ -247,26 +246,24 @@
   Sm90TreeVisitor(
       Params const& params,
       SharedStorage const& shared_storage)
     : Impl(params, shared_storage) {}
 
   CUTLASS_DEVICE bool
   is_producer_load_needed() const {
-    auto const& bcast_op = get<0>(Impl::ops);
     auto const& added_op = get<2>(Impl::ops);
-    return not (bcast_op.params_ptr->dScalar == Stride<_0,_0,_0>{} && not is_C_load_needed()) ||
-           added_op.is_producer_load_needed();
+    return is_C_load_needed() || added_op.is_producer_load_needed();
   }
 
   CUTLASS_DEVICE bool
   is_C_load_needed() const {
-    auto const& bcast_op = get<0>(Impl::ops);
+    auto const& scale_op = get<0>(Impl::ops);
     auto const& src_op = get<1>(Impl::ops);
     auto const& added_op = get<2>(Impl::ops);
-    return (bcast_op.scalar != 0 && src_op.is_C_load_needed()) || added_op.is_C_load_needed();
+    return (not scale_op.is_zero() && src_op.is_C_load_needed()) || added_op.is_C_load_needed();
   }
 
   template <class CallbacksImpl>
   struct ConsumerStoreCallbacks : CallbacksImpl {
     CUTLASS_DEVICE
     ConsumerStoreCallbacks(bool is_C_load_needed, CallbacksImpl&& impl)
       : is_C_load_needed(is_C_load_needed), CallbacksImpl(cute::forward<CallbacksImpl>(impl)) { }
@@ -343,15 +340,16 @@
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     return 0;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return cutlass::Status::kSuccess;
   }
 
   CUTLASS_HOST_DEVICE
   Sm90ReLUAuxStore() { }
 
   CUTLASS_HOST_DEVICE
@@ -375,16 +373,16 @@
   int Alignment,
   bool EnableNullptr,
   // Input node
   class InputOp
 >
 struct Sm90TreeVisitor<
   Sm90Compute<Activation, ElementOutput, ElementCompute, RoundStyle,
-              enable_if_t<is_same_v<Activation<ElementCompute>, cutlass::epilogue::thread::ReLu<ElementCompute>> ||
-                          is_same_v<Activation<ElementCompute>, cutlass::epilogue::thread::Clamp<ElementCompute>>  >>,
+              cute::enable_if_t<cute::is_same_v<Activation<ElementCompute>, cutlass::epilogue::thread::ReLu<ElementCompute>> ||
+                                cute::is_same_v<Activation<ElementCompute>, cutlass::epilogue::thread::Clamp<ElementCompute>>  >>,
   Sm90TreeVisitor<
     Sm90AuxStore<
       Stages,
       EpilogueTile,
       cutlass::uint1b_t,
       RoundStyle,
       StrideMNL,
@@ -470,15 +468,15 @@
       ConvertOutput convert_output{};
 
       Array frg_compute = convert_input(frg_input);
       bool frg_aux[FragmentSize];
       CUTLASS_PRAGMA_UNROLL
       for (int i = 0; i < FragmentSize; ++i) {
         ElementCompute pre_relu = frg_compute[i];
-        if constexpr (is_same_v<Activation<ElementCompute>, cutlass::epilogue::thread::Clamp<ElementCompute>>) {
+        if constexpr (cute::is_same_v<Activation<ElementCompute>, cutlass::epilogue::thread::Clamp<ElementCompute>>) {
           frg_compute[i] = relu(frg_compute[i], params_compute);
         }
         else {
           frg_compute[i] = relu(frg_compute[i]);
         }
         frg_aux[i] = frg_compute[i] == pre_relu;
       }
@@ -602,15 +600,16 @@
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     return 0;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return cutlass::Status::kSuccess;
   }
 
   CUTLASS_HOST_DEVICE
   Sm90AuxLoad() { }
 
   CUTLASS_HOST_DEVICE
@@ -631,55 +630,66 @@
 
   template <class... Args>
   CUTLASS_DEVICE auto
   get_producer_load_callbacks(ProducerLoadArgs<Args...> const& args) {
     return EmptyProducerLoadCallbacks{};
   }
 
-  template <class RTensor, class GTensor, class ResidueMN>
+  template <class RTensor, class GTensor, class CTensor, class ResidueMN>
   struct ConsumerStoreCallbacks : EmptyConsumerStoreCallbacks {
     CUTLASS_DEVICE
-    ConsumerStoreCallbacks(RTensor&& tC_rAux_, GTensor&& tC_gAux_, ResidueMN residue_mn_, Params const& params_)
+    ConsumerStoreCallbacks(RTensor&& tC_rAux_, GTensor&& tC_gAux_, CTensor tC_cAux_, ResidueMN residue_mn_, Params const& params_)
       : tC_rAux(cute::forward<RTensor>(tC_rAux_)),
         tC_gAux(cute::forward<GTensor>(tC_gAux_)),
+        tC_cAux(tC_cAux_),
         residue_mn(residue_mn_),
         params(params_) {}
 
     RTensor tC_rAux;                                                                   // (CPY,CPY_M,CPY_N,{EPI_M,EPI_N})
     GTensor tC_gAux;                                                                   // (CPY,CPY_M,CPY_N,EPI_M,EPI_N)
+    CTensor tC_cAux;                                                                   // (CPY,CPY_M,CPY_N,EPI_M,EPI_N)
     ResidueMN residue_mn;
     Params const& params;
 
     CUTLASS_DEVICE void
     begin() {
       if constexpr (decltype(cute::rank(tC_rAux))::value == 5) {
         if constexpr (EnableNullptr) {
           if (params.ptr_aux == nullptr) {
             return;
           }
         }
 
-        if (elem_less(repeat_like(residue_mn, _0{}), residue_mn)) { // (partially) in-bounds CTA tile
-          copy_aligned(tC_gAux, tC_rAux);
+        constexpr int V = cute::min(Alignment, decltype(max_common_vector(tC_rAux, tC_gAux))::value);
+        if constexpr (V > 0) {
+          using VecType = uint_bit_t<V>;
+          Tensor tC_gAux_vec = recast<VecType>(tC_gAux);
+          Tensor tC_rAux_vec = recast<VecType>(tC_rAux);
+          Tensor tC_cAux_vec = tC_cAux.compose(make_layout(Int<size(tC_rAux_vec)>{}, Int<V>{})); // only works if vector is logically sequential
+          auto predicate_fn = [&] (auto&&... coords) { return elem_less(tC_cAux_vec(coords...), residue_mn); };
+          copy_if(FunctionPredTensor(predicate_fn), tC_gAux_vec, tC_rAux_vec);
+        }
+        else {
+          auto predicate_fn = [&] (auto&&... coords) { return elem_less(tC_cAux(coords...), residue_mn); };
+          copy_if(FunctionPredTensor(predicate_fn), tC_gAux, tC_rAux);
         }
       }
     }
 
     CUTLASS_DEVICE void
     previsit(int epi_m, int epi_n, int load_iteration, bool is_producer_load_needed) {
       if constexpr (decltype(cute::rank(tC_rAux))::value == 3) {
         if constexpr (EnableNullptr) {
           if (params.ptr_aux == nullptr) {
             return;
           }
         }
 
-        if (elem_less(repeat_like(residue_mn, _0{}), residue_mn)) {
-          copy_aligned(tC_gAux(_,_,_,epi_m,epi_n), tC_rAux);
-        }
+        auto predicate_fn = [&] (auto&&... coords) { return elem_less(tC_cAux(_,_,_,epi_m,epi_n)(coords...), residue_mn); };
+        copy_if(FunctionPredTensor(predicate_fn), tC_gAux(_,_,_,epi_m,epi_n), tC_rAux);
       }
     }
 
     template <typename ElementAccumulator, int FragmentSize>
     CUTLASS_DEVICE auto
     visit(Array<ElementAccumulator, FragmentSize> const& frg_acc, int epi_v, int epi_m, int epi_n) {
       using ElementRegister = typename remove_cvref_t<RTensor>::value_type;
@@ -720,16 +730,16 @@
 
     if constexpr (EnableNullptr) {
       if (params.ptr_aux == nullptr) {
         fill(tC_rAux, params.null_default);
       }
     }
 
-    return ConsumerStoreCallbacks<decltype(tC_rAux), decltype(tC_gAux), decltype(args.residue_mn)>(
-        cute::move(tC_rAux), cute::move(tC_gAux), args.residue_mn, params);
+    return ConsumerStoreCallbacks<decltype(tC_rAux), decltype(tC_gAux), decltype(args.tCcD), decltype(args.residue_mn)>(
+        cute::move(tC_rAux), cute::move(tC_gAux), args.tCcD, args.residue_mn, params);
   }
 };
 
 // dReLU specialization
 template<
   class ElementOutput,
   class ElementCompute,
```

## cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_load_tma_warpspecialized.hpp

```diff
@@ -94,14 +94,19 @@
   }
 
   CUTLASS_DEVICE bool
   is_C_load_needed() const {
     return not is_void_v<Element>;
   }
 
+  CUTLASS_DEVICE bool
+  is_zero() const {
+    return is_void_v<Element>;
+  }
+
   using Sm90VisitorImpl<>::Sm90VisitorImpl;
 
   template<class SrcTensor>
   struct ConsumerStoreCallbacks : EmptyConsumerStoreCallbacks {
     CUTLASS_DEVICE
     ConsumerStoreCallbacks(SrcTensor const& tCrC)
       : tCrC(tCrC) {}
@@ -154,45 +159,50 @@
   using SmemLayoutTma = decltype(tile_to_shape(
       SmemLayoutAtom{}, SmemShapeTma{},
       cute::conditional_t<is_m_major, Step<_2,_1>, Step<_1,_2>>{} ));
   using SmemLayout = decltype(tile_to_shape(
       SmemLayoutTma{},
       make_shape(size<0>(shape(EpilogueTile{})), size<1>(shape(EpilogueTile{})), Int<Stages>{}),
       cute::conditional_t<is_m_major, Step<_2,_1,_3>, Step<_1,_2,_3>>{} ));
+  using CopyOpG2S =
+      SM90_TMA_LOAD
+    ;
 
   struct SharedStorage {
     alignas(cutlass::detail::alignment_for_swizzle(SmemLayout{}))
     array_aligned<Element, size(SmemLayout{})> smem_aux;
   };
 
   struct Arguments {
     Element const* ptr_aux = nullptr;
     Element null_default = Element(0);
     StrideMNL dAux = {};
   };
 
   struct Params {
     using TMA_Aux = decltype(make_tma_copy(
-        SM90_TMA_LOAD{},
-        make_tensor(static_cast<Element const*>(nullptr), repeat_like(StrideMNL{}, int32_t(0)), StrideMNL{}),
-        SmemLayoutTma{}));
+        CopyOpG2S{},
+        make_tensor(make_gmem_ptr(static_cast<Element const*>(nullptr)), repeat_like(StrideMNL{}, int32_t(0)), append<3>(StrideMNL{}, _0{})),
+        take<0,2>(SmemLayoutTma{})));
     TMA_Aux tma_load_aux;
     Element null_default = Element(0);
     bool use_default = false;
   };
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
     // Optionally append 1s until problem shape is rank-4 in case its is only rank-3 (MNK)
     auto problem_shape_mnkl = append<4>(problem_shape, 1);
     auto [M, N, K, L] = problem_shape_mnkl;
-
-    Tensor tensor_aux = make_tensor(args.ptr_aux, make_layout(make_shape(M,N,L), args.dAux));
-    typename Params::TMA_Aux tma_load_aux = make_tma_copy(SM90_TMA_LOAD{}, tensor_aux, SmemLayoutTma{});
+    auto M_AUX =
+        size(M)
+      ;
+    Tensor tensor_aux = make_tensor(make_gmem_ptr(args.ptr_aux), make_layout(make_shape(M_AUX,N,L), append<3>(args.dAux, _0{})));
+    typename Params::TMA_Aux tma_load_aux = make_tma_copy(CopyOpG2S{}, tensor_aux, take<0,2>(SmemLayoutTma{}));
 
     bool use_default = false;
     if constexpr (EnableNullptr) {
       use_default = args.ptr_aux == nullptr;
     }
 
     return Params{tma_load_aux, args.null_default, use_default};
@@ -202,15 +212,16 @@
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     return 0;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return cutlass::Status::kSuccess;
   }
 
   CUTLASS_HOST_DEVICE
   Sm90AuxLoad() { }
 
   CUTLASS_HOST_DEVICE
@@ -227,14 +238,19 @@
   }
 
   CUTLASS_DEVICE bool
   is_C_load_needed() const {
     return false;
   }
 
+  CUTLASS_DEVICE bool
+  is_zero() const {
+    return (params_ptr->use_default && params_ptr->null_default == Element(0));
+  }
+
   template <class GTensor, class STensor>
   struct ProducerLoadCallbacks : EmptyProducerLoadCallbacks {
     CUTLASS_DEVICE
     ProducerLoadCallbacks(GTensor&& bGS_gAux, STensor&& bGS_sAux, Params const* params_ptr)
       : bGS_gAux(cute::forward<GTensor>(bGS_gAux)),
         bGS_sAux(cute::forward<STensor>(bGS_sAux)),
         params_ptr(params_ptr) {}
@@ -249,15 +265,15 @@
         if (params_ptr->use_default) {
           return;
         }
       }
 
       if (issue_tma_load) {
         // Increment the expected transaction bytes of the current stage's mbarrier by the subtile's byte-size
-        constexpr uint32_t copy_bytes = size(take<0,2>(SmemLayout{})) * sizeof_bytes_v<Element>;
+        constexpr uint32_t copy_bytes = size(take<0,2>(SmemLayout{})) * sizeof_bits_v<Element> / 8;
         cutlass::arch::ClusterTransactionBarrier::expect_transaction(full_mbarrier_ptr, copy_bytes);
         // Issue the TMA load
         constexpr uint16_t mcast_mask = 0;
         int load_pipe_index = load_iteration % Stages;
         copy(params_ptr->tma_load_aux.with(*full_mbarrier_ptr, mcast_mask),
           bGS_gAux(_,_,_,epi_m,epi_n), bGS_sAux(_,_,_,load_pipe_index));
       }
@@ -266,16 +282,20 @@
 
   template <class... Args>
   CUTLASS_DEVICE auto
   get_producer_load_callbacks(ProducerLoadArgs<Args...> const& args) {
 
     auto [M, N, K, L] = args.problem_shape_mnkl;
     auto [m, n, k, l] = args.tile_coord_mnkl;
-    Tensor mAux = params_ptr->tma_load_aux.get_tma_tensor(make_shape(M,N,L));                                // (M,N,L)
-    Tensor gAux = local_tile(mAux, take<0,2>(args.tile_shape_mnk), make_coord(m,n,l));                 // (CTA_M,CTA_N)
+    auto coord_shape =
+        make_coord(m, n, l)
+      ;
+    Tensor mAux_mn = params_ptr->tma_load_aux.get_tma_tensor(make_shape(M,N,L));                             // (M,N,L)
+    Tensor mAux = coalesce(mAux_mn, take<0,2>(args.tile_shape_mnk));
+    Tensor gAux = local_tile(mAux, take<0,2>(args.tile_shape_mnk), coord_shape);                       // (CTA_M,CTA_N)
 
     Tensor gAux_epi = flat_divide(gAux, args.epi_tile);                          // (EPI_TILE_M,EPI_TILE_N,EPI_M,EPI_N)
     Tensor sAux_epi = make_tensor(make_smem_ptr(smem_aux), SmemLayout{});        // (EPI_TILE_M,EPI_TILE_N,PIPE)
 
     ThrCopy thrblk_g2s = params_ptr->tma_load_aux.get_slice(_0{});
     Tensor bGS_gAux = thrblk_g2s.partition_S(gAux_epi);                                // (TMA,TMA_M,TMA_N,EPI_M,EPI_N)
     Tensor bGS_sAux = thrblk_g2s.partition_D(sAux_epi);                                // (TMA,TMA_M,TMA_N,PIPE)
@@ -327,17 +347,19 @@
     bool ReferenceSrc, // do register tensors reference the src or dst layout of the tiled copy
     class... Args
   >
   CUTLASS_DEVICE auto
   get_consumer_store_callbacks(ConsumerStoreArgs<Args...> const& args) {
 
     auto [M, N, K, L] = args.problem_shape_mnkl;
-    Tensor mAux = params_ptr->tma_load_aux.get_tma_tensor(make_shape(M,N,L));                                // (M,N,L)
-    Tensor tC_gAux = sm90_partition_for_epilogue<ReferenceSrc>(                        // (CPY,CPY_M,CPY_N,EPI_M,EPI_N)
-      mAux, args.tile_shape_mnk, args.tile_coord_mnkl, args.epi_tile, args.tiled_copy, args.thread_idx);
+
+    Tensor mAux_mn = params_ptr->tma_load_aux.get_tma_tensor(make_shape(M,N,L));                             // (M,N,L)
+    Tensor mAux = coalesce(mAux_mn, take<0,2>(args.tile_shape_mnk));
+    Tensor tC_gAux = sm90_partition_for_epilogue<ReferenceSrc                          // (CPY,CPY_M,CPY_N,EPI_M,EPI_N)
+      >(mAux, args.tile_shape_mnk, args.tile_coord_mnkl, args.epi_tile, args.tiled_copy, args.thread_idx);
     Tensor tC_rAux = make_tensor<Element>(take<0,3>(shape(tC_gAux)));                  // (CPY,CPY_M,CPY_N)
 
     auto tiled_s2r = conditional_return<ReferenceSrc>(
       make_tiled_copy_S(Copy_Atom<CopyOpS2R,Element>{}, args.tiled_copy),
       make_tiled_copy_D(Copy_Atom<CopyOpS2R,Element>{}, args.tiled_copy)
     );
     Tensor sAux_epi = cute::as_position_independent_swizzle_tensor(
@@ -389,50 +411,55 @@
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     return 0;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter *cuda_adapter = nullptr) {
     return cutlass::Status::kSuccess;
   }
 
   CUTLASS_DEVICE bool
   is_producer_load_needed() const {
     return false;
   }
 
   CUTLASS_DEVICE bool
   is_C_load_needed() const {
     return false;
   }
 
+  // This must be called after update_scalar is called
+  CUTLASS_DEVICE bool
+  is_zero() const {
+    return scalar == Element(0);
+  }
+
   CUTLASS_HOST_DEVICE
   Sm90ScalarBroadcast() { }
 
   CUTLASS_HOST_DEVICE
   Sm90ScalarBroadcast(Params const& params, SharedStorage const& shared_storage)
       : params_ptr(&params) {
     // Get the scalar for non-batched broadcast
-    if constexpr (cute::is_same_v<StrideMNL, Stride<_0,_0,_0>>) {
+    if (get<2>(params_ptr->dScalar) == 0) {
       update_scalar();
     }
   }
 
   Element scalar;
   Params const* params_ptr;
 
   template <class... Args>
   CUTLASS_DEVICE auto
   get_producer_load_callbacks(ProducerLoadArgs<Args...> const& args) {
     // Get the scalar for batched broadcast
-    if constexpr (
-      cute::is_same_v<StrideMNL, Stride<_0,_0,_1>> ||
-      cute::is_same_v<StrideMNL, Stride<_0,_0,int>>) {
+    if (get<2>(params_ptr->dScalar) != 0) {
       auto [m_coord, n_coord, k_coord, l_coord] = args.tile_coord_mnkl;
       update_scalar(l_coord);
     }
 
     return EmptyProducerLoadCallbacks{};
   }
 
@@ -458,17 +485,15 @@
     bool ReferenceSrc, // do register tensors reference the src or dst layout of the tiled copy
     class... Args
   >
   CUTLASS_DEVICE auto
   get_consumer_store_callbacks(ConsumerStoreArgs<Args...> const& args) {
 
     // Get the scalar for batched broadcast
-    if constexpr (
-      cute::is_same_v<StrideMNL, Stride<_0,_0,_1>> ||
-      cute::is_same_v<StrideMNL, Stride<_0,_0,int>>) {
+    if (get<2>(params_ptr->dScalar) != 0) {
       auto [m_coord, n_coord, k_coord, l_coord] = args.tile_coord_mnkl;
       update_scalar(l_coord);
     }
 
     return ConsumerStoreCallbacks(scalar);
   }
 
@@ -492,18 +517,36 @@
         scalar = reduction_fn(scalar, params_ptr->scalar_ptrs[i][l_offset]);
       } else {
         // batch stride is ignored for nullptr fallback
         scalar = reduction_fn(scalar, params_ptr->scalars[i]);
       }
     }
   }
+
+  template<class... Xs>
+  CUTLASS_DEVICE void
+  update_scalar(cute::tuple<Xs...>) {
+    // Only support multiple L-modes with fully-broadcast scalar
+    static_assert(cute::is_same_v<StrideMNL, Stride<_0,_0, _0>>);
+    scalar = params_ptr->scalars[0];
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+namespace detail {
+
+template <int StagesC, class CtaTileShapeMNK, class EpilogueTile>
+constexpr int
+compute_row_broadcast_stages() {
+  return ceil_div(StagesC, size<1>(zipped_divide(make_layout(take<0,2>(CtaTileShapeMNK{})), EpilogueTile{}))) + 1;
+}
+
+}
+
 // Row vector broadcast
 template<
   // Row bcast reuses the mbarriers from the epilogue subtile load pipeline, so this must be at least
   // ceil_div(StagesC, epi tiles per CTA tile) + 1 to ensure no data races
   int Stages,
   class CtaTileShapeMNK,
   class Element,
@@ -540,15 +583,16 @@
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     return 0;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return cutlass::Status::kSuccess;
   }
 
   CUTLASS_HOST_DEVICE
   Sm90RowBroadcast() { }
 
   CUTLASS_HOST_DEVICE
@@ -565,14 +609,19 @@
   }
 
   CUTLASS_DEVICE bool
   is_C_load_needed() const {
     return false;
   }
 
+  CUTLASS_DEVICE bool
+  is_zero() const {
+    return (params.ptr_row == nullptr && params.null_default == Element(0));
+  }
+
   template <int EpiTiles, class GTensor, class STensor>
   struct ProducerLoadCallbacks : EmptyProducerLoadCallbacks {
     CUTLASS_DEVICE
     ProducerLoadCallbacks(GTensor&& gRow, STensor&& sRow, Params const& params)
       : gRow(cute::forward<GTensor>(gRow)),
         sRow(cute::forward<STensor>(sRow)),
         params(params) {}
@@ -587,15 +636,15 @@
         if (params.ptr_row == nullptr) {
           return;
         }
       }
 
       if (issue_tma_load) {
         // Increment the expect-tx count of the first subtile's mbarrier by the row vector's byte-size
-        constexpr uint32_t copy_bytes = size<1>(CtaTileShapeMNK{}) * sizeof_bytes_v<Element>;
+        constexpr uint32_t copy_bytes = size<1>(CtaTileShapeMNK{}) * sizeof_bits_v<Element> / 8;
         cutlass::arch::ClusterTransactionBarrier::expect_transaction(full_mbarrier_ptr, copy_bytes);
         // Issue the TMA bulk copy
         auto bulk_copy = Copy_Atom<SM90_BULK_COPY_AUTO, Element>{}.with(*full_mbarrier_ptr);
         // Filter so we don't issue redundant copies over stride-0 modes
         int bcast_pipe_index = (load_iteration / EpiTiles) % Stages;
         copy(bulk_copy, filter(gRow), filter(sRow(_,_,bcast_pipe_index)));
       }
@@ -610,15 +659,15 @@
     auto [m, n, k, l] = args.tile_coord_mnkl;
     Tensor mRow = make_tensor(make_gmem_ptr(params.ptr_row), make_shape(M,N,L), params.dRow);
     Tensor gRow = local_tile(mRow, take<0,2>(args.tile_shape_mnk), make_coord(m,n,l));            // (CTA_M,CTA_N)
     Tensor sRow = make_tensor(make_smem_ptr(smem_row),                                            // (CTA_M,CTA_N,PIPE)
                     make_shape(size<0>(CtaTileShapeMNK{}), size<1>(CtaTileShapeMNK{}), Stages),
                     make_stride(_0{},_1{},size<1>(CtaTileShapeMNK{})));
 
-    constexpr int EpiTiles = decltype(size(shape_div(take<0,2>(args.tile_shape_mnk), args.epi_tile)))::value;
+    constexpr int EpiTiles = decltype(size<1>(zipped_divide(make_layout(take<0,2>(args.tile_shape_mnk)), args.epi_tile)))::value;
     return ProducerLoadCallbacks<EpiTiles, decltype(gRow), decltype(sRow)>(
       cute::move(gRow), cute::move(sRow), params);
   }
 
   template <int EpiTiles, class RTensor, class STensor>
   struct ConsumerStoreCallbacks : EmptyConsumerStoreCallbacks {
     CUTLASS_DEVICE
@@ -672,15 +721,15 @@
     Tensor sRow = make_tensor(make_smem_ptr(smem_row),                                            // (CTA_M,CTA_N,PIPE)
                     make_shape(size<0>(CtaTileShapeMNK{}), size<1>(CtaTileShapeMNK{}), Stages),
                     make_stride(_0{},_1{},size<1>(CtaTileShapeMNK{})));
     Tensor tCsRow = sm90_partition_for_epilogue<ReferenceSrc>(                    // (CPY,CPY_M,CPY_N,EPI_M,EPI_N,PIPE)
                       sRow, args.epi_tile, args.tiled_copy, args.thread_idx);
     Tensor tCrRow = make_tensor_like(take<0,3>(tCsRow));                                           // (CPY,CPY_M,CPY_N)
 
-    constexpr int EpiTiles = decltype(size(shape_div(take<0,2>(args.tile_shape_mnk), args.epi_tile)))::value;
+    constexpr int EpiTiles = decltype(size<1>(zipped_divide(make_layout(take<0,2>(args.tile_shape_mnk)), args.epi_tile)))::value;
     return ConsumerStoreCallbacks<EpiTiles, decltype(tCrRow), decltype(tCsRow)>(
       cute::move(tCrRow), cute::move(tCsRow), params);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -721,28 +770,34 @@
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     return 0;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return cutlass::Status::kSuccess;
   }
 
   CUTLASS_DEVICE bool
   is_producer_load_needed() const {
     return false;
   }
 
   CUTLASS_DEVICE bool
   is_C_load_needed() const {
     return false;
   }
 
+  CUTLASS_DEVICE bool
+  is_zero() const {
+    return (params.ptr_col == nullptr && params.null_default == Element(0));
+  }
+
   CUTLASS_HOST_DEVICE
   Sm90ColBroadcast() { }
 
   CUTLASS_HOST_DEVICE
   Sm90ColBroadcast(Params const& params, SharedStorage const& shared_storage)
       : params(params) { }
```

## cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_store_tma_warpspecialized.hpp

```diff
@@ -128,15 +128,16 @@
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     return 0;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return cutlass::Status::kSuccess;
   }
 
   CUTLASS_HOST_DEVICE
   Sm90AuxStore() { }
 
   CUTLASS_HOST_DEVICE
@@ -203,15 +204,15 @@
       Tensor tC_rAux_frg = recast<Array<Element, FragmentSize>>(coalesce(tC_rAux));                          // (EPI_V)
       tC_rAux_frg(epi_v) = convert_input(frg_input);
 
       return frg_input;
     }
 
     CUTLASS_DEVICE void
-    postvisit(int epi_m, int epi_n, int store_iteration, bool issue_smem_store) {
+    postreduce(int epi_m, int epi_n, int store_iteration, bool issue_smem_store) {
       if constexpr (EnableNullptr) {
         if (params_ptr->is_nullptr) {
           return;
         }
       }
 
       using RLayoutR2S = decltype(cute::layout(TiledR2S{}.get_slice(0).retile_S(RTensor{})));
@@ -220,15 +221,15 @@
       if (issue_smem_store) {
         int store_pipe_index = store_iteration % Stages;
         copy(tiled_r2s, tRS_rAux, tRS_sAux(_,_,_,store_pipe_index));
       }
     }
 
     CUTLASS_DEVICE void
-    step(int epi_m, int epi_n, int store_iteration, bool issue_tma_store) {
+    tma_store(int epi_m, int epi_n, int store_iteration, bool issue_tma_store) {
       if constexpr (EnableNullptr) {
         if (params_ptr->is_nullptr) {
           return;
         }
       }
 
       if (issue_tma_store) {
@@ -325,20 +326,21 @@
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     return 0;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     if constexpr (IsAtomic) {
       auto [M, N, K, L] = problem_shape;
       Layout mScalar_layout = make_layout(make_shape(M,N,L), args.dScalar);
       if (args.ptr_scalar != nullptr) {
-        return fill_workspace(args.ptr_scalar, ElementOutput(args.reduction_identity), cosize(mScalar_layout), stream);
+        return fill_workspace(args.ptr_scalar, ElementOutput(args.reduction_identity), cosize(mScalar_layout), stream, cuda_adapter);
       }
     }
 
     return cutlass::Status::kSuccess;
   }
 
   CUTLASS_DEVICE bool
@@ -448,69 +450,124 @@
 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // Row vector reduction
 template <
   template <class> class RegReduceFn,
+  template <class> class ShuffleReduceFn,
   template <class> class GmemReduceFn,
   int Stages,
   class CtaTileShapeMNK,
   class ElementOutput,
   class ElementCompute,
   FloatRoundStyle RoundStyle,
   class StrideMNL = Stride<_0,_1,_0>,
   int Alignment = 128 / sizeof_bits_v<ElementOutput>,
-  bool EnableNullptr = true // Noop on nullptr params
+  bool EnableNullptr = true, // Noop on nullptr params
+  // If this is false, ptr_row is assumed to point to a compact n-major (ceil_div(M,CTA_M), round_nearest(N,CTA_N), L)
+  // tensor of ElementCompute. It is the user's responsibility to reduce this to a (N, L) tensor of ElementOutput
+  bool FinalReduction = true,
+  // False means skip OOB predication if OOB inputs are known to be the reduction identity
+  bool VisitCheckOOB = true
 >
 struct Sm90RowReduction {
 private:
   static_assert(Stages == 0, "Smem usage not supported yet");
   static_assert(Alignment * sizeof_bits_v<ElementOutput> % 128 == 0, "sub-16B alignment not supported yet");
   static_assert(
     (cute::is_same_v<StrideMNL, Stride<_0,_1, _0>>) || // row vector reduction, e.g. per-col sum over all batches
     (cute::is_same_v<StrideMNL, Stride<_0,_1,int>>));  // batched row vector reduction, e.g. per-col sum per batch
   static constexpr bool IsAtomic = is_atomic<GmemReduceFn<ElementCompute>>::value;
-  static_assert(IsAtomic, "non-atomic row reduction not supported yet");
+  static_assert(not (IsAtomic && not FinalReduction), "atomic reduction must be final");
 
 public:
   struct SharedStorage { };
 
   struct Arguments {
-    ElementOutput* ptr_row = nullptr;
+    void* ptr_row = nullptr; // ElementOutput* if FinalReduction, else ElementCompute*
     ElementCompute reduction_identity = 0;
     StrideMNL dRow = {};
   };
 
-  using Params = Arguments;
+  struct Params {
+    void* ptr_row = nullptr;
+    ElementCompute reduction_identity = 0;
+    StrideMNL dRow = {};
+    ElementCompute* reduction_buffer = nullptr;
+    int* tile_counters = nullptr;
+  };
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
-    return args;
+    ElementCompute* reduction_buffer;
+    int* tile_counters = nullptr;
+    if constexpr (IsAtomic) {
+      reduction_buffer = nullptr;
+    }
+    else if constexpr (not FinalReduction) {
+      reduction_buffer = reinterpret_cast<ElementCompute*>(args.ptr_row);
+    }
+    else {
+      auto [M, N, K, L] = problem_shape;
+      auto [tile_M, tile_N, tile_K] = CtaTileShapeMNK{};
+      size_t tile_counters_offset = product(ceil_div(make_shape(size<>(M), size<>(N), L), make_shape(tile_M, tile_N))) * tile_N * sizeof(ElementCompute);
+      tile_counters_offset = round_nearest(tile_counters_offset, sizeof(int));
+
+      reduction_buffer = reinterpret_cast<ElementCompute*>(workspace);
+      tile_counters = reinterpret_cast<int*>(reinterpret_cast<uint8_t*>(workspace) + tile_counters_offset);
+    }
+
+    return {
+      args.ptr_row,
+      args.reduction_identity,
+      args.dRow,
+      reduction_buffer,
+      tile_counters
+    };
   }
 
   template <class ProblemShape>
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
-    return 0;
+    if constexpr (IsAtomic || not FinalReduction) {
+      return 0;
+    }
+
+    size_t workspace_size = 0;
+    auto [M, N, K, L] = problem_shape;
+    auto [tile_M, tile_N, tile_K] = CtaTileShapeMNK{};
+    // Increment by size of reduction buffer
+    workspace_size += product(ceil_div(make_shape(size<>(M),size<>(N),L), make_shape(tile_M, tile_N))) * tile_N * sizeof(ElementCompute);
+    // Align and increment by size of tile counters
+    workspace_size = round_nearest(workspace_size, sizeof(int));
+    workspace_size += cute::ceil_div(size<>(N), tile_N) * sizeof(int);
+    return workspace_size;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     if constexpr (IsAtomic) {
       auto [M, N, K, L] = problem_shape;
       Layout mRow_layout = make_layout(make_shape(M,N,L), args.dRow);
       if (args.ptr_row != nullptr) {
-        return fill_workspace(args.ptr_row, ElementOutput(args.reduction_identity), cosize(mRow_layout), stream);
+        return fill_workspace(args.ptr_row, ElementOutput(args.reduction_identity), cosize(mRow_layout), stream, cuda_adapter);
       }
+      return Status::kSuccess;
     }
+    auto [M, N, K, L] = problem_shape;
+    auto [tile_M, tile_N, tile_K] = CtaTileShapeMNK{};
+    size_t tile_counters_offset = product(ceil_div(make_shape(size<>(M),size<>(N),L), make_shape(tile_M, tile_N))) * tile_N * sizeof(ElementCompute);
 
-    return cutlass::Status::kSuccess;
+    int* tile_counters = reinterpret_cast<int*>(reinterpret_cast<uint8_t*>(workspace) + tile_counters_offset);
+    size_t tile_counters_size = cute::ceil_div(size<>(N), tile_N) * sizeof(int);
+    return zero_workspace(tile_counters, tile_counters_size, stream);
   }
 
   CUTLASS_DEVICE bool
   is_producer_load_needed() const {
     return false;
   }
 
@@ -530,146 +587,358 @@
 
   template <class... Args>
   CUTLASS_DEVICE auto
   get_producer_load_callbacks(ProducerLoadArgs<Args...> const& args) {
     return EmptyProducerLoadCallbacks{};
   }
 
-  template<class RTensor, class GTensor, class CTensor, class ResidueMN>
+  template<class ArgsTuple>
   struct ConsumerStoreCallbacks : EmptyConsumerStoreCallbacks {
     CUTLASS_DEVICE
-    ConsumerStoreCallbacks(
-        RTensor&& tCrRow,
-        GTensor&& tCgRow,
-        CTensor   tCcRow,
-        ResidueMN residue_mn,
-        Params const& params)
-      : tCrRow(cute::forward<RTensor>(tCrRow)),
-        tCgRow(cute::forward<GTensor>(tCgRow)),
-        tCcRow(tCcRow),
-        residue_mn(residue_mn),
+    ConsumerStoreCallbacks(ArgsTuple&& args_tuple, Params const& params)
+      : args_tuple(cute::forward<ArgsTuple>(args_tuple)),
         params(params) {}
 
-    // gmem store after every column of subtiles, assuming M-major loop
-    // needed to reduce reg pressure, otherwise each thread stores up to a full row in RF
-    // since row-elements aren't evenly distributed amongst threads
-    RTensor tCrRow;                                                                    // (CPY,CPY_M,CPY_N,EPI_M)
-    GTensor tCgRow;                                                                    // (CPY,CPY_M,CPY_N,EPI_M,EPI_N)
-    CTensor tCcRow;                                                                    // (CPY,CPY_M,CPY_N,EPI_M,EPI_N)
-    ResidueMN residue_mn;
+    ArgsTuple args_tuple;
     Params const& params;
+    bool do_final_reduction = false;
+
 
     template <typename ElementAccumulator, typename ElementInput, int FragmentSize>
     CUTLASS_DEVICE auto
     visit(Array<ElementAccumulator, FragmentSize> const& frg_acc, int epi_v, int epi_m, int epi_n,
           Array<ElementInput, FragmentSize> const& frg_input) {
-
       if constexpr (EnableNullptr) {
         if (params.ptr_row == nullptr) {
           return frg_input;
         }
       }
 
+      auto& [ref_src, tCrRow, tCcRow, gRow_l, cRow, gBuf_ml, sBuf_layout,
+        lane_layout_MN, lane_mn, warp_layout_MN, warp_mn,
+        tile_coord_mnkl, residue_mn, epi_tile, tiled_copy, thread_idx] = args_tuple;
+      Tensor tCrRow_mn = tCrRow(_,_,_,epi_m,epi_n);
+      Tensor tCcRow_mn = tCcRow(_,_,_,epi_m,epi_n);
+
       using ConvertInput = NumericArrayConverter<ElementCompute, ElementInput, FragmentSize, RoundStyle>;
       using ReduceInput = RegReduceFn<ElementCompute>;
       ConvertInput convert_input{};
       ReduceInput reduce_input{};
 
       Array frg_I = convert_input(frg_input);
-      Tensor tCrRow_mn = tCrRow(_,_,_,epi_m);
-      Tensor tCcRow_mn = tCcRow(_,_,_,epi_m,epi_n);
-
       CUTLASS_PRAGMA_UNROLL
       for (int i = 0; i < FragmentSize; ++i) {
-        if (elem_less(tCcRow_mn(epi_v * FragmentSize + i), residue_mn)) {
+        if constexpr (VisitCheckOOB) {
+          if (elem_less(tCcRow_mn(epi_v * FragmentSize + i), residue_mn)) {
+            ElementCompute& tCrRow_vmn = tCrRow_mn(epi_v * FragmentSize + i);
+            tCrRow_vmn = reduce_input(tCrRow_vmn, frg_I[i]);
+          }
+        }
+        else {
           ElementCompute& tCrRow_vmn = tCrRow_mn(epi_v * FragmentSize + i);
           tCrRow_vmn = reduce_input(tCrRow_vmn, frg_I[i]);
         }
       }
 
       return frg_input;
     }
 
+    template <class STensor, class SyncFn>
     CUTLASS_DEVICE void
-    step(int epi_m, int epi_n, int store_iteration, bool issue_tma_store) {
+    reduce(STensor&& smem_buffer, SyncFn const& sync_fn, int epi_m, int epi_n, bool is_last_iteration) {
+      if (not is_last_iteration) {
+        return;
+      }
+
+      auto& [ref_src, tCrRow, tCcRow, gRow_l, cRow, gBuf_ml, sBuf_layout,
+        lane_layout_MN, lane_mn, warp_layout_MN, warp_mn,
+        tile_coord_mnkl, residue_mn, epi_tile, tiled_copy, thread_idx] = args_tuple;
+      auto [m, n, k, l] = tile_coord_mnkl;
+      constexpr bool ReferenceSrc = decltype(ref_src)::value;
       if constexpr (EnableNullptr) {
         if (params.ptr_row == nullptr) {
           return;
         }
       }
 
-      if (epi_m == size<3>(tCrRow)-1) { // assumes M-major subtile loop
-        using ConvertI = NumericConverter<ElementOutput, ElementCompute, RoundStyle>;
-        using ReduceInput = GmemReduceFn<ElementOutput>;
+      // fully OOB CTA in partially OOB cluster
+      if (not elem_less(cRow(_0{},_0{}), residue_mn)) {
+        return;
+      }
 
-        ConvertI convert_I{};
-        ReduceInput reduce_input{};
+      //
+      // 1. Warp shuffle reduction
+      //
+      using FragmentShuffle = Array<ElementCompute, sizeof(uint64_t) / sizeof(ElementCompute)>;
+      using ReduceShuffle = ShuffleReduceFn<FragmentShuffle>;
+      ReduceShuffle reduce_shuffle{};
+      Tensor tCrRow_frg = recast<FragmentShuffle>(filter(tCrRow));
+      CUTLASS_PRAGMA_UNROLL
+      for (int reduction_rows = size<0>(lane_layout_MN) / 2; reduction_rows > 0; reduction_rows /= 2) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int frg_idx = 0; frg_idx < size(tCrRow_frg); ++frg_idx) {
+          uint64_t frg_shfl = reinterpret_cast<uint64_t&>(tCrRow_frg(frg_idx));
+          frg_shfl = __shfl_down_sync(0xFFFFFFFF, frg_shfl, lane_layout_MN(reduction_rows, _0{}));
+          tCrRow_frg(frg_idx) = reduce_shuffle(tCrRow_frg(frg_idx), reinterpret_cast<FragmentShuffle&>(frg_shfl));
+        }
+      }
+      bool is_reduced_lane = get<0>(lane_mn) == 0;
 
+      //
+      // 2. Atomic reduction
+      //
+      if constexpr (IsAtomic) {
         // Filter so we don't issue redunant copies over stride-0 modes
-        Tensor tCrRow_flt = filter_zeros(tCrRow(_,_,_,epi_m));
-        Tensor tCgRow_flt = filter_zeros(tCgRow(_,_,_,epi_m,epi_n));
-        Tensor tCcRow_mn  = tCcRow(_,_,_,epi_m,epi_n);
-        Tensor tCcRow_flt = make_tensor(tCcRow_mn.data(), make_layout(tCgRow_flt.shape(), tCcRow_mn.stride()));
+        Tensor tCrRow_flt = filter_zeros(tCrRow);
+        Tensor tCcRow_flt = make_tensor(tCcRow.data(), make_layout(tCrRow_flt.shape(), tCcRow.stride()));
+
+        Tensor tCgRow = sm90_partition_for_epilogue<ReferenceSrc>(gRow_l(_,_,l), epi_tile, tiled_copy, thread_idx);
+        Tensor tCgRow_flt = filter_zeros(tCgRow);
+        // NOTE: atomic reduction is performed in the output type
+        using ConvertOutput = NumericConverter<ElementOutput, ElementCompute, RoundStyle>;
+        using ReduceOutput = GmemReduceFn<ElementOutput>;
+        ConvertOutput convert_output{};
+        ReduceOutput reduce_output{};
+
+        if (is_reduced_lane) {
+          CUTLASS_PRAGMA_UNROLL
+          for (int i = 0; i < size(tCrRow_flt); ++i) {
+            if (elem_less(tCcRow_flt(i), residue_mn)) {
+              reduce_output(&tCgRow_flt(i), convert_output(tCrRow_flt(i)));
+            }
+          }
+        }
+        sync_fn();
+      }
+
+      //
+      // 2. One warp in M, skip threadblock smem reduction
+      //
+      else if constexpr (decltype(size<0>(warp_layout_MN))::value <= 1) {
+        // Dump warp reduction to gmem workspace
+        using ElementGmem = cute::conditional_t<FinalReduction, ElementCompute volatile, ElementCompute>;
+        Tensor tCgBuf = sm90_partition_for_epilogue<ReferenceSrc>(gBuf_ml(_,_,m,l), epi_tile, tiled_copy, thread_idx);
+        if (is_reduced_lane) {
+          // Filter so we don't issue redundant copies over stride-0 modes
+          // (only works if 0-strides are in same location, which is by construction)
+          copy_aligned(filter(tCrRow), recast<ElementGmem>(filter(tCgBuf)));
+        }
+        sync_fn();
+      }
+
+      //
+      // 2. Multiple warps in M, do threadblock smem reduction
+      //
+      else {
+        Tensor sBuf = make_tensor(make_smem_ptr<ElementCompute>(raw_pointer_cast(smem_buffer.data())), sBuf_layout);
+        static_assert(decltype(cosize(sBuf.layout()))::value * sizeof(ElementCompute) <=
+                      decltype(cosize(smem_buffer.layout()))::value * sizeof(typename remove_cvref_t<STensor>::value_type),
+                      "smem reduction buffer not large enough, use a larger epilogue tile");
+
+        // Dump warp reduction to smem workspace
+        Tensor tCsBuf = sm90_partition_for_epilogue<ReferenceSrc>(sBuf(_,_,get<0>(warp_mn)), epi_tile, tiled_copy, thread_idx);
+        if (is_reduced_lane) {
+          // Filter so we don't issue redunant copies over stride-0 modes
+          // (only works if 0-strides are in same location, which is by construction)
+          copy_aligned(filter(tCrRow), filter(tCsBuf));
+        }
+        sync_fn();
+
+        constexpr int SmemFragSize = cute::max(size_t{1}, sizeof(uint32_t) / sizeof(ElementCompute));
+        using FragmentSmem = Array<ElementCompute, SmemFragSize>;
+        using VectorSmem = uint_bit_t<sizeof_bits_v<FragmentSmem>>;
+        using ReduceSmem = GmemReduceFn<FragmentSmem>;
+        ReduceSmem reduce_smem{};
 
+        Tensor sBuf_frg = recast<FragmentSmem>(filter_zeros(sBuf));
+        Tensor sBuf_vec = recast<VectorSmem>(filter_zeros(sBuf));
+        constexpr int FragsPerRow = decltype(size<1>(sBuf_frg))::value;
 
-        auto [residue_m, residue_n] = residue_mn;
+        // Do the threadblock smem reduction
         CUTLASS_PRAGMA_UNROLL
-        for (int i = 0; i < size(tCrRow_flt); ++i) {
-          // partially OOB in M must still issue gmem reduction, so only consider residue_n
-          // in case last epi tile in column is fully OOB in M and CTA tile is partially OOB in M
-          if (residue_n > get<1>(tCcRow_flt(i)) &&
-              // fully OOB in M does not need to issue gmem reduction, skip
-              residue_m > 0) {
-            reduce_input(&tCgRow_flt(i), convert_I(tCrRow_flt(i)));
+        for (int reduction_rows = size<0>(warp_layout_MN) / 2; reduction_rows > 1; reduction_rows /= 2) {
+          int FragsPerReduction = reduction_rows * FragsPerRow;
+          CUTLASS_PRAGMA_NO_UNROLL
+          for (int frg_idx = thread_idx; frg_idx < FragsPerReduction; frg_idx += size(tiled_copy)) {
+            FragmentSmem frg_smem = reduce_smem(sBuf_frg(frg_idx), sBuf_frg(frg_idx + FragsPerReduction));
+            sBuf_vec(frg_idx) = reinterpret_cast<VectorSmem&>(frg_smem);
           }
+          sync_fn();
+        }
+
+        // Do final smem reduction and dump to gmem workspace
+        using VectorGmem = cute::conditional_t<FinalReduction, VectorSmem volatile, VectorSmem>;
+        Tensor gBuf_vec = recast<VectorGmem>(filter(gBuf_ml(_,_,m,l)));
+        CUTLASS_PRAGMA_NO_UNROLL
+        for (int frg_idx = thread_idx; frg_idx < FragsPerRow; frg_idx += size(tiled_copy)) {
+          FragmentSmem frg_smem = reduce_smem(sBuf_frg(frg_idx), sBuf_frg(frg_idx + FragsPerRow));
+          gBuf_vec(frg_idx) = reinterpret_cast<VectorSmem&>(frg_smem);
         }
+        sync_fn();
+      }
 
-        // Reset the registers to the reduction identity
-        fill(tCrRow, params.reduction_identity);
+      //
+      // 3. Increment atomic counters to signal final gmem reduction
+      //
+      if constexpr (not IsAtomic && FinalReduction) {
+        // Ensure gmem writes are visible to other threads before incrementing counter
+        __threadfence();
+        sync_fn();
+        // Collective thread 0 increments atomic tile counter and copies value to smem
+        int* prev_tile_count = reinterpret_cast<int*>(raw_pointer_cast(smem_buffer.data()));
+        if (thread_idx == 0) {
+          *prev_tile_count = atomicAdd(&params.tile_counters[n], 1);
+        }
+        sync_fn();
+        // Broadcast tile count to other threads in CTA and determine final reduction status
+        do_final_reduction = *prev_tile_count == size<2>(gBuf_ml) * size<3>(gBuf_ml) - 1;
+        sync_fn();
       }
     }
 
+    CUTLASS_DEVICE void
+    end() {
+      //
+      // 4. Do final gmem reduction if necessary
+      //
+      if constexpr (not IsAtomic && FinalReduction) {
+        if (not do_final_reduction) {
+          return;
+        }
+
+        auto& [ref_src, tCrRow, tCcRow, gRow_l, cRow, gBuf_ml, sBuf_layout,
+          lane_layout_MN, lane_mn, warp_layout_MN, warp_mn,
+          tile_coord_mnkl, residue_mn, epi_tile, tiled_copy, thread_idx] = args_tuple;
+
+        using ReduceOutput = GmemReduceFn<ElementCompute>;
+        using ConvertOutput = NumericConverter<ElementOutput, ElementCompute, RoundStyle>;
+        ReduceOutput reduce_output{};
+        ConvertOutput convert_output{};
+
+        // Reduction over batches
+        if (size<2>(stride(gRow_l)) == 0) {
+          CUTLASS_PRAGMA_NO_UNROLL
+          for (int n = thread_idx; n < size<1>(gBuf_ml); n += size(tiled_copy)) {
+            Tensor tRgBuf_ml = gBuf_ml(_0{},n,_,_);
+            ElementCompute output = tRgBuf_ml(_0{});
+            CUTLASS_PRAGMA_NO_UNROLL
+            for (int ml = 1; ml < size(tRgBuf_ml); ++ml) {
+              output = reduce_output(output, tRgBuf_ml(ml));
+            }
+            if (elem_less(cRow(_0{},n), residue_mn)) {
+              gRow_l(_0{},n,_0{}) = convert_output(output);
+            }
+          }
+        }
+        // No reduction over batches
+        else {
+          CUTLASS_PRAGMA_NO_UNROLL
+          for (int n = thread_idx; n < size<1>(gBuf_ml); n += size(tiled_copy)) {
+            bool do_store = elem_less(cRow(_0{},n), residue_mn);
+            CUTLASS_PRAGMA_NO_UNROLL
+            for (int l = 0; l < size<3>(gBuf_ml); ++l) {
+              Tensor tRgBuf_m = gBuf_ml(_0{},n,_,l);
+              ElementCompute output = tRgBuf_m(_0{});
+              CUTLASS_PRAGMA_NO_UNROLL
+              for (int m = 1; m < size(tRgBuf_m); ++m) {
+                output = reduce_output(output, tRgBuf_m(m));
+              }
+              if (do_store) {
+                gRow_l(_0{},n,l) = convert_output(output);
+              }
+            }
+          }
+        }
+
+      }
+    }
   };
 
   template <
     bool ReferenceSrc, // do register tensors reference the src or dst layout of the tiled copy
     class... Args
   >
   CUTLASS_DEVICE auto
   get_consumer_store_callbacks(ConsumerStoreArgs<Args...> const& args) {
+    Layout ref_layout_MN = [&] () {
+      if constexpr (ReferenceSrc) { return get<0>(args.tiled_copy.get_layoutS_MN()); }
+      else                        { return get<0>(args.tiled_copy.get_layoutD_MN()); }
+    }();                                                                                         // tile_mn -> tv_idx
 
+    // Get the MN layout + coord of lanes to determine shuffle reduction iterations
+    using _W = Int<decltype(args.tiled_copy)::TiledNumThr::value / NumThreadsPerWarp>;
+    Layout tv2lane = Layout<Shape<Int<NumThreadsPerWarp>,_W,_1>,Stride<_1,_0,_0>>{};            //   tv_idx -> lane_idx
+    Layout ref2lane = composition(tv2lane, ref_layout_MN);                                      //  tile_mn -> lane_idx
+    Layout lane_layout_MN = make_layout(filter(get<0>(ref2lane)), filter(get<1>(ref2lane)));    //  lane_mn -> lane_idx
+    Layout inv_lane_layout_MN = right_inverse(lane_layout_MN);                                  // lane_idx -> lane_mn
+    int lane_idx = canonical_lane_idx();
+    auto lane_mn = idx2crd(inv_lane_layout_MN(lane_idx), shape(lane_layout_MN));
+
+    // Get the MN layout + coord of warps to determine smem reduction iterations
+    Layout tv2warp = Layout<Shape<Int<NumThreadsPerWarp>,_W,_1>,Stride<_0,_1,_0>>{};            //   tv_idx -> warp_idx
+    Layout ref2warp = composition(tv2warp, ref_layout_MN);                                      //  tile_mn -> warp_idx
+    Layout warp_layout_MN = make_layout(filter(get<0>(ref2warp)), filter(get<1>(ref2warp)));    //  warp_mn -> warp_idx
+    Layout inv_warp_layout_MN = right_inverse(warp_layout_MN);                                  // warp_idx -> warp_mn
+
+    int warp_idx = args.thread_idx / NumThreadsPerWarp;
+    auto warp_mn = idx2crd(inv_warp_layout_MN(warp_idx), shape(warp_layout_MN));
+
+    // Partition output gmem and register tensors
+    auto [tile_M, tile_N, tile_K] = args.tile_shape_mnk;
     auto [M, N, K, L] = args.problem_shape_mnkl;
-    Tensor mRow = make_tensor(make_gmem_ptr(params.ptr_row), make_shape(M,N,L), params.dRow);
+    auto [m, n, k, l] = args.tile_coord_mnkl;
+
+    Tensor mRow = make_tensor(make_gmem_ptr<ElementOutput>(params.ptr_row), make_shape(M,N,L), params.dRow); // (M,N,L)
+    Tensor gRow_l = local_tile(mRow, take<0,2>(args.tile_shape_mnk), make_coord(m,n,_));             // (CTA_M,CTA_N,L)
     Tensor tCgRow = sm90_partition_for_epilogue<ReferenceSrc>(                         // (CPY,CPY_M,CPY_N,EPI_M,EPI_N)
-      mRow, args.tile_shape_mnk, args.tile_coord_mnkl, args.epi_tile, args.tiled_copy, args.thread_idx);
-    Tensor tCrRow = make_tensor_like<ElementCompute>(tCgRow(_,_,_,_,_0{}));            // (CPY,CPY_M,CPY_N,EPI_M)
+      gRow_l(_,_,l), args.epi_tile, args.tiled_copy, args.thread_idx);
+    Tensor tCrRow = make_tensor_like<ElementCompute>(tCgRow);                          // (CPY,CPY_M,CPY_N,EPI_M,EPI_N)
+
     fill(tCrRow, params.reduction_identity);
 
-    return ConsumerStoreCallbacks<decltype(tCrRow),decltype(tCgRow),decltype(args.tCcD),decltype(args.residue_mn)>(
-      cute::move(tCrRow), cute::move(tCgRow), args.tCcD, args.residue_mn, params);
+    // Partition gmem+smem reduction buffer tensors
+    Layout gBuf_layout = make_layout(take<0,2>(args.tile_shape_mnk), make_stride(_0{}, _1{}));
+    auto block_shape = ceil_div(make_shape(M,N,L), shape(gBuf_layout)); // (M_CNT, N_CNT, L_CNT)
+
+    // Let the M_CNT (the num of partial reduction results) become the outer mode
+    Layout block_layout = make_layout(block_shape, make_stride(get<1>(block_shape), _1{}, get<0>(block_shape) * get<1>(block_shape)));
+    Layout mBuf_layout = blocked_product(gBuf_layout, block_layout);
+    Tensor mBuf = make_tensor(make_gmem_ptr(params.reduction_buffer), mBuf_layout);                // (ceil_M,ceil_N,L)
+    Tensor gBuf_ml = local_tile(mBuf, take<0,2>(args.tile_shape_mnk), make_coord(_,n,_));     // (CTA_M,CTA_N,REST_M,L)
+    Layout sBuf_layout = blocked_product(gBuf_layout,                                          // (CTA_M,CTA_N,WARPS_M)
+      make_layout(make_shape(_1{},_1{},size<0>(warp_layout_MN))));
+
+    auto args_tuple = make_tuple(
+        bool_constant<ReferenceSrc>{}, cute::move(tCrRow), args.tCcD, gRow_l, args.cD, gBuf_ml, sBuf_layout,
+        lane_layout_MN, lane_mn, warp_layout_MN, warp_mn,
+        args.tile_coord_mnkl, args.residue_mn, args.epi_tile, args.tiled_copy, args.thread_idx);
+    return ConsumerStoreCallbacks<decltype(args_tuple)>(cute::move(args_tuple), params);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // Col vector reduction
 template <
   template <class> class RegReduceFn,
+  template <class> class ShuffleReduceFn,
   template <class> class GmemReduceFn,
   int Stages,
   class CtaTileShapeMNK,
   class ElementOutput,
   class ElementCompute,
   FloatRoundStyle RoundStyle,
   class StrideMNL = Stride<_1,_0,_0>,
   int Alignment = 128 / sizeof_bits_v<ElementOutput>,
   bool EnableNullptr = true, // Noop on nullptr params
   // If this is false, ptr_col is assumed to point to a compact m-major (round_nearest(M,CTA_M), ceil_div(N,CTA_N), L)
   // tensor of ElementCompute. It is the user's responsibility to reduce this to a (M, L) tensor of ElementOutput
-  bool FinalReduction = true
+  bool FinalReduction = true,
+  // False means skip OOB predication if OOB inputs are known to be the reduction identity
+  bool VisitCheckOOB = true
 >
 struct Sm90ColReduction {
 private:
   static_assert(Stages == 0, "Smem usage not supported yet");
   static_assert(Alignment * sizeof_bits_v<ElementOutput> % 128 == 0, "sub-16B alignment not supported yet");
   static_assert(
     (cute::is_same_v<StrideMNL, Stride<_1,_0, _0>>) || // col vector reduction, e.g. per-row sum over all batches
@@ -694,22 +963,20 @@
     int* tile_counters = nullptr;
   };
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
     ElementCompute* reduction_buffer;
-    int* tile_counters;
+    int* tile_counters = nullptr;
     if constexpr (IsAtomic) {
       reduction_buffer = nullptr;
-      tile_counters = nullptr;
     }
     else if constexpr (not FinalReduction) {
       reduction_buffer = reinterpret_cast<ElementCompute*>(args.ptr_col);
-      tile_counters = nullptr;
     }
     else {
       auto [M, N, K, L] = problem_shape;
       auto [tile_M, tile_N, tile_K] = CtaTileShapeMNK{};
       size_t tile_counters_offset = product(ceil_div(make_shape(M,N,L), make_shape(tile_M, tile_N))) * tile_M * sizeof(ElementCompute);
       tile_counters_offset = round_nearest(tile_counters_offset, sizeof(int));
 
@@ -744,20 +1011,21 @@
     workspace_size += cute::ceil_div(M, tile_M) * sizeof(int);
 
     return workspace_size;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     if constexpr (IsAtomic) {
       auto [M, N, K, L] = problem_shape;
       Layout mCol_layout = make_layout(make_shape(M,N,L), args.dCol);
       if (args.ptr_col != nullptr) {
-        return fill_workspace(args.ptr_col, ElementOutput(args.reduction_identity), cosize(mCol_layout), stream);
+        return fill_workspace(args.ptr_col, ElementOutput(args.reduction_identity), cosize(mCol_layout), stream, cuda_adapter);
       }
       return Status::kSuccess;
     }
 
     auto [M, N, K, L] = problem_shape;
     auto [tile_M, tile_N, tile_K] = CtaTileShapeMNK{};
     size_t tile_counters_offset = product(ceil_div(make_shape(M,N,L), make_shape(tile_M, tile_N))) * tile_M * sizeof(ElementCompute);
@@ -824,17 +1092,25 @@
       using ReduceInput = RegReduceFn<ElementCompute>;
       ConvertInput convert_input{};
       ReduceInput reduce_input{};
 
       Array frg_I = convert_input(frg_input);
       CUTLASS_PRAGMA_UNROLL
       for (int i = 0; i < FragmentSize; ++i) {
-        if (elem_less(tCcCol_mn(epi_v * FragmentSize + i), residue_mn)) {
-          ElementCompute& tCrCol_vmn = tCrCol_mn(epi_v * FragmentSize + i);
-          tCrCol_vmn = reduce_input(tCrCol_vmn, frg_I[i]);
+        if constexpr (VisitCheckOOB) {
+          if (elem_less(tCcCol_mn(epi_v * FragmentSize + i), residue_mn)) {
+            ElementCompute& tCrCol_vmn = tCrCol_mn(epi_v * FragmentSize + i);
+            tCrCol_vmn = reduce_input(tCrCol_vmn, frg_I[i]);
+          }
+        }
+        else {
+          if (elem_less(tCcCol_mn(epi_v * FragmentSize + i), residue_mn)) {
+            ElementCompute& tCrCol_vmn = tCrCol_mn(epi_v * FragmentSize + i);
+            tCrCol_vmn = reduce_input(tCrCol_vmn, frg_I[i]);
+          }
         }
       }
 
       return frg_input;
     }
 
     template <class STensor, class SyncFn>
@@ -862,15 +1138,15 @@
         return;
       }
 
       //
       // 1. Warp shuffle reduction
       //
       using FragmentShuffle = Array<ElementCompute, sizeof(uint64_t) / sizeof(ElementCompute)>;
-      using ReduceShuffle = RegReduceFn<FragmentShuffle>;
+      using ReduceShuffle = ShuffleReduceFn<FragmentShuffle>;
       ReduceShuffle reduce_shuffle{};
       Tensor tCrCol_frg = recast<FragmentShuffle>(filter(tCrCol));
       CUTLASS_PRAGMA_UNROLL
       for (int reduction_cols = size<1>(lane_layout_MN) / 2; reduction_cols > 0; reduction_cols /= 2) {
         CUTLASS_PRAGMA_UNROLL
         for (int frg_idx = 0; frg_idx < size(tCrCol_frg); ++frg_idx) {
           uint64_t frg_shfl = reinterpret_cast<uint64_t&>(tCrCol_frg(frg_idx));
@@ -909,15 +1185,15 @@
       }
 
       //
       // 2. One warp in N, skip threadblock smem reduction
       //
       else if constexpr (decltype(size<1>(warp_layout_MN))::value <= 1) {
         // Dump warp reduction to gmem workspace
-        using ElementGmem = conditional_t<FinalReduction, ElementCompute volatile, ElementCompute>;
+        using ElementGmem = cute::conditional_t<FinalReduction, ElementCompute volatile, ElementCompute>;
         Tensor tCgBuf = sm90_partition_for_epilogue<ReferenceSrc>(gBuf_nl(_,_,n,l), epi_tile, tiled_copy, thread_idx);
         if (is_reduced_lane) {
           // Filter so we don't issue redundant copies over stride-0 modes
           // (only works if 0-strides are in same location, which is by construction)
           copy_aligned(filter(tCrCol), recast<ElementGmem>(filter(tCgBuf)));
         }
         sync_fn();
@@ -937,15 +1213,15 @@
         if (is_reduced_lane) {
           // Filter so we don't issue redunant copies over stride-0 modes
           // (only works if 0-strides are in same location, which is by construction)
           copy_aligned(filter(tCrCol), filter(tCsBuf));
         }
         sync_fn();
 
-        constexpr int SmemFragSize = cute::max(1, sizeof(uint32_t) / sizeof(ElementCompute));
+        constexpr int SmemFragSize = cute::max(size_t{1}, sizeof(uint32_t) / sizeof(ElementCompute));
         using FragmentSmem = Array<ElementCompute, SmemFragSize>;
         using VectorSmem = uint_bit_t<sizeof_bits_v<FragmentSmem>>;
         using ReduceSmem = GmemReduceFn<FragmentSmem>;
         ReduceSmem reduce_smem{};
 
         Tensor sBuf_frg = recast<FragmentSmem>(filter_zeros(sBuf));
         Tensor sBuf_vec = recast<VectorSmem>(filter_zeros(sBuf));
@@ -960,15 +1236,15 @@
             FragmentSmem frg_smem = reduce_smem(sBuf_frg(frg_idx), sBuf_frg(frg_idx + FragsPerReduction));
             sBuf_vec(frg_idx) = reinterpret_cast<VectorSmem&>(frg_smem);
           }
           sync_fn();
         }
 
         // Do final smem reduction and dump to gmem workspace
-        using VectorGmem = conditional_t<FinalReduction, VectorSmem volatile, VectorSmem>;
+        using VectorGmem = cute::conditional_t<FinalReduction, VectorSmem volatile, VectorSmem>;
         Tensor gBuf_vec = recast<VectorGmem>(filter(gBuf_nl(_,_,n,l)));
         CUTLASS_PRAGMA_NO_UNROLL
         for (int frg_idx = thread_idx; frg_idx < FragsPerCol; frg_idx += size(tiled_copy)) {
           FragmentSmem frg_smem = reduce_smem(sBuf_frg(frg_idx), sBuf_frg(frg_idx + FragsPerCol));
           gBuf_vec(frg_idx) = reinterpret_cast<VectorSmem&>(frg_smem);
         }
         sync_fn();
@@ -1046,26 +1322,14 @@
             }
           }
         }
 
       }
     }
 
-    CUTLASS_DEVICE bool
-    is_reduction_buffer_needed(int epi_m, int epi_n, bool is_last_iteration) const {
-      auto const& [ref_src, tCrCol, tCcCol, gCol_l, cCol, gBuf_nl, sBuf_layout,
-                    lane_layout_MN, lane_mn, warp_layout_MN, warp_mn,
-                    tile_coord_mnkl, residue_mn, epi_tile, tiled_copy, thread_idx] = args_tuple;
-
-      return (not IsAtomic &&                                  // atomic reduction doesn't use smem
-              is_last_iteration &&                             // smem reduction happens after epilogue loop
-              (decltype(size<1>(warp_layout_MN))::value > 1 || // smem reduction happens when multiple warps are in N
-               FinalReduction));                               // smem is used to broadcast tile counters for final reduction
-    }
-
   };
 
   template <
     bool ReferenceSrc, // do register tensors reference the src or dst layout of the tiled copy
     class... Args
   >
   CUTLASS_DEVICE auto
@@ -1092,14 +1356,15 @@
     int warp_idx = args.thread_idx / NumThreadsPerWarp;
     auto warp_mn = idx2crd(inv_warp_layout_MN(warp_idx), shape(warp_layout_MN));
 
     // Partition output gmem and register tensors
     auto [tile_M, tile_N, tile_K] = args.tile_shape_mnk;
     auto [M, N, K, L] = args.problem_shape_mnkl;
     auto [m, n, k, l] = args.tile_coord_mnkl;
+
     Tensor mCol = make_tensor(make_gmem_ptr<ElementOutput>(params.ptr_col), make_shape(M,N,L), params.dCol); // (M,N,L)
     Tensor gCol_l = local_tile(mCol, take<0,2>(args.tile_shape_mnk), make_coord(m,n,_));             // (CTA_M,CTA_N,L)
     Tensor tCgCol = sm90_partition_for_epilogue<ReferenceSrc>(                         // (CPY,CPY_M,CPY_N,EPI_M,EPI_N)
                       gCol_l(_,_,l), args.epi_tile, args.tiled_copy, args.thread_idx);
     Tensor tCrCol = make_tensor_like<ElementCompute>(tCgCol);                          // (CPY,CPY_M,CPY_N,EPI_M,EPI_N)
     fill(tCrCol, params.reduction_identity);
```

## cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_tma_warpspecialized.hpp

```diff
@@ -95,15 +95,18 @@
     Tensor<Engine, LayoutMNL> mT,  // (M,N,L)
     TileShapeMNK tile_shape_mnk,   // (CTA_M,CTA_N,CTA_K)
     TileCoordMNKL tile_coord_mnkl, // (m,n,k,l)
     EpilogueTile epi_tile,         // (EPI_TILE_M,EPI_TILE_N)
     TiledCopy tiled_copy,
     int thread_idx) {
   auto [m, n, k, l] = tile_coord_mnkl;
-  Tensor cT = local_tile(mT, take<0,2>(tile_shape_mnk), make_coord(m,n,l));                            // (CTA_M,CTA_N)
+  auto coord_shape =
+      make_coord(m, n, l)
+    ;
+  Tensor cT = local_tile(mT, take<0,2>(tile_shape_mnk), coord_shape);                                  // (CTA_M,CTA_N)
   Tensor tCcT =
     sm90_partition_for_epilogue<ReferenceSrc>(cT, epi_tile, tiled_copy, thread_idx);   // (CPY,CPY_M,CPY_N,EPI_M,EPI_N)
 
   return tCcT;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -130,15 +133,15 @@
   CUTLASS_DEVICE
   ProducerLoadArgs(
       ProblemShapeMNKL problem_shape_mnkl,
       TileShapeMNK tile_shape_mnk,
       TileCoordMNKL tile_coord_mnkl,
       ResidueMN residue_mn,
       EpilogueTile epi_tile,
-      int thread_idx) 
+      int thread_idx)
   : problem_shape_mnkl(problem_shape_mnkl),
     tile_shape_mnk(tile_shape_mnk),
     tile_coord_mnkl(tile_coord_mnkl),
     residue_mn(residue_mn),
     epi_tile(epi_tile),
     thread_idx(thread_idx) {}
 };
@@ -173,15 +176,15 @@
       TileCoordMNKL tile_coord_mnkl,
       ResidueMN residue_mn,
       EpilogueTile epi_tile,
       TiledCopy tiled_copy,
       int thread_idx,
       CoordTensor cD,
       ThrCoordTensor tCcD,
-      ThrSrcTensor const& tCrC) 
+      ThrSrcTensor const& tCrC)
   : problem_shape_mnkl(problem_shape_mnkl),
     tile_shape_mnk(tile_shape_mnk),
     tile_coord_mnkl(tile_coord_mnkl),
     residue_mn(residue_mn),
     epi_tile(epi_tile),
     tiled_copy(tiled_copy),
     thread_idx(thread_idx),
@@ -198,18 +201,24 @@
   using Arguments = tuple<typename Ops::Arguments...>;
   // Device side fusion params (Kernel-entry API)
   using Params = tuple<typename Ops::Params...>;
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
+    uint8_t* op_workspace = reinterpret_cast<uint8_t*>(workspace);
     return transform_apply(tuple<Ops...>{}, args,
       [&] (auto&& op, auto const& op_args) {
         using Op = cute::remove_cvref_t<decltype(op)>;
-        return Op::to_underlying_arguments(problem_shape, op_args, workspace);
+        auto ret = Op::to_underlying_arguments(problem_shape, op_args, op_workspace);
+        if (op_workspace != nullptr) {
+          size_t op_workspace_size = Op::get_workspace_size(problem_shape, op_args);
+          op_workspace += round_nearest(op_workspace_size, MinWorkspaceAlignment);
+        }
+        return ret;
       },
       [] (auto&&... op_params) { return cute::make_tuple(op_params...); }
     );
   }
 
   template <class ProblemShape>
   static size_t
@@ -224,34 +233,35 @@
         return (0 + ... + op_workspace_size);
       }
     );
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     Status status = Status::kSuccess;
     uint8_t* op_workspace = reinterpret_cast<uint8_t*>(workspace);
     return transform_apply(tuple<Ops...>{}, args,
       // Initialize each operation's workspace, stopping at the first error
       [&] (auto&& op, auto const& op_args) {
         if (status != Status::kSuccess) {
           return status;
         }
 
         using Op = cute::remove_cvref_t<decltype(op)>;
-        status = Op::initialize_workspace(problem_shape, op_args, op_workspace, stream);
+        status = Op::initialize_workspace(problem_shape, op_args, op_workspace, stream, cuda_adapter);
         if (op_workspace != nullptr) {
           size_t op_workspace_size = Op::get_workspace_size(problem_shape, op_args);
           op_workspace += round_nearest(op_workspace_size, MinWorkspaceAlignment);
         }
         return status;
       },
       // Return the final status
-      [&] (auto const&...) { return status; }
+      [&] (auto const&...ops) { return status; }
     );
   }
 
   CUTLASS_HOST_DEVICE
   Sm90VisitorImplBase() {}
 
   CUTLASS_HOST_DEVICE
@@ -292,29 +302,29 @@
   // Is a specialized warp for producer TMA loads needed
   // e.g. Aux tensor loads, broadcasts using TMA bulk copy
   // This condition cannot change between work tiles because it is used
   // to determine whether the load warp should exit early or not
   // e.g. for batched beta this must always be true regardless of current batch idx
   CUTLASS_DEVICE bool
   is_producer_load_needed() const {
-    return apply(ops,
+    return cute::apply(ops,
       [] (auto const&... op) {
         return (false || ... || op.is_producer_load_needed());
       }
     );
   }
 
   // Is a producer TMA load specifically for C needed
   // If this is true then is_producer_load_needed must also be true
   // This condition can change between work tiles because it is only used
   // to determine whether the TMA and smem loads for C of a given tile should happen
   // e.g. for batched beta this can be false depending on current batch idx
   CUTLASS_DEVICE bool
   is_C_load_needed() const {
-    return apply(ops,
+    return cute::apply(ops,
       [] (auto const&... op) {
         return (false || ... || op.is_C_load_needed());
       }
     );
   }
 
   //
@@ -413,57 +423,49 @@
     // Perform the fused elementwise computation
     template <typename ElementAccumulator, typename... ElementInputs, int FragmentSize>
     CUTLASS_DEVICE auto // returns an Array
     visit(Array<ElementAccumulator, FragmentSize> const& frg_acc, int epi_v, int epi_m, int epi_n,
           Array<ElementInputs, FragmentSize> const&... frg_inputs) // depends on the N-naryness of the op
       = delete; // Must be implemented for each operation
 
-    // After visit call, before smem async fence. Smem stores usually performed here.
-    // Upon exit, all smem stores for TMA must have been issued
+    // After visit call. Smem reductions usually performed here
+    // reduction_buffer is an arbitrary smem tensor that can be used for workspace
+    // It is each nodes reponsibility to assert that this buffer is sufficiently sized
+    // and to ensure that this buffer is no longer needed upon callback exit
+    // i.e. results are synchronized and no longer in the reduction buffer
+    template <class STensor, class SyncFn>
     CUTLASS_DEVICE void
-    postvisit(int epi_m, int epi_n, int store_iteration, bool issue_smem_store) {
+    reduce(STensor&& reduction_buffer, SyncFn const& sync_fn, int epi_m, int epi_n, bool is_last_iteration) {
       for_each(callbacks_tuple,
         [&] (auto& callbacks) {
-          callbacks.postvisit(epi_m, epi_n, store_iteration, issue_smem_store);
+          callbacks.reduce(reduction_buffer, sync_fn, epi_m, epi_n, is_last_iteration);
         }
       );
     }
 
-    // After async fence, before TMA store commit. Aux stores usually performed here
-    // Upon exit, all TMA stores for this subtile must have been issued
+    // After reduce call, before smem async fence. Smem stores usually performed here.
+    // Upon exit, all smem stores for TMA must have been issued
     CUTLASS_DEVICE void
-    step(int epi_m, int epi_n, int store_iteration, bool issue_tma_store) {
+    postreduce(int epi_m, int epi_n, int store_iteration, bool issue_smem_store) {
       for_each(callbacks_tuple,
         [&] (auto& callbacks) {
-          callbacks.step(epi_m, epi_n, store_iteration, issue_tma_store);
+          callbacks.postreduce(epi_m, epi_n, store_iteration, issue_smem_store);
         }
       );
     }
 
-    // After TMA store commit. Smem reductions usually performed here
-    // reduction_buffer is an arbitrary smem tensor that can be used for workspace
-    // It is each nodes reponsibility to assert that this buffer is sufficiently sized
-    // and to ensure that this buffer is no longer needed upon callback exit
-    // i.e. results are synchronized and no longer in the reduction buffer
-    template <class STensor, class SyncFn>
+    // After smem async fence, before TMA store commit. Aux stores usually performed here
+    // Upon exit, all TMA stores for this subtile must have been issued
+    // Because of the TMA store delay optimization, this entry point must ONLY be used for TMA stores
+    // other gmem stores can be placed in the reduce or postreduce entry points
     CUTLASS_DEVICE void
-    reduce(STensor&& reduction_buffer, SyncFn const& sync_fn, int epi_m, int epi_n, bool is_last_iteration) {
+    tma_store(int epi_m, int epi_n, int store_iteration, bool issue_tma_store) {
       for_each(callbacks_tuple,
         [&] (auto& callbacks) {
-          callbacks.reduce(reduction_buffer, sync_fn, epi_m, epi_n, is_last_iteration);
-        }
-      );
-    }
-
-    // Collective can query this to determine whether a buffer needs to be freed for reduction
-    CUTLASS_DEVICE bool
-    is_reduction_buffer_needed(int epi_m, int epi_n, bool is_last_iteration) const {
-      return apply(callbacks_tuple,
-        [&] (auto const&... callbacks) {
-          return (false || ... || callbacks.is_reduction_buffer_needed(epi_m, epi_n, is_last_iteration));
+          callbacks.tma_store(epi_m, epi_n, store_iteration, issue_tma_store);
         }
       );
     }
 
     // Exit of subtile store loop. Gmem reductions usually performed here.
     CUTLASS_DEVICE void
     end() {
@@ -588,18 +590,17 @@
 
     template <typename ElementAccumulator, int FragmentSize>
     CUTLASS_DEVICE auto
     visit(Array<ElementAccumulator, FragmentSize> const& frg_acc, int epi_v, int epi_m, int epi_n) {
       Array frg_input = get<0>(callbacks_tuple).visit(frg_acc, epi_v, epi_m, epi_n);
 
       constexpr int Rm2 = sizeof...(AuxOutTrees);
-      cute::detail::for_sequence(make_seq<Rm2>{}, // restrict the sequence to aux out trees
-        [&] (auto&& _I) {
-          constexpr int i = remove_cvref_t<decltype(_I)>::value;
-          get<i+1>(callbacks_tuple).visit(frg_input, epi_v, epi_m, epi_n);
+      cute::for_each(make_seq<Rm2>{}, // restrict the sequence to aux out trees
+        [&] (auto I) {
+          get<I+1>(callbacks_tuple).visit(frg_input, epi_v, epi_m, epi_n);
         }
       );
 
       return get<Rm2+1>(callbacks_tuple).visit(frg_input, epi_v, epi_m, epi_n);
     }
   };
 
@@ -660,15 +661,15 @@
             },
             // Get inputs in the sequence given by the children indices of the current op
             edge_seq
           );
           return frg_compute; // unused
         },
         // Visit the last op
-        [&] (auto const&...) {
+        [&] (auto const&...ops) {
           return cute::detail::apply(frg_compute_tuple,
             // Compute the last op with children inputs
             [&] (auto const&... frg_inputs) {
               return get<Rm1>(callbacks_tuple).visit(frg_acc, epi_v, epi_m, epi_n, frg_inputs...);
             },
             // Get inputs in the sequence given by the children indices of the last op
             get<Rm1>(EdgeTuple{})
@@ -730,20 +731,21 @@
     workspace_size = round_nearest(workspace_size, MinWorkspaceAlignment);
 
     return workspace_size;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     Status status = Status::kSuccess;
     uint8_t* workspace_ptr = reinterpret_cast<uint8_t*>(workspace);
     size_t workspace_offset = 0;
 
-    status = Op0::initialize_workspace(problem_shape, args.op_0, workspace_ptr + workspace_offset, stream);
+    status = Op0::initialize_workspace(problem_shape, args.op_0, workspace_ptr + workspace_offset, stream, cuda_adapter);
     workspace_offset += Op0::get_workspace_size(problem_shape, args.op_0);
     workspace_offset = round_nearest(workspace_offset, MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
     return status;
@@ -778,17 +780,20 @@
     typename Op0::Params op_0;
     typename Op1::Params op_1;
   };
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
+    size_t op_0_workspace_size = Op0::get_workspace_size(problem_shape, args.op_0);
+    uint8_t* op_0_workspace = reinterpret_cast<uint8_t*>(workspace);
+    uint8_t* op_1_workspace = op_0_workspace + op_0_workspace_size;
     return Params{
-      Op0::to_underlying_arguments(problem_shape, args.op_0, workspace),
-      Op1::to_underlying_arguments(problem_shape, args.op_1, workspace)
+      Op0::to_underlying_arguments(problem_shape, args.op_0, op_0_workspace),
+      Op1::to_underlying_arguments(problem_shape, args.op_1, op_1_workspace)
     };
   }
 
   template <class ProblemShape>
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     size_t workspace_size = 0;
@@ -799,27 +804,28 @@
     workspace_size = round_nearest(workspace_size, MinWorkspaceAlignment);
 
     return workspace_size;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     Status status = Status::kSuccess;
     uint8_t* workspace_ptr = reinterpret_cast<uint8_t*>(workspace);
     size_t workspace_offset = 0;
 
-    status = Op0::initialize_workspace(problem_shape, args.op_0, workspace_ptr + workspace_offset, stream);
+    status = Op0::initialize_workspace(problem_shape, args.op_0, workspace_ptr + workspace_offset, stream, cuda_adapter);
     workspace_offset += Op0::get_workspace_size(problem_shape, args.op_0);
     workspace_offset = round_nearest(workspace_offset, MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
-    status = Op1::initialize_workspace(problem_shape, args.op_1, workspace_ptr + workspace_offset, stream);
+    status = Op1::initialize_workspace(problem_shape, args.op_1, workspace_ptr + workspace_offset, stream, cuda_adapter);
     workspace_offset += Op1::get_workspace_size(problem_shape, args.op_1);
     workspace_offset = round_nearest(workspace_offset, MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
     return status;
@@ -858,18 +864,23 @@
     typename Op1::Params op_1;
     typename Op2::Params op_2;
   };
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
+    size_t op_0_workspace_size = Op0::get_workspace_size(problem_shape, args.op_0);
+    size_t op_1_workspace_size = Op1::get_workspace_size(problem_shape, args.op_1);
+    uint8_t* op_0_workspace = reinterpret_cast<uint8_t*>(workspace);
+    uint8_t* op_1_workspace = op_0_workspace + op_0_workspace_size;
+    uint8_t* op_2_workspace = op_1_workspace + op_1_workspace_size;
     return Params{
-      Op0::to_underlying_arguments(problem_shape, args.op_0, workspace),
-      Op1::to_underlying_arguments(problem_shape, args.op_1, workspace),
-      Op2::to_underlying_arguments(problem_shape, args.op_2, workspace)
+      Op0::to_underlying_arguments(problem_shape, args.op_0, op_0_workspace),
+      Op1::to_underlying_arguments(problem_shape, args.op_1, op_1_workspace),
+      Op2::to_underlying_arguments(problem_shape, args.op_2, op_2_workspace)
     };
   }
 
   template <class ProblemShape>
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     size_t workspace_size = 0;
@@ -883,34 +894,35 @@
     workspace_size = round_nearest(workspace_size, MinWorkspaceAlignment);
 
     return workspace_size;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     Status status = Status::kSuccess;
     uint8_t* workspace_ptr = reinterpret_cast<uint8_t*>(workspace);
     size_t workspace_offset = 0;
 
-    status = Op0::initialize_workspace(problem_shape, args.op_0, workspace_ptr + workspace_offset, stream);
+    status = Op0::initialize_workspace(problem_shape, args.op_0, workspace_ptr + workspace_offset, stream, cuda_adapter);
     workspace_offset += Op0::get_workspace_size(problem_shape, args.op_0);
     workspace_offset = round_nearest(workspace_offset, MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
-    status = Op1::initialize_workspace(problem_shape, args.op_1, workspace_ptr + workspace_offset, stream);
+    status = Op1::initialize_workspace(problem_shape, args.op_1, workspace_ptr + workspace_offset, stream, cuda_adapter);
     workspace_offset += Op1::get_workspace_size(problem_shape, args.op_1);
     workspace_offset = round_nearest(workspace_offset, MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
-    status = Op2::initialize_workspace(problem_shape, args.op_2, workspace_ptr + workspace_offset, stream);
+    status = Op2::initialize_workspace(problem_shape, args.op_2, workspace_ptr + workspace_offset, stream, cuda_adapter);
     workspace_offset += Op2::get_workspace_size(problem_shape, args.op_2);
     workspace_offset = round_nearest(workspace_offset, MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
     return status;
@@ -953,19 +965,26 @@
     typename Op2::Params op_2;
     typename Op3::Params op_3;
   };
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
+    size_t op_0_workspace_size = Op0::get_workspace_size(problem_shape, args.op_0);
+    size_t op_1_workspace_size = Op1::get_workspace_size(problem_shape, args.op_1);
+    size_t op_2_workspace_size = Op2::get_workspace_size(problem_shape, args.op_2);
+    uint8_t* op_0_workspace = reinterpret_cast<uint8_t*>(workspace);
+    uint8_t* op_1_workspace = op_0_workspace + op_0_workspace_size;
+    uint8_t* op_2_workspace = op_1_workspace + op_1_workspace_size;
+    uint8_t* op_3_workspace = op_2_workspace + op_2_workspace_size;
     return Params{
-      Op0::to_underlying_arguments(problem_shape, args.op_0, workspace),
-      Op1::to_underlying_arguments(problem_shape, args.op_1, workspace),
-      Op2::to_underlying_arguments(problem_shape, args.op_2, workspace),
-      Op3::to_underlying_arguments(problem_shape, args.op_3, workspace)
+      Op0::to_underlying_arguments(problem_shape, args.op_0, op_0_workspace),
+      Op1::to_underlying_arguments(problem_shape, args.op_1, op_1_workspace),
+      Op2::to_underlying_arguments(problem_shape, args.op_2, op_2_workspace),
+      Op3::to_underlying_arguments(problem_shape, args.op_3, op_3_workspace)
     };
   }
 
   template <class ProblemShape>
   static size_t
   get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
     size_t workspace_size = 0;
@@ -982,41 +1001,42 @@
     workspace_size = round_nearest(workspace_size, MinWorkspaceAlignment);
 
     return workspace_size;
   }
 
   template <class ProblemShape>
   static cutlass::Status
-  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream) {
+  initialize_workspace(ProblemShape const& problem_shape, Arguments const& args, void* workspace, cudaStream_t stream,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     Status status = Status::kSuccess;
     uint8_t* workspace_ptr = reinterpret_cast<uint8_t*>(workspace);
     size_t workspace_offset = 0;
 
-    status = Op0::initialize_workspace(problem_shape, args.op_0, workspace_ptr + workspace_offset, stream);
+    status = Op0::initialize_workspace(problem_shape, args.op_0, workspace_ptr + workspace_offset, stream, cuda_adapter);
     workspace_offset += Op0::get_workspace_size(problem_shape, args.op_0);
     workspace_offset = round_nearest(workspace_offset, MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
-    status = Op1::initialize_workspace(problem_shape, args.op_1, workspace_ptr + workspace_offset, stream);
+    status = Op1::initialize_workspace(problem_shape, args.op_1, workspace_ptr + workspace_offset, stream, cuda_adapter);
     workspace_offset += Op1::get_workspace_size(problem_shape, args.op_1);
     workspace_offset = round_nearest(workspace_offset, MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
-    status = Op2::initialize_workspace(problem_shape, args.op_2, workspace_ptr + workspace_offset, stream);
+    status = Op2::initialize_workspace(problem_shape, args.op_2, workspace_ptr + workspace_offset, stream, cuda_adapter);
     workspace_offset += Op2::get_workspace_size(problem_shape, args.op_2);
     workspace_offset = round_nearest(workspace_offset, MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
-    status = Op3::initialize_workspace(problem_shape, args.op_3, workspace_ptr + workspace_offset, stream);
+    status = Op3::initialize_workspace(problem_shape, args.op_3, workspace_ptr + workspace_offset, stream, cuda_adapter);
     workspace_offset += Op3::get_workspace_size(problem_shape, args.op_3);
     workspace_offset = round_nearest(workspace_offset, MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
     return status;
```

## cutlass_library/source/include/cutlass/epilogue/thread/activation.h

```diff
@@ -62,81 +62,82 @@
     return value;
   }
 };
 
 template <typename T, int N>
 struct Identity<Array<T, N> > {
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value) const {
+  Array<T, N> operator()(Array<T, N> value) const {
     return value;
   }
 };
 
 /// Scale operator
 template <typename T>
 struct Scale {
   struct Arguments {
+    using scale_type = T;
     T scale = T(1);
   };
 
   CUTLASS_HOST_DEVICE
-  T operator()(T const& value, T const& scale) const {
+  T operator()(T value, T scale) const {
     multiplies<T> mul;
     return mul(scale, value);
   }
 
   CUTLASS_HOST_DEVICE
-  T operator()(T const& value, Arguments const& args = Arguments()) const {
+  T operator()(T value, Arguments args = Arguments()) const {
     return this->operator()(value, args.scale);
   }
 };
 
 template <typename T, int N>
 struct Scale<Array<T, N>> {
   using Arguments = typename Scale<T>::Arguments;
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const& values, T const& scale) const {
+  Array<T, N> operator()(Array<T, N> values, T scale) const {
     multiplies<Array<T, N>> mul;
     return mul(scale, values);
   }
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const& values, Arguments const& args = Arguments()) const {
+  Array<T, N> operator()(Array<T, N> values, Arguments args = Arguments()) const {
     return this->operator()(values, args.scale);
   }
 };
 
 /// Specialization to compose other activations with a defined unary operator
 /// e.g. Scale<Identity<T>>
 template <template <class> class Activation, typename T>
 struct Scale<Activation<T>> {
   using Arguments = typename Scale<T>::Arguments;
 
   CUTLASS_HOST_DEVICE
-  T operator()(T const &value, decltype(Arguments{}.scale) const& scale) const {
+  T operator()(T value, typename Arguments::scale_type scale) const {
     multiplies<T> mul;
     Activation<T> act;
     return mul(scale, act(value));
   }
 
   CUTLASS_HOST_DEVICE
-  T operator()(T const& value, Arguments const& args = Arguments()) const {
+  T operator()(T value, Arguments args = Arguments()) const {
     return this->operator()(value, args.scale);
   }
 };
 
 /// ReLu operator - propagates NaNs
 /// Always put threshold in the right hand side of max to propagate NaN.
 template <typename T>
 struct ReLu {
   static const bool kIsHeavy = false;
 
   CUTLASS_HOST_DEVICE
-  T operator()(T const & threshold, T value) const {
+  T operator()(T threshold, T value) const {
     maximum<T> mx;
 
     return mx(value, threshold);
   }
 
   CUTLASS_HOST_DEVICE
   T operator()(T value) const {
@@ -167,15 +168,15 @@
   }
 };
 
 // Generic clamp
 template <typename T>
 struct Clamp {
   struct Arguments {
-    T lower_bound = CUTLASS_STL_NAMESPACE::numeric_limits<T>::min();
+    T lower_bound = CUTLASS_STL_NAMESPACE::numeric_limits<T>::lowest();
     T upper_bound = CUTLASS_STL_NAMESPACE::numeric_limits<T>::max();
   };
 
   CUTLASS_HOST_DEVICE
   T operator()(T const& value, T const& lower_bound, T const& upper_bound) const {
     maximum<T> mx;
     minimum<T> mn;
@@ -662,14 +663,40 @@
       y[i] = relu_op(d_t[i], d_relu[i]);
     }
 
     return y;
   }
 };
 
+/// Computes backwards pass for ReLU operator assuming d_t is the layer gradient and
+/// z is computed from the forward pass.
+template <typename T>
+struct dReLU_Z {
+  CUTLASS_HOST_DEVICE
+  T operator()(T d_t, T z) const {
+    return z < 0 ? T(0) : d_t;
+  }
+};
+
+template <typename T, int N>
+struct dReLU_Z<Array<T, N>> {
+  CUTLASS_HOST_DEVICE
+  Array<T, N> operator()(Array<T, N> const& d_t, Array<T, N> const& z) const {
+    Array<T, N> y;
+    dReLU_Z<T> relu_op;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < N; ++i) {
+      y[i] = relu_op(d_t[i], z[i]);
+    }
+
+    return y;
+  }
+};
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace thread
 } // namespace epilogue
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h

```diff
@@ -59,16 +59,60 @@
 struct kIsHeavy_member_or_false {
   static constexpr bool value = false;
 };
 template<class Op>
 struct kIsHeavy_member_or_false<Op, typename cutlass::platform::enable_if<Op::kIsHeavy>::type> {
   static constexpr bool value = Op::kIsHeavy;
 };
+
 } // namespace (anonymous)
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+namespace detail {
+
+struct EmptyArguments {};
+
+template<class T, class = void>
+struct ElementwiseOpDispatcher {
+  using Arguments = EmptyArguments;
+
+  T op;
+
+  CUTLASS_HOST_DEVICE
+  ElementwiseOpDispatcher(Arguments) {}
+
+  template <typename ValueType>
+  CUTLASS_HOST_DEVICE
+  ValueType operator()(ValueType value) {
+    return op(value);
+  }
+};
+
+template<class T>
+struct ElementwiseOpDispatcher<T, std::void_t<typename T::Arguments>> {
+  using Arguments = typename T::Arguments;
+
+  Arguments args;
+  T op;
+
+  CUTLASS_HOST_DEVICE
+  ElementwiseOpDispatcher(Arguments args_):args(args_) {}
+
+  template <typename ValueType>
+  CUTLASS_HOST_DEVICE
+  ValueType operator()(ValueType value) {
+    return op(value, args);
+  }
+};
+
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 /// This base class is meant to define the concept required of the
 /// EpilogueWithBroadcast::OutputOp
 template <
   typename ElementC_,
   typename ElementAccumulator_,
   typename ElementCompute_,
   typename ElementZ_,
@@ -91,17 +135,21 @@
   using ElementVector = ElementVector_;
   static int const kElementsPerAccess = ElementsPerAccess;
   static int const kCount = kElementsPerAccess;
 
   using ElementwiseOp = ElementwiseOp_;
   using BinaryOp = BinaryOp_;
 
+  using ElementwiseOpDispatcher = detail::ElementwiseOpDispatcher<ElementwiseOp>;
+  using ElementwiseArguments = typename ElementwiseOpDispatcher::Arguments;
+
   // Indicates that this epilogue applies only one binary operation
   static bool const kIsSingleSource = true;
 
+
   using FragmentAccumulator = Array<ElementAccumulator, kElementsPerAccess>;
   using FragmentCompute = Array<ElementCompute, kElementsPerAccess>;
   using FragmentC = Array<ElementC, kElementsPerAccess>;
   using FragmentZ = Array<ElementZ, kElementsPerAccess>;
   using FragmentT = Array<ElementT, kElementsPerAccess>;
 
   // Definitions needed for collective epilogue
@@ -123,14 +171,15 @@
   /// Host-constructable parameters structure
   struct Params {
 
     ElementCompute alpha;                  ///< scales accumulators
     ElementCompute beta;                   ///< scales source tensor
     ElementCompute const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
     ElementCompute const *beta_ptr;        ///< pointer to source scalar - if not null, loads it from memory
+    ElementwiseArguments  elementwise;     ///< Arguments for elementwise operation
 
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
     Params(): 
@@ -138,31 +187,33 @@
       beta(ElementCompute(0)), 
       alpha_ptr(nullptr), 
       beta_ptr(nullptr) { }
 
     CUTLASS_HOST_DEVICE
     Params(
       ElementCompute alpha,
-      ElementCompute beta
-    ): alpha(alpha), beta(beta), alpha_ptr(nullptr), beta_ptr(nullptr) {
+      ElementCompute beta,
+      ElementwiseArguments  elementwise_ = ElementwiseArguments{}
+    ): alpha(alpha), beta(beta), alpha_ptr(nullptr), beta_ptr(nullptr), elementwise(elementwise_) {
 
     }
 
     CUTLASS_HOST_DEVICE
     Params(
       ElementCompute alpha
     ): alpha(alpha), beta(0), alpha_ptr(nullptr), beta_ptr(nullptr) {
 
     }
 
     CUTLASS_HOST_DEVICE
     Params(
       ElementCompute const *alpha_ptr,
-      ElementCompute const *beta_ptr
-    ): alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) {
+      ElementCompute const *beta_ptr,
+      ElementwiseArguments  elementwise_ = ElementwiseArguments{}
+    ): alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr), elementwise(elementwise_) {
 
     }
 
     CUTLASS_HOST_DEVICE
     Params(
       ElementCompute const *alpha_ptr
     ): alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(nullptr) {
@@ -174,25 +225,26 @@
 
   //
   // Data members
   //
 
   ElementCompute alpha_;
   ElementCompute beta_;
+  ElementwiseArguments const &elementwise_;
   bool skip_elementwise_;
 
 public:
 
   //
   // Methods
   //
 
   /// Constructor from Params
   CUTLASS_HOST_DEVICE
-  LinearCombinationBiasElementwise(Params const &params) {
+  LinearCombinationBiasElementwise(Params const &params): elementwise_(params.elementwise) {
 
     alpha_ = (params.alpha_ptr ? *params.alpha_ptr : params.alpha);
     beta_ = (params.beta_ptr ? *params.beta_ptr : params.beta);
     skip_elementwise_ = false;
   }
 
   /// Returns true if source is needed
@@ -209,24 +261,92 @@
     }
 
     if (k_partition != k_partition_count - 1) {
       skip_elementwise_ = true;
     }
   }
 
+  /// Applies the operation when elementwise_op require arguments and is_source_needed() is true
+  template <typename ElementwiseArgs>
+  CUTLASS_HOST_DEVICE
+  void operator()(
+    FragmentZ &frag_Z,
+    FragmentT &frag_T,
+    FragmentAccumulator const &AB,
+    FragmentC const &frag_C,
+    FragmentCompute const &V,
+    ElementwiseArgs const &elementwise_args) const {
+
+    ElementwiseOp elementwise_op;
+    BinaryOp binary_op;
+
+    FragmentCompute tmp_Accum = NumericArrayConverter<ElementCompute, ElementAccumulator, kElementsPerAccess>()(AB);
+    FragmentCompute tmp_C = NumericArrayConverter<ElementCompute, ElementC, kElementsPerAccess>()(frag_C);
+    FragmentCompute result_Z;
+    FragmentCompute result_T;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < kElementsPerAccess; ++i) {
+      ElementCompute z = binary_op(alpha_ * tmp_Accum[i] + beta_ * tmp_C[i], V[i]);
+      result_T[i] = z;
+      result_Z[i] = skip_elementwise_ ? z : elementwise_op(z, elementwise_args);
+    }
+
+    NumericArrayConverter<ElementZ, ElementCompute, kElementsPerAccess> convert_z;
+    frag_Z = convert_z(result_Z);
+
+    if constexpr (kStoreT) {
+      NumericArrayConverter<ElementT, ElementCompute, kElementsPerAccess> convert_t;
+      frag_T = convert_t(result_T);
+    }
+  }
+
+  /// Applies the operation when elementwise_op require arguments and is_source_needed() is false
+  template <typename ElementwiseArgs>
+  CUTLASS_HOST_DEVICE
+  void operator()(
+    FragmentZ &frag_Z,
+    FragmentT &frag_T,
+    FragmentAccumulator const &AB,
+    FragmentCompute const &V,
+    ElementwiseArgs const &elementwise_args) const {
+
+    ElementwiseOp elementwise_op;
+    BinaryOp binary_op;
+
+    FragmentCompute tmp_Accum = NumericArrayConverter<ElementCompute, ElementAccumulator, kElementsPerAccess>()(AB);
+    FragmentCompute result_Z;
+    FragmentCompute result_T;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < kElementsPerAccess; ++i) {
+      ElementCompute z = binary_op(alpha_ * tmp_Accum[i], V[i]);
+      result_T[i] = z;
+      result_Z[i] = skip_elementwise_ ? z : elementwise_op(z, elementwise_args);
+    }
+
+    NumericArrayConverter<ElementZ, ElementCompute, kElementsPerAccess> convert_z;
+    frag_Z = convert_z(result_Z);
+
+    if constexpr (kStoreT) {
+      NumericArrayConverter<ElementT, ElementCompute, kElementsPerAccess> convert_t;
+      frag_T = convert_t(result_T);
+    }
+  }
+
   /// Applies the operation when is_source_needed() is true
   CUTLASS_HOST_DEVICE
   void operator()(
     FragmentZ &frag_Z,
     FragmentT &frag_T,
     FragmentAccumulator const &AB,
     FragmentC const &frag_C,
     FragmentCompute const &V) const {
 
-    ElementwiseOp elementwise_op;
+    ElementwiseOpDispatcher elementwise_op(elementwise_);
     BinaryOp binary_op;
 
     FragmentCompute tmp_Accum = NumericArrayConverter<ElementCompute, ElementAccumulator, kElementsPerAccess>()(AB);
     FragmentCompute tmp_C = NumericArrayConverter<ElementCompute, ElementC, kElementsPerAccess>()(frag_C);
     FragmentCompute result_Z;
     FragmentCompute result_T;
 
@@ -250,15 +370,15 @@
   CUTLASS_HOST_DEVICE
   void operator()(
     FragmentZ &frag_Z,
     FragmentT &frag_T,
     FragmentAccumulator const &AB,
     FragmentCompute const &V) const {
 
-    ElementwiseOp elementwise_op;
+    ElementwiseOpDispatcher elementwise_op(elementwise_);
     BinaryOp binary_op;
 
     FragmentCompute tmp_Accum = NumericArrayConverter<ElementCompute, ElementAccumulator, kElementsPerAccess>()(AB);
     FragmentCompute result_Z;
     FragmentCompute result_T;
 
     CUTLASS_PRAGMA_UNROLL
```

## cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_clamp.h

```diff
@@ -428,25 +428,20 @@
     } else if (Scale == ScaleType::Nothing) {
       intermediate = converted_accumulator;
     } else {
       intermediate = mul_add_source(beta_, converted_source);                             // X =  beta * C + uniform
       intermediate = mul_add_accumulator(alpha_, converted_accumulator, intermediate);    // D = alpha * Accum + X
     }
 
-    // Convert floats back to INT
-    FragmentAccumulator scaled_accumulator;
+    //
+    // Convert float => ElementOutput_ with clamping
+    //
+    NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round> destination_converter;
 
-    NumericArrayConverter<int, ElementCompute, kCount, Round> compute_converter;
-
-    scaled_accumulator = compute_converter(intermediate);
-
-    // Convert to destination numeric type
-    NumericArrayConverter<ElementOutput, int, kCount, Round> destination_converter;
-
-    return destination_converter(scaled_accumulator);
+    return destination_converter(intermediate);
   }
 
   /// Computes linear scaling: D = alpha * accumulator
   CUTLASS_HOST_DEVICE
   FragmentOutput operator()(FragmentAccumulator const &accumulator) const {
 
     // Convert source to interal compute numeric type
@@ -462,25 +457,20 @@
     // Float min-max
     if (Scale == ScaleType::Nothing) {
       intermediate = converted_accumulator;
     } else {
       intermediate = mul_add_accumulator(alpha_, converted_accumulator);    // D = alpha * Accum
     }
 
-    // Convert floats back to INT
-    FragmentAccumulator scaled_accumulator;
-
-    NumericArrayConverter<int, ElementCompute, kCount, Round> compute_converter;
-
-    scaled_accumulator = compute_converter(intermediate);
-
-    // Convert to destination numeric type
-    NumericArrayConverter<ElementOutput, int, kCount, Round> destination_converter;
+    //
+    // Convert float => ElementOutput_ with clamping
+    //
+    NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round> destination_converter;
 
-    return destination_converter(scaled_accumulator);
+    return destination_converter(intermediate);
   }
 };
 
 #endif // Conditional guards to enable partial specialization for packed integers
 
 ////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_planar_complex.h

```diff
@@ -36,14 +36,15 @@
 
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/complex.h"
 #include "cutlass/array_planar_complex.h"
 #include "cutlass/functional.h"
 #include "cutlass/numeric_conversion.h"
+#include "cutlass/epilogue/thread/scale_type.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace thread {
 
@@ -58,89 +59,86 @@
 template <
   typename ElementOutput_,                             ///< Data type used to load and store tensors
   int Count,                                           ///< Number of elements computed per operation
                                                        ///< Usually it is 128/sizeof_bits<ElementOutput_>,
                                                        ///< but we use 64 or 32 sometimes when there are not enough data to store
   typename ElementAccumulator_ = ElementOutput_,       ///< Accumulator data type
   typename ElementCompute_ = ElementOutput_,           ///< Data type used to compute linear combination
-  FloatRoundStyle Round = FloatRoundStyle::round_to_nearest
+  FloatRoundStyle Round = FloatRoundStyle::round_to_nearest,
+  ScaleType::Kind Scale = ScaleType::Default           ///< Control Alpha and Beta scaling
 >
 class LinearCombinationPlanarComplex {
 public:
 
   using ElementOutput = ElementOutput_;
   using ElementAccumulator = ElementAccumulator_;
   using ElementCompute = ElementCompute_;
+  using ElementScalar = complex<ElementCompute>;
 
   static int const kCount = Count;
+  static const ScaleType::Kind kScale = Scale;
 
   using FragmentOutput = ArrayPlanarComplex<ElementOutput, kCount>;
   using FragmentAccumulator = ArrayPlanarComplex<ElementAccumulator, kCount>;
   using ComputeFragment = ArrayPlanarComplex<ElementCompute, kCount>;
 
   static FloatRoundStyle const kRound = Round;
 
   /// Host-constructable parameters structure
   struct Params {
 
-    complex<ElementCompute> alpha;                  ///< scales accumulators
-    complex<ElementCompute> beta;                   ///< scales source tensor
-    complex<ElementCompute> const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
-    complex<ElementCompute> const *beta_ptr;        ///< pointer to source scalar - if not null, loads it from memory
+    ElementScalar alpha{ElementCompute(1)};         ///< scales accumulators
+    ElementScalar beta{ElementCompute(0)};          ///< scales source tensor
+    ElementScalar const* alpha_ptr{nullptr};        ///< pointer to accumulator scalar - if not null, loads it from memory
+    ElementScalar const* beta_ptr{nullptr};         ///< pointer to source scalar - if not null, loads it from memory
 
     //
     // Methods
     //
 
-    CUTLASS_HOST_DEVICE
-    Params(): 
-      alpha(ElementCompute(1)), 
-      beta(ElementCompute(0)), 
-      alpha_ptr(nullptr), 
-      beta_ptr(nullptr) { }
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(
-      complex<ElementCompute> alpha,
-      complex<ElementCompute> beta
-    ): alpha(alpha), beta(beta), alpha_ptr(nullptr), beta_ptr(nullptr) {
-
-    }
+      ElementScalar alpha,
+      ElementScalar beta
+    ): alpha(alpha), beta(beta)
+    {}
 
     CUTLASS_HOST_DEVICE
     Params(
-      complex<ElementCompute> const *alpha_ptr,
-      complex<ElementCompute> const *beta_ptr
-    ): alpha(complex<ElementCompute>()), beta(complex<ElementCompute>()), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) {
-
-    }
+      ElementScalar const *alpha_ptr,
+      ElementScalar const *beta_ptr
+    ): alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) 
+    {}
   };
 
 private:
 
   //
   // Data members
   //
 
-  complex<ElementCompute> alpha_;
-  complex<ElementCompute> beta_;
+  ElementScalar alpha_;
+  ElementScalar beta_;
 
 public:
 
   /// Constructs the function object, possibly loading from pointers in host memory
   CUTLASS_HOST_DEVICE
   LinearCombinationPlanarComplex(Params const &params) {
-
     alpha_ = (params.alpha_ptr ? *params.alpha_ptr : params.alpha);
     beta_ = (params.beta_ptr ? *params.beta_ptr : params.beta);
   }
 
   /// Returns true if source is needed
   CUTLASS_HOST_DEVICE
   bool is_source_needed() const {
+    if (Scale == ScaleType::OnlyAlphaScaling) return false;
+
     return beta_.real() != ElementCompute(0) || beta_.imag() != ElementCompute(0);
   }
 
   /// Functionally required for serial reduction in the epilogue
   CUTLASS_HOST_DEVICE
   void set_k_partition(int k_partition, int k_partition_count) {
     if (k_partition) {
@@ -154,81 +152,82 @@
     FragmentAccumulator const &accumulator, 
     FragmentOutput const &source) const {
 
     // Convert source to interal compute numeric type
     NumericArrayConverter<ElementCompute, ElementOutput, kCount, Round> source_converter;
     NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
 
-    ComputeFragment converted_source(
+    ComputeFragment converted_source{
       source_converter(source.real), 
-      source_converter(source.imag));
+      source_converter(source.imag)};
 
-    ComputeFragment converted_accumulator(
+    ComputeFragment converted_accumulator{
       accumulator_converter(accumulator.real), 
-      accumulator_converter(accumulator.imag));
-
-    // Perform binary operations
-    ComputeFragment intermediate;
+      accumulator_converter(accumulator.imag)};
 
     multiplies<Array<ElementCompute, kCount> > mul_op;
     multiply_add<Array<ElementCompute, kCount> > mul_add_op;
 
+    // Perform binary operations
+  
     // complex multiply: I = beta * C
-    intermediate.real = mul_op(beta_.real(), converted_source.real);
-    intermediate.imag = mul_op(beta_.real(), converted_source.imag);
+    ComputeFragment intermediate {
+      mul_op(beta_.real(), converted_source.real),
+      mul_op(beta_.real(), converted_source.imag)
+    };
 
     intermediate.real = mul_add_op(-beta_.imag(), converted_source.imag, intermediate.real);
     intermediate.imag = mul_add_op( beta_.imag(), converted_source.real, intermediate.imag);
 
     // complex multiply-add: I = alpha * AB + I
     intermediate.real = mul_add_op(alpha_.real(), converted_accumulator.real, intermediate.real);
     intermediate.imag = mul_add_op(alpha_.real(), converted_accumulator.imag, intermediate.imag);
 
     intermediate.real = mul_add_op(-alpha_.imag(), converted_accumulator.imag, intermediate.real);
     intermediate.imag = mul_add_op( alpha_.imag(), converted_accumulator.real, intermediate.imag);
 
     // Convert to destination numeric type
     NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round> destination_converter;
 
-    return FragmentOutput(
+    return FragmentOutput{
       destination_converter(intermediate.real), 
-      destination_converter(intermediate.imag));
+      destination_converter(intermediate.imag)};
   }
 
   /// Computes linear scaling: D = alpha * accumulator + beta * source
   CUTLASS_HOST_DEVICE
   FragmentOutput operator()(
     FragmentAccumulator const &accumulator) const {
 
     // Convert source to interal compute numeric type
     NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
 
-    ComputeFragment converted_accumulator(
+    ComputeFragment converted_accumulator{
       accumulator_converter(accumulator.real), 
-      accumulator_converter(accumulator.imag));
+      accumulator_converter(accumulator.imag)};
 
     // Perform binary operations
-    ComputeFragment intermediate;
-
     multiplies<Array<ElementCompute, kCount> > mul_op;
     multiply_add<Array<ElementCompute, kCount> > mul_add_op;
 
     // complex multiply-add: I = alpha * AB + I
-    intermediate.real = mul_add_op(alpha_.real(), converted_accumulator.real);
-    intermediate.imag = mul_add_op(alpha_.real(), converted_accumulator.imag);
+    ComputeFragment intermediate {
+      mul_op(alpha_.real(), converted_accumulator.real),
+      mul_op(alpha_.real(), converted_accumulator.imag)
+    };
 
     intermediate.real = mul_add_op(-alpha_.imag(), converted_accumulator.imag, intermediate.real);
     intermediate.imag = mul_add_op( alpha_.imag(), converted_accumulator.real, intermediate.imag);
 
     // Convert to destination numeric type
     NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round> destination_converter;
 
-    return FragmentOutput(
+    return FragmentOutput{
       destination_converter(intermediate.real), 
-      destination_converter(intermediate.imag));
+      destination_converter(intermediate.imag)};
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace thread
 } // namespace epilogue
```

## cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_simt.h

```diff
@@ -60,19 +60,20 @@
 
 #include "cutlass/epilogue/warp/fragment_iterator_simt.h"
 #include "cutlass/epilogue/warp/tile_iterator_simt.h"
 #include "cutlass/epilogue/threadblock/default_thread_map_simt.h"
 #include "cutlass/transform/pitch_linear_thread_map.h"
 
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
+#include "cutlass/epilogue/threadblock/predicated_tile_iterator_conv.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h" 
 #include "cutlass/epilogue/threadblock/shared_load_iterator.h"
-#include "cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h"
+#include "cutlass/epilogue/threadblock/shared_load_iterator_pitch_linear.h"
 #include "cutlass/epilogue/threadblock/epilogue.h"
 #include "cutlass/epilogue/threadblock/epilogue_depthwise.h"
 
 #include "cutlass/layout/permute.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -85,48 +86,68 @@
 /// Defines sensible defaults for epilogues for SimtOps.
 template <
   typename Shape_,
   typename WarpMmaSimt_,
   typename OutputOp_,
   int ElementsPerAccess,
   bool ScatterD = false,
-  typename PermuteDLayout = layout::NoPermute
+  typename PermuteDLayout = layout::NoPermute,
+  conv::StrideSupport StrideSupport = conv::StrideSupport::kUnity,
+  int Rank = 4
 >
 struct DefaultEpilogueSimt {
 
   using Shape = Shape_;
   using WarpMmaSimt = WarpMmaSimt_;
   using OutputOp = OutputOp_;
   static int const kElementsPerAccess = ElementsPerAccess;
   static const int kPartitionsK = Shape::kK / WarpMmaSimt::Shape::kK;
 
   using ElementOutput = typename OutputOp::ElementOutput;
   using LayoutC = typename WarpMmaSimt::LayoutC;
   using ElementAccumulator = typename WarpMmaSimt::ElementC;
+  static conv::StrideSupport const kStrideSupport = StrideSupport;
+  static int const kRank = Rank;
 
   //
   // Thread map
   //
 
   using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapSimt<
     Shape,
     typename WarpMmaSimt::Shape,
     typename WarpMmaSimt::Policy,
     kPartitionsK,
     ElementOutput,
     kElementsPerAccess
   >::Type;
 
-  using OutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIterator<
+  static bool const UseCUDAStore = platform::is_same<ElementOutput, double>::value;
+
+  using PackedOutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIterator<
     OutputTileThreadMap,
     ElementOutput,
     ScatterD,
-    PermuteDLayout
+    PermuteDLayout,
+    UseCUDAStore
   >;
 
+  using StridedOutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIteratorConv<
+    OutputTileThreadMap,
+    ElementOutput,
+    ScatterD,
+    PermuteDLayout,
+    UseCUDAStore,
+    kRank
+  >;
+
+  using OutputTileIterator = typename platform::conditional<StrideSupport == cutlass::conv::StrideSupport::kUnity,
+                                                            PackedOutputTileIterator,
+                                                            StridedOutputTileIterator>::type;
+
   using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorSimt<
     typename WarpMmaSimt::Shape,
     typename WarpMmaSimt::ThreadMma,
     layout::RowMajor,
     typename WarpMmaSimt::Policy
   >;
 
@@ -385,15 +406,15 @@
     ThreadBlockOutputShape,
     typename WarpMmaSimt::ThreadMma,
     ElementAccumulator,
     layout::RowMajor,
     typename WarpMmaSimt::Policy
   >;
 
-  using SharedLoadIterator = cutlass::epilogue::threadblock::SharedLoadIteratorPitchLiner<
+  using SharedLoadIterator = cutlass::epilogue::threadblock::SharedLoadIteratorPitchLinear<
     OutputTileThreadMap,
     ElementAccumulator
   >;
 
   /// Hard-coded padding elements added 
   using Padding = typename WarpTileIterator::Padding;
   //
```

## cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h

```diff
@@ -62,14 +62,15 @@
 
 #include "cutlass/epilogue/warp/fragment_iterator_tensor_op.h"
 #include "cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h"
 #include "cutlass/epilogue/warp/tile_iterator_tensor_op.h"
 #include "cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h"
 #include "cutlass/epilogue/threadblock/default_thread_map_tensor_op.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
+#include "cutlass/epilogue/threadblock/predicated_tile_iterator_conv.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h"
 #include "cutlass/epilogue/threadblock/shared_load_iterator.h"
 #include "cutlass/epilogue/threadblock/shared_load_iterator_mixed.h"
 
 #include "cutlass/epilogue/threadblock/epilogue.h"
 #include "cutlass/epilogue/threadblock/interleaved_epilogue.h"
@@ -283,15 +284,15 @@
 
   static_assert(platform::is_same<ElementOutput, cutlass::int4b_t>::value ||
                 platform::is_same<ElementOutput, cutlass::uint4b_t>::value ||
                 platform::is_same<ElementOutput, int8_t>::value ||
                 platform::is_same<ElementOutput, uint8_t>::value,
                 "ElementOutput needs to be 4 or 8 bit (unsigned) int.");
 
-   static_assert((ElementsPerAccess == 16 || ElementsPerAccess == 8),
+   static_assert((ElementsPerAccess == 16 || ElementsPerAccess == 8 || ElementsPerAccess == 4),
                 "ElementsPerAccess needs to be 16 or 8.");
   
   using WarpTileIteratorMixed = cutlass::epilogue::warp::TileIteratorTensorOpMixed<
     WarpShape,
     InstructionShape,
     int32_t,
     32,
@@ -304,15 +305,15 @@
     WarpShape,
     InstructionShape,
     int32_t,
     layout::RowMajor
   >;
 
   using WarpTileIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
+                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8) || (ElementsPerAccess == 4),
                              WarpTileIteratorNotMixed,
                              WarpTileIteratorMixed>::type;
 
   using SharedLoadIteratorMixed = cutlass::epilogue::threadblock::SharedLoadIteratorMixed<
     ThreadMap,
     int32_t,
     32,
@@ -323,15 +324,15 @@
 
   using SharedLoadIteratorNotMixed = cutlass::epilogue::threadblock::SharedLoadIterator<
     ThreadMap,
     int32_t
   >;
 
   using SharedLoadIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
+                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8) || (ElementsPerAccess == 4),
                              SharedLoadIteratorNotMixed,
                              SharedLoadIteratorMixed>::type;
 
   static int const kFragmentsPerIteration = 1;
 };
 
 /// Partial specialization for float_e4m3_t <= float x 16/8 epilogues avoids shared memory bank conflicts.
@@ -350,15 +351,15 @@
   ThreadblockShape, 
   WarpShape, 
   InstructionShape, 
   ThreadMap> {
 
   using ElementOutput = cutlass::float_e4m3_t;
 
-  static_assert((ElementsPerAccess == 16 || ElementsPerAccess == 8),
+  static_assert((ElementsPerAccess == 16 || ElementsPerAccess == 8 || ElementsPerAccess == 4),
               "ElementsPerAccess needs to be 16 or 8.");
   
   using WarpTileIteratorMixed = cutlass::epilogue::warp::TileIteratorTensorOpMixed<
     WarpShape,
     InstructionShape,
     float,
     32,
@@ -371,15 +372,15 @@
     WarpShape,
     InstructionShape,
     float,
     layout::RowMajor
   >;
 
   using WarpTileIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
+                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8) || (ElementsPerAccess == 4),
                              WarpTileIteratorNotMixed,
                              WarpTileIteratorMixed>::type;
 
   using SharedLoadIteratorMixed = cutlass::epilogue::threadblock::SharedLoadIteratorMixed<
     ThreadMap,
     float,
     32,
@@ -390,15 +391,15 @@
 
   using SharedLoadIteratorNotMixed = cutlass::epilogue::threadblock::SharedLoadIterator<
     ThreadMap,
     float
   >;
 
   using SharedLoadIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
+                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8) || (ElementsPerAccess == 4),
                              SharedLoadIteratorNotMixed,
                              SharedLoadIteratorMixed>::type;
 
   static int const kFragmentsPerIteration = 1;
 };
 
 /// Partial specialization for float_e5m2_t <= float x 16/8 epilogues avoids shared memory bank conflicts.
@@ -417,15 +418,15 @@
   ThreadblockShape, 
   WarpShape, 
   InstructionShape, 
   ThreadMap> {
 
   using ElementOutput = cutlass::float_e5m2_t;
 
-  static_assert((ElementsPerAccess == 16 || ElementsPerAccess == 8),
+  static_assert((ElementsPerAccess == 16 || ElementsPerAccess == 8 || ElementsPerAccess == 4),
               "ElementsPerAccess needs to be 16 or 8.");
   
   using WarpTileIteratorMixed = cutlass::epilogue::warp::TileIteratorTensorOpMixed<
     WarpShape,
     InstructionShape,
     float,
     32,
@@ -438,15 +439,15 @@
     WarpShape,
     InstructionShape,
     float,
     layout::RowMajor
   >;
 
   using WarpTileIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
+                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8) || (ElementsPerAccess == 4),
                              WarpTileIteratorNotMixed,
                              WarpTileIteratorMixed>::type;
 
   using SharedLoadIteratorMixed = cutlass::epilogue::threadblock::SharedLoadIteratorMixed<
     ThreadMap,
     float,
     32,
@@ -457,15 +458,15 @@
 
   using SharedLoadIteratorNotMixed = cutlass::epilogue::threadblock::SharedLoadIterator<
     ThreadMap,
     float
   >;
 
   using SharedLoadIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
+                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8) || (ElementsPerAccess == 4),
                              SharedLoadIteratorNotMixed,
                              SharedLoadIteratorMixed>::type;
 
   static int const kFragmentsPerIteration = 1;
 };
 
 } // namespace detail
@@ -476,27 +477,31 @@
 template <
   typename Shape_,
   typename WarpMmaTensorOp_,
   int PartitionsK,
   typename OutputOp_,
   int ElementsPerAccess,
   bool ScatterD = false,
-  typename PermuteDLayout = layout::NoPermute
+  typename PermuteDLayout = layout::NoPermute,
+  conv::StrideSupport StrideSupport = conv::StrideSupport::kUnity,
+  int Rank = 4
 >
 struct DefaultEpilogueTensorOp {
 
   using Shape = Shape_;
   using WarpMmaTensorOp = WarpMmaTensorOp_;
   static int const kPartitionsK = PartitionsK;
   using OutputOp = OutputOp_;
   static int const kElementsPerAccess = ElementsPerAccess;
 
   using ElementOutput = typename OutputOp::ElementOutput;
   using LayoutC = typename WarpMmaTensorOp::LayoutC;
   using ElementAccumulator = typename WarpMmaTensorOp::ElementC;
+  static conv::StrideSupport const kStrideSupport = StrideSupport;
+  static int const kRank = Rank;
 
   //
   // Thread map
   //
 
   using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapTensorOp<
     Shape,
@@ -504,22 +509,35 @@
     kPartitionsK,
     ElementOutput,
     kElementsPerAccess
   >::Type;
 
   static bool const UseCUDAStore = platform::is_same<ElementOutput, double>::value;
 
-  using OutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIterator<
+  using PackedOutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIterator<
     OutputTileThreadMap,
     ElementOutput,
     ScatterD,
     PermuteDLayout,
     UseCUDAStore
   >;
 
+  using StridedOutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIteratorConv<
+    OutputTileThreadMap,
+    ElementOutput,
+    ScatterD,
+    PermuteDLayout,
+    UseCUDAStore,
+    kRank
+  >;
+
+  using OutputTileIterator = typename platform::conditional<StrideSupport == cutlass::conv::StrideSupport::kUnity,
+                                                            PackedOutputTileIterator,
+                                                            StridedOutputTileIterator>::type;
+
   using AccumulatorFragmentIterator = typename platform::conditional<is_complex<ElementOutput>::value,
                                     cutlass::epilogue::warp::FragmentIteratorComplexTensorOp<
                                         typename WarpMmaTensorOp::Shape,
                                         typename WarpMmaTensorOp::Policy::Operator::Shape,
                                         typename WarpMmaTensorOp::Policy::Operator::ElementC,
                                         typename WarpMmaTensorOp::Policy::Operator::FragmentC,
                                         LayoutC>,
```

## cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h

```diff
@@ -115,15 +115,15 @@
 
   using OutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIteratorBlas3<
     OutputTileThreadMap,
     ElementOutput,
     kBlasMode
   >;
 
-  using AccumulatorFragmentIterator = typename std::conditional<is_complex<ElementOutput>::value,
+  using AccumulatorFragmentIterator = typename platform::conditional<is_complex<ElementOutput>::value,
                                     cutlass::epilogue::warp::FragmentIteratorComplexTensorOp<
                                         typename WarpMmaTensorOp::Shape,
                                         typename WarpMmaTensorOp::Policy::Operator::Shape,
                                         typename WarpMmaTensorOp::Policy::Operator::ElementC,
                                         typename WarpMmaTensorOp::Policy::Operator::FragmentC,
                                         LayoutC>,
                                     cutlass::epilogue::warp::FragmentIteratorTensorOp<
```

## cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h

```diff
@@ -53,15 +53,126 @@
 #include "cutlass/layout/permute.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
+////////////////////////////////////////////////////////////////////////////////
+
+/// Defines sensible defaults for epilogues for SimtOps.
+template <
+  typename Shape,
+  typename WarpMmaSimt,
+  typename ElementOutput,
+  typename ElementTensor,
+  typename ElementVector,
+  typename OutputOp,
+  int ElementsPerAccess,
+  bool ScatterD = false,
+  typename PermuteDLayout = layout::NoPermute
+>
+struct DefaultEpilogueWithBroadcastSimt {
+
+  /// Use defaults related to the existing epilogue
+  using Base = DefaultEpilogueSimt<
+    Shape,
+    WarpMmaSimt,
+    OutputOp,
+    ElementsPerAccess
+  >;
+
+  //
+  // Stores the result z = (y = GEMM(A, B, C), broadcast)
+  //
+  using OutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIterator<
+    typename Base::OutputTileThreadMap,
+    ElementOutput,
+    ScatterD,
+    PermuteDLayout
+  >;
+
+  //
+  // Additional tensor tile iterator - stores t = Elementwise(z)
+  //
+  using TensorTileIterator = cutlass::epilogue::threadblock::PredicatedTileIterator<
+    typename Base::OutputTileThreadMap,
+    ElementTensor
+  >;
+
+  /// Define the epilogue
+  using Epilogue = EpilogueWithBroadcast<
+    Shape,
+    WarpMmaSimt,
+    Base::kPartitionsK,
+    OutputTileIterator,
+    TensorTileIterator,
+    ElementVector,
+    typename Base::AccumulatorFragmentIterator,
+    typename Base::WarpTileIterator,
+    typename Base::SharedLoadIterator,
+    OutputOp,
+    typename Base::Padding
+  >;
+};
+////////////////////////////////////////////////////////////////////////////////
+
+/// Defines sensible defaults for strided dgrad epilogues for SimtOps.
+template <
+  typename Shape,
+  typename WarpMmaSimt,
+  typename ElementOutput,
+  typename ElementTensor,
+  typename ElementVector,
+  typename OutputOp,
+  int ElementsPerAccess,
+  bool ScatterD = false,
+  typename PermuteDLayout = layout::NoPermute
+>
+struct DefaultEpilogueWithBroadcastSimtStridedDgrad {
+
+  /// Use defaults related to the existing epilogue
+  using Base = DefaultEpilogueSimtStridedDgrad<
+    Shape,
+    WarpMmaSimt,
+    OutputOp,
+    ElementsPerAccess
+  >;
+
+  //
+  // Stores the result z = (y = GEMM(A, B, C), broadcast)
+  //
+  using OutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIteratorStridedDgrad<
+    typename Base::OutputTileThreadMap,
+    ElementOutput
+  >;
+
+  //
+  // Additional tensor tile iterator - stores t = Elementwise(z)
+  //
+  using TensorTileIterator = cutlass::epilogue::threadblock::PredicatedTileIteratorStridedDgrad<
+    typename Base::OutputTileThreadMap,
+    ElementTensor
+  >;
 
+  /// Define the epilogue
+  using Epilogue = EpilogueWithBroadcast<
+    Shape,
+    WarpMmaSimt,
+    Base::kPartitionsK,
+    OutputTileIterator,
+    TensorTileIterator,
+    ElementVector,
+    typename Base::AccumulatorFragmentIterator,
+    typename Base::WarpTileIterator,
+    typename Base::SharedLoadIterator,
+    OutputOp,
+    typename Base::Padding
+  >;
+};
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Defines sensible defaults for epilogues for TensorOps.
 template <
   typename Shape,
   typename WarpMmaTensorOp,
   int PartitionsK,
```

## cutlass_library/source/include/cutlass/epilogue/threadblock/output_iterator_parameter.h

```diff
@@ -66,28 +66,76 @@
   static int const kWgradStrideIdx = 
     platform::is_same<TensorLayout, layout::TensorNHWC>::value ? 2 : 3;
 
   /// This chooses the appropriate stride element of the C tensor.
   static int const kTensorStrideIdx = 
     (kConvolutionalOperator == conv::Operator::kWgrad ? kWgradStrideIdx : 0);
 
-
   CUTLASS_HOST_DEVICE
   static OutputIteratorLayout layout(const TensorRef & ref) {
     return ref.stride(kTensorStrideIdx);
   }
 
   CUTLASS_HOST_DEVICE
   static OutputTensorCoord extent(ConvProblemSize problem_size) {
     return conv::implicit_gemm_problem_size(kConvolutionalOperator, problem_size).mn();
   }
+};
+
+template<
+  typename TensorRef_,                                ///! Input tensor to epilogue output iterator
+  typename ConvProblemSize_                          ///! Convolutional operator on 2D or 3D problem
+>
+struct ConvOutputIteratorParameter<layout::TensorNHWC, layout::TensorNHWC, TensorRef_, conv::Operator::kFprop, ConvProblemSize_> {
+
+  using TensorLayout = layout::TensorNHWC;
+  using OutputIteratorLayout = layout::TensorNHWC;
+  using MappedLayout = layout::RowMajor;
+  using OutputTensorCoord = typename OutputIteratorLayout::TensorCoord;
+  using MappedTensorCoord = typename MappedLayout::TensorCoord;
+  using TensorRef = TensorRef_;
+  static conv::Operator const kConvolutionalOperator = conv::Operator::kFprop;
+  using ConvProblemSize = ConvProblemSize_;
 
+  CUTLASS_HOST_DEVICE
+  static OutputIteratorLayout layout(const TensorRef & ref) {
+    return ref.stride();
+  }
+
+  CUTLASS_HOST_DEVICE
+  static MappedTensorCoord extent(ConvProblemSize problem_size) {
+    return conv::implicit_gemm_problem_size(kConvolutionalOperator, problem_size).mn();
+  }
 };
 
+template<
+  typename TensorRef_,                                ///! Input tensor to epilogue output iterator
+  typename ConvProblemSize_                          ///! Convolutional operator on 2D or 3D problem
+>
+struct ConvOutputIteratorParameter<layout::TensorNDHWC, layout::TensorNDHWC, TensorRef_, conv::Operator::kFprop, ConvProblemSize_> {
+
+  using TensorLayout = layout::TensorNDHWC;
+  using OutputIteratorLayout = layout::TensorNDHWC;
+  using MappedLayout = layout::RowMajor;
+  using OutputTensorCoord = typename OutputIteratorLayout::TensorCoord;
+  using MappedTensorCoord = typename MappedLayout::TensorCoord;
+  using TensorRef = TensorRef_;
+  static conv::Operator const kConvolutionalOperator = conv::Operator::kFprop;
+  using ConvProblemSize = ConvProblemSize_;
 
+  CUTLASS_HOST_DEVICE
+  static OutputIteratorLayout layout(const TensorRef & ref) {
+    return ref.stride();
+  }
+
+  CUTLASS_HOST_DEVICE
+  static MappedTensorCoord extent(ConvProblemSize problem_size) {
+    return conv::implicit_gemm_problem_size(kConvolutionalOperator, problem_size).mn();
+  }
+};
 
 template <
   int InterleavedK,
   typename TensorRef_,
   conv::Operator ConvOperator,
   typename ConvProblemSize_
 >
```

## cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h

```diff
@@ -47,14 +47,16 @@
 #include "cutlass/matrix_shape.h"
 #include "cutlass/tensor_ref.h"
 #include "cutlass/transform/pitch_linear_thread_map.h"
 #include "cutlass/epilogue/threadblock/output_tile_thread_map.h"
 #include "cutlass/arch/arch.h"
 #include "cutlass/arch/memory.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator_params.h"
+#include "cutlass/conv/conv2d_problem_size.h"
+#include "cutlass/conv/conv3d_problem_size.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 
 ////////////////////////////////////////////////////////////////////////////////
 
@@ -98,18 +100,18 @@
   static_assert( ThreadMap::Iterations::kRow > 0,"ThreadMap::Iterations::kRow must be > 0");
   static_assert( ThreadMap::Iterations::kGroup > 0,"ThreadMap::Iterations::kGroup must be > 0");
   static_assert( ThreadMap::Iterations::kCluster > 0,"ThreadMap::Iterations::kCluster must be > 0");
   static_assert( ThreadMap::Iterations::kColumn > 0,"ThreadMap::Iterations::kColumn must be > 0");
 
   /// Fragment object
   using Fragment = Array<
-    Element, 
-    ThreadMap::Iterations::kColumn * 
-    ThreadMap::Iterations::kRow * 
-    ThreadMap::Iterations::kGroup * 
+    Element,
+    ThreadMap::Iterations::kColumn *
+    ThreadMap::Iterations::kRow *
+    ThreadMap::Iterations::kGroup *
     ThreadMap::Iterations::kCluster * ThreadMap::kElementsPerAccess>;
 
   /// Memory access size
   using AccessType = AlignedArray<Element, ThreadMap::kElementsPerAccess>;
 
   //
   // Parameters struct
@@ -119,22 +121,36 @@
   struct Params : PredicatedTileIteratorParams {
     using Base = PredicatedTileIteratorParams;
 
     CUTLASS_HOST_DEVICE
     Params() { }
 
     CUTLASS_HOST_DEVICE
-    Params(Layout const &layout): 
+    Params(Layout const &layout):
       PredicatedTileIteratorParams(
         layout.stride(0) * int(sizeof(AccessType)) / kElementsPerAccess,
         make_OutputTileThreadMapDesc<ThreadMap>()
       ) 
     { }
 
     CUTLASS_HOST_DEVICE
+    Params(Layout const &layout,
+           // Not needed.  Added to be compatible with strided conv epilogue.
+           conv::Conv2dProblemSize const &problem_size):
+      Params(layout)
+    { }
+
+    CUTLASS_HOST_DEVICE
+    Params(Layout const &layout,
+           // Not needed.  Added to be compatible with strided conv epilogue.
+           conv::Conv3dProblemSize const &problem_size):
+      Params(layout)
+    { }
+
+    CUTLASS_HOST_DEVICE
     Params(Base const &base) : 
       Base(base) { }
   };
 
   /// Mask object
   struct Mask {
 
@@ -198,15 +214,15 @@
   /// A thread's starting column
   Index thread_start_column_;
 
   /// Internal state counter
   int state_[3];
 
   /// Scatter indices
-  int const *indices_; 
+  int const *indices_;
 
   /// PermuteDLayout
   PermuteDLayout permute_layout_;
 
   //
   // Static asserts about internal strides
   //
@@ -249,30 +265,30 @@
     thread_start_row_ = thread_offset.row();
     thread_start_column_ = thread_offset.column();
 
     // Initialize predicates
     CUTLASS_PRAGMA_UNROLL
     for (int c = 0; c < ThreadMap::Iterations::kColumn; ++c) {
 
-      mask_.predicates[c] = ((thread_offset.column() 
+      mask_.predicates[c] = ((thread_offset.column()
         + ThreadMap::Delta::kColumn * c) < extent.column());
     }
 
     // Null pointer performs no accesses
     if (!pointer) {
       mask_.clear();
     }
 
     if (ScatterD && !indices) {
       mask_.clear();
     }
 
     // Initialize byte_pointer_
-    byte_pointer_ = reinterpret_cast<uint8_t *>(pointer) + 
-      LongIndex(thread_offset.row()) * LongIndex(params_.stride) + 
+    byte_pointer_ = reinterpret_cast<uint8_t *>(pointer) +
+      LongIndex(thread_offset.row()) * LongIndex(params_.stride) +
       LongIndex(thread_offset.column()) * sizeof(AccessType) / kElementsPerAccess;
 
     if (ScatterD) {
       byte_pointer_ = reinterpret_cast<uint8_t *>(pointer) +
         LongIndex(thread_offset.column()) * sizeof(AccessType) / kElementsPerAccess;
     }
 
@@ -302,15 +318,15 @@
 
       CUTLASS_PRAGMA_UNROLL
       for (int group = 0; group < ThreadMap::Iterations::kGroup; ++group) {
 
         CUTLASS_PRAGMA_UNROLL
         for (int row = 0; row < ThreadMap::Iterations::kRow; ++row) {
 
-          int frag_row_idx = 
+          int frag_row_idx =
             (row + ThreadMap::Iterations::kRow * (group + ThreadMap::Iterations::kGroup * cluster));
 
           int row_offset = row * ThreadMap::Delta::kRow 
             + group * ThreadMap::Delta::kGroup 
             + cluster * ThreadMap::Delta::kCluster;
 
           bool row_guard = ((row_offset + thread_start_row_) < extent_row_);
@@ -326,15 +342,15 @@
 
           CUTLASS_PRAGMA_UNROLL
           for (int column = 0; column < ThreadMap::Iterations::kColumn; ++column) {
 
             bool guard = row_guard && mask_.predicates[column];
 
             cutlass::arch::global_load<
-              AccessType, 
+              AccessType,
               sizeof(AccessType)
             >(
                 frag_ptr[frag_row_idx * ThreadMap::Iterations::kColumn +
                          column],
                 (void *)&memory_pointer[column * ThreadMap::Delta::kColumn /
                                         kElementsPerAccess],
                 guard);
@@ -376,19 +392,19 @@
 
       CUTLASS_PRAGMA_UNROLL
       for (int group = 0; group < ThreadMap::Iterations::kGroup; ++group) {
 
         CUTLASS_PRAGMA_UNROLL
         for (int row = 0; row < ThreadMap::Iterations::kRow; ++row) {
 
-          int frag_row_idx = 
+          int frag_row_idx =
             (row + ThreadMap::Iterations::kRow * (group + ThreadMap::Iterations::kGroup * cluster));
 
-          int row_offset = row * ThreadMap::Delta::kRow 
-            + group * ThreadMap::Delta::kGroup 
+          int row_offset = row * ThreadMap::Delta::kRow
+            + group * ThreadMap::Delta::kGroup
             + cluster * ThreadMap::Delta::kCluster;
 
           bool row_guard = ((row_offset + thread_start_row_) < extent_row_);
 
           AccessType *memory_pointer = reinterpret_cast<AccessType *>(byte_pointer + byte_offset);
 
           if (ScatterD && row_guard) {
@@ -422,15 +438,15 @@
               }
             } else {
               cutlass::arch::global_store<AccessType, sizeof(AccessType)>(
                   frag_ptr[frag_row_idx * ThreadMap::Iterations::kColumn + column],
                   (void *)&memory_pointer[0],
                   guard);
             }
-            
+
             if (!PermuteD) {
               memory_pointer += (ThreadMap::Delta::kColumn / kElementsPerAccess);
             }
           }
 
           if (row + 1 < ThreadMap::Iterations::kRow) {
             if (!ScatterD && !PermuteD) {
@@ -645,29 +661,29 @@
     }
 
     if (!ScatterD && !PermuteD) {
       store_byte_pointer_ += params_.advance_row;
     }
 
     thread_start_row_ += ThreadMap::Shape::kRow;
-    
+
     if (state_[0] == ThreadMap::Count::kRow) {
 
       state_[0] = 0;
       ++state_[1];
 
       if (!ScatterD) {
         byte_pointer_ += params_.advance_group;
       }
 
       if (!ScatterD && !PermuteD) {
         store_byte_pointer_ += params_.advance_group;
       }
 
-      thread_start_row_ += (ThreadMap::Shape::kGroup - 1) * 
+      thread_start_row_ += (ThreadMap::Shape::kGroup - 1) *
         ThreadMap::Shape::kRow * ThreadMap::Count::kRow;
 
       if (state_[1] == ThreadMap::Count::kGroup) {
 
         state_[1] = 0;
         ++state_[2];
 
@@ -675,15 +691,15 @@
           byte_pointer_ += params_.advance_cluster;
         }
 
         if (!ScatterD && !PermuteD) {
           store_byte_pointer_ += params_.advance_cluster;
         }
 
-        thread_start_row_ += ThreadMap::Count::kGroup * 
+        thread_start_row_ += ThreadMap::Count::kGroup *
           ThreadMap::Shape::kGroup * ThreadMap::Count::kRow * ThreadMap::Shape::kRow;
 
         if (state_[2] == ThreadMap::Count::kCluster) {
           state_[2] = 0;
 
           if (!ScatterD) {
             byte_pointer_ += params_.advance_tile;
@@ -1117,14 +1133,22 @@
     }
 
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout) {
 
       initialize(layout.stride());
     }
+
+    CUTLASS_HOST_DEVICE
+    Params(Layout const &layout,
+           // Not needed.  Added to be compatible with strided conv epilogue.
+           conv::Conv2dProblemSize const &problem_size):
+      Params(layout)
+    { }
+
   };
 
   /// Mask object
   struct Mask {
     static int const kCount =
         (ThreadMap::Iterations::kRow < 8) ? 8 : ThreadMap::Iterations::kRow;
```

## cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h

```diff
@@ -28,24 +28,14 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
   \brief 
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by this unit test: `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #include "cutlass/cutlass.h"
 
 #include "cutlass/layout/pitch_linear.h"
 #include "cutlass/layout/matrix.h"
 
@@ -253,16 +243,14 @@
 
   CUTLASS_HOST_DEVICE
   PredicatedTileIteratorParams(LongIndex stride, OutputTileThreadMapDesc thread_map) {
     initialize(stride, thread_map);
   }
 };
 
-
-
 ///////////////////////////////////////////////////////////////////////////////
 
 //
 // Parameters struct for PredicatedTileIteratorDirect2dConv
 //
 
 struct PredicatedTileIteratorDirect2dConvParams{
```

## cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_2x.hpp

```diff
@@ -88,19 +88,19 @@
         }
       );
     }
 
     /// Called after accumulators have been exchanged for each accumulator vector
     template <typename ElementAccumulator, typename... ElementInputs, int FragmentSize>
     CUTLASS_DEVICE auto // returns an Array
-    visit(int iter_idx, int row_idx, int column_idx, int frg_idx, 
+    visit(int iter_idx, int row_idx, int column_idx, int frg_idx,
           Array<ElementAccumulator, FragmentSize> const& frg_acc,
           Array<ElementInputs, FragmentSize> const&... frg_inputs) // depends on the N-naryness of the op
       = delete; // Must be implemented for each operation
-    
+
     /// Called at the start of a row
     CUTLASS_DEVICE void
     end_row(int row_idx) {
       for_each(callbacks_tuple,
         [&] (auto& callbacks) {
           callbacks.end_row(row_idx);
         }
@@ -175,20 +175,20 @@
   using VisitorImpl2x<ChildOps..., NodeOp>::VisitorImpl2x;
 
   template<class CallbacksImpl>
   struct Callbacks : CallbacksImpl {
     CUTLASS_DEVICE
     Callbacks(CallbacksImpl&& impl)
       : CallbacksImpl(cute::forward<CallbacksImpl>(impl)) {}
-    
+
     using CallbacksImpl::callbacks_tuple;
 
     template <typename ElementAccumulator, int FragmentSize>
     CUTLASS_DEVICE auto
-    visit(int iter_idx, int row_idx, int column_idx, int frg_idx, 
+    visit(int iter_idx, int row_idx, int column_idx, int frg_idx,
           Array<ElementAccumulator, FragmentSize> const& frg_acc) {
       constexpr int Rm1 = sizeof...(ChildOps);
       return cute::detail::tapply(callbacks_tuple,
         [&] (auto& child_callbacks) {
           return child_callbacks.visit(iter_idx, row_idx, column_idx, frg_idx, frg_acc);
         },
         [&] (auto&&... frg_inputs) {
@@ -238,24 +238,24 @@
   using VisitorImpl2x<Ops...>::VisitorImpl2x;
 
   template<class CallbacksImpl>
   struct Callbacks : CallbacksImpl {
     CUTLASS_DEVICE
     Callbacks(CallbacksImpl&& impl)
       : CallbacksImpl(cute::forward<CallbacksImpl>(impl)) {}
-    
+
     using CallbacksImpl::callbacks_tuple;
 
     template <typename ElementAccumulator, int FragmentSize>
     CUTLASS_DEVICE auto
-    visit(int iter_idx, int row_idx, int column_idx, int frg_idx, 
+    visit(int iter_idx, int row_idx, int column_idx, int frg_idx,
           Array<ElementAccumulator, FragmentSize> const& frg_acc) {
       constexpr int Rm1 = sizeof...(Ops) - 1;
       auto frg_compute_tuple = cute::repeat<Rm1>(Array<ElementCompute, FragmentSize>{});
-      
+
       return cute::detail::tapply(EdgeTuple{}, callbacks_tuple, frg_compute_tuple,
         // Visit the first R-1 ops in topological order
         [&] (auto&& edge_seq, auto& callbacks, auto& frg_compute) {
           frg_compute = cute::detail::apply(frg_compute_tuple,
           // Compute the current op with children inputs
           [&] (auto const&... frg_inputs) {
             auto frg_output = callbacks.visit(iter_idx, row_idx, column_idx, frg_idx, frg_acc, frg_inputs...);
@@ -267,15 +267,15 @@
           },
           // Get inputs in the sequence given by the children indices of the current op
           edge_seq
         );
         return frg_compute;
       },
       // Visit the last op
-      [&] (auto const&...) {
+      [&] (auto const&...ops) {
         return cute::detail::apply(frg_compute_tuple,
           // Compute the last op with children inputs
           [&] (auto const&... frg_inputs) {
             return get<Rm1>(callbacks_tuple).visit(iter_idx, row_idx, column_idx, frg_idx, frg_acc, frg_inputs...);
           },
           // Get inputs in the sequence given by the children indices of the last op
           get<Rm1>(EdgeTuple{})
@@ -339,15 +339,15 @@
 >
 struct OutputTileThreadLayout: DefaultThreadMapTensorOp<
   ThreadblockShape_,
   WarpShape_,
   ThreadblockShape_::kK/WarpShape_::kK,
   Element_,
   ElementsPerAccess>::Type {
-  
+
   using Base = typename DefaultThreadMapTensorOp<
     ThreadblockShape_,
     WarpShape_,
     ThreadblockShape_::kK/WarpShape_::kK,
     Element_,
     ElementsPerAccess>::Type;
   using Base::Base;
```

## cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_load.hpp

```diff
@@ -109,14 +109,20 @@
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
     return args;
   }
 
+  template <class ProblemShape>
+  static size_t
+  get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
+    return 0;
+  }
+
   CUTLASS_HOST_DEVICE
   VisitorScalarBroadcast() { }
 
   CUTLASS_HOST_DEVICE
   VisitorScalarBroadcast(Params const& params, SharedStorage const& shared_storage)
       : params_ptr(&params) {
     // Get the scalar for non-batched broadcast
@@ -212,14 +218,20 @@
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
     return args;
   }
 
+  template <class ProblemShape>
+  static size_t
+  get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
+    return 0;
+  }
+
   // Software pipeline stages
   static const int Stages = ThreadMap::Stages;
 
   struct SharedStorage {};
 
   // Global load type
   static int constexpr vec_bits = ThreadMap::kElementsPerAccess * sizeof_bits<Element>::value;
@@ -337,14 +349,20 @@
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
     return args;
   }
 
+  template <class ProblemShape>
+  static size_t
+  get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
+    return 0;
+  }
+
   struct SharedStorage {};
 
   // Global load type
   static int constexpr vec_bits = ThreadMap::kElementsPerAccess * sizeof_bits<Element>::value;
   using VecType = uint_bit_t<cute::min(128, vec_bits)>;
   static int constexpr VecLength = sizeof(VecType) / sizeof(Element);
 
@@ -460,14 +478,20 @@
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
     return args;
   }
 
+  template <class ProblemShape>
+  static size_t
+  get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
+    return 0;
+  }
+
   struct SharedStorage { };
 
   CUTLASS_HOST_DEVICE
   VisitorColBroadcast() { }
 
   CUTLASS_HOST_DEVICE
   VisitorColBroadcast(Params const& params, SharedStorage const& shared_storage)
```

## cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_store.hpp

```diff
@@ -70,14 +70,20 @@
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
     return args;
   }
 
+  template <class ProblemShape>
+  static size_t
+  get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
+    return 0;
+  }
+
   struct SharedStorage {};
 
   static int constexpr vec_bits = ThreadMap::kElementsPerAccess * sizeof_bits<Element>::value;
   using VecType = uint_bit_t<cute::min(128, vec_bits)>;
   static int constexpr VecLength = sizeof(VecType) / sizeof(Element);
 
   CUTLASS_HOST_DEVICE
@@ -254,14 +260,20 @@
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
     return args;
   }
 
+  template <class ProblemShape>
+  static size_t
+  get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
+    return 0;
+  }
+
   struct SharedStorage { };
 
   CUTLASS_HOST_DEVICE
   VisitorColReduction() { }
 
   CUTLASS_HOST_DEVICE
   VisitorColReduction(Params const& params, SharedStorage const& shared_storage)
@@ -394,14 +406,20 @@
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
     return args;
   }
 
+  template <class ProblemShape>
+  static size_t
+  get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
+    return 0;
+  }
+
   using SharedStorageShape = decltype(select<0,1,2,3,5,8,10>(typename ThreadMap::ThreadMapShape{}));
 
   struct SharedStorage {
     AlignedArray<ElementCompute, size(SharedStorageShape{}), 16> reduction;
   };
 
   static int constexpr vec_bits = ThreadMap::kElementsPerAccess * sizeof_bits<ElementOutput>::value;
@@ -668,14 +686,20 @@
 
   template <class ProblemShape>
   static constexpr Params
   to_underlying_arguments(ProblemShape const& problem_shape, Arguments const& args, void* workspace) {
     return args;
   }
 
+  template <class ProblemShape>
+  static size_t
+  get_workspace_size(ProblemShape const& problem_shape, Arguments const& args) {
+    return 0;
+  }
+
   struct SharedStorage { };
 
   CUTLASS_HOST_DEVICE
   VisitorScalarReduction(){ };
 
   CUTLASS_HOST_DEVICE
   VisitorScalarReduction(Params const& params, SharedStorage const& shared_storage)
```

## cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h

```diff
@@ -104,15 +104,18 @@
   struct Detail {
     static int const kLanesInQuad = 4;
 
     /// Number of pointers needed to write accumulators
     static int const kPointerCount = 
       (OutputElementCount * sizeof_bits<Element>::value) / (const_min(128, OutputElementCount * sizeof_bits<Element>::value));
 
-    static_assert(kPointerCount <= 4, "Can only accommodate four pointers at present.");
+    // Currently support max 4 ptr
+    static constexpr int kMaxPointerCount{4};
+
+    static_assert(kPointerCount <= kMaxPointerCount, "Can only accommodate four pointers at present.");
     static_assert(sizeof(Element) == 4, "This can only be used with 32b accumulator data types (f32, s32).");
   };
 
   /// Padding quantity
   using Padding = MatrixShape<
     0,
     Detail::kLanesInQuad * Policy::kElementsPerAccess>;
@@ -123,32 +126,26 @@
   using AccessType = AlignedArray<Element, Policy::kElementsPerAccess>;
 
   //
   // Data members
   //
 
   /// Internal pointer to memory
-  AccessType *pointers_[Detail::kPointerCount];
+  AccessType *pointers_[Detail::kPointerCount] = {nullptr};
 
   /// Stride in units of AccessType
-  int stride_;
+  int stride_{0};
 
   /// Logical column in which warp tile is aligned
-  int warp_column_;
+  int warp_column_{0};
 
 public:
 
   /// Default constructor
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOpMixed() {
-    CUTLASS_PRAGMA_UNROLL
-    for (int64_t i = 0; i < Detail::kPointerCount; ++i) {
-      pointers_[i] = nullptr;
-    }
-  }
+  TileIteratorTensorOpMixed() = default;
 
   /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
   TileIteratorTensorOpMixed(
     TensorRef const &ref,
     unsigned lane_id
   ):
@@ -161,26 +158,15 @@
     CUTLASS_PRAGMA_UNROLL
     for (int64_t i = 0; i < Detail::kPointerCount; ++i) {
       AccessType *ptr = reinterpret_cast<AccessType *>(ref.data()) + quad_id * stride_;
       int column_idx = (lane_in_quad % 2) + (((lane_in_quad / 2) + i) % Detail::kPointerCount) * 2;
 
       ptr += column_idx;
 
-      if (i == 0) {
-        pointers_[0 % Detail::kPointerCount] = ptr;
-      }
-      else if (i == 1) {
-        pointers_[1 % Detail::kPointerCount] = ptr;
-      }
-      else if (i == 2) {
-        pointers_[2 % Detail::kPointerCount] = ptr;
-      }
-      else if (i == 3) {
-        pointers_[3 % Detail::kPointerCount] = ptr;
-      }
+      pointers_[i % Detail::kPointerCount] = ptr;
     }
   }
 
   /// Adds a pointer offset
   CUTLASS_HOST_DEVICE
   TileIteratorTensorOpMixed & add_pointer_offset(Index pointer_offset) {
 
@@ -375,32 +361,26 @@
   using AccessType = AlignedArray<Element, 2>;
 
   //
   // Data members
   //
 
   /// Internal pointer to memory
-  AccessType *pointers_[Detail::kPointerCount];
+  AccessType *pointers_[Detail::kPointerCount] = {nullptr};
 
   /// Stride in units of AccessType
-  int stride_;
+  int stride_{0};
 
   /// Uniform offset in bytes added to warp tile iterator
-  int uniform_offset_[Detail::kOffsetCount];
+  int uniform_offset_[Detail::kOffsetCount] = {0};
 
 public:
 
   /// Default constructor
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOpMixed() {
-    CUTLASS_PRAGMA_UNROLL
-    for (int64_t i = 0; i < Detail::kPointerCount; ++i) {
-      pointers_[i] = nullptr;
-    }
-  }
+  TileIteratorTensorOpMixed() = default;
 
   /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
   TileIteratorTensorOpMixed(
     TensorRef const &ref,
     unsigned lane_id
   ):
@@ -579,29 +559,23 @@
   using AccessType = AlignedArray<Element, 2>;
 
   //
   // Data members
   //
 
   /// Internal pointer to memory
-  AccessType *pointers_[Detail::kPointerCount];
+  AccessType *pointers_[Detail::kPointerCount] = {nullptr};
 
   /// Stride in units of AccessType
-  int stride_;
+  int stride_{0};
 
 public:
 
   /// Default constructor
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOpMixed() {
-    CUTLASS_PRAGMA_UNROLL
-    for (int64_t i = 0; i < Detail::kPointerCount; ++i) {
-      pointers_[i] = nullptr;
-    }
-  }
+  TileIteratorTensorOpMixed() = default;
 
   /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
   TileIteratorTensorOpMixed(
     TensorRef const &ref,
     unsigned lane_id
   ):
@@ -712,14 +686,394 @@
   /// Set smem base address
   CUTLASS_HOST_DEVICE
   void set_smem_base_address(Index address) {
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for float x 16 => float_e4m3_t/float_e5m2_t x 16
+template <
+  typename WarpShape_,            ///< shape of warp-level GEMM (concept: GemmShape)
+  typename OperatorShape_         ///< matrix multiply operation shape (concept: gemm::GemmShape),
+>
+class TileIteratorTensorOpMixed<WarpShape_, OperatorShape_, float, 32, 8, 16, 8> {
+public:
+
+  using WarpShape = WarpShape_;
+  using OperatorShape = OperatorShape_;
+  using Element = float;
+  using Layout = layout::RowMajor;
+  static int const kOutputElementCount = 16;
+
+  using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
+  using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
+  using Index = typename TensorRef::Index;
+  using LongIndex = typename TensorRef::LongIndex;
+
+  using Policy = TensorOpPolicy<WarpShape, OperatorShape, Layout>;
+
+  /// Shape of the tile in memory
+  using Shape = MatrixShape<
+    Policy::kRowsPerIteration,
+    WarpShape::kN
+  >;
+
+  /// This is the fragment size produced by one access of the iterator.
+  using Fragment = Array<
+    Element,
+    Policy::OperatorCount::kColumn * Policy::kElementsPerAccess>;
+
+  /// This is the complete warp-level accumulator tile.
+  //using AccumulatorTile = typename Operator::FragmentC;
+
+  /// Number of times this iterator can be incremented
+  static int const kIterations = Policy::kIterations;
+
+  // Internal constants
+  struct Detail {
+    static int const kLanesInQuad = 4;
+
+    /// Number of pointers needed to write accumulators
+    static int const kPointerCount = 2;
+
+    /// Offsets added
+    static int const kOffsetCount = 4;
+
+    static_assert(sizeof(Element) == 4, "This can only be used with 32b accumulator data types (f32, s32).");
+  };
+
+  /// Padding quantity
+  using Padding = MatrixShape<0, Detail::kLanesInQuad * 2>;
+
+private:
+
+  /// Storage type for accessing memory
+  using AccessType = AlignedArray<Element, 2>;
+
+  //
+  // Data members
+  //
+
+  /// Internal pointer to memory
+  AccessType *pointers_[Detail::kPointerCount] = {nullptr};
+
+  /// Stride in units of AccessType
+  int stride_{0};
+
+  /// Uniform offset in bytes added to warp tile iterator
+  int uniform_offset_[Detail::kOffsetCount] = {0};
+
+public:
+
+  /// Default constructor
+  TileIteratorTensorOpMixed() = default;
+
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  TileIteratorTensorOpMixed(
+    TensorRef const &ref,
+    unsigned lane_id
+  ):
+    stride_(ref.stride()[0] / AccessType::kElements) {
+
+    int quad_id = (lane_id / Detail::kLanesInQuad);
+    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < Detail::kPointerCount; ++i) {
+      AccessType *ptr = reinterpret_cast<AccessType *>(ref.data()) + quad_id * stride_;
+      int column_idx = lane_in_quad ^ (i * 2);
+
+      ptr += column_idx;
+
+      if (i == 0) {
+        pointers_[0] = ptr;
+      }
+      else if (i == 1) {
+        pointers_[1] = ptr;
+      }
+    }
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < Detail::kOffsetCount; ++i) {
+      uniform_offset_[i] = (i ^ 0) * 4 * sizeof(AccessType);
+    }
+  }
+
+  /// Adds a pointer offset
+  CUTLASS_HOST_DEVICE
+  TileIteratorTensorOpMixed & add_pointer_offset(Index pointer_offset) {
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int64_t i = 0; i < Detail::kPointerCount; ++i) {
+      pointers_[i] += pointer_offset / AccessType::kElements;
+    }
+
+    return *this;
+  }
+
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  CUTLASS_HOST_DEVICE
+  TileIteratorTensorOpMixed & add_tile_offset(TensorCoord const &tile_offset) {
+
+    int ptr_offset = tile_offset.row() * Shape::kRow * stride_ +
+      tile_offset.column() * Shape::kColumn / AccessType::kElements;
+
+    pointers_[0] += ptr_offset;
+    pointers_[1] += ptr_offset;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < Detail::kOffsetCount; ++i) {
+      uniform_offset_[i] = (i ^ tile_offset.column()) * 4 * sizeof(AccessType);
+    }
+
+    return *this;
+  }
+
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  CUTLASS_HOST_DEVICE
+  TileIteratorTensorOpMixed & operator+=(TensorCoord const &tile_offset) {
+    return add_tile_offset(tile_offset);
+  }
+
+  /// Store
+  CUTLASS_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int n = 0; n < Policy::OperatorCount::kColumn; ++n) {
+
+      int ptr_idx = (n / 4);
+      int offset_idx = (n % 4);
+
+      AccessType *ptr;
+      if (ptr_idx == 0) {
+        ptr = pointers_[0];
+      }
+      else if (ptr_idx == 1) {
+        ptr = pointers_[1];
+      }
+
+      int offset = (n / 4) * 16 + pointer_offset / AccessType::kElements;
+
+#if 0
+      //
+      // Using inline PTX to avoid generic memory
+      //
+      AccessType *smem_ptr = pointers_[ptr_idx];
+      smem_ptr[offset] = frag_ptr[n];
+#else
+      uint32_t smem_addr = arch::cutlass_get_smem_pointer(ptr);
+      uint32_t const *data = reinterpret_cast<uint32_t const *>(frag_ptr + n);
+      uint32_t offset_in_bytes = offset * sizeof(AccessType) + uniform_offset_[offset_idx];
+
+      asm volatile(
+        "{ .reg .u32 smem_ptr; add.u32 smem_ptr, %0, %1; st.shared.v2.u32 [smem_ptr], {%2, %3}; }\n"
+        : : "r"(smem_addr), "r"(offset_in_bytes), "r"(data[0]), "r"(data[1])
+      );
+#endif
+    }
+  }
+
+  /// Store
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    store_with_pointer_offset(frag, 0);
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for float x 8 => float_e4m3_t/float_e5m2_t x 8
+template <
+  typename WarpShape_,            ///< shape of warp-level GEMM (concept: GemmShape)
+  typename OperatorShape_         ///< matrix multiply operation shape (concept: gemm::GemmShape)
+>
+class TileIteratorTensorOpMixed<WarpShape_, OperatorShape_, float, 32, 8, 8, 8> {
+public:
+
+  using WarpShape = WarpShape_;
+  using OperatorShape = OperatorShape_;
+  using Element = float;
+  using Layout = layout::RowMajor;
+  static int const kOutputElementCount = 8;
+
+  using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
+  using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
+  using Index = typename TensorRef::Index;
+  using LongIndex = typename TensorRef::LongIndex;
+
+  using Policy = TensorOpPolicy<WarpShape, OperatorShape, Layout>;
+
+  /// Shape of the tile in memory
+  using Shape = MatrixShape<
+    Policy::kRowsPerIteration,
+    WarpShape::kN
+  >;
+
+  /// This is the fragment size produced by one access of the iterator.
+  using Fragment = Array<
+    Element,
+    Policy::OperatorCount::kColumn * Policy::kElementsPerAccess>;
+
+  /// This is the complete warp-level accumulator tile.
+  //using AccumulatorTile = typename Operator::FragmentC;
+
+  /// Number of times this iterator can be incremented
+  static int const kIterations = Policy::kIterations;
+
+  // Internal constants
+  struct Detail {
+    static int const kLanesInQuad = 4;
+
+    /// Number of pointers needed to write accumulators
+    static int const kPointerCount = 2;
+
+    static_assert(sizeof(Element) == 4, "This can only be used with 32b accumulator data types (f32, s32).");
+  };
+
+  /// Padding quantity
+  using Padding = MatrixShape<0, Detail::kLanesInQuad * 2>;
+
+private:
+
+  /// Storage type for accessing memory
+  using AccessType = AlignedArray<Element, 2>;
+
+  //
+  // Data members
+  //
+
+  /// Internal pointer to memory
+  AccessType *pointers_[Detail::kPointerCount] = {nullptr};
+
+  /// Stride in units of AccessType
+  int stride_{0};
+
+public:
+
+  /// Default constructor
+  TileIteratorTensorOpMixed() = default;
+
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  TileIteratorTensorOpMixed(
+    TensorRef const &ref,
+    unsigned lane_id
+  ):
+    stride_(ref.stride()[0] / AccessType::kElements) {
+
+    int quad_id = (lane_id / Detail::kLanesInQuad);
+    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < Detail::kPointerCount; ++i) {
+      AccessType *ptr = reinterpret_cast<AccessType *>(ref.data()) + quad_id * stride_;
+      int column_idx = lane_in_quad ^ (i * 2);
+
+      ptr += column_idx;
+
+      if (i == 0) {
+        pointers_[0] = ptr;
+      }
+      else if (i == 1) {
+        pointers_[1] = ptr;
+      }
+    }
+  }
+
+  /// Adds a pointer offset
+  CUTLASS_HOST_DEVICE
+  TileIteratorTensorOpMixed & add_pointer_offset(Index pointer_offset) {
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int64_t i = 0; i < Detail::kPointerCount; ++i) {
+      pointers_[i] += pointer_offset / AccessType::kElements;
+    }
+
+    return *this;
+  }
+
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  CUTLASS_HOST_DEVICE
+  TileIteratorTensorOpMixed & add_tile_offset(TensorCoord const &tile_offset) {
+
+    int ptr_offset = tile_offset.row() * Shape::kRow * stride_ +
+      tile_offset.column() * Shape::kColumn / AccessType::kElements;
+
+    pointers_[0] += ptr_offset;
+    pointers_[1] += ptr_offset;
+
+    if (tile_offset.column() % 2) {
+      auto tmp = pointers_[0];
+      pointers_[0] = pointers_[1];
+      pointers_[1] = tmp;
+    }
+
+    return *this;
+  }
+
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  CUTLASS_HOST_DEVICE
+  TileIteratorTensorOpMixed & operator+=(TensorCoord const &tile_offset) {
+    return add_tile_offset(tile_offset);
+  }
+
+  /// Store
+  CUTLASS_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int n = 0; n < Policy::OperatorCount::kColumn; ++n) {
+
+      int ptr_idx = (n / 4);
+
+      AccessType *ptr;
+      if (ptr_idx == 0) {
+        ptr = pointers_[0];
+      }
+      else if (ptr_idx == 1) {
+        ptr = pointers_[1];
+      }
+
+      int offset = (n / 4) * 16 + pointer_offset / AccessType::kElements + (n % 4) * 4;
+
+#if 0
+      //
+      // Using inline PTX to avoid generic memory
+      //
+      AccessType *smem_ptr = pointers_[ptr_idx];
+      smem_ptr[offset] = frag_ptr[n];
+#else
+      uint32_t smem_addr = arch::cutlass_get_smem_pointer(ptr);
+      uint32_t const *data = reinterpret_cast<uint32_t const *>(frag_ptr + n);
+      uint32_t offset_in_bytes = offset * sizeof(AccessType);
+
+      asm volatile(
+        "{ .reg .u32 smem_ptr; add.u32 smem_ptr, %0, %1; st.shared.v2.u32 [smem_ptr], {%2, %3}; }\n"
+        : : "r"(smem_addr), "r"(offset_in_bytes), "r"(data[0]), "r"(data[1])
+      );
+#endif
+    }
+  }
+
+  /// Store
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    store_with_pointer_offset(frag, 0);
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 } // namespace warp
 } // namespace epilogue
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #undef CUTLASS_EPILOGUE_WARP_TILE_ITERATOR_TENSOR_OP_MIXED_OPTIMIZATION_ENABLED
```

## cutlass_library/source/include/cutlass/gemm/dispatch_policy.hpp

```diff
@@ -33,19 +33,45 @@
 #include "cutlass/arch/arch.h"
 #include "cutlass/gemm/gemm.h"
 
 #include "cute/layout.hpp"
 #include "cute/numeric/integral_constant.hpp"
 //////////////////////////////////////////////////////////////////////////////
 
+namespace cutlass::detail {
+
+template <class T, template <int...> class U>
+struct is_kernel_tag_of : cute::false_type {};
+
+template <template <int...> class U, int... Args>
+struct is_kernel_tag_of<U<Args...>, U> : cute::true_type {};
+
+template <class T, template <int...> class U>
+constexpr bool is_kernel_tag_of_v = is_kernel_tag_of<T, U>::value;
+
+}
+
+//////////////////////////////////////////////////////////////////////////////
+
 namespace cutlass::gemm {
 using namespace cute;
 
 //////////////////////////////////////////////////////////////////////////////
 
+namespace detail {
+
+enum class KernelInputTransformType {
+    FastF32,
+    InterleavedComplexTF32
+};
+
+} // namespace detail
+
+//////////////////////////////////////////////////////////////////////////////
+
 //
 // Kernel schedule policies (the base class tags, one for each kernel layer file)
 //
 struct KernelMultistage { };
 struct KernelCpAsyncWarpSpecialized { };
 struct KernelCpAsyncWarpSpecializedPingpong { };
 struct KernelCpAsyncWarpSpecializedCooperative { };
@@ -54,15 +80,15 @@
 struct KernelTmaWarpSpecializedPingpong { };
 struct KernelTmaWarpSpecializedCooperative { };
 struct KernelPtrArrayTmaWarpSpecializedCooperative { };
 
 //////////////////////////////////////////////////////////////////////////////
 
 //
-// Builder dispatch policies (not a part of the main CUTLASS layers, simply used to opt into 
+// Builder dispatch policies (not a part of the main CUTLASS layers, simply used to opt into
 // specific collective builder dispatches)
 //
 
 // FP8 related policies (including Fast Accumulation)
 struct KernelTmaWarpSpecializedFP8FastAccum : KernelTmaWarpSpecialized { };
 struct KernelTmaWarpSpecializedPingpongFP8FastAccum : KernelTmaWarpSpecializedPingpong { };
 struct KernelTmaWarpSpecializedCooperativeFP8FastAccum: KernelTmaWarpSpecializedCooperative { };
@@ -215,15 +241,15 @@
 // For FP8 kernels
 template<
   int Stages_,
   class ClusterShape_ = Shape<_1,_1,_1>,
   class KernelSchedule = KernelTmaWarpSpecialized
 >
 struct MainloopSm90TmaGmmaWarpSpecializedFP8
-  : MainloopSm90TmaGmmaWarpSpecialized<Stages_, ClusterShape_, KernelSchedule> { 
+  : MainloopSm90TmaGmmaWarpSpecialized<Stages_, ClusterShape_, KernelSchedule> {
   static_assert(
     cute::is_same_v<KernelSchedule, KernelTmaWarpSpecialized> ||
     cute::is_same_v<KernelSchedule, KernelTmaWarpSpecializedPingpong> ||
     cute::is_same_v<KernelSchedule, KernelTmaWarpSpecializedCooperative>,
     "KernelSchedule must be one of the warp specialized policies");
 };
```

## cutlass_library/source/include/cutlass/gemm/gemm_enumerated_types.h

```diff
@@ -28,24 +28,14 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Defines common types used for all GEMM-like operators.
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/coord.h"
 #include "cutlass/gemm_coord.h"
 #include "cutlass/layout/matrix.h"
```

## cutlass_library/source/include/cutlass/gemm/collective/collective_builder.hpp

```diff
@@ -35,18 +35,28 @@
 
 namespace cutlass::gemm::collective {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // Used to specify stage counts or dispatch to automatic computation of stage count
 template<int num_stages>
-struct StageCount { static constexpr int value = num_stages; };
+struct StageCount {
+  static constexpr int value = num_stages;
+
+  StageCount() = default;
+  explicit StageCount(cute::Int<num_stages>) {}
+};
 
 template<int carveout_bytes>
-struct StageCountAutoCarveout { static constexpr int bytes = carveout_bytes; };
+struct StageCountAutoCarveout {
+  static constexpr int bytes = carveout_bytes;
+
+  StageCountAutoCarveout() = default;
+  explicit StageCountAutoCarveout(cute::Int<carveout_bytes>) {}
+};
 
 using StageCountAuto = StageCountAutoCarveout<0>;
 
 // Used to automatically let the builder pick the kernel schedule.
 // Can be overridden with kernel schedule tags in cutlass/gemm/dispatch_policy.hpp
 struct KernelScheduleAuto {};
```

## cutlass_library/source/include/cutlass/gemm/collective/sm80_mma_multistage.hpp

```diff
@@ -96,14 +96,17 @@
   using SmemLayoutAtomA = SmemLayoutAtomA_;
   using SmemLayoutAtomB = SmemLayoutAtomB_;
   using SmemCopyAtomA = SmemCopyAtomA_;
   using SmemCopyAtomB = SmemCopyAtomB_;
   using TransformA = TransformA_;
   using TransformB = TransformB_;
   using ArchTag = typename DispatchPolicy::ArchTag;
+  // Follow the change in TestSmall: TileShape switch to CtaShape 
+  // For sm80 arch, CtaShape should euqal to TileShape
+  using CtaShape_MNK = TileShape;
 
   static_assert(cute::rank(SmemLayoutAtomA{}) == 2, "SmemLayoutAtom must be rank 2 (M/N, K)");
   static_assert((size<0>(TileShape{}) % size<0>(SmemLayoutAtomA{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
   static_assert((size<2>(TileShape{}) % size<1>(SmemLayoutAtomA{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
 
   static_assert(cute::rank(SmemLayoutAtomB{}) == 2, "SmemLayoutAtom must be rank 2 (M/N, K)");
   static_assert((size<1>(TileShape{}) % size<0>(SmemLayoutAtomB{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
@@ -328,14 +331,16 @@
         cute::transform(tCrB(_,_,k_block), TransformB{});
         // Thread-level register gemm for k_block
         cute::gemm(tiled_mma, accum, tCrA(_,_,k_block), tCrB(_,_,k_block), src_accum);
       });
 
     }
 
+    cp_async_wait<0>();
+    __syncthreads();
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   int Stages,
@@ -373,29 +378,31 @@
    >
 {
   //
   // Type Aliases
   //
   using DispatchPolicy = MainloopSm80CpAsync<Stages>;
   using TileShape = TileShape_;
+  // Follow the change in TestSmall: TileShape switch to CtaShape 
+  // In legacy arch, it should be same
+  using CtaShape_MNK = TileShape;
   using ElementA = ElementA_;
   using StrideA = StrideA_;
   using ElementB = ElementB_;
   using StrideB = StrideB_;
   using TiledMma = TiledMma_;
   using ElementAccumulator = typename TiledMma::ValTypeC;  using GmemTiledCopyA = GmemTiledCopyA_;
   using GmemTiledCopyB = GmemTiledCopyB_;
   using SmemLayoutAtomA = SmemLayoutAtomA_;
   using SmemLayoutAtomB = SmemLayoutAtomB_;
   using SmemCopyAtomA = SmemCopyAtomA_;
   using SmemCopyAtomB = SmemCopyAtomB_;
   using TransformA = TransformA_;
   using TransformB = TransformB_;
   using ArchTag = typename DispatchPolicy::ArchTag;
-
   static_assert(cute::rank(SmemLayoutAtomA{}) == 2, "SmemLayoutAtom must be rank 2 (M/N, K)");
   static_assert((size<0>(TileShape{}) % size<0>(SmemLayoutAtomA{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
   static_assert((size<2>(TileShape{}) % size<1>(SmemLayoutAtomA{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
 
   static_assert(cute::rank(SmemLayoutAtomB{}) == 2, "SmemLayoutAtom must be rank 2 (M/N, K)");
   static_assert((size<1>(TileShape{}) % size<0>(SmemLayoutAtomB{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
   static_assert((size<2>(TileShape{}) % size<1>(SmemLayoutAtomB{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
@@ -676,14 +683,16 @@
         cute::transform(tCrB(_,_,k_block), TransformB{});
         // Thread-level register gemm for k_block
         cute::gemm(tiled_mma, accum, tCrA(_,_,k_block), tCrB(_,_,k_block), src_accum);
       });
 
     }
 
+    cp_async_wait<0>();
+    __syncthreads();
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace cutlass::gemm::collective
```

## cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_array_tma_gmma_ss_warpspecialized.hpp

```diff
@@ -27,25 +27,26 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cute/arch/cluster_sm90.hpp"
-#include "cute/arch/copy_sm90.hpp"
 #include "cutlass/gemm/dispatch_policy.hpp"
+#include "cutlass/numeric_types.h"
+#include "cutlass/pipeline/pipeline.hpp"
+#include "cutlass/trace.h"
 
+#include "cute/arch/cluster_sm90.hpp"
+#include "cute/arch/copy_sm90.hpp"
 #include "cute/algorithm/functional.hpp"
 #include "cute/atom/mma_atom.hpp"
 #include "cute/algorithm/gemm.hpp"
 #include "cute/tensor_predicate.hpp"
 #include "cute/numeric/arithmetic_tuple.hpp"
-#include "cutlass/pipeline/pipeline.hpp"
-#include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass::gemm::collective {
 using namespace cute;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -109,32 +110,32 @@
   using TransformB = TransformB_;
   using ArchTag = typename DispatchPolicy::ArchTag;
 
   using MainloopPipeline = cutlass::PipelineTmaAsync<DispatchPolicy::Stages>;
   using PipelineState = cutlass::PipelineState<DispatchPolicy::Stages>;
 
   using PipelineParams = typename MainloopPipeline::Params;
-
+  using CtaShape_MNK = decltype(shape_div(TileShape{}, ClusterShape{}));
   static_assert(rank(SmemLayoutAtomA{}) == 2, "SmemLayoutAtom must be rank 2 (M/N, K)");
   static_assert((size<0>(TileShape{}) % size<0>(SmemLayoutAtomA{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
   static_assert((size<2>(TileShape{}) % size<1>(SmemLayoutAtomA{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
 
   static_assert(rank(SmemLayoutAtomB{}) == 2, "SmemLayoutAtom must be rank 2 (M/N, K)");
   static_assert((size<1>(TileShape{}) % size<0>(SmemLayoutAtomB{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
   static_assert((size<2>(TileShape{}) % size<1>(SmemLayoutAtomB{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
 
   // Tile along modes in a way that maximizes the TMA box size.
   using SmemLayoutA = decltype(tile_to_shape(
       SmemLayoutAtomA{},
       make_shape(shape<0>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,StrideA>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
+      cute::conditional_t< ::cutlass::gemm::detail::is_major<0,StrideA>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
   using SmemLayoutB = decltype(tile_to_shape(
       SmemLayoutAtomB{},
       make_shape(shape<1>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,StrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
+      cute::conditional_t< ::cutlass::gemm::detail::is_major<0,StrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
 
   static_assert(DispatchPolicy::Stages >= 2, "Specialization requires Stages set to value 2 or more.");
   static_assert(cute::is_base_of<cute::GMMA::DescriptorIterator, typename TiledMma::FrgTypeA>::value &&
                 cute::is_base_of<cute::GMMA::DescriptorIterator, typename TiledMma::FrgTypeB>::value,
                 "MMA atom must source both A and B operand from smem_desc for this mainloop.");
   static_assert(cute::is_same_v<GmemTiledCopyA, SM90_TMA_LOAD> || cute::is_same_v<GmemTiledCopyA, SM90_TMA_LOAD_MULTICAST>,
       "GmemTiledCopy - invalid SM90 TMA copy atom specified.");
@@ -308,17 +309,16 @@
     }
     return implementable;
   }
 
   static constexpr int K_PIPE_MAX = DispatchPolicy::Stages;
   static constexpr int K_PIPE_MMAS = 1;
   static constexpr uint32_t TmaTransactionBytes =
-        (size<0>(SmemLayoutA{}) * size<1>(SmemLayoutA{}) * static_cast<uint32_t>(sizeof_bits<ElementA>::value)) / 8+
-        (size<0>(SmemLayoutB{}) * size<1>(SmemLayoutB{}) * static_cast<uint32_t>(sizeof_bits<ElementB>::value)) / 8;
-
+        cutlass::bits_to_bytes(size<0>(SmemLayoutA{}) * size<1>(SmemLayoutA{}) * static_cast<uint32_t>(sizeof_bits<ElementA>::value))+
+        cutlass::bits_to_bytes(size<0>(SmemLayoutB{}) * size<1>(SmemLayoutB{}) * static_cast<uint32_t>(sizeof_bits<ElementB>::value));
 
   // Set up the data needed by this collective for load and mma.
   // Returns a tuple of tensors. The collective and the kernel layer have the contract that the
   // returned tuple must contain at least two elements, with the first two elements being:
   // gA_mkl - The tma tensor, A after a local tile so it has shape  (BLK_M,BLK_K,m,k,l)
   // gB_nkl - The tma tensor, B after a local tile so it has shape  (BLK_N,BLK_K,n,k,l)
   // The rest of the tensors can be specified as needed by this collective.
```

## cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_multistage_gmma_rs_warpspecialized.hpp

```diff
@@ -27,26 +27,27 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cute/arch/cluster_sm90.hpp"
-#include "cute/arch/copy_sm90.hpp"
 #include "cutlass/gemm/dispatch_policy.hpp"
+#include "cutlass/numeric_types.h"
+#include "cutlass/pipeline/pipeline.hpp"
+#include "cutlass/transform/collective/sm90_wgmma_transpose.hpp"
+#include "cutlass/trace.h"
 
+#include "cute/arch/cluster_sm90.hpp"
+#include "cute/arch/copy_sm90.hpp"
 #include "cute/algorithm/functional.hpp"
 #include "cute/atom/mma_atom.hpp"
 #include "cute/algorithm/gemm.hpp"
 #include "cute/tensor_predicate.hpp"
 #include "cute/numeric/arithmetic_tuple.hpp"
-#include "cutlass/pipeline/pipeline.hpp"
-#include "cutlass/transform/collective/sm90_wgmma_transpose.hpp"
-#include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass::gemm::collective {
 using namespace cute;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -102,14 +103,15 @@
   using GmemTiledCopyA = GmemTiledCopyA_;
   using GmemTiledCopyB = GmemTiledCopyB_;
   using SmemLayoutAtomA = SmemLayoutAtomA_;
   using SmemLayoutAtomB = SmemLayoutAtomB_;
   using SmemCopyAtomA = SmemCopyAtomA_;
   using SmemCopyAtomB = SmemCopyAtomB_;
 
+  using CtaShape_MNK = decltype(shape_div(TileShape{}, ClusterShape{}));
   // Swap and transpose A/B for A k-major layout and B mn-major layout since WGMMA is k-major only (e.g. tf32, Fp32, Int8, Fp8 WGMMA)
   static constexpr bool IsLayoutAkBmn =
     cute::is_same_v<gemm::detail::StrideToLayoutTagA_t<StrideA>, layout::RowMajor> &&
     cute::is_same_v<gemm::detail::StrideToLayoutTagB_t<StrideB>, layout::RowMajor>;
 
   static constexpr bool IsInputSizeTwoBytes = sizeof(ElementA) == 2 && sizeof(ElementB) == 2;
   static constexpr bool SwapAB =  !IsInputSizeTwoBytes && IsLayoutAkBmn;
@@ -174,15 +176,15 @@
   using GmmaSmemLayoutB = decltype(tile_to_shape(
       GmmaSmemLayoutAtomB{},
       make_shape(shape<1>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{})));
 
   static_assert(!SwapAB || !TransposeB, "Cannot SwapAB and TransposeB at the same time.");
   static_assert(TransposeB xor (cute::is_same_v<SmemLayoutB, GmmaSmemLayoutB>),
     "Should be same layout if not TransposeB.");
-  static_assert(!TransposeB || ((size<1>(SmemLayoutB{}) * sizeof_bits<InternalElementB>::value) / 8) == 128,
+  static_assert(!TransposeB || (cutlass::bits_to_bytes(size<1>(SmemLayoutB{}) * sizeof_bits<InternalElementB>::value)) == 128,
     "SmemLayoutB K must be 128bytes to be transposed.");
   static_assert(!transform::collective::detail::use_universal_transposition<InternalSmemLayoutAtomB, InternalElementB>(),
     "Warp specialized ARF kernels have not supported universal B transposition yet.");
 
   struct SharedStorage
   {
     struct TensorStorage : cute::aligned_struct<256> {
```

## cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_multistage_gmma_ss_warpspecialized.hpp

```diff
@@ -104,14 +104,15 @@
   using SmemLayoutAtomB = SmemLayoutAtomB_;
   using SmemCopyAtomA = SmemCopyAtomA_;
   using SmemCopyAtomB = SmemCopyAtomB_;
   using TransformA = TransformA_;
   using TransformB = TransformB_;
   using ArchTag = typename DispatchPolicy::ArchTag;
 
+  using CtaShape_MNK = decltype(shape_div(TileShape{}, ClusterShape{}));
   using MainloopPipeline = cutlass::PipelineAsync<DispatchPolicy::Stages>;
   using PipelineState    = typename MainloopPipeline::PipelineState;
   using PipelineParams   = typename MainloopPipeline::Params;
 
   static_assert(cute::rank(SmemLayoutAtomA{}) == 2, "SmemLayoutAtom must be rank 2 (M/N, K)");
   static_assert((size<0>(TileShape{}) % size<0>(SmemLayoutAtomA{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
   static_assert((size<2>(TileShape{}) % size<1>(SmemLayoutAtomA{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
```

## cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized.hpp

```diff
@@ -27,29 +27,30 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cute/arch/cluster_sm90.hpp"
-#include "cute/arch/copy_sm90.hpp"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/detail/dependent_false.hpp"
 #include "cutlass/gemm/dispatch_policy.hpp"
 #include "cutlass/detail/layout.hpp"
+#include "cutlass/numeric_types.h"
+#include "cutlass/pipeline/pipeline.hpp"
+#include "cutlass/transform/collective/sm90_wgmma_transpose.hpp"
+#include "cutlass/trace.h"
 
+#include "cute/arch/cluster_sm90.hpp"
+#include "cute/arch/copy_sm90.hpp"
 #include "cute/algorithm/functional.hpp"
 #include "cute/atom/mma_atom.hpp"
 #include "cute/algorithm/gemm.hpp"
 #include "cute/tensor_predicate.hpp"
 #include "cute/numeric/arithmetic_tuple.hpp"
-#include "cutlass/pipeline/pipeline.hpp"
-#include "cutlass/transform/collective/sm90_wgmma_transpose.hpp"
-#include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass::gemm::collective {
 using namespace cute;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -104,14 +105,15 @@
   using GmemTiledCopyA = GmemTiledCopyA_;
   using GmemTiledCopyB = GmemTiledCopyB_;
   using SmemLayoutAtomA = SmemLayoutAtomA_;
   using SmemLayoutAtomB = SmemLayoutAtomB_;
   using SmemCopyAtomA = SmemCopyAtomA_;
   using SmemCopyAtomB = SmemCopyAtomB_;
 
+  using CtaShape_MNK = decltype(shape_div(TileShape{}, ClusterShape{}));
   // Swap and transpose A/B for A k-major layout and B mn-major layout since WGMMA is k-major only (e.g. tf32, Fp32, Int8, Fp8 WGMMA)
   static constexpr bool IsLayoutAkBmn =
     cute::is_same_v<gemm::detail::StrideToLayoutTagA_t<StrideA>, layout::RowMajor> &&
     cute::is_same_v<gemm::detail::StrideToLayoutTagB_t<StrideB>, layout::RowMajor>;
 
   static constexpr bool IsInputSizeTwoBytes = sizeof(ElementA) == 2 && sizeof(ElementB) == 2;
   static constexpr bool SwapAB =  !IsInputSizeTwoBytes && IsLayoutAkBmn;
@@ -147,19 +149,19 @@
   static_assert((size<1>(TileShape{}) % size<0>(InternalSmemLayoutAtomB{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
   static_assert((size<2>(TileShape{}) % size<1>(InternalSmemLayoutAtomB{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
 
   // Tile along modes in a way that maximizes the TMA box size.
   using SmemLayoutA = decltype(tile_to_shape(
       InternalSmemLayoutAtomA{},
       make_shape(shape<0>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,InternalStrideA>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
+      cute::conditional_t< ::cutlass::gemm::detail::is_major<0,InternalStrideA>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
   using SmemLayoutB = decltype(tile_to_shape(
       InternalSmemLayoutAtomB{},
       make_shape(shape<1>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,InternalStrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
+      cute::conditional_t< ::cutlass::gemm::detail::is_major<0,InternalStrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
 
   // If A mn-layout and B mn-layout, transposing B matrix since WGMMA is k-major only (e.g. tf32, fp32, fp8, int8).
   static constexpr bool IsLayoutAmnBmn =
     cute::is_same_v<gemm::detail::StrideToLayoutTagA_t<StrideA>, layout::ColumnMajor> &&
     cute::is_same_v<gemm::detail::StrideToLayoutTagB_t<StrideB>, layout::RowMajor>;
   static constexpr bool TransposeB = !IsInputSizeTwoBytes && IsLayoutAmnBmn;
   using TransposeOperandB = decltype(cutlass::transform::collective::detail::make_transpose_operand_b(
@@ -178,20 +180,20 @@
   using GmmaSmemLayoutAtomB = decltype(transform::collective::detail::gmma_smem_transpose_or_passthrough<
       TransposeB, InternalSmemLayoutAtomB, InternalElementB>());
 
   // SmemLayoutB for GMMA is different from SmemLayoutB for TMA if TransposeB
   using GmmaSmemLayoutB = decltype(tile_to_shape(
       GmmaSmemLayoutAtomB{},
       make_shape(shape<1>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,InternalStrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
+      cute::conditional_t< ::cutlass::gemm::detail::is_major<0,InternalStrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
 
   static_assert(!SwapAB || !TransposeB, "Cannot SwapAB and TransposeB at the same time.");
   static_assert(TransposeB xor (cute::is_same_v<SmemLayoutB, GmmaSmemLayoutB>),
     "Should be same layout if not TransposeB.");
-  static_assert(!TransposeB || (((size<1>(SmemLayoutB{}) * sizeof_bits<InternalElementB>::value)) / 8) == 128,
+  static_assert(!TransposeB || (cutlass::bits_to_bytes((size<1>(SmemLayoutB{}) * sizeof_bits<InternalElementB>::value))) == 128,
     "SmemLayoutB K must be 128bytes to be transposed.");
 
   static constexpr bool uses_universal_transposition() {
     if constexpr (TransposeB) {
       return transform::collective::detail::use_universal_transposition<InternalSmemLayoutAtomB, InternalElementB>();
     }
     else {
@@ -325,16 +327,16 @@
       CUTLASS_TRACE_HOST("  CAN IMPLEMENT: Problem Size doesn't meet the minimum alignment requirements for TMA.\n");
     }
     return implementable;
   }
 
   static constexpr int K_PIPE_MAX = DispatchPolicy::Stages;
   static constexpr uint32_t TmaTransactionBytes =
-        (size<0>(SmemLayoutA{}) * size<1>(SmemLayoutA{}) * static_cast<uint32_t>(sizeof_bits<InternalElementA>::value)) / 8 +
-        (size<0>(SmemLayoutB{}) * size<1>(SmemLayoutB{}) * static_cast<uint32_t>(sizeof_bits<InternalElementB>::value)) / 8 ;
+        cutlass::bits_to_bytes(size<0>(SmemLayoutA{}) * size<1>(SmemLayoutA{}) * static_cast<uint32_t>(sizeof_bits<InternalElementA>::value)) +
+        cutlass::bits_to_bytes(size<0>(SmemLayoutB{}) * size<1>(SmemLayoutB{}) * static_cast<uint32_t>(sizeof_bits<InternalElementB>::value)) ;
 
   /// Issue Tma Descriptor Prefetch -- ideally from a single thread for best performance
   CUTLASS_DEVICE
   static void prefetch_tma_descriptors(Params const& mainloop_params) {
     cute::prefetch_tma_descriptor(mainloop_params.tma_load_a.get_tma_descriptor());
     cute::prefetch_tma_descriptor(mainloop_params.tma_load_b.get_tma_descriptor());
   }
@@ -543,14 +545,15 @@
     CUTE_STATIC_ASSERT_V(size<2>(tCsA_copy_view) == size<2>(tCrA_copy_view));                                  // CPY_K
     CUTE_STATIC_ASSERT_V(size<1>(tCrA) == size<1>(accum));                                                     // MMA_M
     CUTE_STATIC_ASSERT_V(size<1>(tCsB) == size<2>(accum));                                                         // N
     CUTE_STATIC_ASSERT_V(size<2>(tCsA) == size<2>(tCsB));                                                          // K
     CUTE_STATIC_ASSERT_V(size<3>(tCsA) == size<3>(tCsB));                                                       // PIPE
     CUTE_STATIC_ASSERT_V(Int<DispatchPolicy::Stages>{} == size<2>(sA));                                         // PIPE
     CUTE_STATIC_ASSERT_V(Int<DispatchPolicy::Stages>{} == size<2>(sB));                                         // PIPE
+    CUTE_STATIC_ASSERT_V(size<2>(tCrA) > _2{}, "RS loops require more than 2 MMA k-iterations for correctness.");
 
     //
     // PIPELINED MAIN LOOP
     //
 
     // We release buffers to producer warps(dma load) with some mmas in flight
     PipelineState smem_pipe_release = smem_pipe_read;
```

## cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized_mixed_input.hpp

```diff
@@ -28,31 +28,34 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_conversion.h"
-#include "cute/arch/cluster_sm90.hpp"
-#include "cute/arch/copy_sm90.hpp"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/detail/dependent_false.hpp"
 #include "cutlass/gemm/dispatch_policy.hpp"
+#include "cutlass/numeric_types.h"
 #include "cutlass/detail/layout.hpp"
+#include "cutlass/pipeline/pipeline.hpp"
+#include "cutlass/transform/collective/sm90_wgmma_transpose.hpp"
+#include "cutlass/trace.h"
+#include "cutlass/detail/collective.hpp"
 
+#include "cute/arch/cluster_sm90.hpp"
+#include "cute/arch/copy_sm90.hpp"
 #include "cute/algorithm/functional.hpp"
 #include "cute/atom/mma_atom.hpp"
 #include "cute/atom/copy_traits_sm90_tma.hpp"
 #include "cute/algorithm/gemm.hpp"
 #include "cute/tensor_predicate.hpp"
 #include "cute/numeric/arithmetic_tuple.hpp"
 #include "cutlass/pipeline/pipeline.hpp"
-#include "cutlass/transform/collective/sm90_wgmma_transpose.hpp"
 #include "cutlass/trace.h"
-
 #include "cutlass/detail/collective.hpp"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass::gemm::collective {
 using namespace cute;
 
@@ -152,14 +155,15 @@
                 (cutlass::gemm::detail::is_k_major<StrideA>() && 
                  cutlass::gemm::detail::is_k_major<StrideB>()), 
                 "The unscaled element must be 2 bytes OR both inputs must be K-major");
 
   static_assert(cutlass::gemm::detail::is_mn_major<NonVoidStrideScale>(), 
     "Scale must be MN major [Col Major if A is scaled, Row Major if B is scaled].");
 
+  using CtaShape_MNK = decltype(shape_div(TileShape{}, ClusterShape{}));
 
   using TiledMma = TiledMma_;
   using ElementAccumulator = typename TiledMma::ValTypeC;
 
   using GmemTiledCopyA = GmemTiledCopyA_;
   using GmemTiledCopyB = GmemTiledCopyB_;
   using GmemTiledCopyScale = cute::SM90_TMA_LOAD;
@@ -168,21 +172,14 @@
   using SmemLayoutAtomB = SmemLayoutAtomB_;
   // Scale layout atom set after swapping.
 
   using SmemCopyAtomA = SmemCopyAtomA_;
   using SmemCopyAtomB = SmemCopyAtomB_;
   using SmemCopyAtomScale = Copy_Atom<cute::DefaultCopy, NonVoidElementScale>;
 
-  // Swap and transpose A/B for A k-major layout and B mn-major layout since WGMMA is k-major only (e.g. tf32, Fp32, Int8, Fp8 WGMMA)
-  static constexpr bool IsLayoutAkBmn =
-    cute::is_same_v<gemm::detail::StrideToLayoutTagA_t<StrideA>, layout::RowMajor> &&
-    cute::is_same_v<gemm::detail::StrideToLayoutTagB_t<StrideB>, layout::RowMajor>;
-
-  static constexpr bool IsInputSizeTwoBytes = sizeof(ElementA) == 2 && sizeof(ElementB) == 2;
-
   // We must ensure the type to be scaled goes to RF
   static constexpr bool SwapAB = !IsATransformed;
   using InternalSmemLayoutAtomA = cute::conditional_t<!SwapAB, SmemLayoutAtomA, SmemLayoutAtomB>;
   using InternalSmemLayoutAtomB = cute::conditional_t<!SwapAB, SmemLayoutAtomB, SmemLayoutAtomA>;
   using InternalSmemCopyAtomA   = cute::conditional_t<!SwapAB, SmemCopyAtomA, SmemCopyAtomB>;
   using InternalSmemCopyAtomB   = cute::conditional_t<!SwapAB, SmemCopyAtomB, SmemCopyAtomA>;
   // TMA converts f32 input to tf32 when copying from GMEM to SMEM
@@ -229,65 +226,40 @@
   static_assert((size<0>(TileShape{}) % size<0>(SmemLayoutAtomScale{})) == 0, "SmemLayoutAtomScale must equal the tile shape.");
   static_assert((size<2>(TileShape{}) % size<1>(SmemLayoutAtomScale{})) == 0, "SmemLayoutAtomScale must evenly divide tile k shape.");
 
   // Tile along modes in a way that maximizes the TMA box size.
   using SmemLayoutA = decltype(tile_to_shape(
       InternalSmemLayoutAtomA{},
       make_shape(shape<0>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,InternalStrideA>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
+      cute::conditional_t< ::cutlass::gemm::detail::is_major<0,InternalStrideA>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
   using SmemLayoutB = decltype(tile_to_shape(
       InternalSmemLayoutAtomB{},
       make_shape(shape<1>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,InternalStrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
+      cute::conditional_t< ::cutlass::gemm::detail::is_major<0,InternalStrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
     
   // It is assumed that the scales and zero-points share the same smem layout
   using SmemLayoutScale = decltype(tile_to_shape(
     SmemLayoutAtomScale{}, 
     make_shape(shape<0>(ScaleTileShape{}), shape<1>(ScaleTileShape{}), Int<Stages>{}),
-    conditional_t< ::cutlass::gemm::detail::is_major<0,NonVoidStrideScale>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
-
-  // If A mn-layout and B mn-layout, transposing B matrix since WGMMA is k-major only (e.g. tf32, fp32, fp8, int8).
-  static constexpr bool IsLayoutAmnBmn =
-    cute::is_same_v<gemm::detail::StrideToLayoutTagA_t<StrideA>, layout::ColumnMajor> &&
-    cute::is_same_v<gemm::detail::StrideToLayoutTagB_t<StrideB>, layout::RowMajor>;
-  static constexpr bool TransposeB = !IsInputSizeTwoBytes && IsLayoutAmnBmn;
-  using TransposeOperandB = decltype(cutlass::transform::collective::detail::make_transpose_operand_b(
-                                      0, 0, TiledMma{}, SmemLayoutB{}, InternalSmemLayoutAtomB{},
-                                      InternalElementB{}, cute::bool_constant<TransposeB>{})); 
+    cute::conditional_t< ::cutlass::gemm::detail::is_major<0,NonVoidStrideScale>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
 
   static_assert(DispatchPolicy::Stages >= 2, "Specialization requires Stages set to value 2 or more.");
   static_assert(not cute::is_base_of<cute::GMMA::DescriptorIterator, typename TiledMma::FrgTypeA>::value &&
                     cute::is_base_of<cute::GMMA::DescriptorIterator, typename TiledMma::FrgTypeB>::value,
                 "MMA atom must source A from rmem and B operand from smem_desc for this mainloop.");
   static_assert(cute::is_same_v<GmemTiledCopyA, SM90_TMA_LOAD> || cute::is_same_v<GmemTiledCopyA, SM90_TMA_LOAD_MULTICAST>,
       "GmemTiledCopy - invalid SM90 TMA copy atom specified.");
   static_assert(cute::is_same_v<GmemTiledCopyB, SM90_TMA_LOAD> || cute::is_same_v<GmemTiledCopyB, SM90_TMA_LOAD_MULTICAST>,
       "GmemTiledCopy - invalid SM90 TMA copy atom specified.");
 
-  using GmmaSmemLayoutAtomB = decltype(transform::collective::detail::gmma_smem_transpose_or_passthrough<
-      TransposeB, InternalSmemLayoutAtomB, InternalElementB>());
-
-  // SmemLayoutB for GMMA is different from SmemLayoutB for TMA if TransposeB
-  using GmmaSmemLayoutB = decltype(tile_to_shape(
-      GmmaSmemLayoutAtomB{},
-      make_shape(shape<1>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,InternalStrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
-
-  // These two restrictions are related, so we place the assertions together.
   // To relax them, we need to handle loading more than 1 row of scales for every main loop iteration.
   // We must also handle updating the pipeline transaction bytes on the fly.
   // NOTE: Deleting this assertion without required changes will cause the code to hang.
   static_assert(size<1>(SmemLayoutAtomScale{}) == 1, "size<1>(SmemLayoutAtomScale) must be 1.");
 
-  static_assert(!SwapAB || !TransposeB, "Cannot SwapAB and TransposeB at the same time.");
-  static_assert(TransposeB xor (cute::is_same_v<SmemLayoutB, GmmaSmemLayoutB>),
-    "Should be same layout if not TransposeB.");
-  static_assert(!TransposeB || size<1>(SmemLayoutB{}) * cute::sizeof_bits_v<InternalElementB> / 8 == 128,
-    "SmemLayoutB K must be 128bytes to be transposed.");
-
 private:
   static constexpr ConversionMode 
   get_conversion_mode() {
     if constexpr (cute::is_void_v<ElementScale>) {
       return ConversionMode::DirectConvert;
     } 
     else if constexpr (cute::is_void_v<ElementZero>) {
@@ -328,31 +300,31 @@
       static_assert(cutlass::detail::dependent_false<KernelSchedule>, "Type not handled in scale smem allocation.");
     }
   }
 
   // These methods use some the public members of the class. For that reason, we define them after the public section.
   static constexpr uint32_t
   compute_tma_transaction_bytes() {
-    constexpr uint32_t a_bytes = (size<0>(SmemLayoutA{}) * size<1>(SmemLayoutA{}) * static_cast<uint32_t>(cute::sizeof_bits_v<InternalElementA>) / 8);
-    constexpr uint32_t b_bytes = (size<0>(SmemLayoutB{}) * size<1>(SmemLayoutB{}) * static_cast<uint32_t>(cute::sizeof_bits_v<InternalElementB>) / 8);
+    constexpr uint32_t a_bytes = cutlass::bits_to_bytes(size<0>(SmemLayoutA{}) * size<1>(SmemLayoutA{}) * static_cast<uint32_t>(cute::sizeof_bits_v<InternalElementA>));
+    constexpr uint32_t b_bytes = cutlass::bits_to_bytes(size<0>(SmemLayoutB{}) * size<1>(SmemLayoutB{}) * static_cast<uint32_t>(cute::sizeof_bits_v<InternalElementB>));
 
     constexpr uint32_t baseline_bytes = a_bytes + b_bytes;
 
     if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {
       return baseline_bytes;
     }
     else if constexpr (ModeHasScales) {
-      constexpr uint32_t scale_tx_bytes = (size<0>(SmemLayoutScale{}) * size<1>(SmemLayoutScale{}) * static_cast<uint32_t>(cute::sizeof_bits_v<ElementScale>) / 8);
+      constexpr uint32_t scale_tx_bytes = cutlass::bits_to_bytes(size<0>(SmemLayoutScale{}) * size<1>(SmemLayoutScale{}) * static_cast<uint32_t>(cute::sizeof_bits_v<ElementScale>));
       static_assert(scale_tx_bytes % 128 == 0, "Each scale stage must be 128B aligned."); // required by TMA
       if constexpr (KernelConversionMode == ConversionMode::ConvertAndScale) {
         return baseline_bytes + scale_tx_bytes;
       }
       else if constexpr (KernelConversionMode == ConversionMode::ConvertAndScaleWithZero) {
         // Scale and zero share smem layout
-        constexpr uint32_t zero_tx_bytes =  (size<0>(SmemLayoutScale{}) * size<1>(SmemLayoutScale{}) * static_cast<uint32_t>(cute::sizeof_bits_v<ElementZero>) / 8);
+        constexpr uint32_t zero_tx_bytes = cutlass::bits_to_bytes(size<0>(SmemLayoutScale{}) * size<1>(SmemLayoutScale{}) * static_cast<uint32_t>(cute::sizeof_bits_v<ElementZero>));
         static_assert(zero_tx_bytes % 128 == 0, "Each zero stage must be 128B aligned."); // required by TMA
         return baseline_bytes + scale_tx_bytes + zero_tx_bytes;
       }
       else {
         static_assert(cutlass::detail::dependent_false<KernelSchedule>, "Type not handled in tma transaction bytes computation.");
       }
     }
@@ -847,35 +819,29 @@
     // Obtain warp index
     int warp_idx = canonical_warp_idx_sync();
     [[maybe_unused]] int warp_group_thread_idx = thread_idx % 128;
     
     Tensor sA_ = make_tensor(make_smem_ptr(shared_tensors.smem_A.begin()), SmemLayoutA{});        // (BLK_M,BLK_K,PIPE)
     Tensor sA = as_position_independent_swizzle_tensor(sA_);                                      // (BLK_M,BLK_K,PIPE)
     
-    Tensor sB_ = make_tensor(make_smem_ptr(shared_tensors.smem_B.begin()), SmemLayoutB{});        // (BLK_N,BLK_K,PIPE)
-    Tensor sB  = as_position_independent_swizzle_tensor(sB_);                                     // (BLK_M,BLK_K,PIPE)
-
-    // If TransposeB, GMMA will read from transposed B layout SMEM
-    Tensor gmma_sB_position_dependent = make_tensor(make_smem_ptr(shared_tensors.smem_B.begin()), 
-                                          GmmaSmemLayoutB{});                                     // (BLK_N,BLK_K,PIPE)
-    Tensor gmma_sB = as_position_independent_swizzle_tensor(gmma_sB_position_dependent);          // (BLK_N,BLK_K,PIPE)
+    Tensor sB = make_tensor(make_smem_ptr(shared_tensors.smem_B.begin()), SmemLayoutB{});         // (BLK_N,BLK_K,PIPE)
 
     //
     // Define C accumulators and A/B partitioning
     //
 
     TiledMma tiled_mma;
     auto thread_mma = tiled_mma.get_thread_slice(thread_idx);
     Tensor tCsA = thread_mma.partition_A(sA);
 
     // Allocate fragments and descriptors
     Tensor tCrA_mma = thread_mma.partition_fragment_A(sA(_,_,Int<0>{}));                      // (MMA,MMA_M,MMA_K,PIPE)
     Tensor tCrA_load = make_fragment_like<RealInternalElementA>(tCrA_mma);
     
-    Tensor tCsB = thread_mma.partition_B(gmma_sB_position_dependent);                         // (MMA,MMA_N,MMA_K,PIPE)
+    Tensor tCsB = thread_mma.partition_B(sB);                                                 // (MMA,MMA_N,MMA_K,PIPE)
     Tensor tCrB = thread_mma.make_fragment_B(tCsB);                                           // (MMA,MMA_N,MMA_K,PIPE)
 
     //
     // Copy Atom A retiling
     //
     auto smem_tiled_copy_A = make_tiled_copy_A(InternalSmemCopyAtomA{}, tiled_mma);
     auto smem_thr_copy_A   = smem_tiled_copy_A.get_thread_slice(warp_group_thread_idx);
@@ -907,20 +873,17 @@
     //
 
     // We release buffers to producer warps(dma load) with some mmas in flight
     PipelineState smem_pipe_release = smem_pipe_read;
 
     tiled_mma.accumulate_ = GMMA::ScaleOut::Zero;
 
-    TransposeOperandB transpose = cutlass::transform::collective::detail::make_transpose_operand_b(
-                                    warp_idx, warp_group_thread_idx, tiled_mma, SmemLayoutB{}, 
-                                    InternalSmemLayoutAtomB{}, InternalElementB{}, 
-                                    cute::bool_constant<TransposeB>{});
-
     warpgroup_fence_operand(accum);
+
+    constexpr int K_BLOCK_MAX = size<2>(tCrA_load);
     
     ConsumerToken barrier_token = {BarrierStatus::WaitAgain};
     // first k tile
     {
       barrier_token = pipeline.consumer_try_wait(smem_pipe_read);
       pipeline.consumer_wait(smem_pipe_read, barrier_token);
 
@@ -930,49 +893,39 @@
       barrier_token = pipeline.consumer_try_wait(smem_pipe_read);
 
       // copy smem->rmem for A operand
       copy_A_and_extra_info(smem_tiled_copy_A, tCsA, tCrA_copy_view, 
         partitioned_extra_info, copy_partitions_extra_info, 0, read_stage);
 
       transform_A_kblock(tCrA_load, A_CPY_VEC{}, tCrA_mma, partitioned_extra_info, 0);
-      // transpose B operand in SMEM
-      transpose(sB, gmma_sB, read_stage, 0);
       
       // Unroll the K mode manually to set scale D to 1
       CUTLASS_PRAGMA_UNROLL
-      for (int k_block = 0; k_block < size<2>(tCrA_load) - 1; ++k_block) {
-        copy_A_and_extra_info(smem_tiled_copy_A, tCsA, tCrA_copy_view, 
-          partitioned_extra_info, copy_partitions_extra_info, k_block + 1, read_stage);
-        transform_A_kblock(tCrA_load, A_CPY_VEC{}, tCrA_mma, partitioned_extra_info, k_block + 1);
-        transpose.synchronize(k_block);
-        transpose(sB, gmma_sB, read_stage, k_block + 1);
+      for (int k_block = 0; k_block < K_BLOCK_MAX; ++k_block) {
+        if (k_block < K_BLOCK_MAX - 1) {
+          copy_A_and_extra_info(smem_tiled_copy_A, tCsA, tCrA_copy_view, 
+            partitioned_extra_info, copy_partitions_extra_info, k_block + 1, read_stage);
+          transform_A_kblock(tCrA_load, A_CPY_VEC{}, tCrA_mma, partitioned_extra_info, k_block + 1);
+        }
         warpgroup_arrive();
         // (V,M) x (V,N) => (V,M,N)
         cute::gemm(tiled_mma, tCrA_mma(_,_,k_block), tCrB(_,_,k_block,read_stage), accum);
         tiled_mma.accumulate_ = GMMA::ScaleOut::One;
         warpgroup_commit_batch();
-      }
+      }     
 
-      warpgroup_wait<2>();
-      
       --k_tile_count;
       if (k_tile_count > 0) {
+        // Wait for K_BLOCK_MAX - 1 to be in flight to ensure that it is safe to overwrite the A registers for the first mma.
+        warpgroup_wait<K_BLOCK_MAX - 1>(); 
         pipeline.consumer_wait(smem_pipe_read, barrier_token);
         copy_A_and_extra_info(smem_tiled_copy_A, tCsA, tCrA_copy_view, 
           partitioned_extra_info, copy_partitions_extra_info, 0, smem_pipe_read.index());
         transform_A_kblock(tCrA_load, A_CPY_VEC{}, tCrA_mma, partitioned_extra_info, 0);
-        transpose(sB, gmma_sB, smem_pipe_read.index(), 0);
       }
-      warpgroup_arrive();
-      // (V,M) x (V,N) => (V,M,N)
-      const int final_k = size<2>(tCrA_load) - 1;
-      cute::gemm(tiled_mma, tCrA_mma(_,_, final_k), tCrB(_,_,final_k,read_stage), accum);
-      tiled_mma.accumulate_ = GMMA::ScaleOut::One;
-      warpgroup_commit_batch();
-      warpgroup_wait<2>();
     }
 
     if (k_tile_count == 0) {
       return;
     }
 
     warpgroup_fence_operand(accum);
@@ -986,45 +939,43 @@
 
       int read_stage = smem_pipe_read.index();
       ++smem_pipe_read;
 
       warpgroup_fence_operand(accum);
       // Unroll the K mode manually to set scale D to 1
       CUTLASS_PRAGMA_UNROLL
-      for (int k_block = 0; k_block < size<2>(tCrA_load); ++k_block) {
+      for (int k_block = 0; k_block < K_BLOCK_MAX; ++k_block) {
+        
+        warpgroup_arrive();
+        // (V,M) x (V,N) => (V,M,N)
+        cute::gemm(tiled_mma, tCrA_mma(_,_,k_block), tCrB(_,_,k_block,read_stage), accum);
+        tiled_mma.accumulate_ = GMMA::ScaleOut::One;
+        warpgroup_commit_batch();
+
+        warpgroup_wait<K_BLOCK_MAX - 1>();
+        if (k_block == K_BLOCK_MAX - 1) {
+          // We have K_BLOCK_MAX - 1 GMMA instructions pending for this stage, so we can release prior barrier
+          pipeline.consumer_release(smem_pipe_release);             // UNLOCK smem_pipe_release, done _computing_ on it
+          ++smem_pipe_release;
+        }
+
         if (k_block == 0) {
           barrier_token = pipeline.consumer_try_wait(smem_pipe_read);
         }
-        if (k_block == size<2>(tCrA_load) - 1) {
+
+        if (k_block == K_BLOCK_MAX - 1) { 
           pipeline.consumer_wait(smem_pipe_read, barrier_token);
           copy_A_and_extra_info(smem_tiled_copy_A, tCsA, tCrA_copy_view, 
             partitioned_extra_info, copy_partitions_extra_info, 0, smem_pipe_read.index());
           transform_A_kblock(tCrA_load, A_CPY_VEC{}, tCrA_mma, partitioned_extra_info, 0);
-          // transpose B operand in SMEM
-          transpose(sB, gmma_sB, smem_pipe_read.index(), 0);
         } 
         else {
           copy_A_and_extra_info(smem_tiled_copy_A, tCsA, tCrA_copy_view, 
             partitioned_extra_info, copy_partitions_extra_info, k_block + 1, read_stage);
           transform_A_kblock(tCrA_load, A_CPY_VEC{}, tCrA_mma, partitioned_extra_info, k_block + 1);
-          // transpose B operand in SMEM
-          transpose.synchronize(k_block);                                      // make transpose of k_block available
-          transpose(sB, gmma_sB, read_stage, k_block + 1);
-        }
-        
-        warpgroup_arrive();
-        // (V,M) x (V,N) => (V,M,N)
-        cute::gemm(tiled_mma, tCrA_mma(_,_,k_block), tCrB(_,_,k_block,read_stage), accum);
-        tiled_mma.accumulate_ = GMMA::ScaleOut::One;
-        warpgroup_commit_batch();
-        warpgroup_wait<2>();
-        if (k_block == 1) {
-          // release prior barrier
-          pipeline.consumer_release(smem_pipe_release);             // UNLOCK smem_pipe_release, done _computing_ on it
-          ++smem_pipe_release;
         }
       }
       warpgroup_fence_operand(accum);
 
     }
 
     warpgroup_fence_operand(accum);
@@ -1036,40 +987,34 @@
 
       int read_stage = smem_pipe_read.index();
 
       warpgroup_fence_operand(accum);
       
       // Unroll the K mode manually to set scale D to 1
       CUTLASS_PRAGMA_UNROLL
-      for (int k_block = 0; k_block < size<2>(tCrA_load) - 1; ++k_block) {
-        
-        copy_A_and_extra_info(smem_tiled_copy_A, tCsA, tCrA_copy_view, 
-          partitioned_extra_info, copy_partitions_extra_info, k_block + 1, read_stage);
-        transform_A_kblock(tCrA_load, A_CPY_VEC{}, tCrA_mma, partitioned_extra_info, k_block + 1);
-        transpose.synchronize(k_block);                                           // make k_block transpose available
-        transpose(sB, gmma_sB, read_stage, k_block + 1);
+      for (int k_block = 0; k_block < K_BLOCK_MAX; ++k_block) {
+
         warpgroup_arrive();
         // (V,M) x (V,N) => (V,M,N)
         cute::gemm(tiled_mma, tCrA_mma(_,_,k_block), tCrB(_,_,k_block,read_stage), accum);
         tiled_mma.accumulate_ = GMMA::ScaleOut::One;
         warpgroup_commit_batch();
-        warpgroup_wait<2>();
-        if (k_block == 1) {
+        warpgroup_wait<K_BLOCK_MAX - 1>();
+        if (k_block == K_BLOCK_MAX - 1) {
           // release prior barrier
           pipeline.consumer_release(smem_pipe_release);             // UNLOCK smem_pipe_release, done _computing_ on it
           ++smem_pipe_release;
         }
+
+        if (k_block < K_BLOCK_MAX - 1) {
+          copy_A_and_extra_info(smem_tiled_copy_A, tCsA, tCrA_copy_view, 
+            partitioned_extra_info, copy_partitions_extra_info, k_block + 1, read_stage);
+          transform_A_kblock(tCrA_load, A_CPY_VEC{}, tCrA_mma, partitioned_extra_info, k_block + 1);
+        }
       }
-      
-      warpgroup_arrive();
-      // (V,M) x (V,N) => (V,M,N)
-      const int final_k = size<2>(tCrA_load) - 1;
-      cute::gemm(tiled_mma, tCrA_mma(_,_,final_k), tCrB(_,_,final_k,read_stage), accum);
-      tiled_mma.accumulate_ = GMMA::ScaleOut::One;
-      warpgroup_commit_batch();
     }
 
     warpgroup_fence_operand(accum);
   }
   
   /// Perform a Consumer Epilogue to release all buffers
   CUTLASS_DEVICE void
@@ -1144,23 +1089,23 @@
     if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {
       // noting to do
       return cute::tuple{};
     }
     else if constexpr (ModeHasScales) {
       Tensor sS = make_tensor(make_smem_ptr(shared_tensors.smem_scale.begin()), SmemLayoutScale{});    // (BLK_M,BLK_SCALE_K,PIPE)
       Tensor tCsS = thread_mma.partition_A(sS);
-      Tensor tCrS = make_fragment_like<ElementScale>(thread_mma.partition_fragment_A(sS(_,_,Int<0>{}))); 
+      Tensor tCrS = make_tensor<ElementScale>(thread_mma.partition_fragment_A(sS(_,_,Int<0>{})).shape()); 
 
       if constexpr (KernelConversionMode == ConversionMode::ConvertAndScale) {
         return cute::make_tuple(tCsS, tCrS);
       }
       else if constexpr (KernelConversionMode == ConversionMode::ConvertAndScaleWithZero) {
         Tensor sZ = make_tensor(make_smem_ptr(shared_tensors.smem_zero.begin()), SmemLayoutScale{});    // (BLK_M,BLK_SCALE_K,PIPE)
         Tensor tCsZ = thread_mma.partition_A(sZ);
-        Tensor tCrZ = make_fragment_like<ElementZero>(thread_mma.partition_fragment_A(sZ(_,_,Int<0>{}))); 
+        Tensor tCrZ = make_tensor<ElementZero>(thread_mma.partition_fragment_A(sZ(_,_,Int<0>{})).shape()); 
         return cute::make_tuple(tCsS, tCrS, tCsZ, tCrZ);
       }
       else {
         static_assert(cutlass::detail::dependent_false<KernelSchedule>, "Conversion mode not handled in A -> RF path.");
       }
     } 
     else {
```

## cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss.hpp

```diff
@@ -27,25 +27,26 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cute/arch/cluster_sm90.hpp"
-#include "cute/arch/copy_sm90.hpp"
 #include "cutlass/gemm/dispatch_policy.hpp"
+#include "cutlass/numeric_types.h"
+#include "cutlass/pipeline/pipeline.hpp"
+#include "cutlass/trace.h"
 
+#include "cute/arch/cluster_sm90.hpp"
+#include "cute/arch/copy_sm90.hpp"
 #include "cute/algorithm/functional.hpp"
 #include "cute/atom/mma_atom.hpp"
 #include "cute/algorithm/gemm.hpp"
 #include "cute/tensor_predicate.hpp"
 #include "cute/numeric/arithmetic_tuple.hpp"
-#include "cutlass/pipeline/pipeline.hpp"
-#include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass::gemm::collective {
 using namespace cute;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -102,14 +103,15 @@
   using SmemLayoutAtomB = SmemLayoutAtomB_;
   using SmemCopyAtomA = SmemCopyAtomA_;
   using SmemCopyAtomB = SmemCopyAtomB_;
   using TransformA = TransformA_;
   using TransformB = TransformB_;
   using ArchTag = typename DispatchPolicy::ArchTag;
 
+  using CtaShape_MNK = decltype(shape_div(TileShape{}, ClusterShape{}));
   using MainloopPipeline = cutlass::PipelineTmaAsync<DispatchPolicy::Stages>;
 
   using PipelineParams = typename MainloopPipeline::Params;
   using PipelineState  = typename cutlass::PipelineState<DispatchPolicy::Stages>;
 
   static constexpr int ThreadCount = CUTE_STATIC_V(size(TiledMma{}));
 
@@ -121,19 +123,19 @@
   static_assert((size<1>(TileShape{}) % size<0>(SmemLayoutAtomB{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
   static_assert((size<2>(TileShape{}) % size<1>(SmemLayoutAtomB{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
 
   // Tile along modes in a way that maximizes the TMA box size.
   using SmemLayoutA = decltype(tile_to_shape(
       SmemLayoutAtomA{},
       make_shape(shape<0>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,StrideA>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
+      cute::conditional_t< ::cutlass::gemm::detail::is_major<0,StrideA>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
   using SmemLayoutB = decltype(tile_to_shape(
       SmemLayoutAtomB{},
       make_shape(shape<1>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,StrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
+      cute::conditional_t< ::cutlass::gemm::detail::is_major<0,StrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
 
   static_assert(DispatchPolicy::Stages >= 2, "Specialization requires Stages set to value 1 or more.");
   static_assert(cute::is_base_of<cute::GMMA::DescriptorIterator, typename TiledMma::FrgTypeA>::value &&
                 cute::is_base_of<cute::GMMA::DescriptorIterator, typename TiledMma::FrgTypeB>::value,
                 "MMA atom must source both A and B operand from smem_desc for this mainloop.");
   static_assert(cute::is_same_v<GmemTiledCopyA, SM90_TMA_LOAD> || cute::is_same_v<GmemTiledCopyA, SM90_TMA_LOAD_MULTICAST>,
       "GmemTiledCopy - invalid SM90 TMA copy atom specified.");
@@ -316,17 +318,16 @@
     static_assert(0 <= K_PIPE_MMAS && K_PIPE_MMAS <  K_PIPE_MAX);
     static_assert(0 <  K_PIPE_TMAS && K_PIPE_TMAS <= K_PIPE_MAX);
 
     static_assert(K_PIPE_MMAS < K_PIPE_MAX - 1);
 
     // Set the bytes transferred in this TMA transaction (may involve multiple issues)
     constexpr uint32_t TmaTransactionBytes = static_cast<uint32_t>(
-        (size<0>(sA) * size<1>(sA) * sizeof_bits<InternalElementA>::value) / 8 +
-        (size<0>(sB) * size<1>(sB) * sizeof_bits<InternalElementB>::value) / 8);
-
+        cutlass::bits_to_bytes(size<0>(sA) * size<1>(sA) * sizeof_bits<InternalElementA>::value) +
+        cutlass::bits_to_bytes(size<0>(sB) * size<1>(sB) * sizeof_bits<InternalElementB>::value));
 
     // Obtain warp index
     int warp_idx = canonical_warp_idx_sync();
     int warp_group_thread_idx = thread_idx % NumThreadsPerWarpGroup;
 
     PipelineParams params;
     params.transaction_bytes = TmaTransactionBytes;
```

## cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp

```diff
@@ -27,25 +27,26 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cute/arch/cluster_sm90.hpp"
-#include "cute/arch/copy_sm90.hpp"
 #include "cutlass/gemm/dispatch_policy.hpp"
+#include "cutlass/numeric_types.h"
+#include "cutlass/pipeline/pipeline.hpp"
+#include "cutlass/trace.h"
 
+#include "cute/arch/cluster_sm90.hpp"
+#include "cute/arch/copy_sm90.hpp"
 #include "cute/algorithm/functional.hpp"
 #include "cute/atom/mma_atom.hpp"
 #include "cute/algorithm/gemm.hpp"
 #include "cute/tensor_predicate.hpp"
 #include "cute/numeric/arithmetic_tuple.hpp"
-#include "cutlass/pipeline/pipeline.hpp"
-#include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass::gemm::collective {
 using namespace cute;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -103,14 +104,15 @@
   using SmemLayoutAtomB = SmemLayoutAtomB_;
   using SmemCopyAtomA = SmemCopyAtomA_;
   using SmemCopyAtomB = SmemCopyAtomB_;
   using TransformA = TransformA_;
   using TransformB = TransformB_;
   using ArchTag = typename DispatchPolicy::ArchTag;
 
+  using CtaShape_MNK = decltype(shape_div(TileShape{}, ClusterShape{}));
   using MainloopPipeline = cutlass::PipelineTmaAsync<DispatchPolicy::Stages>;
   using PipelineState = cutlass::PipelineState<DispatchPolicy::Stages>;
 
   using PipelineParams = typename MainloopPipeline::Params;
 
   static_assert(cute::rank(SmemLayoutAtomA{}) == 2, "SmemLayoutAtom must be rank 2 (M/N, K)");
   static_assert((size<0>(TileShape{}) % size<0>(SmemLayoutAtomA{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
@@ -120,19 +122,19 @@
   static_assert((size<1>(TileShape{}) % size<0>(SmemLayoutAtomB{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
   static_assert((size<2>(TileShape{}) % size<1>(SmemLayoutAtomB{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
 
   // Tile along modes in a way that maximizes the TMA box size.
   using SmemLayoutA = decltype(tile_to_shape(
       SmemLayoutAtomA{},
       make_shape(shape<0>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,StrideA>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
+      cute::conditional_t< ::cutlass::gemm::detail::is_major<0,StrideA>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
   using SmemLayoutB = decltype(tile_to_shape(
       SmemLayoutAtomB{},
       make_shape(shape<1>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,StrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
+      cute::conditional_t< ::cutlass::gemm::detail::is_major<0,StrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
 
   static_assert(DispatchPolicy::Stages >= 2, "Specialization requires Stages set to value 2 or more.");
   static_assert(cute::is_base_of<cute::GMMA::DescriptorIterator, typename TiledMma::FrgTypeA>::value &&
                 cute::is_base_of<cute::GMMA::DescriptorIterator, typename TiledMma::FrgTypeB>::value,
                 "MMA atom must source both A and B operand from smem_desc for this mainloop.");
   static_assert(cute::is_same_v<GmemTiledCopyA, SM90_TMA_LOAD> || cute::is_same_v<GmemTiledCopyA, SM90_TMA_LOAD_MULTICAST>,
       "GmemTiledCopy - invalid SM90 TMA copy atom specified.");
@@ -244,16 +246,16 @@
     }
     return implementable;
   }
 
   static constexpr int K_PIPE_MAX = DispatchPolicy::Stages;
   static constexpr int K_PIPE_MMAS = 1;
   static constexpr uint32_t TmaTransactionBytes =
-        (size<0>(SmemLayoutA{}) * size<1>(SmemLayoutA{}) * static_cast<uint32_t>(sizeof_bits<ElementA>::value)) / 8+
-        (size<0>(SmemLayoutB{}) * size<1>(SmemLayoutB{}) * static_cast<uint32_t>(sizeof_bits<ElementB>::value)) / 8;
+        cutlass::bits_to_bytes(size<0>(SmemLayoutA{}) * size<1>(SmemLayoutA{}) * static_cast<uint32_t>(sizeof_bits<ElementA>::value))+
+        cutlass::bits_to_bytes(size<0>(SmemLayoutB{}) * size<1>(SmemLayoutB{}) * static_cast<uint32_t>(sizeof_bits<ElementB>::value));
 
   /// Issue Tma Descriptor Prefetch -- ideally from a single thread for best performance
   CUTLASS_DEVICE
   static void prefetch_tma_descriptors(Params const& mainloop_params) {
     cute::prefetch_tma_descriptor(mainloop_params.tma_load_a.get_tma_descriptor());
     cute::prefetch_tma_descriptor(mainloop_params.tma_load_b.get_tma_descriptor());
   }
```

## cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8.hpp

```diff
@@ -28,25 +28,26 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cute/arch/cluster_sm90.hpp"
-#include "cute/arch/copy_sm90.hpp"
 #include "cutlass/gemm/dispatch_policy.hpp"
+#include "cutlass/gemm/collective/fp8_accumulation.hpp"
+#include "cutlass/trace.h"
+#include "cutlass/numeric_types.h"
 
+#include "cute/arch/cluster_sm90.hpp"
+#include "cute/arch/copy_sm90.hpp"
 #include "cute/algorithm/functional.hpp"
 #include "cute/atom/mma_atom.hpp"
 #include "cute/algorithm/gemm.hpp"
 #include "cute/tensor_predicate.hpp"
 #include "cute/numeric/arithmetic_tuple.hpp"
-#include "cutlass/gemm/collective/fp8_accumulation.hpp"
-#include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass::gemm::collective {
 using namespace cute;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -104,14 +105,15 @@
   using SmemLayoutAtomB = SmemLayoutAtomB_;
   using SmemCopyAtomA = SmemCopyAtomA_;
   using SmemCopyAtomB = SmemCopyAtomB_;
   using TransformA = TransformA_;
   using TransformB = TransformB_;
   using ArchTag = typename DispatchPolicy::ArchTag;
 
+  using CtaShape_MNK = decltype(shape_div(TileShape{}, ClusterShape{}));
   using MainloopPipeline = cutlass::PipelineTmaAsync<DispatchPolicy::Stages>;
   using PipelineState = cutlass::PipelineState<DispatchPolicy::Stages>;
 
   using PipelineParams = typename MainloopPipeline::Params;
 
   static_assert(cute::rank(SmemLayoutAtomA{}) == 2, "SmemLayoutAtom must be rank 2 (M/N, K)");
   static_assert((size<0>(TileShape{}) % size<0>(SmemLayoutAtomA{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
@@ -121,19 +123,19 @@
   static_assert((size<1>(TileShape{}) % size<0>(SmemLayoutAtomB{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
   static_assert((size<2>(TileShape{}) % size<1>(SmemLayoutAtomB{})) == 0, "SmemLayoutAtom must evenly divide tile shape.");
 
   // Tile along modes in a way that maximizes the TMA box size.
   using SmemLayoutA = decltype(tile_to_shape(
       SmemLayoutAtomA{},
       make_shape(shape<0>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,StrideA>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
+      cute::conditional_t< ::cutlass::gemm::detail::is_major<0,StrideA>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
   using SmemLayoutB = decltype(tile_to_shape(
       SmemLayoutAtomB{},
       make_shape(shape<1>(TileShape{}), shape<2>(TileShape{}), Int<DispatchPolicy::Stages>{}),
-      conditional_t< ::cutlass::gemm::detail::is_major<0,StrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
+      cute::conditional_t< ::cutlass::gemm::detail::is_major<0,StrideB>(), Step<_2,_1,_3>, Step<_1,_2,_3>>{}));
 
   static_assert(DispatchPolicy::Stages >= 2, "Specialization requires Stages set to value 1 or more.");
   static_assert(cute::is_base_of<cute::GMMA::DescriptorIterator, typename TiledMma::FrgTypeA>::value &&
                 cute::is_base_of<cute::GMMA::DescriptorIterator, typename TiledMma::FrgTypeB>::value,
                 "MMA atom must source both A and B operand from smem_desc for this mainloop.");
   static_assert(cute::is_same_v<GmemTiledCopyA, SM90_TMA_LOAD> || cute::is_same_v<GmemTiledCopyA, SM90_TMA_LOAD_MULTICAST>,
       "GmemTiledCopy - invalid SM90 TMA copy atom specified.");
@@ -242,16 +244,16 @@
     }
     return implementable;
   }
 
   static constexpr int K_PIPE_MAX = DispatchPolicy::Stages;
   static constexpr int K_PIPE_MMAS = 1;
   static constexpr uint32_t TmaTransactionBytes =
-        (size<0>(SmemLayoutA{}) * size<1>(SmemLayoutA{}) * static_cast<uint32_t>(sizeof_bits<ElementA>::value)) / 8+
-        (size<0>(SmemLayoutB{}) * size<1>(SmemLayoutB{}) * static_cast<uint32_t>(sizeof_bits<ElementB>::value)) / 8;
+        cutlass::bits_to_bytes(size<0>(SmemLayoutA{}) * size<1>(SmemLayoutA{}) * static_cast<uint32_t>(sizeof_bits<ElementA>::value))+
+        cutlass::bits_to_bytes(size<0>(SmemLayoutB{}) * size<1>(SmemLayoutB{}) * static_cast<uint32_t>(sizeof_bits<ElementB>::value));
 
   /// Issue Tma Descriptor Prefetch -- ideally from a single thread for best performance
   CUTLASS_DEVICE
   static void prefetch_tma_descriptors(Params const& mainloop_params)
   {
     cute::prefetch_tma_descriptor(mainloop_params.tma_load_a.get_tma_descriptor());
     cute::prefetch_tma_descriptor(mainloop_params.tma_load_b.get_tma_descriptor());
```

## cutlass_library/source/include/cutlass/gemm/collective/builders/sm90_common.inl

```diff
@@ -26,16 +26,20 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
+#include "cutlass/arch/mma.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/dispatch_policy.hpp"
 #include "cutlass/detail/layout.hpp"
 #include "cutlass/detail/collective.hpp"
+#include "cutlass/detail/dependent_false.hpp"
 
 #include "cute/atom/mma_traits_sm90_gmma.hpp"
 #include "cute/atom/copy_traits_sm90_tma.hpp"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass::gemm::collective {
@@ -119,14 +123,16 @@
   }
 }
 
 // Generates the most efficient possible TiledCopy with cp.async copy atom given a set of parameters.
 template<int ThreadCount, class Element, int Alignment, class StrideType, class TileMN, class TileK>
 constexpr auto
 make_cp_async_gmem_tiled_copy() {
+  using namespace cute;
+
   using AlignmentType = cute::uint_byte_t<static_cast<int>(sizeof(Element)) * Alignment>;
   constexpr int TileSizeMN  = cute::size(TileMN{});
   constexpr int TileSizeK   = cute::size(TileK{});
 
   // Maximize the number of threads along the gmem major mode to promote coalesced reads
   // While making sure our thread layout tiles the threadblock tile evenly
 
@@ -162,17 +168,19 @@
 }
 
 // Helper for SS GMMA smem selection that considers a tensor TileShape:
 //   (BLK_MN, BLK_K)
 //   or hierarchically
 //   ((BLK_MN0,BLK_MN1,...),(BLK_K0,BLK_K1,...))
 //   and returns the optimal GMMA::Layout that fits BLK_MN0 and BLK_K0
-template <GMMA::Major major, class ElementType, class BLK_MN, class BLK_K, const bool is_ws_transposed_B = false>
+template <cute::GMMA::Major major, class ElementType, class BLK_MN, class BLK_K, const bool is_ws_transposed_B = false>
 constexpr auto
 rs_smem_selector() {
+  using namespace cute;
+
   auto BLK_MN0 = size<0>(BLK_MN{});
   auto BLK_K0  = size<0>(BLK_K{});
 
   static_assert(BLK_MN0 % 8 == 0, "BLK_MN0 must be a multiple of 8.");
   static_assert(BLK_K0 % 8 == 0,  "BLK_K0 must be a multiple of 8.");
   if constexpr (major == GMMA::Major::MN) {
     if constexpr (sizeof(ElementType) == 4){
@@ -256,26 +264,27 @@
 }
 
 // Helper for SS GMMA smem selection that considers a tensor TileShape:
 //   (BLK_MN, BLK_K)
 //   or hierarchically
 //   ((BLK_MN0,BLK_MN1,...),(BLK_K0,BLK_K1,...))
 //   and returns the largest GMMA::Layout that fits BLK_MN0 and BLK_K0
-template <GMMA::Major major, class ElementType, class BLK_MN, class BLK_K>
+template <cute::GMMA::Major major, class ElementType, class BLK_MN, class BLK_K>
 CUTE_HOST_DEVICE constexpr
 auto
 ss_smem_selector()
 {
+  using namespace cute;
+
   auto BLK_MN0 = size<0>(BLK_MN{});
   auto BLK_K0  = size<0>(BLK_K{});
 
   static_assert(BLK_MN0 % 8 == 0, "BLK_MN0 must be a multiple of 8.");
   static_assert(BLK_K0 % 8 == 0,  "BLK_K0 must be a multiple of 8.");
 
-
   if constexpr (major == GMMA::Major::MN) {
     if constexpr (BLK_MN0 % size<0>(GMMA::Layout_MN_SW128_Atom<ElementType>{}) == 0) {
       return GMMA::Layout_MN_SW128_Atom<ElementType>{};
     }
     else if constexpr (BLK_MN0 % size<0>(GMMA::Layout_MN_SW64_Atom<ElementType>{}) == 0) {
       return GMMA::Layout_MN_SW64_Atom<ElementType>{};
     }
```

## cutlass_library/source/include/cutlass/gemm/collective/builders/sm90_gmma_builder.inl

```diff
@@ -26,18 +26,14 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
-#include "cutlass/arch/mma.h"
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/dispatch_policy.hpp"
-
 #include "cutlass/gemm/collective/builders/sm90_common.inl"
 
 // SM90 Collective Builders should be used only starting CUDA 12.0
 #if (__CUDACC_VER_MAJOR__ >= 12)
 #define CUTLASS_SM90_COLLECTIVE_BUILDER_SUPPORTED
 #endif
 
@@ -63,22 +59,21 @@
   return stages;
 }
 
 // Returns the maximum number of smem tiles that can be used with a given smem capacity, or overrides with manual count. 
 template<int CapacityBytes, class ElementA, class ElementB, class TileShapeMNK, int carveout_bytes>
 constexpr int
 compute_stage_count_or_override(StageCountAutoCarveout<carveout_bytes> stage_count) {
-  // 32 bytes to account for barriers etc.
-  constexpr int stage_barrier_bytes = 32;
-  constexpr int a_bits = static_cast<int>(sizeof_bits<ElementA>::value);
-  constexpr int b_bits = static_cast<int>(sizeof_bits<ElementB>::value);
+  constexpr auto mainloop_pipeline_bytes = sizeof(typename cutlass::PipelineTmaAsync<1>::SharedStorage);
+  constexpr auto a_bits = cute::sizeof_bits_v<ElementA>;
+  constexpr auto b_bits = cute::sizeof_bits_v<ElementB>;
   constexpr int stage_bytes =
-    (a_bits * size<0>(TileShapeMNK{}) * size<2>(TileShapeMNK{})) / 8 +
-    (b_bits * size<1>(TileShapeMNK{}) * size<2>(TileShapeMNK{})) / 8 +
-    stage_barrier_bytes;
+    cutlass::bits_to_bytes(a_bits * size<0>(TileShapeMNK{}) * size<2>(TileShapeMNK{})) +
+    cutlass::bits_to_bytes(b_bits * size<1>(TileShapeMNK{}) * size<2>(TileShapeMNK{})) +
+    static_cast<int>(mainloop_pipeline_bytes);
 
   return (CapacityBytes - carveout_bytes) / stage_bytes;
 }
 
 // Returns the maximum number of smem tiles that can be used with a given smem capacity (with an optional scale matrix), or overrides with manual count. 
 template<int CapacityBytes, class ElementA, class ElementB, class ElementScale, class ElementZero, class TileShapeMNK, int stages>
 constexpr int
@@ -98,31 +93,31 @@
 
 // Returns the maximum number of smem tiles that can be used with a given smem capacity (with an optional scale matrix), or overrides with manual count. 
 template<int CapacityBytes, class ElementA, class ElementB, class ElementScale, class ElementZero, class TileShapeMNK, int carveout_bytes>
 constexpr int
 compute_stage_count_or_override_single_affine_transformed_input(StageCountAutoCarveout<carveout_bytes> stage_count) {
 
   // 32 bytes to account for barriers etc.
-  constexpr int stage_barrier_bytes = 32;
+  constexpr auto mainloop_pipeline_bytes = sizeof(typename cutlass::PipelineTmaAsync<1>::SharedStorage);
   constexpr int scale_zero_k_tile = 1;
-  constexpr int a_bits = static_cast<int>(sizeof_bits<ElementA>::value);
-  constexpr int b_bits = static_cast<int>(sizeof_bits<ElementB>::value);
-  constexpr int s_bits = get_bits_for_possibly_void_element<ElementScale>();
-  constexpr int z_bits = get_bits_for_possibly_void_element<ElementZero>();
+  constexpr auto a_bits = cute::sizeof_bits_v<ElementA>;
+  constexpr auto b_bits = cute::sizeof_bits_v<ElementB>;
+  constexpr auto s_bits = get_bits_for_possibly_void_element<ElementScale>();
+  constexpr auto z_bits = get_bits_for_possibly_void_element<ElementZero>();
 
-  constexpr int scale_bytes = (s_bits * size<0>(TileShapeMNK{}) * scale_zero_k_tile) / 8;
-  constexpr int zero_bytes  = (z_bits * size<0>(TileShapeMNK{}) * scale_zero_k_tile) / 8;
+  constexpr auto scale_bytes = cutlass::bits_to_bytes(s_bits * size<0>(TileShapeMNK{}) * scale_zero_k_tile);
+  constexpr auto zero_bytes  = cutlass::bits_to_bytes(z_bits * size<0>(TileShapeMNK{}) * scale_zero_k_tile);
   static_assert(scale_bytes % 128 == 0, "Scale bytes must be a multiple of 128");
   static_assert(zero_bytes  % 128 == 0, "Zero bytes must be a multiple of 128");
 
   // When scales are void, s_bits will be 0 so no smem will be allocated for scales. 
   constexpr int stage_bytes =
-    (a_bits * size<0>(TileShapeMNK{}) * size<2>(TileShapeMNK{})) / 8 +
-    (b_bits * size<1>(TileShapeMNK{}) * size<2>(TileShapeMNK{})) / 8 +
-    scale_bytes + zero_bytes + stage_barrier_bytes;
+    cutlass::bits_to_bytes(a_bits * size<0>(TileShapeMNK{}) * size<2>(TileShapeMNK{})) +
+    cutlass::bits_to_bytes(b_bits * size<1>(TileShapeMNK{}) * size<2>(TileShapeMNK{})) +
+    static_cast<int>(scale_bytes + zero_bytes + mainloop_pipeline_bytes);
 
   return (CapacityBytes - carveout_bytes) / stage_bytes;
 }
 
 template <class ElementA, class LayoutA, class ElementB, class LayoutB>
 constexpr bool
 is_swapAB(){
@@ -152,45 +147,45 @@
 } // namespace detail
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA_TMA_WS_SS
 template <
   class ElementA,
-  class GmemLayoutA,
+  class GmemLayoutATag,
   int AlignmentA,
   class ElementB,
-  class GmemLayoutB,
+  class GmemLayoutBTag,
   int AlignmentB,
   class ElementAccumulator,
   class TileShape_MNK,
   class ClusterShape_MNK,
   class StageCountType,
   class KernelScheduleType
 >
 struct CollectiveBuilder<
     arch::Sm90,
     arch::OpClassTensorOp,
     ElementA,
-    GmemLayoutA,
+    GmemLayoutATag,
     AlignmentA,
     ElementB,
-    GmemLayoutB,
+    GmemLayoutBTag,
     AlignmentB,
     ElementAccumulator,
     TileShape_MNK,
     ClusterShape_MNK,
     StageCountType,
     KernelScheduleType,
     cute::enable_if_t<
       (cute::is_same_v<KernelScheduleType, KernelTmaWarpSpecialized> ||
        cute::is_same_v<KernelScheduleType, KernelTmaWarpSpecializedPingpong> ||
        cute::is_same_v<KernelScheduleType, KernelTmaWarpSpecializedCooperative> ||
        cute::is_same_v<KernelScheduleType, KernelPtrArrayTmaWarpSpecializedCooperative>) &&
-       not detail::is_use_rmem_A<ElementA, GmemLayoutA, ElementB, GmemLayoutB>()>
+       not detail::is_use_rmem_A<ElementA, GmemLayoutATag, ElementB, GmemLayoutBTag>()>
 > {
   static_assert(is_static<TileShape_MNK>::value);
   static_assert(is_static<ClusterShape_MNK>::value);
 #ifndef CUTLASS_SM90_COLLECTIVE_BUILDER_SUPPORTED
   static_assert(cutlass::detail::dependent_false<ElementA>, "Unsupported Toolkit for SM90 Collective Builder\n");
 #endif
   static_assert(detail::is_aligned<ElementA, AlignmentA, ElementB, AlignmentB, detail::tma_alignment_bytes>(),
@@ -198,54 +193,54 @@
 
   static constexpr bool IsArrayOfPointersGemm = (cute::is_same_v<KernelScheduleType, KernelPtrArrayTmaWarpSpecializedCooperative>);
   static constexpr bool IsFP8Input = detail::is_input_fp8<ElementA, ElementB>();
   static_assert(!IsFP8Input || (IsFP8Input && !IsArrayOfPointersGemm),
                 "Kernel[Array/Group]TmaWarpSpecializedCooperative is only compatible with FP8 FastAccum version right now\n");
 
   // For fp32 types, map to tf32 MMA value type
-  using MmaElementA = cute::conditional_t<cute::is_same_v<ElementA, float>, tfloat32_t, ElementA>;
-  using MmaElementB = cute::conditional_t<cute::is_same_v<ElementB, float>, tfloat32_t, ElementB>;
+  using ElementAMma = cute::conditional_t<cute::is_same_v<ElementA, float>, tfloat32_t, ElementA>;
+  using ElementBMma = cute::conditional_t<cute::is_same_v<ElementB, float>, tfloat32_t, ElementB>;
 
-  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_ss_tag_to_major_A<MmaElementA, GmemLayoutA>();
-  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_ss_tag_to_major_B<MmaElementB, GmemLayoutB>();
+  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_ss_tag_to_major_A<ElementAMma, GmemLayoutATag>();
+  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_ss_tag_to_major_B<ElementBMma, GmemLayoutBTag>();
 
   using AtomLayoutMNK = cute::conditional_t<
       cute::is_same_v<KernelScheduleType, KernelTmaWarpSpecializedCooperative> || IsArrayOfPointersGemm,
       Layout<Shape<_2,_1,_1>>, Layout<Shape<_1,_1,_1>>>;
 
   using TiledMma = decltype(cute::make_tiled_mma(cute::GMMA::ss_op_selector<
-      MmaElementA, MmaElementB, ElementAccumulator, TileShape_MNK, GmmaMajorA, GmmaMajorB>(), AtomLayoutMNK{}));
+      ElementAMma, ElementBMma, ElementAccumulator, TileShape_MNK, GmmaMajorA, GmmaMajorB>(), AtomLayoutMNK{}));
 
   using GmemTiledCopyA = decltype(detail::sm90_cluster_shape_to_tma_atom(shape<1>(ClusterShape_MNK{})));
   using GmemTiledCopyB = decltype(detail::sm90_cluster_shape_to_tma_atom(shape<0>(ClusterShape_MNK{})));
 
   using SmemLayoutAtomA = decltype(detail::ss_smem_selector<
-      GmmaMajorA, MmaElementA, decltype(cute::get<0>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
+      GmmaMajorA, ElementAMma, decltype(cute::get<0>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
   using SmemLayoutAtomB = decltype(detail::ss_smem_selector<
-      GmmaMajorB, MmaElementB, decltype(cute::get<1>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
+      GmmaMajorB, ElementBMma, decltype(cute::get<1>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
 
   static constexpr int PipelineStages = detail::compute_stage_count_or_override<detail::sm90_smem_capacity_bytes,
-      MmaElementA, MmaElementB, TileShape_MNK>(StageCountType{});
+      ElementAMma, ElementBMma, TileShape_MNK>(StageCountType{});
   using DispatchPolicy = cute::conditional_t<IsArrayOfPointersGemm,
       MainloopSm90ArrayTmaGmmaWarpSpecialized<PipelineStages, ClusterShape_MNK, KernelScheduleType>,
       /* For FP8 use a separate mainloop compared to other datatypes */
       cute::conditional_t<IsFP8Input,
           MainloopSm90TmaGmmaWarpSpecializedFP8<PipelineStages, ClusterShape_MNK, KernelScheduleType>,
           MainloopSm90TmaGmmaWarpSpecialized<PipelineStages, ClusterShape_MNK, KernelScheduleType>>>;
 
   using SmemCopyAtomA = void; 
   using SmemCopyAtomB = void; 
 
   using CollectiveOp = CollectiveMma<
       DispatchPolicy,
       TileShape_MNK,
       ElementA,
-      TagToStrideA_t<GmemLayoutA>,
+      TagToStrideA_t<GmemLayoutATag>,
       ElementB,
-      TagToStrideB_t<GmemLayoutB>,
+      TagToStrideB_t<GmemLayoutBTag>,
       TiledMma,
       GmemTiledCopyA,
       SmemLayoutAtomA,
       SmemCopyAtomA,
       cute::identity,
       GmemTiledCopyB,
       SmemLayoutAtomB,
@@ -255,92 +250,92 @@
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA_TMA_WS_RS
 template <
   class ElementA,
-  class GmemLayoutA,
+  class GmemLayoutATag,
   int AlignmentA,
   class ElementB,
-  class GmemLayoutB,
+  class GmemLayoutBTag,
   int AlignmentB,
   class ElementAccumulator,
   class TileShape_MNK,
   class ClusterShape_MNK,
   class StageCountType,
   class KernelScheduleType
 >
 struct CollectiveBuilder<
     arch::Sm90,
     arch::OpClassTensorOp,
     ElementA,
-    GmemLayoutA,
+    GmemLayoutATag,
     AlignmentA,
     ElementB,
-    GmemLayoutB,
+    GmemLayoutBTag,
     AlignmentB,
     ElementAccumulator,
     TileShape_MNK,
     ClusterShape_MNK,
     StageCountType,
     KernelScheduleType,
     cute::enable_if_t<
       (cute::is_same_v<KernelScheduleType, KernelTmaWarpSpecialized> ||
        cute::is_same_v<KernelScheduleType, KernelTmaWarpSpecializedPingpong> ||
        cute::is_same_v<KernelScheduleType, KernelTmaWarpSpecializedCooperative>) &&
-      detail::is_use_rmem_A<ElementA, GmemLayoutA, ElementB, GmemLayoutB>()> 
+      detail::is_use_rmem_A<ElementA, GmemLayoutATag, ElementB, GmemLayoutBTag>()> 
 > {
   static_assert(is_static<TileShape_MNK>::value);
   static_assert(is_static<ClusterShape_MNK>::value);
   static_assert(detail::is_aligned<ElementA, AlignmentA, ElementB, AlignmentB, detail::tma_alignment_bytes>(),
                 "Should meet TMA alignment requirement\n");
 #ifndef CUTLASS_SM90_COLLECTIVE_BUILDER_SUPPORTED
   static_assert(cutlass::detail::dependent_false<ElementA>, "Unsupported Toolkit for SM90 Collective Builder\n");
 #endif
-  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_rs_tag_to_major_A<GmemLayoutA>();
-  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_rs_tag_to_major_B<GmemLayoutB>();
-  static constexpr bool SwapAB = detail::is_swapAB<ElementA, GmemLayoutA, ElementB, GmemLayoutB>();
+  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_rs_tag_to_major_A<GmemLayoutATag>();
+  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_rs_tag_to_major_B<GmemLayoutBTag>();
+  static constexpr bool SwapAB = detail::is_swapAB<ElementA, GmemLayoutATag, ElementB, GmemLayoutBTag>();
   static constexpr bool IsWarpSpecializedTransposeB = detail::is_warpspecialized_transpose_B<
-      ElementA, GmemLayoutA, ElementB, GmemLayoutB, KernelScheduleType>();
+      ElementA, GmemLayoutATag, ElementB, GmemLayoutBTag, KernelScheduleType>();
 
   // For fp32 types, map to tf32 MMA value type
-  using MmaElementA = cute::conditional_t<cute::is_same_v<ElementA, float>, tfloat32_t, ElementA>;
-  using MmaElementB = cute::conditional_t<cute::is_same_v<ElementB, float>, tfloat32_t, ElementB>;
+  using ElementAMma = cute::conditional_t<cute::is_same_v<ElementA, float>, tfloat32_t, ElementA>;
+  using ElementBMma = cute::conditional_t<cute::is_same_v<ElementB, float>, tfloat32_t, ElementB>;
 
   using AtomLayoutMNK = cute::conditional_t<cute::is_same_v<KernelScheduleType, KernelTmaWarpSpecializedCooperative>,
       Layout<Shape<_2,_1,_1>>, Layout<Shape<_1,_1,_1>>>;
 
   using TiledMma = decltype(cute::make_tiled_mma(cute::GMMA::rs_op_selector<
-      MmaElementA, MmaElementB, ElementAccumulator, TileShape_MNK, GMMA::Major::K, GMMA::Major::K>(), AtomLayoutMNK{}));
+      ElementAMma, ElementBMma, ElementAccumulator, TileShape_MNK, GMMA::Major::K, GMMA::Major::K>(), AtomLayoutMNK{}));
 
   using GmemTiledCopyA = decltype(detail::sm90_cluster_shape_to_tma_atom(shape<1>(ClusterShape_MNK{})));
   using GmemTiledCopyB = decltype(detail::sm90_cluster_shape_to_tma_atom(shape<0>(ClusterShape_MNK{})));
 
-  using SmemLayoutAtomA = decltype(detail::rs_smem_selector<GmmaMajorA, MmaElementA,
+  using SmemLayoutAtomA = decltype(detail::rs_smem_selector<GmmaMajorA, ElementAMma,
       decltype(cute::get<0>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{})), IsWarpSpecializedTransposeB>());
-  using SmemLayoutAtomB = decltype(detail::rs_smem_selector<GmmaMajorB, MmaElementB,
+  using SmemLayoutAtomB = decltype(detail::rs_smem_selector<GmmaMajorB, ElementBMma,
       decltype(cute::get<1>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{})), IsWarpSpecializedTransposeB>());
 
   static constexpr int PipelineStages = detail::compute_stage_count_or_override<detail::sm90_smem_capacity_bytes,
-      MmaElementA, MmaElementB, TileShape_MNK>(StageCountType{});
+      ElementAMma, ElementBMma, TileShape_MNK>(StageCountType{});
 
   using DispatchPolicy = MainloopSm90TmaGmmaRmemAWarpSpecialized<
       PipelineStages, ClusterShape_MNK, KernelScheduleType>;
 
   using SmemCopyAtomA = cute::conditional_t<SwapAB, void, Copy_Atom<cute::DefaultCopy, ElementA>>;
   using SmemCopyAtomB = cute::conditional_t<SwapAB, Copy_Atom<cute::DefaultCopy, ElementB>, void>;
 
   using CollectiveOp = CollectiveMma<
       DispatchPolicy,
       TileShape_MNK,
       ElementA,
-      TagToStrideA_t<GmemLayoutA>,
+      TagToStrideA_t<GmemLayoutATag>,
       ElementB,
-      TagToStrideB_t<GmemLayoutB>,
+      TagToStrideB_t<GmemLayoutBTag>,
       TiledMma,
       GmemTiledCopyA,
       SmemLayoutAtomA,
       SmemCopyAtomA,
       cute::identity,
       GmemTiledCopyB,
       SmemLayoutAtomB,
@@ -350,33 +345,33 @@
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA_TMA_WS_RS Mixed Scaled GEMM
 template <
   class ElementPairA_,
-  class GmemLayoutA_,
+  class GmemLayoutATag_,
   int AlignmentA,
   class ElementPairB_,
-  class GmemLayoutB_,
+  class GmemLayoutBTag_,
   int AlignmentB,
   class ElementAccumulator,
   class TileShape_MNK,
   class ClusterShape_MNK,
   class StageCountType,
   class KernelScheduleType
 >
 struct CollectiveBuilder<
     arch::Sm90,
     arch::OpClassTensorOp,
     ElementPairA_,
-    GmemLayoutA_,
+    GmemLayoutATag_,
     AlignmentA,
     ElementPairB_,
-    GmemLayoutB_,
+    GmemLayoutBTag_,
     AlignmentB,
     ElementAccumulator,
     TileShape_MNK,
     ClusterShape_MNK,
     StageCountType,
     KernelScheduleType,
     cute::enable_if_t<
@@ -397,16 +392,16 @@
   using ElementB = detail::deduce_mixed_width_dtype_t<0, ElementPairB_>;
   static_assert(cute::is_tuple<ElementPairA_>::value ^ cute::is_tuple<ElementPairB_>::value ||
                (NeitherIsTuple && (sizeof_bits<ElementA>::value != sizeof_bits<ElementB>::value)), 
     "Either A OR B must be a tuple or the widths of A and B must be different.");
 
   static constexpr bool IsANarrow = sizeof_bits<ElementA>::value < sizeof_bits<ElementB>::value;
 
-  using GmemLayoutA = GmemLayoutA_;
-  using GmemLayoutB = GmemLayoutB_;
+  using GmemLayoutATag = GmemLayoutATag_;
+  using GmemLayoutBTag = GmemLayoutBTag_;
 
   using ElementPairA = cute::conditional_t<IsANarrow && NeitherIsTuple, cute::tuple<ElementA>, ElementPairA_>;
   using ElementPairB = cute::conditional_t<!IsANarrow && NeitherIsTuple, cute::tuple<ElementB>, ElementPairB_>;
 
   static constexpr bool IsATransformed = cute::is_tuple<ElementPairA>::value;
   using ElementScale = cute::conditional_t<IsATransformed, ScaleA, ScaleB>;
   using ElementZero = cute::conditional_t<IsATransformed, ZeroA, ZeroB>;
@@ -414,18 +409,18 @@
   static_assert(is_static<TileShape_MNK>::value);
   static_assert(is_static<ClusterShape_MNK>::value);
   static_assert(detail::is_aligned<ElementA, AlignmentA, ElementB, AlignmentB, detail::tma_alignment_bytes>(),
                 "Should meet TMA alignment requirement\n");
 #ifndef CUTLASS_SM90_COLLECTIVE_BUILDER_SUPPORTED
   static_assert(cutlass::detail::dependent_false<ElementA>, "Unsupported Toolkit for SM90 Collective Builder\n");
 #endif
-  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_rs_tag_to_major_A<GmemLayoutA>();
-  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_rs_tag_to_major_B<GmemLayoutB>();
+  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_rs_tag_to_major_A<GmemLayoutATag>();
+  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_rs_tag_to_major_B<GmemLayoutBTag>();
   static constexpr bool IsWarpSpecializedTransposeB = detail::is_warpspecialized_transpose_B<
-      ElementA, GmemLayoutA, ElementB, GmemLayoutB, KernelScheduleType>();
+      ElementA, GmemLayoutATag, ElementB, GmemLayoutBTag, KernelScheduleType>();
   static_assert(!IsWarpSpecializedTransposeB, "Mixed input GEMM does not support WS transpose B.");
 
   // If A is scaled, then we don't need to swap. Otherwise, we must ensure B goes to RF and we must swap the operands.
   static constexpr bool SwapAB = !IsATransformed;
 
   // When we relax the above assertion, we must handle setting the tile mma GmmaMajorB correctly.
   static constexpr cute::GMMA::Major TiledMmaGmmaMajorB = SwapAB ? GmmaMajorA : GmmaMajorB;
@@ -452,16 +447,16 @@
 
   using SmemCopyAtomA = cute::conditional_t<SwapAB, void, Copy_Atom<cute::DefaultCopy, ElementA>>;
   using SmemCopyAtomB = cute::conditional_t<SwapAB, Copy_Atom<cute::DefaultCopy, ElementB>, void>;
 
   using DispatchPolicy = MainloopSm90TmaGmmaRmemAWarpSpecializedMixedInput<PipelineStages, ClusterShape_MNK, KernelScheduleType>;
 
   // We pack the scale data with the operand that will be optionally scaled and converted before MMA.
-  using StrideA = TagToStrideA_t<GmemLayoutA>;
-  using StrideB = TagToStrideB_t<GmemLayoutB>;
+  using StrideA = TagToStrideA_t<GmemLayoutATag>;
+  using StrideB = TagToStrideB_t<GmemLayoutBTag>;
 
   using CollectiveOp = CollectiveMma<
       DispatchPolicy,
       TileShape_MNK,
       ElementPairA,
       StrideA,
       ElementPairB,
@@ -480,33 +475,33 @@
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA_TMA_WS_FP8_FAST_ACCUM_SS
 template <
   class ElementA,
-  class GmemLayoutA,
+  class GmemLayoutATag,
   int AlignmentA,
   class ElementB,
-  class GmemLayoutB,
+  class GmemLayoutBTag,
   int AlignmentB,
   class ElementAccumulator,
   class TileShape_MNK,
   class ClusterShape_MNK,
   class StageCountType,
   class KernelScheduleType
 >
 struct CollectiveBuilder<
     arch::Sm90,
     arch::OpClassTensorOp,
     ElementA,
-    GmemLayoutA,
+    GmemLayoutATag,
     AlignmentA,
     ElementB,
-    GmemLayoutB,
+    GmemLayoutBTag,
     AlignmentB,
     ElementAccumulator,
     TileShape_MNK,
     ClusterShape_MNK,
     StageCountType,
     KernelScheduleType,
     cute::enable_if_t<
@@ -518,22 +513,22 @@
   static_assert(is_static<TileShape_MNK>::value);
   static_assert(is_static<ClusterShape_MNK>::value);
   static_assert(detail::is_aligned<ElementA, AlignmentA, ElementB, AlignmentB, detail::tma_alignment_bytes>(),
                 "Not meet TMA alignment requirement yet\n");
   static_assert(detail::is_input_fp8<ElementA, ElementB>(),
                 "Only FP8 datatypes are compatible with these kernel schedules\n");
   // Dispatch TN fp8 kernels only to TMA warp specialized FP8 builder
-  static_assert(!detail::is_use_rmem_A<ElementA, GmemLayoutA, ElementB, GmemLayoutB>(),
+  static_assert(!detail::is_use_rmem_A<ElementA, GmemLayoutATag, ElementB, GmemLayoutBTag>(),
                  "Not supported for fp8 non-TN warp specialized kernels yet\n");
 #ifndef CUTLASS_SM90_COLLECTIVE_BUILDER_SUPPORTED
   static_assert(cutlass::detail::dependent_false<ElementA>, "Unsupported Toolkit for SM90 Collective Builder\n");
 #endif
 
-  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_ss_tag_to_major_A<ElementA, GmemLayoutA>();
-  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_ss_tag_to_major_B<ElementB, GmemLayoutB>();
+  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_ss_tag_to_major_A<ElementA, GmemLayoutATag>();
+  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_ss_tag_to_major_B<ElementB, GmemLayoutBTag>();
 
   static constexpr bool IsArrayOfPointersGemm = (cute::is_same_v<KernelScheduleType, KernelPtrArrayTmaWarpSpecializedCooperativeFP8FastAccum>);
   using AtomLayoutMNK = cute::conditional_t<cute::is_same_v<KernelScheduleType, KernelTmaWarpSpecializedCooperativeFP8FastAccum> ||
                                             IsArrayOfPointersGemm,
       Layout<Shape<_2,_1,_1>>, Layout<Shape<_1,_1,_1>>>;
 
   using TiledMma = decltype(cute::make_tiled_mma(cute::GMMA::ss_op_selector<
@@ -556,17 +551,17 @@
   using SmemCopyAtomA = void;
   using SmemCopyAtomB = void;
 
   using CollectiveOp = CollectiveMma<
       DispatchPolicy,
       TileShape_MNK,
       ElementA,
-      TagToStrideA_t<GmemLayoutA>,
+      TagToStrideA_t<GmemLayoutATag>,
       ElementB,
-      TagToStrideB_t<GmemLayoutB>,
+      TagToStrideB_t<GmemLayoutBTag>,
       TiledMma,
       GmemTiledCopyA,
       SmemLayoutAtomA,
       SmemCopyAtomA,
       cute::identity,
       GmemTiledCopyB,
       SmemLayoutAtomB,
@@ -576,82 +571,82 @@
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA_TMA_SS
 template <
   class ElementA,
-  class GmemLayoutA,
+  class GmemLayoutATag,
   int AlignmentA,
   class ElementB,
-  class GmemLayoutB,
+  class GmemLayoutBTag,
   int AlignmentB,
   class ElementAccumulator,
   class TileShape_MNK,
   class ClusterShape_MNK,
   class StageCountType,
   class KernelScheduleType
 >
 struct CollectiveBuilder<
     arch::Sm90,
     arch::OpClassTensorOp,
     ElementA,
-    GmemLayoutA,
+    GmemLayoutATag,
     AlignmentA,
     ElementB,
-    GmemLayoutB,
+    GmemLayoutBTag,
     AlignmentB,
     ElementAccumulator,
     TileShape_MNK,
     ClusterShape_MNK,
     StageCountType,
     KernelScheduleType,
     cute::enable_if_t<cute::is_same_v<KernelScheduleType, KernelTma> &&
-                     not detail::is_use_rmem_A<ElementA, GmemLayoutA, ElementB, GmemLayoutB>()>
+                     not detail::is_use_rmem_A<ElementA, GmemLayoutATag, ElementB, GmemLayoutBTag>()>
 > {
   static_assert(is_static<TileShape_MNK>::value);
   static_assert(is_static<ClusterShape_MNK>::value);
   static_assert(detail::is_aligned<ElementA, AlignmentA, ElementB, AlignmentB, detail::tma_alignment_bytes>(),
                 "Should meet TMA alignment requirement\n");
 #ifndef CUTLASS_SM90_COLLECTIVE_BUILDER_SUPPORTED
   static_assert(cutlass::detail::dependent_false<ElementA>, "Unsupported Toolkit for SM90 Collective Builder\n");
 #endif
 
   // For fp32 types, map to tf32 MMA value type
-  using MmaElementA = cute::conditional_t<cute::is_same_v<ElementA, float>, tfloat32_t, ElementA>;
-  using MmaElementB = cute::conditional_t<cute::is_same_v<ElementB, float>, tfloat32_t, ElementB>;
+  using ElementAMma = cute::conditional_t<cute::is_same_v<ElementA, float>, tfloat32_t, ElementA>;
+  using ElementBMma = cute::conditional_t<cute::is_same_v<ElementB, float>, tfloat32_t, ElementB>;
 
-  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_ss_tag_to_major_A<MmaElementA, GmemLayoutA>();
-  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_ss_tag_to_major_B<MmaElementB, GmemLayoutB>();
+  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_ss_tag_to_major_A<ElementAMma, GmemLayoutATag>();
+  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_ss_tag_to_major_B<ElementBMma, GmemLayoutBTag>();
 
   using TiledMma = decltype(cute::make_tiled_mma(cute::GMMA::ss_op_selector<
-      MmaElementA, MmaElementB, ElementAccumulator, TileShape_MNK, GmmaMajorA, GmmaMajorB>()));
+      ElementAMma, ElementBMma, ElementAccumulator, TileShape_MNK, GmmaMajorA, GmmaMajorB>()));
 
   using GmemTiledCopyA = decltype(detail::sm90_cluster_shape_to_tma_atom(shape<1>(ClusterShape_MNK{})));
   using GmemTiledCopyB = decltype(detail::sm90_cluster_shape_to_tma_atom(shape<0>(ClusterShape_MNK{})));
 
   using SmemLayoutAtomA = decltype(detail::ss_smem_selector<
-      GmmaMajorA, MmaElementA, decltype(cute::get<0>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
+      GmmaMajorA, ElementAMma, decltype(cute::get<0>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
   using SmemLayoutAtomB = decltype(detail::ss_smem_selector<
-      GmmaMajorB, MmaElementB, decltype(cute::get<1>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
+      GmmaMajorB, ElementBMma, decltype(cute::get<1>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
 
   static constexpr int PipelineStages = detail::compute_stage_count_or_override<detail::sm90_smem_capacity_bytes,
-      MmaElementA, MmaElementB, TileShape_MNK>(StageCountType{});
+      ElementAMma, ElementBMma, TileShape_MNK>(StageCountType{});
   using DispatchPolicy = MainloopSm90TmaGmma<PipelineStages, ClusterShape_MNK>;
 
   using SmemCopyAtomA = void;
   using SmemCopyAtomB = void;
 
   using CollectiveOp = CollectiveMma<
       DispatchPolicy,
       TileShape_MNK,
       ElementA,
-      TagToStrideA_t<GmemLayoutA>,
+      TagToStrideA_t<GmemLayoutATag>,
       ElementB,
-      TagToStrideB_t<GmemLayoutB>,
+      TagToStrideB_t<GmemLayoutBTag>,
       TiledMma,
       GmemTiledCopyA,
       SmemLayoutAtomA,
       SmemCopyAtomA,
       cute::identity,
       GmemTiledCopyB,
       SmemLayoutAtomB,
@@ -663,147 +658,147 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA_CpAsync
 template <
   class ElementA,
-  class GmemLayoutA,
+  class GmemLayoutATag,
   int AlignmentA,
   class ElementB,
-  class GmemLayoutB,
+  class GmemLayoutBTag,
   int AlignmentB,
   class ElementAccumulator,
   class TileShape_MNK,
   class ClusterShape_MNK,
   class StageCountType,
   class KernelScheduleType
 >
 struct [[deprecated("Use one of KernelCpAsyncWarpSpecialized schedules instead")]]
 CollectiveBuilder<
     arch::Sm90,
     arch::OpClassTensorOp,
     ElementA,
-    GmemLayoutA,
+    GmemLayoutATag,
     AlignmentA,
     ElementB,
-    GmemLayoutB,
+    GmemLayoutBTag,
     AlignmentB,
     ElementAccumulator,
     TileShape_MNK,
     ClusterShape_MNK,
     StageCountType,
     KernelScheduleType,
     cute::enable_if_t<
       cute::is_same_v<KernelScheduleType, KernelMultistage>>
 > {
   // Map to warp-specialized kernels for better performance
   using CollectiveOp = typename CollectiveBuilder<
     arch::Sm90,
     arch::OpClassTensorOp,
     ElementA,
-    GmemLayoutA,
+    GmemLayoutATag,
     AlignmentA,
     ElementB,
-    GmemLayoutB,
+    GmemLayoutBTag,
     AlignmentB,
     ElementAccumulator,
     TileShape_MNK,
     ClusterShape_MNK,
     StageCountType,
     KernelCpAsyncWarpSpecialized
   >::CollectiveOp;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA_CpAsync_WS_SS
 template <
   class ElementA,
-  class GmemLayoutA,
+  class GmemLayoutATag,
   int   AlignmentA,
   class ElementB,
-  class GmemLayoutB,
+  class GmemLayoutBTag,
   int   AlignmentB,
   class ElementAccumulator,
   class TileShape_MNK,
   class ClusterShape_MNK,
   class StageCountType,
   class KernelScheduleType
 >
 struct CollectiveBuilder<
     arch::Sm90,
     arch::OpClassTensorOp,
     ElementA,
-    GmemLayoutA,
+    GmemLayoutATag,
     AlignmentA,
     ElementB,
-    GmemLayoutB,
+    GmemLayoutBTag,
     AlignmentB,
     ElementAccumulator,
     TileShape_MNK,
     ClusterShape_MNK,
     StageCountType,
     KernelScheduleType,
     cute::enable_if_t<
       (cute::is_same_v<KernelScheduleType, KernelCpAsyncWarpSpecialized> ||
        cute::is_same_v<KernelScheduleType, KernelCpAsyncWarpSpecializedCooperative> ||
        cute::is_same_v<KernelScheduleType, KernelCpAsyncWarpSpecializedPingpong>) &&
-      not detail::is_use_rmem_A<ElementA, GmemLayoutA, ElementB, GmemLayoutB>()
+      not detail::is_use_rmem_A<ElementA, GmemLayoutATag, ElementB, GmemLayoutBTag>()
     >
 > {
   static_assert(is_static<TileShape_MNK>::value);
   static_assert(is_static<ClusterShape_MNK>::value);
 #ifndef CUTLASS_SM90_COLLECTIVE_BUILDER_SUPPORTED
   static_assert(cutlass::detail::dependent_false<ElementA>, "Unsupported Toolkit for SM90 Collective Builder\n");
 #endif
 
   // For fp32 types, map to tf32 MMA value type
-  using MmaElementA = cute::conditional_t<cute::is_same_v<ElementA, float>, tfloat32_t, ElementA>;
-  using MmaElementB = cute::conditional_t<cute::is_same_v<ElementB, float>, tfloat32_t, ElementB>;
+  using ElementAMma = cute::conditional_t<cute::is_same_v<ElementA, float>, tfloat32_t, ElementA>;
+  using ElementBMma = cute::conditional_t<cute::is_same_v<ElementB, float>, tfloat32_t, ElementB>;
 
   static_assert(detail::is_aligned<ElementA, AlignmentA, ElementB, AlignmentB, detail::cp_async_min_alignment_bytes>(),
                 "Minimum alignment required for cp.async is 4B.");
 
-  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_ss_tag_to_major_A<ElementA, GmemLayoutA>();
-  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_ss_tag_to_major_B<ElementB, GmemLayoutB>();
+  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_ss_tag_to_major_A<ElementA, GmemLayoutATag>();
+  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_ss_tag_to_major_B<ElementB, GmemLayoutBTag>();
 
   using AtomLayoutMNK = cute::conditional_t<cute::is_same_v<KernelScheduleType, KernelCpAsyncWarpSpecializedCooperative>,
       Layout<Shape<cute::Int<(size<0>(TileShape_MNK{}) < 128) ? 1 : 2>,_1,_1>>, Layout<Shape<_1,_1,_1>>>;
 
   using TiledMma = decltype(cute::make_tiled_mma(cute::GMMA::ss_op_selector<
-      MmaElementA, MmaElementB, ElementAccumulator, TileShape_MNK, GmmaMajorA, GmmaMajorB>(), AtomLayoutMNK{}));
+      ElementAMma, ElementBMma, ElementAccumulator, TileShape_MNK, GmmaMajorA, GmmaMajorB>(), AtomLayoutMNK{}));
 
   static constexpr int NumLoadWarpGroups = cute::is_same_v<KernelScheduleType, KernelCpAsyncWarpSpecialized> ? 2 : 1;
 
   using GmemTiledCopyA = decltype(detail::make_cp_async_gmem_tiled_copy<
-      NumThreadsPerWarpGroup * NumLoadWarpGroups, ElementA, AlignmentA, TagToStrideA_t<GmemLayoutA>,
+      NumThreadsPerWarpGroup * NumLoadWarpGroups, ElementA, AlignmentA, TagToStrideA_t<GmemLayoutATag>,
       decltype(cute::get<0>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
   using GmemTiledCopyB = decltype(detail::make_cp_async_gmem_tiled_copy<
-      NumThreadsPerWarpGroup * NumLoadWarpGroups, ElementB, AlignmentB, TagToStrideB_t<GmemLayoutB>,
+      NumThreadsPerWarpGroup * NumLoadWarpGroups, ElementB, AlignmentB, TagToStrideB_t<GmemLayoutBTag>,
       decltype(cute::get<1>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
 
   using SmemLayoutAtomA = decltype(detail::ss_smem_selector<
-      GmmaMajorA, MmaElementA, decltype(cute::get<0>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
+      GmmaMajorA, ElementAMma, decltype(cute::get<0>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
   using SmemLayoutAtomB = decltype(detail::ss_smem_selector<
-      GmmaMajorB, MmaElementB, decltype(cute::get<1>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
+      GmmaMajorB, ElementBMma, decltype(cute::get<1>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
 
   static constexpr int PipelineStages = detail::compute_stage_count_or_override<
-      detail::sm90_smem_capacity_bytes, MmaElementA, MmaElementB, TileShape_MNK>(StageCountType{});
+      detail::sm90_smem_capacity_bytes, ElementAMma, ElementBMma, TileShape_MNK>(StageCountType{});
 
   using DispatchPolicy = MainloopSm90CpAsyncGmmaWarpSpecialized<
       PipelineStages, ClusterShape_MNK, KernelScheduleType>;
 
   using CollectiveOp = CollectiveMma<
       DispatchPolicy,
       TileShape_MNK,
       ElementA,
-      TagToStrideA_t<GmemLayoutA>,
+      TagToStrideA_t<GmemLayoutATag>,
       ElementB,
-      TagToStrideB_t<GmemLayoutB>,
+      TagToStrideB_t<GmemLayoutBTag>,
       TiledMma,
       GmemTiledCopyA,
       SmemLayoutAtomA,
       void,
       cute::identity,
       GmemTiledCopyB,
       SmemLayoutAtomB,
@@ -813,101 +808,101 @@
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA_CpAsync_WS_RS
 template <
   class ElementA,
-  class GmemLayoutA,
+  class GmemLayoutATag,
   int   AlignmentA,
   class ElementB,
-  class GmemLayoutB,
+  class GmemLayoutBTag,
   int   AlignmentB,
   class ElementAccumulator,
   class TileShape_MNK,
   class ClusterShape_MNK,
   class StageCountType,
   class KernelScheduleType
 >
 struct CollectiveBuilder<
     arch::Sm90,
     arch::OpClassTensorOp,
     ElementA,
-    GmemLayoutA,
+    GmemLayoutATag,
     AlignmentA,
     ElementB,
-    GmemLayoutB,
+    GmemLayoutBTag,
     AlignmentB,
     ElementAccumulator,
     TileShape_MNK,
     ClusterShape_MNK,
     StageCountType,
     KernelScheduleType,
     cute::enable_if_t<
       (cute::is_same_v<KernelScheduleType, KernelCpAsyncWarpSpecialized> ||
        cute::is_same_v<KernelScheduleType, KernelCpAsyncWarpSpecializedCooperative> ||
        cute::is_same_v<KernelScheduleType, KernelCpAsyncWarpSpecializedPingpong>) &&
-      detail::is_use_rmem_A<ElementA, GmemLayoutA, ElementB, GmemLayoutB>()
+      detail::is_use_rmem_A<ElementA, GmemLayoutATag, ElementB, GmemLayoutBTag>()
     >
 > {
   static_assert(is_static<TileShape_MNK>::value);
   static_assert(is_static<ClusterShape_MNK>::value);
 #ifndef CUTLASS_SM90_COLLECTIVE_BUILDER_SUPPORTED
   static_assert(cutlass::detail::dependent_false<ElementA>, "Unsupported Toolkit for SM90 Collective Builder\n");
 #endif
 
   // For fp32 types, map to tf32 MMA value type
-  using MmaElementA = cute::conditional_t<cute::is_same_v<ElementA, float>, tfloat32_t, ElementA>;
-  using MmaElementB = cute::conditional_t<cute::is_same_v<ElementB, float>, tfloat32_t, ElementB>;
+  using ElementAMma = cute::conditional_t<cute::is_same_v<ElementA, float>, tfloat32_t, ElementA>;
+  using ElementBMma = cute::conditional_t<cute::is_same_v<ElementB, float>, tfloat32_t, ElementB>;
 
   static_assert(detail::is_aligned<ElementA, AlignmentA, ElementB, AlignmentB, detail::cp_async_min_alignment_bytes>(),
                 "Minimum alignment required for cp.async is 4B.");
 
-  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_rs_tag_to_major_A<GmemLayoutA>();
-  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_rs_tag_to_major_B<GmemLayoutB>();
-  static constexpr bool SwapAB = detail::is_swapAB<ElementA, GmemLayoutA, ElementB, GmemLayoutB>();
+  static constexpr cute::GMMA::Major GmmaMajorA = detail::gmma_rs_tag_to_major_A<GmemLayoutATag>();
+  static constexpr cute::GMMA::Major GmmaMajorB = detail::gmma_rs_tag_to_major_B<GmemLayoutBTag>();
+  static constexpr bool SwapAB = detail::is_swapAB<ElementA, GmemLayoutATag, ElementB, GmemLayoutBTag>();
   static constexpr bool IsWarpSpecializedTransposeB = detail::is_warpspecialized_transpose_B<
-      ElementA, GmemLayoutA, ElementB, GmemLayoutB, KernelScheduleType>();
+      ElementA, GmemLayoutATag, ElementB, GmemLayoutBTag, KernelScheduleType>();
 
   using AtomLayoutMNK = cute::conditional_t<cute::is_same_v<KernelScheduleType, KernelCpAsyncWarpSpecializedCooperative>,
       Layout<Shape<cute::Int<(size<0>(TileShape_MNK{}) < 128) ? 1 : 2>,_1,_1>>, Layout<Shape<_1,_1,_1>>>;
 
   using TiledMma = decltype(cute::make_tiled_mma(cute::GMMA::rs_op_selector<
-      MmaElementA, MmaElementB, ElementAccumulator, TileShape_MNK, GMMA::Major::K, GMMA::Major::K>(), AtomLayoutMNK{}));
+      ElementAMma, ElementBMma, ElementAccumulator, TileShape_MNK, GMMA::Major::K, GMMA::Major::K>(), AtomLayoutMNK{}));
 
   static constexpr int NumLoadWarpGroups = 1;
 
   using GmemTiledCopyA = decltype(detail::make_cp_async_gmem_tiled_copy<
-      NumThreadsPerWarpGroup * NumLoadWarpGroups, ElementA, AlignmentA, TagToStrideA_t<GmemLayoutA>,
+      NumThreadsPerWarpGroup * NumLoadWarpGroups, ElementA, AlignmentA, TagToStrideA_t<GmemLayoutATag>,
       decltype(cute::get<0>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
   using GmemTiledCopyB = decltype(detail::make_cp_async_gmem_tiled_copy<
-      NumThreadsPerWarpGroup * NumLoadWarpGroups, ElementB, AlignmentB, TagToStrideB_t<GmemLayoutB>,
+      NumThreadsPerWarpGroup * NumLoadWarpGroups, ElementB, AlignmentB, TagToStrideB_t<GmemLayoutBTag>,
       decltype(cute::get<1>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{}))>());
 
-  using SmemLayoutAtomA = decltype(detail::rs_smem_selector<GmmaMajorA, MmaElementA, 
+  using SmemLayoutAtomA = decltype(detail::rs_smem_selector<GmmaMajorA, ElementAMma, 
       decltype(cute::get<0>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{})), IsWarpSpecializedTransposeB>());
-  using SmemLayoutAtomB = decltype(detail::rs_smem_selector<GmmaMajorB, MmaElementB,
+  using SmemLayoutAtomB = decltype(detail::rs_smem_selector<GmmaMajorB, ElementBMma,
       decltype(cute::get<1>(TileShape_MNK{})), decltype(cute::get<2>(TileShape_MNK{})), IsWarpSpecializedTransposeB>());
 
   static constexpr int PipelineStages = detail::compute_stage_count_or_override<
-      detail::sm90_smem_capacity_bytes, MmaElementA, MmaElementB, TileShape_MNK>(StageCountType{});
+      detail::sm90_smem_capacity_bytes, ElementAMma, ElementBMma, TileShape_MNK>(StageCountType{});
 
   using DispatchPolicy = MainloopSm90CpAsyncGmmaRmemAWarpSpecialized<
       PipelineStages, ClusterShape_MNK, KernelScheduleType>;
 
   using SmemCopyAtomA = cute::conditional_t<SwapAB, void, Copy_Atom<cute::DefaultCopy, ElementA>>;
   using SmemCopyAtomB = cute::conditional_t<SwapAB, Copy_Atom<cute::DefaultCopy, ElementB>, void>;
 
   using CollectiveOp = CollectiveMma<
       DispatchPolicy,
       TileShape_MNK,
       ElementA,
-      TagToStrideA_t<GmemLayoutA>,
+      TagToStrideA_t<GmemLayoutATag>,
       ElementB,
-      TagToStrideB_t<GmemLayoutB>,
+      TagToStrideB_t<GmemLayoutBTag>,
       TiledMma,
       GmemTiledCopyA,
       SmemLayoutAtomA,
       SmemCopyAtomA,
       cute::identity,
       GmemTiledCopyB,
       SmemLayoutAtomB,
@@ -917,33 +912,33 @@
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // GMMA auto kernel schedule
 template <
   class ElementA,
-  class GmemLayoutA,
+  class GmemLayoutATag,
   int AlignmentA,
   class ElementB,
-  class GmemLayoutB,
+  class GmemLayoutBTag,
   int AlignmentB,
   class ElementAccumulator,
   class TileShape_MNK,
   class ClusterShape_MNK,
   class StageCountType,
   class KernelScheduleType
 >
 struct CollectiveBuilder<
     arch::Sm90,
     arch::OpClassTensorOp,
     ElementA,
-    GmemLayoutA,
+    GmemLayoutATag,
     AlignmentA,
     ElementB,
-    GmemLayoutB,
+    GmemLayoutBTag,
     AlignmentB,
     ElementAccumulator,
     TileShape_MNK,
     ClusterShape_MNK,
     StageCountType,
     KernelScheduleType,
     cute::enable_if_t<cute::is_same_v<KernelScheduleType, KernelScheduleAuto>>
@@ -984,18 +979,18 @@
   using KernelCpAsyncWarpSpecializedSchedule = KernelCpAsyncWarpSpecialized;
   using KernelSchedule = cute::conditional_t<IsTmaCompatible, KernelTmaWarpSpecializedSchedule, KernelCpAsyncWarpSpecializedSchedule>;
   static_assert((cute::is_same_v<KernelSchedule, KernelTmaWarpSpecializedSchedule> && IsMixedWidthInput) || !IsMixedWidthInput, "Only TMA warp specialized kernels are supported for mixed width input.");
   using CollectiveOp = typename CollectiveBuilder<
       arch::Sm90,
       arch::OpClassTensorOp,
       ElementA,
-      GmemLayoutA,
+      GmemLayoutATag,
       AlignmentA,
       ElementB,
-      GmemLayoutB,
+      GmemLayoutBTag,
       AlignmentB,
       ElementAccumulator,
       TileShape_MNK,
       ClusterShape_MNK,
       StageCountType,
       KernelSchedule
     >::CollectiveOp;
```

## cutlass_library/source/include/cutlass/gemm/device/default_gemm_configuration.h

```diff
@@ -759,14 +759,102 @@
   using EpilogueOutputOp = epilogue::thread::LinearCombinationClamp<
       ElementC, 128 / sizeof_bits<ElementC>::value, int32_t, float>;
 
   using Operator = arch::OpMultiplyAdd;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
+
+/// Base configuration for all {fe4m3, fe5m2} x {fe4m3, fe5m2} combinations on SM89
+template <
+  typename ElementA,
+  typename ElementB,
+  typename ElementC,
+  typename ElementAccumulator>
+struct DefaultGemmConfigurationSm89F8 {
+  static_assert((platform::is_same<ElementA, cutlass::float_e4m3_t>::value ||
+                 platform::is_same<ElementA, cutlass::float_e5m2_t>::value),
+                "ElementA must be of type float_e4m3_t or float_e5m2_t");
+  static_assert((platform::is_same<ElementB, cutlass::float_e4m3_t>::value ||
+                 platform::is_same<ElementB, cutlass::float_e5m2_t>::value),
+                "ElementB must be of type float_e4m3_t or float_e5m2_t");
+
+  static int const kAlignmentA = 128 / sizeof_bits<ElementA>::value;
+  static int const kAlignmentB = 128 / sizeof_bits<ElementB>::value;
+
+  using ThreadblockShape = GemmShape<128, 256, 64>;
+  using WarpShape = GemmShape<64, 64, 64>;
+  using InstructionShape = GemmShape<16, 8, 32>;
+  static int const kStages = 3;
+
+  using EpilogueOutputOp = epilogue::thread::LinearCombination<
+      ElementC, 128 / sizeof_bits<ElementC>::value, ElementAccumulator,
+      ElementAccumulator>;
+
+  using Operator = arch::OpMultiplyAdd;
+};
+
+/// Partial specialization for SM89 fe4m3 x fe4m3
+template <typename ElementC, typename ElementAccumulator>
+struct DefaultGemmConfiguration<
+  arch::OpClassTensorOp,
+  arch::Sm89,
+  cutlass::float_e4m3_t,
+  cutlass::float_e4m3_t,
+  ElementC,
+  ElementAccumulator> : DefaultGemmConfigurationSm89F8<
+                            cutlass::float_e4m3_t,
+                            cutlass::float_e4m3_t,
+                            ElementC,
+                            ElementAccumulator> {};
+
+/// Partial specialization for SM89 fe4m3 x fe5m2
+template <typename ElementC, typename ElementAccumulator>
+struct DefaultGemmConfiguration<
+  arch::OpClassTensorOp,
+  arch::Sm89,
+  cutlass::float_e4m3_t,
+  cutlass::float_e5m2_t,
+  ElementC,
+  ElementAccumulator> : DefaultGemmConfigurationSm89F8<
+                            cutlass::float_e4m3_t,
+                            cutlass::float_e5m2_t,
+                            ElementC,
+                            ElementAccumulator> {};
+
+/// Partial specialization for SM89 fe5m2 x fe4m3
+template <typename ElementC, typename ElementAccumulator>
+struct DefaultGemmConfiguration<
+  arch::OpClassTensorOp,
+  arch::Sm89,
+  cutlass::float_e5m2_t,
+  cutlass::float_e4m3_t,
+  ElementC,
+  ElementAccumulator> : DefaultGemmConfigurationSm89F8<
+                            cutlass::float_e5m2_t,
+                            cutlass::float_e4m3_t,
+                            ElementC,
+                            ElementAccumulator> {};
+
+/// Partial specialization for SM89 fe5m2 x fe5m2
+template <typename ElementC, typename ElementAccumulator>
+struct DefaultGemmConfiguration<
+  arch::OpClassTensorOp,
+  arch::Sm89,
+  cutlass::float_e5m2_t,
+  cutlass::float_e5m2_t,
+  ElementC,
+  ElementAccumulator> : DefaultGemmConfigurationSm89F8<
+                            cutlass::float_e5m2_t,
+                            cutlass::float_e5m2_t,
+                            ElementC,
+                            ElementAccumulator> {};
+
+////////////////////////////////////////////////////////////////////////////////
+
 template <typename ElementC,
           typename ElementAccumulator>
 struct DefaultGemmConfiguration<arch::OpClassTensorOp, arch::Sm90, double,
                                 double, ElementC, ElementAccumulator> {
 
   static int const kAlignmentA = 1;
   static int const kAlignmentB = 1;
```

## cutlass_library/source/include/cutlass/gemm/device/ell_gemm.h

```diff
@@ -362,15 +362,15 @@
 
     }
   };
 
 private:
 
   /// Kernel parameters object
-  typename GemmKernel::Params params_;
+  typename GemmKernel::Params params_{};
 
 public:
 
   /// Constructs the GEMM.
   EllGemm() { }
 
   /// Determines whether the GEMM can execute the given problem.
```

## cutlass_library/source/include/cutlass/gemm/device/gemm_universal.h

```diff
@@ -30,14 +30,15 @@
  **************************************************************************************************/
 /*! \file
     \brief
 */
 
 #pragma once
 
+#include "cutlass/arch/mma.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/arch/arch.h"
 #include "cutlass/device_kernel.h"
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
```

## cutlass_library/source/include/cutlass/gemm/device/gemm_universal_adapter.h

```diff
@@ -92,30 +92,30 @@
 public:
   using GemmKernel = GemmKernel_;
   using TileShape = typename GemmKernel::TileShape;
   using ElementA = typename GemmKernel::ElementA;
   using ElementB = typename GemmKernel::ElementB;
   using ElementC = typename GemmKernel::ElementC;
   using ElementD = typename GemmKernel::ElementD;
-  using ElementAccumulator = typename GemmKernel::TiledMma::ValTypeC;
+  using ElementAccumulator = typename GemmKernel::ElementAccumulator;
   using DispatchPolicy = typename GemmKernel::DispatchPolicy;
   using CollectiveMainloop = typename GemmKernel::CollectiveMainloop;
   using CollectiveEpilogue = typename GemmKernel::CollectiveEpilogue;
 
   // Map back to 2.x type as best as possible
   using LayoutA = gemm::detail::StrideToLayoutTagA_t<typename GemmKernel::StrideA>;
   using LayoutB = gemm::detail::StrideToLayoutTagB_t<typename GemmKernel::StrideB>;
   using LayoutC = gemm::detail::StrideToLayoutTagC_t<typename GemmKernel::StrideC>;
   using LayoutD = gemm::detail::StrideToLayoutTagC_t<typename GemmKernel::StrideD>;
 
   static bool const kEnableCudaHostAdapter = CUTLASS_ENABLE_CUDA_HOST_ADAPTER;
 
   static ComplexTransform const kTransformA = cute::is_same_v<typename GemmKernel::CollectiveMainloop::TransformA, cute::conjugate> ?
                                               ComplexTransform::kConjugate : ComplexTransform::kNone;
-  static ComplexTransform const kTransformB = cute::is_same_v<typename GemmKernel::CollectiveMainloop::TransformB, cute::conjugate> ? 
+  static ComplexTransform const kTransformB = cute::is_same_v<typename GemmKernel::CollectiveMainloop::TransformB, cute::conjugate> ?
                                               ComplexTransform::kConjugate : ComplexTransform::kNone;
 
   // Legacy: Assume MultiplyAdd only since we do not use this tag type in 3.0
   using MathOperator = cutlass::arch::OpMultiplyAdd;
 
   using OperatorClass = cutlass::detail::get_operator_class_t<typename CollectiveMainloop::TiledMma>;
 
@@ -157,17 +157,17 @@
       CUTE_STATIC_V(cute::tile_size<1>(typename CollectiveMainloop::TiledMma{})) / WarpsInMmaN,
       CUTE_STATIC_V(cute::tile_size<2>(typename CollectiveMainloop::TiledMma{}))>;
 
   static int constexpr kStages = CollectiveMainloop::DispatchPolicy::Stages;
 
   // Inspect TiledCopy for A and B to compute the alignment size
   static int constexpr kAlignmentA = cutlass::detail::get_alignment_count_from_gmem_tiled_copy<
-      typename CollectiveMainloop::GmemTiledCopyA, ElementA>();
+      typename CollectiveMainloop::GmemTiledCopyA, ElementA, typename CollectiveMainloop::TiledMma::ValTypeA>();
   static int constexpr kAlignmentB = cutlass::detail::get_alignment_count_from_gmem_tiled_copy<
-      typename CollectiveMainloop::GmemTiledCopyB, ElementB>();
+      typename CollectiveMainloop::GmemTiledCopyB, ElementB, typename CollectiveMainloop::TiledMma::ValTypeB>();
   static int constexpr kAlignmentC = cutlass::detail::get_alignment_count_from_gmem_tiled_copy<
       typename CollectiveEpilogue::GmemTiledCopyC, ElementC>();
   static int constexpr kAlignmentD = cutlass::detail::get_alignment_count_from_gmem_tiled_copy<
       typename CollectiveEpilogue::GmemTiledCopyD, ElementD>();
 
   using EpilogueOutputOp = typename CollectiveEpilogue::ThreadEpilogueOp;
 
@@ -274,28 +274,26 @@
 
   /// Initializes GEMM state from arguments.
   Status
   initialize(
     Arguments const& args,
     void* workspace = nullptr,
     cudaStream_t stream = nullptr,
-    CudaHostAdapter *cuda_adapter = nullptr) {
+    CudaHostAdapter* cuda_adapter = nullptr) {
 
     CUTLASS_TRACE_HOST("GemmUniversal::initialize() - workspace "
       << workspace << ", stream: " << (stream ? "non-null" : "null"));
 
     // Initialize the workspace
-    Status status = GemmKernel::initialize_workspace(args, workspace, stream);
+    Status status = GemmKernel::initialize_workspace(args, workspace, stream, cuda_adapter);
     if (status != Status::kSuccess) {
       return status;
     }
-
     // Initialize the Params structure
     params_ = GemmKernel::to_underlying_arguments(args, workspace);
-
     // Don't set the function attributes - require the CudaHostAdapter to set it.
     if constexpr (kEnableCudaHostAdapter) {
       CUTLASS_ASSERT(cuda_adapter);
       return Status::kSuccess;
     }
     else {
       //
@@ -314,15 +312,14 @@
         if (cudaSuccess != result) {
           result = cudaGetLastError(); // to clear the error bit
           CUTLASS_TRACE_HOST("  cudaFuncSetAttribute() returned error: " << cudaGetErrorString(result));
           return Status::kErrorInternal;
         }
       }
     }
-
     return Status::kSuccess;
   }
 
   /// Update API is preserved in 3.0, but does not guarantee a lightweight update of params.
   Status
   update(Arguments const& args, void* workspace = nullptr) {
     CUTLASS_TRACE_HOST("GemmUniversal()::update() - workspace: " << workspace);
@@ -338,54 +335,55 @@
 
   /// Primary run() entry point API that is static allowing users to create and manage their own params.
   /// Supplied params struct must be construct by calling GemmKernel::to_underling_arguments()
   static Status
   run(Params& params,
       cudaStream_t stream = nullptr,
       CudaHostAdapter *cuda_adapter = nullptr) {
-
     CUTLASS_TRACE_HOST("GemmUniversal::run()");
     dim3 const block = GemmKernel::get_block_shape();
     dim3 const grid = get_grid_shape(params);
 
     // configure smem size and carveout
     int smem_size = GemmKernel::SharedStorageSize;
 
-    Status launch_result;
+    Status launch_result{ Status::kSuccess };
     // Use extended launch API only for mainloops that use it
-    if constexpr(GemmKernel::ArchTag::kMinComputeCapability >= 90) {
+    if constexpr (GemmKernel::ArchTag::kMinComputeCapability >= 90) {
       dim3 cluster(cute::size<0>(typename GemmKernel::DispatchPolicy::ClusterShape{}),
                    cute::size<1>(typename GemmKernel::DispatchPolicy::ClusterShape{}),
                    cute::size<2>(typename GemmKernel::DispatchPolicy::ClusterShape{}));
-
       void* kernel_params[] = {&params};
 
       if constexpr (kEnableCudaHostAdapter) {
         //
         // Use the cuda host adapter
         //
         CUTLASS_ASSERT(cuda_adapter);
         if (cuda_adapter) {
 
-          launch_result = cuda_adapter->launch(
-            grid, cluster, block, smem_size, stream, kernel_params, 0
-          );
+          launch_result = cuda_adapter->launch(grid,
+                                               cluster,
+                                               block,
+                                               smem_size,
+                                               stream,
+                                               kernel_params,
+                                               0);
         }
         else {
           return Status::kErrorInternal;
         }
       }
       else {
-
         CUTLASS_ASSERT(cuda_adapter == nullptr);
         void const* kernel = (void const*) device_kernel<GemmKernel>;
-
-        launch_result = ClusterLauncher::launch(
-          grid, cluster, block, smem_size, stream, kernel, kernel_params);
-
+        if constexpr (GemmKernel::ArchTag::kMinComputeCapability == 90) {
+          launch_result = ClusterLauncher::launch(
+            grid, cluster, block, smem_size, stream, kernel, kernel_params);
+        }
       }
     }
     else {
       launch_result = Status::kSuccess;
       if constexpr (kEnableCudaHostAdapter) {
         CUTLASS_ASSERT(cuda_adapter);
         if (cuda_adapter) {
@@ -424,15 +422,14 @@
   Status
   run(
     Arguments const& args,
     void* workspace = nullptr,
     cudaStream_t stream = nullptr,
     CudaHostAdapter *cuda_adapter = nullptr
   ) {
-
     Status status = initialize(args, workspace, stream, cuda_adapter);
 
     if (Status::kSuccess == status) {
       status = run(params_, stream, cuda_adapter);
     }
     return status;
   }
@@ -552,23 +549,23 @@
     }
     else {
       return args;
     }
   }
 
   /// Determines whether the GEMM can execute the given problem.
-  static Status can_implement(Arguments const &args) {
+  static Status can_implement(Arguments const &args, CudaHostAdapter *cuda_adapter = nullptr) {
 
-    return UnderlyingOperator::can_implement(to_underlying_arguments(args));
+    return UnderlyingOperator::can_implement(to_underlying_arguments(args), cuda_adapter);
   }
 
   /// Gets the workspace size
-  static size_t get_workspace_size(Arguments const &args) {
+  static size_t get_workspace_size(Arguments const &args, CudaHostAdapter *cuda_adapter = nullptr) {
 
-    return UnderlyingOperator::get_workspace_size(to_underlying_arguments(args));
+    return UnderlyingOperator::get_workspace_size(to_underlying_arguments(args), cuda_adapter);
   }
 
   /// Computes the grid shape
   static dim3 get_grid_shape(Arguments const &args) {
     return UnderlyingOperator::get_grid_shape(to_underlying_arguments(args));
   }
 
@@ -600,15 +597,15 @@
     CudaHostAdapter *cuda_adapter = nullptr) {
 
     return underlying_operator_.run(stream, cuda_adapter);
   }
 
   /// Runs the kernel using initialized state.
   Status operator()(
-    cudaStream_t stream = nullptr, 
+    cudaStream_t stream = nullptr,
     CudaHostAdapter *cuda_adapter = nullptr) {
 
     return run(stream);
   }
 
   /// Runs the kernel using initialized state.
   Status operator()(
```

## cutlass_library/source/include/cutlass/gemm/kernel/default_gemm.h

```diff
@@ -214,14 +214,92 @@
 
   /// Define the kernel-level GEMM operator.
   using GemmKernel = kernel::Gemm<Mma, Epilogue, ThreadblockSwizzle, SplitKSerial>;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
+/// Partial specialization for Ada Architecture
+template <
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentB,
+    /// Element type for C and D matrix operands
+    typename ElementC,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// If true, kernel is configured to support serial reduction in the
+    /// epilogue
+    bool SplitKSerial,
+    /// Operation performed by GEMM
+    typename Operator,
+    /// Use zfill or predicate for out-of-bound cp.async
+    SharedMemoryClearOption SharedMemoryClear,
+    /// Gather operand A by using an index array
+    bool GatherA,
+    /// Gather operand B by using an index array
+    bool GatherB,
+    /// Scatter result D by using an index array
+    bool ScatterD,
+    /// Permute result D
+    typename PermuteDLayout,
+    /// Permute operand A
+    typename PermuteALayout,
+    /// Permute operand B
+    typename PermuteBLayout
+>
+struct DefaultGemm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, ElementC,
+                   layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
+                   arch::Sm89, ThreadblockShape, WarpShape, InstructionShape,
+                   EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
+                   Operator, SharedMemoryClear, GatherA, GatherB, ScatterD, 
+                   PermuteDLayout, PermuteALayout, PermuteBLayout> {
+  /// Define the threadblock-scoped matrix multiply-accumulate
+  using Mma = typename cutlass::gemm::threadblock::DefaultMma<
+      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm89,
+      ThreadblockShape, WarpShape, InstructionShape, Stages,
+      Operator, false, SharedMemoryClear, GatherA, GatherB,
+      PermuteALayout, PermuteBLayout>::ThreadblockMma;
+
+  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
+
+  /// Define the epilogue
+  using Epilogue =
+      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
+          ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
+          EpilogueOutputOp::kCount, ScatterD, PermuteDLayout>::Epilogue;
+
+  /// Define the kernel-level GEMM operator.
+  using GemmKernel = kernel::Gemm<Mma, Epilogue, ThreadblockSwizzle, SplitKSerial>;
+};
+
+////////////////////////////////////////////////////////////////////////////////
+
 /// Partial specialization for Ampere Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
     /// Access granularity of A matrix in units of elements
```

## cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_sparse.h

```diff
@@ -179,13 +179,74 @@
           ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
           EpilogueOutputOp::kCount>::Epilogue;
 
   /// Define the kernel-level GEMM operator.
   using GemmKernel = kernel::SparseGemm<Mma, Epilogue, ThreadblockSwizzle, SplitKSerial>;
 };
 
+///////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for Ada Architecture
+template <
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentB,
+    /// Element type for C and D matrix operands
+    typename ElementC,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// If true, kernel is configured to support serial reduction in the
+    /// epilogue
+    bool SplitKSerial,
+    /// Operation performed by GEMM
+    typename Operator>
+struct DefaultSparseGemm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, ElementC,
+                   layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
+                   arch::Sm89, ThreadblockShape, WarpShape, InstructionShape,
+                   EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
+                   Operator> {
+  /// Define the threadblock-scoped matrix multiply-accumulate
+  using Mma = typename cutlass::gemm::threadblock::DefaultSparseMma<
+      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm89,
+      ThreadblockShape, WarpShape, InstructionShape, Stages,
+      Operator>::ThreadblockMma;
+
+  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
+
+  /// Define the epilogue
+  using Epilogue =
+      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
+          ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
+          EpilogueOutputOp::kCount>::Epilogue;
+
+  /// Define the kernel-level GEMM operator.
+  using GemmKernel = kernel::SparseGemm<Mma, Epilogue, ThreadblockSwizzle, SplitKSerial>;
+};
+
 ////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace kernel
 }  // namespace gemm
 }  // namespace cutlass
```

## cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k_grouped.h

```diff
@@ -161,15 +161,15 @@
     >
 struct DefaultRank2KGrouped<ElementA, LayoutA, TransformA, kAlignmentA,
           ElementB, LayoutB, TransformB, kAlignmentB,
           ElementC, LayoutC,
           FillModeC, ElementAccumulator, OperatorClass, ArchTag, ThreadblockShape,
           WarpShape, InstructionShape, EpilogueOutputOp,
           ThreadblockSwizzle, Stages, Operator, BlasMode_, GroupScheduleMode_,
-          typename std::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
+          typename platform::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
 > {
   // If true, we must construct a 'transposed-and-exchanged' Rank2K operator.
   static bool const kInternalTranspose = platform::is_same<LayoutC, layout::ColumnMajor>::value;
 
   using MapArguments = kernel::detail::Rank2KMapArguments<
     ElementA,
     LayoutA,
@@ -279,15 +279,15 @@
     >
 struct DefaultRank2KGrouped<ElementA, LayoutA, TransformA, kAlignmentA,
           ElementB, LayoutB, TransformB, kAlignmentB,
           ElementC, LayoutC,
           FillModeC, ElementAccumulator, OperatorClass, ArchTag, ThreadblockShape,
           WarpShape, InstructionShape, EpilogueOutputOp,
           ThreadblockSwizzle, Stages, Operator, BlasMode_, GroupScheduleMode_,
-          typename std::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
+          typename platform::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
 > {
   // If true, we must construct a 'transposed-and-exchanged' Rank2K operator.
   static bool const kInternalTranspose = platform::is_same<LayoutC, layout::ColumnMajor>::value;
 
   using MapArguments = kernel::detail::Rank2KMapArguments<
     ElementA,
     LayoutA,
```

## cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k_universal.h

```diff
@@ -179,15 +179,15 @@
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
   SplitKSerial,
   Operator,
   BlasMode::kSymmetric,
-  typename std::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
+  typename platform::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
 > {
 
   using DefaultRank2Kkernel = typename kernel::DefaultRank2K<
     ElementA,
     LayoutA,
     kAlignmentA,
     ElementB,
@@ -297,15 +297,15 @@
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
   SplitKSerial,
   Operator,
   kBlasMode,
-  typename std::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
+  typename platform::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
 > {
 
   using DefaultRank2Kkernel = typename kernel::DefaultRank2KComplex<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
```

## cutlass_library/source/include/cutlass/gemm/kernel/default_rank_k_universal.h

```diff
@@ -161,15 +161,15 @@
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
   SplitKSerial,
   Operator,
   BlasMode::kSymmetric,
-  typename std::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
+  typename platform::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
 > {
 
   using DefaultRankKkernel = typename kernel::DefaultRankK<
     ElementA,
     LayoutA,
     kAlignmentA,
     ElementC,
@@ -261,15 +261,15 @@
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
   SplitKSerial,
   Operator,
   kBlasMode,
-  typename std::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
+  typename platform::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
 > {
 
   using DefaultRankKkernel = typename kernel::DefaultRankKComplex<
     ElementA,
     LayoutA,
     ElementC,
     LayoutC,
```

## cutlass_library/source/include/cutlass/gemm/kernel/default_symm_universal.h

```diff
@@ -178,15 +178,15 @@
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
   SplitKSerial,
   Operator,
   BlasMode::kSymmetric,
-  typename std::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
+  typename platform::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
 > {
 
   using DefaultSymmkernel = typename kernel::DefaultSymm<
     ElementA,
     LayoutA,
     SideModeA,
     FillModeA,
@@ -294,15 +294,15 @@
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
   SplitKSerial,
   Operator,
   kBlasMode,
-  typename std::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
+  typename platform::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
 > {
 
   using DefaultSymmkernel = typename kernel::DefaultSymmComplex<
     ElementA,
     LayoutA,
     SideModeA,
     FillModeA,
```

## cutlass_library/source/include/cutlass/gemm/kernel/default_trmm_universal.h

```diff
@@ -186,15 +186,15 @@
   WarpShape,
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
   SplitKSerial,
   Operator,
-  typename std::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
+  typename platform::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
 > {
 
   using DefaultTrmmKernel = typename kernel::DefaultTrmm<
     ElementA,
     LayoutA,
     kAlignmentA,
     ElementB,
@@ -307,15 +307,15 @@
   WarpShape,
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
   SplitKSerial,
   Operator,
-  typename std::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
+  typename platform::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
 > {
 
   using DefaultTrmmKernel = typename kernel::DefaultTrmmComplex<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
```

## cutlass_library/source/include/cutlass/gemm/kernel/ell_gemm.h

```diff
@@ -69,40 +69,38 @@
 
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
   /// Parameters structure
   struct Params {
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
-    typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorA::TensorRef ref_A;
-    typename Mma::IteratorB::Params params_B;
-    typename Mma::IteratorB::TensorRef ref_B;
-    typename Epilogue::OutputTileIterator::Params params_C;
-    typename Epilogue::OutputTileIterator::TensorRef ref_C;
-    typename Epilogue::OutputTileIterator::Params params_D;
-    typename Epilogue::OutputTileIterator::TensorRef ref_D;
-    typename OutputOp::Params output_op;
-    int *semaphore;
-    int gemm_k_iterations;
-    int gemm_k_size;
-    const int* ell_idx;
-    int ell_ncol;
-    int ell_blocksize;
-    int ell_base_idx;
+    cutlass::gemm::GemmCoord problem_size{};
+    cutlass::gemm::GemmCoord grid_tiled_shape{};
+    int swizzle_log_tile{0};
+    typename Mma::IteratorA::Params params_A{};
+    typename Mma::IteratorA::TensorRef ref_A{};
+    typename Mma::IteratorB::Params params_B{};
+    typename Mma::IteratorB::TensorRef ref_B{};
+    typename Epilogue::OutputTileIterator::Params params_C{};
+    typename Epilogue::OutputTileIterator::TensorRef ref_C{};
+    typename Epilogue::OutputTileIterator::Params params_D{};
+    typename Epilogue::OutputTileIterator::TensorRef ref_D{};
+    typename OutputOp::Params output_op{};
+    int *semaphore = nullptr;
+    int gemm_k_iterations{0};
+    int gemm_k_size{0};
+    const int* ell_idx = nullptr;
+    int ell_ncol{0};
+    int ell_blocksize{0};
+    int ell_base_idx{0};
 
     //
     // Methods
     //
-
-    CUTLASS_HOST_DEVICE
-    Params(): swizzle_log_tile(0), semaphore(0), gemm_k_iterations(0), gemm_k_size(0) { }
+   Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(
       cutlass::gemm::GemmCoord const & problem_size,
       cutlass::gemm::GemmCoord const & grid_tiled_shape,
       typename Mma::IteratorA::TensorRef ref_A,
       typename Mma::IteratorB::TensorRef ref_B,
@@ -150,17 +148,15 @@
     };
     typename cutlass::transform::threadblock::ell::SharedStorage ell;
   };
 
   //
   // Methods
   //
-
-  CUTLASS_HOST_DEVICE
-  EllGemm() { }
+  EllGemm() = default;
 
   /// Determines whether kernel satisfies alignment
     static Status can_implement(
       cutlass::gemm::GemmCoord const & problem_size,
       typename Mma::IteratorA::TensorRef ref_A,
       typename Mma::IteratorB::TensorRef ref_B,
       typename Epilogue::OutputTileIterator::TensorRef ref_C,
@@ -454,40 +450,38 @@
 
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
   /// Parameters structure
   struct Params {
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
-    typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorA::TensorRef ref_A;
-    typename Mma::IteratorB::Params params_B;
-    typename Mma::IteratorB::TensorRef ref_B;
-    typename Epilogue::OutputTileIterator::Params params_C;
-    typename Epilogue::OutputTileIterator::TensorRef ref_C;
-    typename Epilogue::OutputTileIterator::Params params_D;
-    typename Epilogue::OutputTileIterator::TensorRef ref_D;
-    typename OutputOp::Params output_op;
-    int *semaphore;
-    int gemm_k_iterations;
-    int gemm_k_size;
-    const int* ell_idx;
-    int ell_ncol;
-    int ell_blocksize;
-    int ell_base_idx;
+    cutlass::gemm::GemmCoord problem_size{};
+    cutlass::gemm::GemmCoord grid_tiled_shape{};
+    int swizzle_log_tile{0};
+    typename Mma::IteratorA::Params params_A{};
+    typename Mma::IteratorA::TensorRef ref_A{};
+    typename Mma::IteratorB::Params params_B{};
+    typename Mma::IteratorB::TensorRef ref_B{};
+    typename Epilogue::OutputTileIterator::Params params_C{};
+    typename Epilogue::OutputTileIterator::TensorRef ref_C{};
+    typename Epilogue::OutputTileIterator::Params params_D{};
+    typename Epilogue::OutputTileIterator::TensorRef ref_D{};
+    typename OutputOp::Params output_op{};
+    int *semaphore = nullptr;
+    int gemm_k_iterations{0};
+    int gemm_k_size{0};
+    const int* ell_idx = nullptr;
+    int ell_ncol{0};
+    int ell_blocksize{0};
+    int ell_base_idx{0};
 
     //
     // Methods
     //
-
-    CUTLASS_HOST_DEVICE
-    Params(): swizzle_log_tile(0), semaphore(0), gemm_k_iterations(0), gemm_k_size(0) { }
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(
       cutlass::gemm::GemmCoord const & problem_size,
       cutlass::gemm::GemmCoord const & grid_tiled_shape,
       typename Mma::IteratorA::TensorRef ref_A,
       typename Mma::IteratorB::TensorRef ref_B,
```

## cutlass_library/source/include/cutlass/gemm/kernel/gemm_batched.h

```diff
@@ -61,39 +61,37 @@
 
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
   /// Parameters structure
   struct Params {
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
-    typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorA::TensorRef ref_A;
-    int64_t stride_A;
-    typename Mma::IteratorB::Params params_B;
-    typename Mma::IteratorB::TensorRef ref_B;
-    int64_t stride_B;
-    typename Epilogue::OutputTileIterator::Params params_C;
-    typename Epilogue::OutputTileIterator::TensorRef ref_C;
-    int64_t stride_C;
-    typename Epilogue::OutputTileIterator::Params params_D;
-    typename Epilogue::OutputTileIterator::TensorRef ref_D;
-    int64_t stride_D;
-    typename OutputOp::Params epilogue;
-    int batch_count;
-    int gemm_k_iterations;
+    cutlass::gemm::GemmCoord problem_size{};
+    cutlass::gemm::GemmCoord grid_tiled_shape{};
+    int swizzle_log_tile{0};
+    typename Mma::IteratorA::Params params_A{};
+    typename Mma::IteratorA::TensorRef ref_A{};
+    int64_t stride_A{0};
+    typename Mma::IteratorB::Params params_B{};
+    typename Mma::IteratorB::TensorRef ref_B{};
+    int64_t stride_B{0};
+    typename Epilogue::OutputTileIterator::Params params_C{};
+    typename Epilogue::OutputTileIterator::TensorRef ref_C{};
+    int64_t stride_C{0};
+    typename Epilogue::OutputTileIterator::Params params_D{};
+    typename Epilogue::OutputTileIterator::TensorRef ref_D{};
+    int64_t stride_D{0};
+    typename OutputOp::Params epilogue{};
+    int batch_count{1};
+    int gemm_k_iterations{0};
 
     //
     // Methods
     //
-
-    CUTLASS_HOST_DEVICE
-    Params() : swizzle_log_tile(0) { }
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(
       cutlass::gemm::GemmCoord const & problem_size_,
       cutlass::gemm::GemmCoord const & grid_tiled_shape_,
       typename Mma::IteratorA::TensorRef ref_A_,
       int64_t stride_A_,
@@ -119,31 +117,27 @@
       ref_C(ref_C_),
       stride_C(stride_C_),
       params_D(ref_D_.layout()),
       ref_D(ref_D_),
       stride_D(stride_D_),
       epilogue(epilogue_),
       batch_count(batch_count_),
-      gemm_k_iterations((problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK) {
-
-    }
+      gemm_k_iterations((problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK) {}
   };
 
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
 
   //
   // Methods
   //
-
-  CUTLASS_HOST_DEVICE
-  GemmBatched() { } 
+  GemmBatched() = default;
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
```

## cutlass_library/source/include/cutlass/gemm/kernel/gemm_grouped.h

```diff
@@ -129,54 +129,40 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    GemmCoord *problem_sizes;
-    int problem_count;
-    int threadblock_count;
-
-    typename EpilogueOutputOp::Params output_op;
-
-    ElementA ** ptr_A;
-    ElementB ** ptr_B;
-    ElementC ** ptr_C;
-    ElementC ** ptr_D;
-
-    typename LayoutA::Stride::LongIndex *lda;
-    typename LayoutB::Stride::LongIndex *ldb;
-    typename LayoutC::Stride::LongIndex *ldc;
-    typename LayoutC::Stride::LongIndex *ldd;
+    GemmCoord *problem_sizes{nullptr};
+    int problem_count{0};
+    int threadblock_count{0};
+
+    typename EpilogueOutputOp::Params output_op{};
+
+    ElementA ** ptr_A{nullptr};
+    ElementB ** ptr_B{nullptr};
+    ElementC ** ptr_C{nullptr};
+    ElementC ** ptr_D{nullptr};
+
+    typename LayoutA::Stride::LongIndex *lda{nullptr};
+    typename LayoutB::Stride::LongIndex *ldb{nullptr};
+    typename LayoutC::Stride::LongIndex *ldc{nullptr};
+    typename LayoutC::Stride::LongIndex *ldd{nullptr};
 
     // Only used by device-level operator
-    GemmCoord *host_problem_sizes;
+    GemmCoord *host_problem_sizes{nullptr};
+
 
     //
     // Methods
     //
 
     /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Arguments(): 
-      problem_count(0),
-      threadblock_count(0), 
-      ptr_A(nullptr), 
-      ptr_B(nullptr), 
-      ptr_C(nullptr), 
-      ptr_D(nullptr), 
-      lda(nullptr),
-      ldb(nullptr),
-      ldc(nullptr),
-      ldd(nullptr),
-      host_problem_sizes(nullptr)
-    {
-
-    }
+    Arguments() = default;
 
     /// Ctor
     CUTLASS_HOST_DEVICE
     Arguments(    
       GemmCoord *problem_sizes,
       int problem_count,
       int threadblock_count,
@@ -212,44 +198,34 @@
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
   struct Params {
 
-    typename ProblemVisitor::Params problem_visitor;
-    int threadblock_count;
+    typename ProblemVisitor::Params problem_visitor{};
+    int threadblock_count{0};
 
-    typename EpilogueOutputOp::Params output_op;
+    typename EpilogueOutputOp::Params output_op{};
 
-    ElementA ** ptr_A;
-    ElementB ** ptr_B;
-    ElementC ** ptr_C;
-    ElementC ** ptr_D;
-
-    typename LayoutA::Stride::LongIndex *lda;
-    typename LayoutB::Stride::LongIndex *ldb;
-    typename LayoutC::Stride::LongIndex *ldc;
-    typename LayoutC::Stride::LongIndex *ldd;
+    ElementA ** ptr_A{nullptr};
+    ElementB ** ptr_B{nullptr};
+    ElementC ** ptr_C{nullptr};
+    ElementC ** ptr_D{nullptr};
+
+    typename LayoutA::Stride::LongIndex *lda{nullptr};
+    typename LayoutB::Stride::LongIndex *ldb{nullptr};
+    typename LayoutC::Stride::LongIndex *ldc{nullptr};
+    typename LayoutC::Stride::LongIndex *ldd{nullptr};
 
     //
     // Methods
     //
 
-    CUTLASS_HOST_DEVICE
-    Params():
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      lda(nullptr),
-      ldb(nullptr),
-      ldc(nullptr),
-      ldd(nullptr)
-    { }
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(Arguments const &args,
           void *workspace = nullptr,
           int tile_count = 0):
       problem_visitor(args.problem_sizes, args.problem_count, workspace, tile_count),
       threadblock_count(args.threadblock_count),
```

## cutlass_library/source/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h

```diff
@@ -131,58 +131,41 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    GemmCoord *problem_sizes;
-    int problem_count;
-    int threadblock_count;
-
-    typename EpilogueOutputOp::Params output_op;
-
-    ElementA ** ptr_A;
-    ElementB ** ptr_B;
-    ElementC ** ptr_C;
-    ElementC ** ptr_D;
-    void ** ptr_norm;
-    void ** ptr_sum;
-
-    typename LayoutA::Stride::LongIndex *lda;
-    typename LayoutB::Stride::LongIndex *ldb;
-    typename LayoutC::Stride::LongIndex *ldc;
-    typename LayoutC::Stride::LongIndex *ldd;
+    GemmCoord *problem_sizes{nullptr};
+    int problem_count{0};
+    int threadblock_count{0};
+
+    typename EpilogueOutputOp::Params output_op{};
+
+    ElementA ** ptr_A{nullptr};
+    ElementB ** ptr_B{nullptr};
+    ElementC ** ptr_C{nullptr};
+    ElementC ** ptr_D{nullptr};
+    void ** ptr_norm{nullptr};
+    void ** ptr_sum{nullptr};
+
+    typename LayoutA::Stride::LongIndex *lda{nullptr};
+    typename LayoutB::Stride::LongIndex *ldb{nullptr};
+    typename LayoutC::Stride::LongIndex *ldc{nullptr};
+    typename LayoutC::Stride::LongIndex *ldd{nullptr};
 
     // Only used by device-level operator
-    GemmCoord *host_problem_sizes;
+    GemmCoord *host_problem_sizes{nullptr};
 
     //
     // Methods
     //
 
     /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Arguments():
-      problem_count(0),
-      threadblock_count(0),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      ptr_norm(nullptr),
-      ptr_sum(nullptr),
-      lda(nullptr),
-      ldb(nullptr),
-      ldc(nullptr),
-      ldd(nullptr),
-      host_problem_sizes(nullptr)
-    {
-
-    }
+    Arguments() = default;
 
     /// Ctor
     CUTLASS_HOST_DEVICE
     Arguments(
       GemmCoord *problem_sizes,
       int problem_count,
       int threadblock_count,
@@ -222,49 +205,37 @@
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
   struct Params {
 
-    typename ProblemVisitor::Params problem_visitor;
-    int threadblock_count;
+    typename ProblemVisitor::Params problem_visitor{};
+    int threadblock_count{0};
 
-    typename EpilogueOutputOp::Params output_op;
+    typename EpilogueOutputOp::Params output_op{};
 
-    ElementA ** ptr_A;
-    ElementB ** ptr_B;
-    ElementC ** ptr_C;
-    ElementC ** ptr_D;
-
-    void ** ptr_norm;
-    void ** ptr_sum;
-
-    typename LayoutA::Stride::LongIndex *lda;
-    typename LayoutB::Stride::LongIndex *ldb;
-    typename LayoutC::Stride::LongIndex *ldc;
-    typename LayoutC::Stride::LongIndex *ldd;
+    ElementA ** ptr_A{nullptr};
+    ElementB ** ptr_B{nullptr};
+    ElementC ** ptr_C{nullptr};
+    ElementC ** ptr_D{nullptr};
+
+    void ** ptr_norm{nullptr};
+    void ** ptr_sum{nullptr};
+
+    typename LayoutA::Stride::LongIndex *lda{nullptr};
+    typename LayoutB::Stride::LongIndex *ldb{nullptr};
+    typename LayoutC::Stride::LongIndex *ldc{nullptr};
+    typename LayoutC::Stride::LongIndex *ldd{nullptr};
 
     //
     // Methods
     //
 
-    CUTLASS_HOST_DEVICE
-    Params():
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      ptr_norm(nullptr),
-      ptr_sum(nullptr),
-      lda(nullptr),
-      ldb(nullptr),
-      ldc(nullptr),
-      ldd(nullptr)
-    { }
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(Arguments const &args,
           void *workspace = nullptr,
           int tile_count = 0):
       problem_visitor(args.problem_sizes, args.problem_count, workspace, tile_count),
       threadblock_count(args.threadblock_count),
```

## cutlass_library/source/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h

```diff
@@ -107,67 +107,60 @@
   /// Argument structure
   struct Arguments : UniversalArgumentsBase
   {
     //
     // Data members
     //
 
-    typename EpilogueOutputOp::Params epilogue;
+    typename EpilogueOutputOp::Params epilogue{};
 
-    void const * ptr_A;
-    void const * ptr_B;
-    void const * ptr_var;
-    void const * ptr_mean;
-    void const * ptr_gamma;
-    void const * ptr_beta;
-    void const * ptr_C;
-    void * ptr_D;
-
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_var;
-    int64_t batch_stride_mean;
-    int64_t batch_stride_gamma;
-    int64_t batch_stride_beta;
-    int64_t batch_stride_C;
-
-    typename LayoutA::Stride stride_a;
-    typename LayoutB::Stride stride_b;
-    typename LayoutScaleBias::Stride stride_var;
-    typename LayoutScaleBias::Stride stride_mean;
-    typename LayoutScaleBias::Stride stride_gamma;
-    typename LayoutScaleBias::Stride stride_beta;
-    typename LayoutC::Stride stride_c;
-    typename LayoutC::Stride stride_d;
-
-    typename LayoutA::Stride::LongIndex lda;
-    typename LayoutB::Stride::LongIndex ldb;
-    typename LayoutScaleBias::Stride::LongIndex ld_var;
-    typename LayoutScaleBias::Stride::LongIndex ld_mean;
-    typename LayoutScaleBias::Stride::LongIndex ld_gamma;
-    typename LayoutScaleBias::Stride::LongIndex ld_beta;
-    typename LayoutC::Stride::LongIndex ldc;
-    typename LayoutC::Stride::LongIndex ldd;
-
-    int const * ptr_gather_A_indices;
-    int const * ptr_gather_B_indices;
-    int const * ptr_scatter_D_indices;
+    void const * ptr_A{nullptr};
+    void const * ptr_B{nullptr};
+    void const * ptr_var{nullptr};
+    void const * ptr_mean{nullptr};
+    void const * ptr_gamma{nullptr};
+    void const * ptr_beta{nullptr};
+    void const * ptr_C{nullptr};
+    void * ptr_D{nullptr};
+
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_B{0};
+    int64_t batch_stride_var{0};
+    int64_t batch_stride_mean{0};
+    int64_t batch_stride_gamma{0};
+    int64_t batch_stride_beta{0};
+    int64_t batch_stride_C{0};
+
+    typename LayoutA::Stride stride_a{};
+    typename LayoutB::Stride stride_b{};
+    typename LayoutScaleBias::Stride stride_var{};
+    typename LayoutScaleBias::Stride stride_mean{};
+    typename LayoutScaleBias::Stride stride_gamma{};
+    typename LayoutScaleBias::Stride stride_beta{};
+    typename LayoutC::Stride stride_c{};
+    typename LayoutC::Stride stride_d{};
+
+    typename LayoutA::Stride::LongIndex lda{};
+    typename LayoutB::Stride::LongIndex ldb{};
+    typename LayoutScaleBias::Stride::LongIndex ld_var{};
+    typename LayoutScaleBias::Stride::LongIndex ld_mean{};
+    typename LayoutScaleBias::Stride::LongIndex ld_gamma{};
+    typename LayoutScaleBias::Stride::LongIndex ld_beta{};
+    typename LayoutC::Stride::LongIndex ldc{};
+    typename LayoutC::Stride::LongIndex ldd{};
+
+    int const * ptr_gather_A_indices{nullptr};
+    int const * ptr_gather_B_indices{nullptr};
+    int const * ptr_scatter_D_indices{nullptr};
 
     //
     // Methods
     //
     
-    Arguments(): 
-      ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr),
-      ptr_var(nullptr), ptr_mean(nullptr),
-      ptr_gamma(nullptr), ptr_beta(nullptr),
-      ptr_gather_A_indices(nullptr),
-      ptr_gather_B_indices(nullptr),
-      ptr_scatter_D_indices(nullptr)
-    {}
+    Arguments() = default;
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
```

## cutlass_library/source/include/cutlass/gemm/kernel/gemm_params.h

```diff
@@ -66,54 +66,44 @@
   using MmaIteratorParams = typename cutlass::transform::threadblock::PredicatedTileAccessIteratorParams;  
   using EpilogueIteratorParams = typename cutlass::epilogue::threadblock::PredicatedTileIteratorParams;
 
   //
   // Data members
   //
 
-  cutlass::gemm::GemmCoord problem_size;
-  cutlass::gemm::GemmCoord grid_tiled_shape;
-  int swizzle_log_tile;
-
-  // Data members for Mma::Iterator::Params
-  MmaIteratorParams params_itr_a;
-  MmaIteratorParams params_itr_b;  
-
-  // Data member for Epilogue::OutputTileIterator::Params 
-  EpilogueIteratorParams params_itr_c;
-  EpilogueIteratorParams params_itr_d;
-
-
-  GemmUniversalMode mode;
-  int batch_count;
-  int gemm_k_size;
-
-  void * ptr_A;
-  void * ptr_B;
-  void * ptr_C;
-  void * ptr_D;
-
-  LongIndex lda; 
-  LongIndex ldb; 
-  LongIndex ldc; 
-  LongIndex ldd;
-
-  LongIndex batch_stride_A;
-  LongIndex batch_stride_B;
-  LongIndex batch_stride_C;
-  LongIndex batch_stride_D;
+  cutlass::gemm::GemmCoord problem_size{};
+  cutlass::gemm::GemmCoord grid_tiled_shape{};
+  int swizzle_log_tile{};
+
+  GemmUniversalMode mode{GemmUniversalMode::kGemm};
+  int batch_count{1};
+  int gemm_k_size{0};
+
+  void * ptr_A{nullptr};
+  void * ptr_B{nullptr};
+  void * ptr_C{nullptr};
+  void * ptr_D{nullptr};
+
+  LongIndex lda{0};
+  LongIndex ldb{0};
+  LongIndex ldc{0};
+  LongIndex ldd{0};
+
+  LongIndex batch_stride_A{0};
+  LongIndex batch_stride_B{0};
+  LongIndex batch_stride_C{0};
+  LongIndex batch_stride_D{0};
 
-  int *semaphore;
+  int *semaphore{nullptr};
 
   //
   // Methods
   //
 
-  CUTLASS_HOST_DEVICE
-  GemmParams()  {}
+  GemmParams() = default;
 
   CUTLASS_HOST_DEVICE
   GemmParams(
     cutlass::gemm::GemmCoord problem_size_,
     cutlass::gemm::GemmCoord grid_tiled_shape_,
     int swizzle_log_tile_,
     GemmUniversalMode mode_,
```

## cutlass_library/source/include/cutlass/gemm/kernel/gemm_planar_complex.h

```diff
@@ -108,59 +108,47 @@
   /// Argument structure
   struct Arguments : UniversalArgumentsBase
   {
     //
     // Data members
     //
 
-    typename EpilogueOutputOp::Params epilogue;
+    typename EpilogueOutputOp::Params epilogue{};
 
-    void const * ptr_A_real;
-    void const * ptr_A_imag;
-
-    void const * ptr_B_real;
-    void const * ptr_B_imag;
-
-    void const * ptr_C_real;
-    void const * ptr_C_imag;
-
-    void * ptr_D_real;
-    void * ptr_D_imag;
-
-    typename LayoutA::Stride::Index lda_real;
-    typename LayoutA::Stride::Index lda_imag;
-    typename LayoutB::Stride::Index ldb_real;
-    typename LayoutB::Stride::Index ldb_imag;
-    typename LayoutC::Stride::Index ldc_real;
-    typename LayoutC::Stride::Index ldc_imag;
-    typename LayoutC::Stride::Index ldd_real;
-    typename LayoutC::Stride::Index ldd_imag;
+    void const * ptr_A_real{nullptr};
+    void const * ptr_A_imag{nullptr};
+    void const * ptr_B_real{nullptr};
+    void const * ptr_B_imag{nullptr};
+    void const * ptr_C_real{nullptr};
+    void const * ptr_C_imag{nullptr};
+    void * ptr_D_real{nullptr};
+    void * ptr_D_imag{nullptr};
+
+    typename LayoutA::Stride::Index lda_real{};
+    typename LayoutA::Stride::Index lda_imag{};
+    typename LayoutB::Stride::Index ldb_real{};
+    typename LayoutB::Stride::Index ldb_imag{};
+    typename LayoutC::Stride::Index ldc_real{};
+    typename LayoutC::Stride::Index ldc_imag{};
+    typename LayoutC::Stride::Index ldd_real{};
+    typename LayoutC::Stride::Index ldd_imag{};
     
-    int64_t batch_stride_A;
-    int64_t batch_stride_A_imag;
-    int64_t batch_stride_B;
-    int64_t batch_stride_B_imag;
-    int64_t batch_stride_C;
-    int64_t batch_stride_C_imag;
-    int64_t batch_stride_D_imag;
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_A_imag{0};
+    int64_t batch_stride_B{0};
+    int64_t batch_stride_B_imag{0};
+    int64_t batch_stride_C{0};
+    int64_t batch_stride_C_imag{0};
+    int64_t batch_stride_D_imag{0};
 
     //
     // Methods
     //
-    
-    Arguments() :
-      ptr_A_real(nullptr), 
-      ptr_A_imag(nullptr), 
-      ptr_B_real(nullptr), 
-      ptr_B_imag(nullptr), 
-      ptr_C_real(nullptr), 
-      ptr_C_imag(nullptr), 
-      ptr_D_real(nullptr),
-      ptr_D_imag(nullptr)
-    {}
+
+    Arguments() = default;
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -256,42 +244,42 @@
       LayoutA,
       LayoutB>;
 
     //
     // Data members
     //
 
-    typename Mma::IteratorA::Params params_A_real;
-    typename Mma::IteratorA::Params params_A_imag;
-    typename Mma::IteratorB::Params params_B_real;
-    typename Mma::IteratorB::Params params_B_imag;
-    typename Epilogue::OutputTileIterator::Params params_C_real;
-    typename Epilogue::OutputTileIterator::Params params_C_imag;
-    typename Epilogue::OutputTileIterator::Params params_D_real;
-    typename Epilogue::OutputTileIterator::Params params_D_imag;
+    typename Mma::IteratorA::Params params_A_real{};
+    typename Mma::IteratorA::Params params_A_imag{};
+    typename Mma::IteratorB::Params params_B_real{};
+    typename Mma::IteratorB::Params params_B_imag{};
+    typename Epilogue::OutputTileIterator::Params params_C_real{};
+    typename Epilogue::OutputTileIterator::Params params_C_imag{};
+    typename Epilogue::OutputTileIterator::Params params_D_real{};
+    typename Epilogue::OutputTileIterator::Params params_D_imag{};
     
-    typename EpilogueOutputOp::Params output_op;
+    typename EpilogueOutputOp::Params output_op{};
 
-    void * ptr_A_real;
-    void * ptr_A_imag;
-    void * ptr_B_real;
-    void * ptr_B_imag;
-    void * ptr_C_real;
-    void * ptr_C_imag;
-    void * ptr_D_real;
-    void * ptr_D_imag;
-
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C;
-
-    int64_t batch_stride_A_imag;
-    int64_t batch_stride_B_imag;
-    int64_t batch_stride_C_imag;
-    int64_t batch_stride_D_imag;
+    void * ptr_A_real{nullptr};
+    void * ptr_A_imag{nullptr};
+    void * ptr_B_real{nullptr};
+    void * ptr_B_imag{nullptr};
+    void * ptr_C_real{nullptr};
+    void * ptr_C_imag{nullptr};
+    void * ptr_D_real{nullptr};
+    void * ptr_D_imag{nullptr};
+
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_B{0};
+    int64_t batch_stride_C{0};
+
+    int64_t batch_stride_A_imag{0};
+    int64_t batch_stride_B_imag{0};
+    int64_t batch_stride_C_imag{0};
+    int64_t batch_stride_D_imag{0};
 
     //
     // Host dispatch API
     //
 
     /// Default constructor
     Params() = default;
```

## cutlass_library/source/include/cutlass/gemm/kernel/gemm_planar_complex_array.h

```diff
@@ -108,58 +108,46 @@
   /// Argument structure
   struct Arguments : UniversalArgumentsBase
   {
     //
     // Data members
     //
 
-    typename EpilogueOutputOp::Params epilogue;
+    typename EpilogueOutputOp::Params epilogue{};
 
-    int const *ptr_M;
-    int const *ptr_N;
-    int const *ptr_K;
-
-    void const * const * ptr_A_real;
-    void const * const * ptr_A_imag;
-
-    void const * const * ptr_B_real;
-    void const * const * ptr_B_imag;
-
-    void const * const * ptr_C_real;
-    void const * const * ptr_C_imag;
-
-    void * const * ptr_D_real;
-    void * const * ptr_D_imag;
-
-    typename LayoutA::Stride::Index lda_real;
-    typename LayoutA::Stride::Index lda_imag;
-    typename LayoutB::Stride::Index ldb_real;
-    typename LayoutB::Stride::Index ldb_imag;
-    typename LayoutC::Stride::Index ldc_real;
-    typename LayoutC::Stride::Index ldc_imag;
-    typename LayoutC::Stride::Index ldd_real;
-    typename LayoutC::Stride::Index ldd_imag;
+    int const *ptr_M{nullptr};
+    int const *ptr_N{nullptr};
+    int const *ptr_K{nullptr};
+
+    void const * const * ptr_A_real{nullptr};
+    void const * const * ptr_A_imag{nullptr};
+
+    void const * const * ptr_B_real{nullptr};
+    void const * const * ptr_B_imag{nullptr};
+
+    void const * const * ptr_C_real{nullptr};
+    void const * const * ptr_C_imag{nullptr};
+
+    void * const * ptr_D_real{nullptr};
+    void * const * ptr_D_imag{nullptr};
+
+    typename LayoutA::Stride::Index lda_real{};
+    typename LayoutA::Stride::Index lda_imag{};
+    typename LayoutB::Stride::Index ldb_real{};
+    typename LayoutB::Stride::Index ldb_imag{};
+    typename LayoutC::Stride::Index ldc_real{};
+    typename LayoutC::Stride::Index ldc_imag{};
+    typename LayoutC::Stride::Index ldd_real{};
+    typename LayoutC::Stride::Index ldd_imag{};
 
     //
     // Methods
     //
-    
-    Arguments(): 
-      ptr_M(nullptr),
-      ptr_N(nullptr),
-      ptr_K(nullptr),
-      ptr_A_real(nullptr), 
-      ptr_A_imag(nullptr), 
-      ptr_B_real(nullptr), 
-      ptr_B_imag(nullptr), 
-      ptr_C_real(nullptr), 
-      ptr_C_imag(nullptr), 
-      ptr_D_real(nullptr),
-      ptr_D_imag(nullptr)
-    {}
+
+    Arguments() = default;
 
     /// constructs an arguments structure
     Arguments(
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
       int const *ptr_M,
@@ -244,37 +232,37 @@
       LayoutA,
       LayoutB>;
 
     //
     // Data members
     //
 
-    typename Mma::IteratorA::Params params_A_real;
-    typename Mma::IteratorA::Params params_A_imag;
-    typename Mma::IteratorB::Params params_B_real;
-    typename Mma::IteratorB::Params params_B_imag;
-    typename Epilogue::OutputTileIterator::Params params_C_real;
-    typename Epilogue::OutputTileIterator::Params params_C_imag;
-    typename Epilogue::OutputTileIterator::Params params_D_real;
-    typename Epilogue::OutputTileIterator::Params params_D_imag;
-
-    typename EpilogueOutputOp::Params output_op;
-
-    int const *ptr_M;
-    int const *ptr_N;
-    int const *ptr_K;
-
-    void const * const * ptr_A_real;
-    void const * const * ptr_A_imag;
-    void const * const * ptr_B_real;
-    void const * const * ptr_B_imag;
-    void const * const * ptr_C_real;
-    void const * const * ptr_C_imag;
-    void * const * ptr_D_real;
-    void * const * ptr_D_imag;
+    typename Mma::IteratorA::Params params_A_real{};
+    typename Mma::IteratorA::Params params_A_imag{};
+    typename Mma::IteratorB::Params params_B_real{};
+    typename Mma::IteratorB::Params params_B_imag{};
+    typename Epilogue::OutputTileIterator::Params params_C_real{};
+    typename Epilogue::OutputTileIterator::Params params_C_imag{};
+    typename Epilogue::OutputTileIterator::Params params_D_real{};
+    typename Epilogue::OutputTileIterator::Params params_D_imag{};
+
+    typename EpilogueOutputOp::Params output_op{};
+
+    int const *ptr_M{nullptr};
+    int const *ptr_N{nullptr};
+    int const *ptr_K{nullptr};
+
+    void const * const * ptr_A_real{nullptr};
+    void const * const * ptr_A_imag{nullptr};
+    void const * const * ptr_B_real{nullptr};
+    void const * const * ptr_B_imag{nullptr};
+    void const * const * ptr_C_real{nullptr};
+    void const * const * ptr_C_imag{nullptr};
+    void * const * ptr_D_real{nullptr};
+    void * const * ptr_D_imag{nullptr};
 
     //
     // Host dispatch API
     //
 
     /// Default constructor
     Params() = default;
```

## cutlass_library/source/include/cutlass/gemm/kernel/gemm_streamk_with_fused_epilogue.h

```diff
@@ -25,15 +25,15 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Stream-K Gemm kernel compatible with fused epilogues 
+    \brief Stream-K Gemm kernel compatible with fused epilogues
     that broadcast a bias vector over the MMA output.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
@@ -61,15 +61,15 @@
   typename ThreadblockSwizzle_,   ///! Threadblock swizzling function
   bool IsSingleSource = Epilogue_::kIsSingleSource
 >
 struct GemmStreamkWithFusedEpilogue;
 
 // GemmStreamkWithFusedEpilogue with two sources
 template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate
   typename Epilogue_,             ///! Epilogue
   typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
 struct GemmStreamkWithFusedEpilogue<Mma_, Epilogue_, ThreadblockSwizzle_, false> {
   using Mma = Mma_;
   using Epilogue = Epilogue_;
   using EpilogueOutputOp = typename Epilogue::OutputOp;
@@ -122,63 +122,54 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;        // Either (mode == GemmUniversalMode::kBatched) the batch count, or (mode == GemmUniversalMode::kGemm) the tile-splitting factor
-
-    typename EpilogueOutputOp::Params epilogue;
-
-    void const * ptr_A;
-    void const * ptr_B;
-    void const * ptr_C1;
-    void const * ptr_C2;
-    void * ptr_D;
+    GemmUniversalMode mode{GemmUniversalMode::kGemm};
+    GemmCoord problem_size{};
+    int batch_count{1};        // Either (mode == GemmUniversalMode::kBatched) the batch count, or (mode == GemmUniversalMode::kGemm) the tile-splitting factor
+
+    typename EpilogueOutputOp::Params epilogue{};
+
+    void const * ptr_A{nullptr};
+    void const * ptr_B{nullptr};
+    void const * ptr_C1{nullptr};
+    void const * ptr_C2{nullptr};
+    void * ptr_D{nullptr};
 
     void * ptr_Vector;
     void * ptr_Tensor;
 
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C1;
-    int64_t batch_stride_C2;
-    int64_t batch_stride_D;
-    int64_t batch_stride_Vector;
-    int64_t batch_stride_Tensor;
-
-    typename LayoutA::Stride::Index lda;
-    typename LayoutB::Stride::Index ldb;
-    typename LayoutC::Stride::Index ldc1;
-    typename LayoutC::Stride::Index ldc2;
-    typename LayoutC::Stride::Index ldd;
-    typename LayoutC::Stride::Index ldr;
-    typename LayoutC::Stride::Index ldt;
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_B{0};
+    int64_t batch_stride_C1{0};
+    int64_t batch_stride_C2{0};
+    int64_t batch_stride_D{0};
+    int64_t batch_stride_Vector{0};
+    int64_t batch_stride_Tensor{0};
+
+    typename LayoutA::Stride::Index lda{};
+    typename LayoutB::Stride::Index ldb{};
+    typename LayoutC::Stride::Index ldc1{};
+    typename LayoutC::Stride::Index ldc2{};
+    typename LayoutC::Stride::Index ldd{};
+    typename LayoutC::Stride::Index ldr{};
+    typename LayoutC::Stride::Index ldt{};
 
-    int avail_sms;          /// The number of SMs that StreamK dispatch heuristics will attempt to load-balance across (-1 defaults to device width, 1 implies classic data-parallel scheduling)
+    int avail_sms{-1};          /// The number of SMs that StreamK dispatch heuristics will attempt to load-balance across (-1 defaults to device width, 1 implies classic data-parallel scheduling)
 
 
     //
     // Methods
     //
-    
+
     /// Default Constructor
-    Arguments():
-      mode(GemmUniversalMode::kGemm),
-      batch_count(1),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C1(nullptr),
-      ptr_C2(nullptr),
-      ptr_D(nullptr),
-      avail_sms(-1)
-    {}
+    Arguments() = default;
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_split,                              /// Either (mode == GemmUniversalMode::kBatched) the batch count, or (mode == GemmUniversalMode::kGemm) the tile-splitting factor (1 defaults to StreamK, >1 emulates Split-K)
       typename EpilogueOutputOp::Params epilogue,
@@ -204,22 +195,22 @@
       typename LayoutC::Stride::Index ldr,
       typename LayoutC::Stride::Index ldt,
       int avail_sms = -1)                           /// The number of SMs that StreamK dispatch heuristics will attempt to load-balance across (-1 defaults to device width, 1 implies classic data-parallel scheduling)
     :
       mode(mode),
       problem_size(problem_size),
       batch_count(batch_split),
-      epilogue(epilogue), 
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C1(ptr_C1), ptr_C2(ptr_C2), ptr_D(ptr_D), 
-      ptr_Vector(ptr_Vector), 
+      epilogue(epilogue),
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C1(ptr_C1), ptr_C2(ptr_C2), ptr_D(ptr_D),
+      ptr_Vector(ptr_Vector),
       ptr_Tensor(ptr_Tensor),
-      batch_stride_A(batch_stride_A), 
-      batch_stride_B(batch_stride_B), 
-      batch_stride_C1(batch_stride_C1), 
-      batch_stride_C2(batch_stride_C2), 
+      batch_stride_A(batch_stride_A),
+      batch_stride_B(batch_stride_B),
+      batch_stride_C1(batch_stride_C1),
+      batch_stride_C2(batch_stride_C2),
       batch_stride_Vector(batch_stride_Vector),
       batch_stride_Tensor(batch_stride_Tensor),
       lda(lda), ldb(ldb), ldc1(ldc1), ldc2(ldc2), ldd(ldd), ldr(ldr), ldt(ldt), avail_sms(avail_sms)
     {
       CUTLASS_TRACE_HOST("GemmStreamkWithFusedEpilogue::Arguments::Arguments() - problem_size: " << problem_size);
       CUTLASS_TRACE_HOST("  ptr_Vector: " << (void *)this->ptr_Vector);
       CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
@@ -247,50 +238,50 @@
   {
   public:
 
     //
     // Data members
     //
 
-    void * ptr_A;
-    void * ptr_B;
+    void * ptr_A{nullptr};
+    void * ptr_B{nullptr};
 
-    typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorB::Params params_B;
+    typename Mma::IteratorA::Params params_A{};
+    typename Mma::IteratorB::Params params_B{};
 
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_B{0};
 
-    GemmUniversalMode mode;
+    GemmUniversalMode mode{GemmUniversalMode::kGemm};
 
-    ThreadblockSwizzle block_mapping;
+    ThreadblockSwizzle block_mapping{};
 
-    void *barrier_workspace;
-    void *partials_workspace;
+    void *barrier_workspace{nullptr};
+    void *partials_workspace{nullptr};
 
-    typename EpilogueOutputOp::Params output_op;
+    typename EpilogueOutputOp::Params output_op{};
 
-    void * ptr_C1;
-    void * ptr_C2;
-    void * ptr_D;
-    void * ptr_Tensor;
-    void * ptr_Vector;
+    void * ptr_C1{nullptr};
+    void * ptr_C2{nullptr};
+    void * ptr_D{nullptr};
+    void * ptr_Tensor{nullptr};
+    void * ptr_Vector{nullptr};
+
+    typename Epilogue::OutputTileIterator::Params params_C1{};
+    typename Epilogue::OutputTileIterator::Params params_C2{};
+    typename Epilogue::OutputTileIterator::Params params_D{};
+    typename Epilogue::TensorTileIterator::Params params_Tensor{};
 
-    typename Epilogue::OutputTileIterator::Params params_C1;
-    typename Epilogue::OutputTileIterator::Params params_C2;
-    typename Epilogue::OutputTileIterator::Params params_D;
-    typename Epilogue::TensorTileIterator::Params params_Tensor;
-
-    int64_t batch_stride_C1;
-    int64_t batch_stride_C2;
-    int64_t batch_stride_D;
-    int64_t batch_stride_Vector;
-    int64_t batch_stride_Tensor;
+    int64_t batch_stride_C1{0};
+    int64_t batch_stride_C2{0};
+    int64_t batch_stride_D{0};
+    int64_t batch_stride_Vector{0};
+    int64_t batch_stride_Tensor{0};
 
-    typename LayoutC::Stride::Index ldr;
+    typename LayoutC::Stride::Index ldr{};
 
   protected:
 
     //
     // Host-only dispatch-utilities
     //
 
@@ -357,25 +348,25 @@
       batch_stride_C2(args.batch_stride_C2),
       batch_stride_D(args.batch_stride_D),
       batch_stride_Vector(args.batch_stride_Vector),
       batch_stride_Tensor(args.batch_stride_Tensor),
       barrier_workspace(nullptr),
       partials_workspace(nullptr)
     {
-      CUTLASS_TRACE_HOST("GemmStreamkWithFusedEpilogue::Params::Params() - problem_size: " << problem_size);
+      CUTLASS_TRACE_HOST("GemmStreamkWithFusedEpilogue::Params::Params()");
       CUTLASS_TRACE_HOST("  ptr_Vector: " << (void *)this->ptr_Vector);
       CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
       CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
       CUTLASS_TRACE_HOST("  ldt: " << args.ldt);
-      CUTLASS_TRACE_HOST("  avail_sms: " << avail_sms);
 
       // Number of SMs to make available for StreamK decomposition
       int avail_sms = (args.avail_sms == -1) ?
                         device_sms :
                         fast_min(args.avail_sms, device_sms);
+      CUTLASS_TRACE_HOST("  avail_sms: " << avail_sms);
 
       // Initialize the block mapping structure
       block_mapping = ThreadblockSwizzle(
         args.mode,
         args.problem_size,
         {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
         args.batch_count,
@@ -400,15 +391,14 @@
     /// the memory allocated to workspace is at least as large as get_workspace_size().
     Status init_workspace(
       void *workspace,
       cudaStream_t stream = nullptr)
     {
       uint8_t *ptr = static_cast<uint8_t*>(workspace);
 
-
       // Establish partials workspace
       partials_workspace = nullptr;
       size_t partials_workspace_bytes = get_partials_workspace_size();
       if (partials_workspace_bytes > 0)
       {
         if (!workspace) {
           return Status::kErrorWorkspaceNull;
@@ -836,15 +826,15 @@
   {
     ElementC *ptr_C1 = static_cast<ElementC *>(params.ptr_C1);
     ElementC *ptr_C2 = static_cast<ElementC *>(params.ptr_C2);
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
     typename Epilogue::ElementTensor *ptr_Tensor = static_cast<typename Epilogue::ElementTensor *>(params.ptr_Tensor);
 
     // Define the reduction output pointer and move to the appropriate place
-    typename Epilogue::ElementVector *ptr_Vector = 
+    typename Epilogue::ElementVector *ptr_Vector =
       static_cast<typename Epilogue::ElementVector *>(params.ptr_Vector);
 
     // Update pointers for batched/array mode(s)
     if (params.mode == GemmUniversalMode::kBatched) {
       ptr_C1 += tile_work.tiled_coord.k() * params.batch_stride_C1;
       if (ptr_C2) {
         ptr_C2 += tile_work.tiled_coord.k() * params.batch_stride_C2;
@@ -965,15 +955,15 @@
 
     ElementC *ptr_C1 = static_cast<ElementC *>(params.ptr_C1);
     ElementC *ptr_C2 = static_cast<ElementC *>(params.ptr_C2);
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
     typename Epilogue::ElementTensor *ptr_Tensor = static_cast<typename Epilogue::ElementTensor *>(params.ptr_Tensor);
 
     // Define the reduction output pointer and move to the appropriate place
-    typename Epilogue::ElementVector *ptr_Vector = 
+    typename Epilogue::ElementVector *ptr_Vector =
       static_cast<typename Epilogue::ElementVector *>(params.ptr_Vector);
 
     // Tile iterator loading from residual1.
     typename Epilogue::OutputTileIterator iterator_C1(
         params.params_C1,
         ptr_C1,
         params.block_mapping.problem_size.mn(),
@@ -1252,15 +1242,15 @@
 
   }
 };
 
 
 // GemmStreamkWithFusedEpilogue with one source
 template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate
   typename Epilogue_,             ///! Epilogue
   typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
 struct GemmStreamkWithFusedEpilogue<Mma_, Epilogue_, ThreadblockSwizzle_, true> {
   using Mma = Mma_;
   using Epilogue = Epilogue_;
   using EpilogueOutputOp = typename Epilogue::OutputOp;
@@ -1314,59 +1304,51 @@
   struct Arguments
   {
 
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;        // Either (mode == GemmUniversalMode::kBatched) the batch count, or (mode == GemmUniversalMode::kGemm) the tile-splitting factor
-
-    typename EpilogueOutputOp::Params epilogue;
-
-    void const * ptr_A;
-    void const * ptr_B;
-    void const * ptr_C;
-    void * ptr_D;
-
-    void * ptr_Vector;
-    void * ptr_Tensor;
-
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C;
-    int64_t batch_stride_D;
-    int64_t batch_stride_Vector;
-    int64_t batch_stride_Tensor;
-
-    typename LayoutA::Stride::Index lda;
-    typename LayoutB::Stride::Index ldb;
-    typename LayoutC::Stride::Index ldc;
-    typename LayoutC::Stride::Index ldd;
-    typename LayoutC::Stride::Index ldr;
-    typename LayoutC::Stride::Index ldt;
+    GemmUniversalMode mode{GemmUniversalMode::kGemm};
+    GemmCoord problem_size{};
+    int batch_count{1};        // Either (mode == GemmUniversalMode::kBatched) the batch count, or (mode == GemmUniversalMode::kGemm) the tile-splitting factor
+
+    typename EpilogueOutputOp::Params epilogue{};
+
+    void const * ptr_A{nullptr};
+    void const * ptr_B{nullptr};
+    void const * ptr_C{nullptr};
+    void * ptr_D{nullptr};
+
+    void * ptr_Vector{nullptr};
+    void * ptr_Tensor{nullptr};
+
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_B{0};
+    int64_t batch_stride_C{0};
+    int64_t batch_stride_D{0};
+    int64_t batch_stride_Vector{0};
+    int64_t batch_stride_Tensor{0};
+
+    typename LayoutA::Stride::Index lda{};
+    typename LayoutB::Stride::Index ldb{};
+    typename LayoutC::Stride::Index ldc{};
+    typename LayoutC::Stride::Index ldd{};
+    typename LayoutC::Stride::Index ldr{};
+    typename LayoutC::Stride::Index ldt{};
 
-    int avail_sms;          /// The number of SMs that StreamK dispatch heuristics will attempt to load-balance across (-1 defaults to device width, 1 implies classic data-parallel scheduling)
+    int avail_sms{-1};          /// The number of SMs that StreamK dispatch heuristics will attempt to load-balance across (-1 defaults to device width, 1 implies classic data-parallel scheduling)
 
 
     //
     // Methods
     //
-    
+
     /// Default Constructor
-    Arguments(): 
-      mode(GemmUniversalMode::kGemm),
-      batch_count(1),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      avail_sms(-1)
-    {}
+    Arguments() = default;
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_split,                              /// Either (mode == GemmUniversalMode::kBatched) the batch count, or (mode == GemmUniversalMode::kGemm) the tile-splitting factor (1 defaults to StreamK, >1 emulates Split-K)
       typename EpilogueOutputOp::Params epilogue,
@@ -1389,21 +1371,21 @@
       typename LayoutC::Stride::Index ldr,
       typename LayoutC::Stride::Index ldt,
       int avail_sms = -1)                           /// The number of SMs that StreamK dispatch heuristics will attempt to load-balance across (-1 defaults to device width, 1 implies classic data-parallel scheduling)
     :
       mode(mode),
       problem_size(problem_size),
       batch_count(batch_split),
-      epilogue(epilogue), 
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
-      ptr_Vector(ptr_Vector), 
+      epilogue(epilogue),
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
+      ptr_Vector(ptr_Vector),
       ptr_Tensor(ptr_Tensor),
-      batch_stride_A(batch_stride_A), 
-      batch_stride_B(batch_stride_B), 
-      batch_stride_C(batch_stride_C), 
+      batch_stride_A(batch_stride_A),
+      batch_stride_B(batch_stride_B),
+      batch_stride_C(batch_stride_C),
       batch_stride_Vector(batch_stride_Vector),
       batch_stride_Tensor(batch_stride_Tensor),
       lda(lda), ldb(ldb), ldc(ldc), ldd(ldd), ldr(ldr), ldt(ldt), avail_sms(avail_sms)
     {
       CUTLASS_TRACE_HOST("GemmStreamkWithFusedEpilogue::Arguments::Arguments() - problem_size: " << problem_size);
       CUTLASS_TRACE_HOST("  ptr_Vector: " << (void *)this->ptr_Vector);
       CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
@@ -1411,15 +1393,15 @@
       CUTLASS_TRACE_HOST("  ldt: " << this->ldt);
       CUTLASS_TRACE_HOST("  avail_sms: " << this->avail_sms);
     }
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
       Arguments args(*this);
-      
+
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A, args.ptr_B);
       std::swap(args.lda, args.ldb);
       std::swap(args.batch_stride_A, args.batch_stride_B);
 
       return args;
     }
@@ -1432,48 +1414,47 @@
 
   public:
 
     //
     // Data members
     //
 
-    void * ptr_A;
-    void * ptr_B;
+    void * ptr_A{nullptr};
+    void * ptr_B{nullptr};
 
-    typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorB::Params params_B;
+    typename Mma::IteratorA::Params params_A{};
+    typename Mma::IteratorB::Params params_B{};
 
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_B{0};
 
-    GemmUniversalMode mode;
+    GemmUniversalMode mode{GemmUniversalMode::kGemm};
 
-    ThreadblockSwizzle block_mapping;
+    ThreadblockSwizzle block_mapping{};
 
-    void *barrier_workspace;
-    void *partials_workspace;
+    void *barrier_workspace{nullptr};
+    void *partials_workspace{nullptr};
 
-    typename EpilogueOutputOp::Params output_op;
+    typename EpilogueOutputOp::Params output_op{};
 
-    void * ptr_C;
-    void * ptr_D;
-    void * ptr_Tensor;
-    void * ptr_Vector;
-
-    typename Epilogue::OutputTileIterator::Params params_C;
-    typename Epilogue::OutputTileIterator::Params params_D;
-    typename Epilogue::TensorTileIterator::Params params_Tensor;
+    void * ptr_C{nullptr};
+    void * ptr_D{nullptr};
+    void * ptr_Tensor{nullptr};
+    void * ptr_Vector{nullptr};
 
-    int64_t batch_stride_C;
-    int64_t batch_stride_D;
-    int64_t batch_stride_Vector;
-    int64_t batch_stride_Tensor;
+    typename Epilogue::OutputTileIterator::Params params_C{};
+    typename Epilogue::OutputTileIterator::Params params_D{};
+    typename Epilogue::TensorTileIterator::Params params_Tensor{};
 
+    int64_t batch_stride_C{0};
+    int64_t batch_stride_D{0};
+    int64_t batch_stride_Vector{0};
+    int64_t batch_stride_Tensor{0};
 
-    typename LayoutC::Stride::Index ldr;
+    typename LayoutC::Stride::Index ldr{};
 
   protected:
 
     //
     // Host-only dispatch-utilities
     //
 
@@ -1536,25 +1517,25 @@
       batch_stride_C(args.batch_stride_C),
       batch_stride_D(args.batch_stride_D),
       batch_stride_Vector(args.batch_stride_Vector),
       batch_stride_Tensor(args.batch_stride_Tensor),
       barrier_workspace(nullptr),
       partials_workspace(nullptr)
     {
-      CUTLASS_TRACE_HOST("GemmStreamkWithFusedEpilogue::Params::Params() - problem_size: " << problem_size);
+      CUTLASS_TRACE_HOST("GemmStreamkWithFusedEpilogue::Params::Params()");
       CUTLASS_TRACE_HOST("  ptr_Vector: " << (void *)this->ptr_Vector);
       CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
       CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
       CUTLASS_TRACE_HOST("  ldt: " << args.ldt);
-      CUTLASS_TRACE_HOST("  avail_sms: " << avail_sms);
 
       // Number of SMs to make available for StreamK decomposition
       int avail_sms = (args.avail_sms == -1) ?
                         device_sms :
                         fast_min(args.avail_sms, device_sms);
+      CUTLASS_TRACE_HOST("  avail_sms: " << avail_sms);
 
       // Initialize the block mapping structure
       block_mapping = ThreadblockSwizzle(
         args.mode,
         args.problem_size,
         {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
         args.batch_count,
@@ -2014,15 +1995,15 @@
     AccumulatorTile &accumulator_tile)
   {
     ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C);
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
     typename Epilogue::ElementTensor *ptr_Tensor = static_cast<typename Epilogue::ElementTensor *>(params.ptr_Tensor);
 
     // Define the reduction output pointer and move to the appropriate place
-    typename Epilogue::ElementVector *ptr_Vector = 
+    typename Epilogue::ElementVector *ptr_Vector =
       static_cast<typename Epilogue::ElementVector *>(params.ptr_Vector);
 
     // Update pointers for batched/array mode(s)
     if (params.mode == GemmUniversalMode::kBatched) {
       ptr_C += tile_work.tiled_coord.k() * params.batch_stride_C;
       ptr_D += tile_work.tiled_coord.k() * params.batch_stride_D;
       if (ptr_Tensor) {
@@ -2127,15 +2108,15 @@
     );
 
     ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C);
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
     typename Epilogue::ElementTensor *ptr_Tensor = static_cast<typename Epilogue::ElementTensor *>(params.ptr_Tensor);
 
     // Define the reduction output pointer and move to the appropriate place
-    typename Epilogue::ElementVector *ptr_Vector = 
+    typename Epilogue::ElementVector *ptr_Vector =
       static_cast<typename Epilogue::ElementVector *>(params.ptr_Vector);
 
     // Tile iterator loading from source tensor.
     typename Epilogue::OutputTileIterator iterator_C(
         params.params_C,
         ptr_C,
         params.block_mapping.problem_size.mn(),
```

## cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal.h

```diff
@@ -26,15 +26,15 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief 
+    \brief
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 
 #include "cutlass/arch/arch.h"
@@ -173,16 +173,16 @@
       typename LayoutC::Stride stride_c,
       typename LayoutC::Stride stride_d,
       int const *ptr_gather_A_indices = nullptr,
       int const *ptr_gather_B_indices = nullptr,
       int const *ptr_scatter_D_indices = nullptr)
     :
       UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
-      epilogue(epilogue), 
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
+      epilogue(epilogue),
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
       batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
       stride_a(stride_a), stride_b(stride_b), stride_c(stride_c), stride_d(stride_d),
       ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
       ptr_scatter_D_indices(ptr_scatter_D_indices)
     {
       lda = 0;
       ldb = 0;
@@ -482,26 +482,26 @@
 
       return;
     }
 
     int offset_k = 0;
     int problem_size_k = params.problem_size.k();
 
-    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A); 
+    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A);
     ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
 
     //
     // Fetch pointers based on mode.
     //
-    if (params.mode == GemmUniversalMode::kGemm || 
+    if (params.mode == GemmUniversalMode::kGemm ||
       params.mode == GemmUniversalMode::kGemmSplitKParallel) {
 
       if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
 
-        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size; 
+        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size;
       }
 
       offset_k = threadblock_tile_offset.k() * params.gemm_k_size;
     }
     else if (params.mode == GemmUniversalMode::kBatched) {
       ptr_A += threadblock_tile_offset.k() * params.batch_stride_A;
       ptr_B += threadblock_tile_offset.k() * params.batch_stride_B;
@@ -562,18 +562,18 @@
     accumulators.clear();
 
     // Compute threadblock-scoped matrix multiply-add
     int gemm_k_iterations = (problem_size_k - offset_k + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
     // Compute threadblock-scoped matrix multiply-add
     mma(
-      gemm_k_iterations, 
-      accumulators, 
-      iterator_A, 
-      iterator_B, 
+      gemm_k_iterations,
+      accumulators,
+      iterator_A,
+      iterator_B,
       accumulators);
 
     //
     // Epilogue
     //
 
     EpilogueOutputOp output_op(params.output_op);
@@ -588,29 +588,29 @@
     MatrixCoord threadblock_offset(
       threadblock_tile_offset.m() * Mma::Shape::kM,
       threadblock_tile_offset.n() * Mma::Shape::kN
     );
 
     int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
 
-    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C); 
+    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C);
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
 
     //
     // Fetch pointers based on mode.
     //
-    
+
     // Construct the semaphore.
     Semaphore semaphore(params.semaphore + block_idx, thread_idx);
 
     if (params.mode == GemmUniversalMode::kGemm) {
 
       // If performing a reduction via split-K, fetch the initial synchronization
       if (params.grid_tiled_shape.k() > 1) {
-        
+
         // Fetch the synchronization lock initially but do not block.
         semaphore.fetch();
 
         // Indicate which position in a serial reduction the output operator is currently updating
         output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
       }
     }
@@ -643,38 +643,38 @@
       params.problem_size.mn(),
       thread_idx,
       threadblock_offset,
       params.ptr_scatter_D_indices
     );
 
     Epilogue epilogue(
-      shared_storage.epilogue, 
-      thread_idx, 
-      warp_idx, 
+      shared_storage.epilogue,
+      thread_idx,
+      warp_idx,
       lane_idx);
 
     // Wait on the semaphore - this latency may have been covered by iterator construction
     if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
-        
+
       // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
       if (threadblock_tile_offset.k()) {
         iterator_C = iterator_D;
       }
 
       semaphore.wait(threadblock_tile_offset.k());
     }
 
 
     // Execute the epilogue operator to update the destination tensor.
     epilogue(
-      output_op, 
-      iterator_D, 
-      accumulators, 
-      iterator_C); 
-    
+      output_op,
+      iterator_D,
+      accumulators,
+      iterator_C);
+
     //
     // Release the semaphore
     //
 
     if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
 
       int lock = 0;
@@ -683,15 +683,15 @@
         // The final threadblock resets the semaphore for subsequent grids.
         lock = 0;
       }
       else {
         // Otherwise, the semaphore is incremented
         lock = threadblock_tile_offset.k() + 1;
       }
-      
+
       semaphore.release(lock);
     }
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_streamk.h

```diff
@@ -118,57 +118,49 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;        // Either (mode == GemmUniversalMode::kBatched) the batch count, or (mode == GemmUniversalMode::kGemm) the tile-splitting factor
-
-    typename EpilogueOutputOp::Params epilogue;
-
-    void const * ptr_A;
-    void const * ptr_B;
-    void const * ptr_C;
-    void * ptr_D;
-
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C;
-    int64_t batch_stride_D;
-
-    typename LayoutA::Stride stride_a;
-    typename LayoutB::Stride stride_b;
-    typename LayoutC::Stride stride_c;
-    typename LayoutC::Stride stride_d;
-
-    typename LayoutA::Stride::LongIndex lda;
-    typename LayoutB::Stride::LongIndex ldb;
-    typename LayoutC::Stride::LongIndex ldc;
-    typename LayoutC::Stride::LongIndex ldd;
+    GemmUniversalMode mode = GemmUniversalMode::kGemm;
+    GemmCoord problem_size {};
+    int batch_count {1};        // Either (mode == GemmUniversalMode::kBatched) the batch count, or (mode == GemmUniversalMode::kGemm) the tile-splitting factor
+
+    typename EpilogueOutputOp::Params epilogue{};
+
+    void const * ptr_A = nullptr;
+    void const * ptr_B = nullptr;
+    void const * ptr_C = nullptr;
+    void * ptr_D = nullptr;
+
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_B{0};
+    int64_t batch_stride_C{0};
+    int64_t batch_stride_D{0};
+
+    typename LayoutA::Stride stride_a{0};
+    typename LayoutB::Stride stride_b{0};
+    typename LayoutC::Stride stride_c{0};
+    typename LayoutC::Stride stride_d{0};
+
+    typename LayoutA::Stride::LongIndex lda{0};
+    typename LayoutB::Stride::LongIndex ldb{0};
+    typename LayoutC::Stride::LongIndex ldc{0};
+    typename LayoutC::Stride::LongIndex ldd{0};
 
-    int avail_sms;          /// The number of SMs that StreamK dispatch heuristics will attempt to load-balance across (-1 defaults to device width, 1 implies classic data-parallel scheduling)
+    int avail_sms{-1};          /// The number of SMs that StreamK dispatch heuristics will attempt to load-balance across (-1 defaults to device width, 1 implies classic data-parallel scheduling)
 
 
     //
     // Methods
     //
 
     /// Default Constructor
-    Arguments():
-      mode(GemmUniversalMode::kGemm),
-      batch_count(1),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      avail_sms(-1)
-    {}
+    Arguments() = default;
 
     /// Constructor
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_split,                              /// Either (mode == GemmUniversalMode::kBatched) the batch count, or (mode == GemmUniversalMode::kGemm) the tile-splitting factor (1 defaults to StreamK, >1 emulates Split-K)
       typename EpilogueOutputOp::Params epilogue,
@@ -253,40 +245,40 @@
   {
   public:
 
     //
     // Data members
     //
 
-    void * ptr_A;
-    void * ptr_B;
+    void * ptr_A = nullptr;
+    void * ptr_B = nullptr;
 
-    typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorB::Params params_B;
+    typename Mma::IteratorA::Params params_A{};
+    typename Mma::IteratorB::Params params_B{};
 
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_B{0};
 
-    GemmUniversalMode mode;
+    GemmUniversalMode mode = GemmUniversalMode::kGemm;
 
-    ThreadblockSwizzle block_mapping;
+    ThreadblockSwizzle block_mapping{};
 
-    void *barrier_workspace;
-    void *partials_workspace;
+    void *barrier_workspace = nullptr;
+    void *partials_workspace = nullptr;
 
-    typename EpilogueOutputOp::Params output_op;
+    typename EpilogueOutputOp::Params output_op{};
 
-    void * ptr_D;
-    void * ptr_C;
+    void * ptr_D = nullptr;
+    void * ptr_C = nullptr;
 
-    typename Epilogue::OutputTileIterator::Params params_D;
-    typename Epilogue::OutputTileIterator::Params params_C;
+    typename Epilogue::OutputTileIterator::Params params_D{};
+    typename Epilogue::OutputTileIterator::Params params_C{};
 
-    int64_t batch_stride_D;
-    int64_t batch_stride_C;
+    int64_t batch_stride_D{0};
+    int64_t batch_stride_C{0};
 
 
   protected:
 
     //
     // Host-only dispatch-utilities
     //
@@ -322,15 +314,14 @@
     //
     // Host dispatch API
     //
 
     /// Default constructor
     Params() = default;
 
-
     /// Constructor
     Params(
       Arguments const &args,  /// GEMM application arguments
       int device_sms,         /// Number of SMs on the device
       int sm_occupancy)       /// Kernel SM occupancy (in thread blocks)
     :
       params_A(args.lda ? make_Coord_with_padding<LayoutA::kStrideRank>(args.lda) : args.stride_a),
```

## cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_with_visitor_streamk.h

```diff
@@ -125,43 +125,43 @@
   struct Params
   {
   public:
 
     //
     // Data members
     //
-    cute::Shape<int32_t,int32_t,int32_t> problem_shape;
+    cute::Shape<int32_t,int32_t,int32_t> problem_shape{};
 
-    void * ptr_A;
-    void * ptr_B;
+    void * ptr_A{nullptr};
+    void * ptr_B{nullptr};
 
-    typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorB::Params params_B;
+    typename Mma::IteratorA::Params params_A{};
+    typename Mma::IteratorB::Params params_B{};
 
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_B{0};
 
-    GemmUniversalMode mode;
+    GemmUniversalMode mode{GemmUniversalMode::kGemm};
 
-    ThreadblockSwizzle block_mapping;
+    ThreadblockSwizzle block_mapping{};
 
-    void *barrier_workspace;
-    void *partials_workspace;
+    void *barrier_workspace{nullptr};
+    void *partials_workspace{nullptr};
 
-    typename FusionCallbacks::Params output_op;
+    typename FusionCallbacks::Params output_op{};
 
 
-    void * ptr_D;
-    void * ptr_C;
+    void * ptr_D{nullptr};
+    void * ptr_C{nullptr};
 
-    typename Epilogue::OutputTileIterator::Params params_D;
-    typename Epilogue::OutputTileIterator::Params params_C;
+    typename Epilogue::OutputTileIterator::Params params_D{};
+    typename Epilogue::OutputTileIterator::Params params_C{};
 
-    int64_t batch_stride_D;
-    int64_t batch_stride_C;
+    int64_t batch_stride_D{0};
+    int64_t batch_stride_C{0};
 
 
   protected:
 
     //
     // Host-only dispatch-utilities
     //
```

## cutlass_library/source/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h

```diff
@@ -50,24 +50,24 @@
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate
   typename Epilogue_,             ///! Epilogue
   typename ThreadblockSwizzle_,   ///! Threadblock swizzling function
   bool IsSingleSource = Epilogue_::kIsSingleSource
 >
 struct GemmWithFusedEpilogue;
 
 // GemmWithFusedEpilogue with two sources
 template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate
   typename Epilogue_,             ///! Epilogue
   typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
 struct GemmWithFusedEpilogue<Mma_, Epilogue_, ThreadblockSwizzle_, false> {
 public:
 
   using Mma = Mma_;
@@ -99,15 +99,15 @@
 
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
   /// Split-K preserves splits that are 128b aligned
   static int const kSplitKAlignment = const_max(
-    128 / sizeof_bits<ElementA>::value, 
+    128 / sizeof_bits<ElementA>::value,
     128 / sizeof_bits<ElementB>::value
   );
 
   //
   // Structures
   //
 
@@ -143,16 +143,16 @@
     typename LayoutC::Stride::Index ldd;
     typename LayoutC::Stride::Index ldr;
     typename LayoutC::Stride::Index ldt;
 
     //
     // Methods
     //
-    
-    Arguments(): 
+
+    Arguments():
       ptr_A(nullptr),
       ptr_B(nullptr),
       ptr_C1(nullptr),
       ptr_C2(nullptr),
       ptr_D(nullptr)
     {}
 
@@ -181,37 +181,37 @@
       typename LayoutC::Stride::Index ldc1,
       typename LayoutC::Stride::Index ldc2,
       typename LayoutC::Stride::Index ldd,
       typename LayoutC::Stride::Index ldr,
       typename LayoutC::Stride::Index ldt)
     :
       UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
-      epilogue(epilogue), 
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C1(ptr_C1), ptr_C2(ptr_C2), ptr_D(ptr_D), 
-      ptr_Vector(ptr_Vector), 
+      epilogue(epilogue),
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C1(ptr_C1), ptr_C2(ptr_C2), ptr_D(ptr_D),
+      ptr_Vector(ptr_Vector),
       ptr_Tensor(ptr_Tensor),
-      batch_stride_A(batch_stride_A), 
-      batch_stride_B(batch_stride_B), 
-      batch_stride_C1(batch_stride_C1), 
-      batch_stride_C2(batch_stride_C2), 
+      batch_stride_A(batch_stride_A),
+      batch_stride_B(batch_stride_B),
+      batch_stride_C1(batch_stride_C1),
+      batch_stride_C2(batch_stride_C2),
       batch_stride_Vector(batch_stride_Vector),
       batch_stride_Tensor(batch_stride_Tensor),
       lda(lda), ldb(ldb), ldc1(ldc1), ldc2(ldc2), ldd(ldd), ldr(ldr), ldt(ldt)
     {
       CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Arguments::Arguments() - problem_size: " << problem_size);
       CUTLASS_TRACE_HOST("  ptr_Vector: " << (void *)this->ptr_Vector);
       CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
       CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
       CUTLASS_TRACE_HOST("  ldt: " << this->ldt);
     }
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
       Arguments args(*this);
-      
+
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A, args.ptr_B);
       std::swap(args.lda, args.ldb);
       std::swap(args.batch_stride_A, args.batch_stride_B);
 
       return args;
     }
@@ -303,15 +303,15 @@
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
       batch_stride_C1(args.batch_stride_C1),
       batch_stride_C2(args.batch_stride_C2),
       batch_stride_Vector(args.batch_stride_Vector),
       batch_stride_Tensor(args.batch_stride_Tensor)
     {
-      CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Params::Params() - problem_size: " << problem_size);
+      CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Params::Params()");
       CUTLASS_TRACE_HOST("  ptr_Vector: " << (void *)this->ptr_Vector);
       CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
       CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
       CUTLASS_TRACE_HOST("  ldt: " << args.ldt);
     }
 
     /// Lightweight update given a subset of arguments.
@@ -456,28 +456,28 @@
 
       return;
     }
 
     int offset_k = 0;
     int problem_size_k = params.problem_size.k();
 
-    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A); 
+    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A);
     ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
 
 
     #if SPLIT_K_ENABLED
     //
     // Fetch pointers based on mode.
     //
-    if (params.mode == GemmUniversalMode::kGemm || 
+    if (params.mode == GemmUniversalMode::kGemm ||
       params.mode == GemmUniversalMode::kGemmSplitKParallel) {
 
       if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
 
-        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size; 
+        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size;
       }
 
       offset_k = threadblock_tile_offset.k() * params.gemm_k_size;
     }
     else if (params.mode == GemmUniversalMode::kBatched) {
       ptr_A += threadblock_tile_offset.k() * params.batch_stride_A;
       ptr_B += threadblock_tile_offset.k() * params.batch_stride_B;
@@ -535,18 +535,18 @@
     accumulators.clear();
 
     // Compute threadblock-scoped matrix multiply-add
     int gemm_k_iterations = (problem_size_k - offset_k + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
     // Compute threadblock-scoped matrix multiply-add
     mma(
-      gemm_k_iterations, 
-      accumulators, 
-      iterator_A, 
-      iterator_B, 
+      gemm_k_iterations,
+      accumulators,
+      iterator_A,
+      iterator_B,
       accumulators);
 
     //
     // Epilogue
     //
 
     EpilogueOutputOp output_op(params.output_op);
@@ -567,24 +567,24 @@
 
     ElementC *ptr_C1 = static_cast<ElementC *>(params.ptr_C1);
     ElementC *ptr_C2 = static_cast<ElementC *>(params.ptr_C2);
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
     typename Epilogue::ElementTensor *ptr_Tensor = static_cast<typename Epilogue::ElementTensor *>(params.ptr_Tensor);
 
     // Define the reduction output pointer and move to the appropriate place
-    typename Epilogue::ElementVector *ptr_Vector = 
+    typename Epilogue::ElementVector *ptr_Vector =
       static_cast<typename Epilogue::ElementVector *>(params.ptr_Vector);
 
     //
     // Fetch pointers based on mode.
     //
-    
+
     //
     // Special path when split-K not enabled.
-    // 
+    //
 
     if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() == 1) {
 
       // Tile iterators loading from source tensors.
       typename Epilogue::OutputTileIterator iterator_C1(
         params.params_C1,
         ptr_C1,
@@ -617,17 +617,17 @@
           ptr_Tensor,
           params.problem_size.mn(),
           thread_idx,
           threadblock_offset);
 
       // Construct the epilogue
       Epilogue epilogue(
-        shared_storage.epilogue, 
-        thread_idx, 
-        warp_idx, 
+        shared_storage.epilogue,
+        thread_idx,
+        warp_idx,
         lane_idx);
 
       // Move to appropriate location for this output tile
       if (ptr_Vector) {
         ptr_Vector += threadblock_offset.column() + threadblock_tile_offset.m() * params.ldr;
       }
 
@@ -645,24 +645,24 @@
       return;
     }
 
     //
     // Slower path when split-K or batching is needed
     //
 
-      
+
     #if SPLIT_K_ENABLED
     // Construct the semaphore.
     Semaphore semaphore(params.semaphore + block_idx, thread_idx);
 
     if (params.mode == GemmUniversalMode::kGemm) {
 
       // If performing a reduction via split-K, fetch the initial synchronization
       if (params.grid_tiled_shape.k() > 1) {
-        
+
         // Fetch the synchronization lock initially but do not block.
         semaphore.fetch();
 
         // Indicate which position in a serial reduction the output operator is currently updating
         output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
       }
     }
@@ -735,23 +735,23 @@
             : ptr_Tensor,
         params.problem_size.mn(),
         thread_idx,
         threadblock_offset);
 
     // Construct the epilogue
     Epilogue epilogue(
-      shared_storage.epilogue, 
-      thread_idx, 
-      warp_idx, 
+      shared_storage.epilogue,
+      thread_idx,
+      warp_idx,
       lane_idx);
 
     #if SPLIT_K_ENABLED
     // Wait on the semaphore - this latency may have been covered by iterator construction
     if ((params.mode == GemmUniversalMode::kGemm) && params.grid_tiled_shape.k() > 1) {
-        
+
       // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
       if (threadblock_tile_offset.k()) {
         iterator_C1 = iterator_D;
       }
 
       semaphore.wait(threadblock_tile_offset.k());
 
@@ -779,36 +779,36 @@
              threadblock_offset);
 
     //
     // Release the semaphore
     //
 
     #if SPLIT_K_ENABLED
-    if ((params.mode == GemmUniversalMode::kGemm)  && params.grid_tiled_shape.k() > 1) { 
+    if ((params.mode == GemmUniversalMode::kGemm)  && params.grid_tiled_shape.k() > 1) {
 
       int lock = 0;
       if (params.grid_tiled_shape.k() == threadblock_tile_offset.k() + 1) {
 
         // The final threadblock resets the semaphore for subsequent grids.
         lock = 0;
       }
       else {
         // Otherwise, the semaphore is incremented
         lock = threadblock_tile_offset.k() + 1;
       }
-      
+
       semaphore.release(lock);
     }
     #endif
   }
 };
 
 // GemmWithFusedEpilogue with one source
 template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate
   typename Epilogue_,             ///! Epilogue
   typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
 struct GemmWithFusedEpilogue<Mma_, Epilogue_, ThreadblockSwizzle_, true> {
 public:
 
   using Mma = Mma_;
@@ -840,15 +840,15 @@
 
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
   /// Split-K preserves splits that are 128b aligned
   static int const kSplitKAlignment = const_max(
-    128 / sizeof_bits<ElementA>::value, 
+    128 / sizeof_bits<ElementA>::value,
     128 / sizeof_bits<ElementB>::value
   );
 
   //
   // Structures
   //
 
@@ -881,16 +881,16 @@
     typename LayoutC::Stride::Index ldd;
     typename LayoutC::Stride::Index ldr;
     typename LayoutC::Stride::Index ldt;
 
     //
     // Methods
     //
-    
-    Arguments(): 
+
+    Arguments():
       ptr_A(nullptr),
       ptr_B(nullptr),
       ptr_C(nullptr),
       ptr_D(nullptr)
     {}
 
     /// constructs an arguments structure
@@ -915,36 +915,36 @@
       typename LayoutB::Stride::Index ldb,
       typename LayoutC::Stride::Index ldc,
       typename LayoutC::Stride::Index ldd,
       typename LayoutC::Stride::Index ldr,
       typename LayoutC::Stride::Index ldt)
     :
       UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
-      epilogue(epilogue), 
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
-      ptr_Vector(ptr_Vector), 
+      epilogue(epilogue),
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
+      ptr_Vector(ptr_Vector),
       ptr_Tensor(ptr_Tensor),
-      batch_stride_A(batch_stride_A), 
-      batch_stride_B(batch_stride_B), 
-      batch_stride_C(batch_stride_C), 
+      batch_stride_A(batch_stride_A),
+      batch_stride_B(batch_stride_B),
+      batch_stride_C(batch_stride_C),
       batch_stride_Vector(batch_stride_Vector),
       batch_stride_Tensor(batch_stride_Tensor),
       lda(lda), ldb(ldb), ldc(ldc), ldd(ldd), ldr(ldr), ldt(ldt)
     {
       CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Arguments::Arguments() - problem_size: " << problem_size);
       CUTLASS_TRACE_HOST("  ptr_Vector: " << (void *)this->ptr_Vector);
       CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
       CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
       CUTLASS_TRACE_HOST("  ldt: " << this->ldt);
     }
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
       Arguments args(*this);
-      
+
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A, args.ptr_B);
       std::swap(args.lda, args.ldb);
       std::swap(args.batch_stride_A, args.batch_stride_B);
 
       return args;
     }
@@ -1031,15 +1031,15 @@
       ptr_Tensor(args.ptr_Tensor),
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
       batch_stride_C(args.batch_stride_C),
       batch_stride_Vector(args.batch_stride_Vector),
       batch_stride_Tensor(args.batch_stride_Tensor)
     {
-      CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Params::Params() - problem_size: " << problem_size);
+      CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Params::Params()");
       CUTLASS_TRACE_HOST("  ptr_Vector: " << (void *)this->ptr_Vector);
       CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
       CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
       CUTLASS_TRACE_HOST("  ldt: " << args.ldt);
     }
 
     /// Lightweight update given a subset of arguments.
@@ -1182,28 +1182,28 @@
 
       return;
     }
 
     int offset_k = 0;
     int problem_size_k = params.problem_size.k();
 
-    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A); 
+    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A);
     ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
 
 
     #if SPLIT_K_ENABLED
     //
     // Fetch pointers based on mode.
     //
-    if (params.mode == GemmUniversalMode::kGemm || 
+    if (params.mode == GemmUniversalMode::kGemm ||
       params.mode == GemmUniversalMode::kGemmSplitKParallel) {
 
       if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
 
-        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size; 
+        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size;
       }
 
       offset_k = threadblock_tile_offset.k() * params.gemm_k_size;
     }
     else if (params.mode == GemmUniversalMode::kBatched) {
       ptr_A += threadblock_tile_offset.k() * params.batch_stride_A;
       ptr_B += threadblock_tile_offset.k() * params.batch_stride_B;
@@ -1261,18 +1261,18 @@
     accumulators.clear();
 
     // Compute threadblock-scoped matrix multiply-add
     int gemm_k_iterations = (problem_size_k - offset_k + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
     // Compute threadblock-scoped matrix multiply-add
     mma(
-      gemm_k_iterations, 
-      accumulators, 
-      iterator_A, 
-      iterator_B, 
+      gemm_k_iterations,
+      accumulators,
+      iterator_A,
+      iterator_B,
       accumulators);
 
     //
     // Epilogue
     //
 
     EpilogueOutputOp output_op(params.output_op);
@@ -1292,24 +1292,24 @@
     int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
 
     ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C);
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
     typename Epilogue::ElementTensor *ptr_Tensor = static_cast<typename Epilogue::ElementTensor *>(params.ptr_Tensor);
 
     // Define the reduction output pointer and move to the appropriate place
-    typename Epilogue::ElementVector *ptr_Vector = 
+    typename Epilogue::ElementVector *ptr_Vector =
       static_cast<typename Epilogue::ElementVector *>(params.ptr_Vector);
 
     //
     // Fetch pointers based on mode.
     //
-    
+
     //
     // Special path when split-K not enabled.
-    // 
+    //
 
     if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() == 1) {
 
       // Tile iterators loading from source tensors.
       typename Epilogue::OutputTileIterator iterator_C(
         params.params_C,
         ptr_C,
@@ -1334,17 +1334,17 @@
           ptr_Tensor,
           params.problem_size.mn(),
           thread_idx,
           threadblock_offset);
 
       // Construct the epilogue
       Epilogue epilogue(
-        shared_storage.epilogue, 
-        thread_idx, 
-        warp_idx, 
+        shared_storage.epilogue,
+        thread_idx,
+        warp_idx,
         lane_idx);
 
       // Move to appropriate location for this output tile
       if (ptr_Vector) {
         ptr_Vector += threadblock_offset.column() + threadblock_tile_offset.m() * params.ldr;
       }
 
@@ -1361,24 +1361,24 @@
       return;
     }
 
     //
     // Slower path when split-K or batching is needed
     //
 
-      
+
     #if SPLIT_K_ENABLED
     // Construct the semaphore.
     Semaphore semaphore(params.semaphore + block_idx, thread_idx);
 
     if (params.mode == GemmUniversalMode::kGemm) {
 
       // If performing a reduction via split-K, fetch the initial synchronization
       if (params.grid_tiled_shape.k() > 1) {
-        
+
         // Fetch the synchronization lock initially but do not block.
         semaphore.fetch();
 
         // Indicate which position in a serial reduction the output operator is currently updating
         output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
       }
     }
@@ -1437,23 +1437,23 @@
             : ptr_Tensor,
         params.problem_size.mn(),
         thread_idx,
         threadblock_offset);
 
     // Construct the epilogue
     Epilogue epilogue(
-      shared_storage.epilogue, 
-      thread_idx, 
-      warp_idx, 
+      shared_storage.epilogue,
+      thread_idx,
+      warp_idx,
       lane_idx);
 
     #if SPLIT_K_ENABLED
     // Wait on the semaphore - this latency may have been covered by iterator construction
     if ((params.mode == GemmUniversalMode::kGemm) && params.grid_tiled_shape.k() > 1) {
-        
+
       // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
       if (threadblock_tile_offset.k()) {
         iterator_C = iterator_D;
       }
 
       semaphore.wait(threadblock_tile_offset.k());
 
@@ -1480,27 +1480,27 @@
              threadblock_offset);
 
     //
     // Release the semaphore
     //
 
     #if SPLIT_K_ENABLED
-    if ((params.mode == GemmUniversalMode::kGemm)  && params.grid_tiled_shape.k() > 1) { 
+    if ((params.mode == GemmUniversalMode::kGemm)  && params.grid_tiled_shape.k() > 1) {
 
       int lock = 0;
       if (params.grid_tiled_shape.k() == threadblock_tile_offset.k() + 1) {
 
         // The final threadblock resets the semaphore for subsequent grids.
         lock = 0;
       }
       else {
         // Otherwise, the semaphore is incremented
         lock = threadblock_tile_offset.k() + 1;
       }
-      
+
       semaphore.release(lock);
     }
     #endif
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/gemm/kernel/params_sparse_base.h

```diff
@@ -56,34 +56,32 @@
   typename TensorRefE>
 struct SparseParamsBase
 {
   //
   // Data members
   //
 
-  cutlass::gemm::GemmCoord problem_size;
-  cutlass::gemm::GemmCoord grid_tiled_shape;
+  cutlass::gemm::GemmCoord problem_size{};
+  cutlass::gemm::GemmCoord grid_tiled_shape{};
   int swizzle_log_tile;
-  ParamsA params_A;
-  TensorRefA ref_A;
-  ParamsB params_B;
-  TensorRefB ref_B;
-  ParamsE params_E;
-  TensorRefE ref_E;
-  int gemm_k_iterations;
-  int gemm_k_size;
+  ParamsA params_A{};
+  TensorRefA ref_A{};
+  ParamsB params_B{};
+  TensorRefB ref_B{};
+  ParamsE params_E{};
+  TensorRefE ref_E{};
+  int gemm_k_iterations{0};
+  int gemm_k_size{0};
 
   //
   // Host dispatch API
   //
 
   /// Default constructor
-  CUTLASS_HOST_DEVICE
-  SparseParamsBase() : swizzle_log_tile(0), gemm_k_iterations(0), gemm_k_size(0) { }
-
+  SparseParamsBase() = default;
 
   /// Constructor
   CUTLASS_HOST_DEVICE
   SparseParamsBase(
     cutlass::gemm::GemmCoord const & problem_size,
     cutlass::gemm::GemmCoord const & grid_tiled_shape,
     TensorRefA ref_A,
```

## cutlass_library/source/include/cutlass/gemm/kernel/params_universal_base.h

```diff
@@ -50,44 +50,39 @@
 
 namespace util {
 
 template <class LayoutA, class LayoutB>
 CUTLASS_HOST_DEVICE
 static bool 
 is_continous_k_aligned(GemmCoord problem_size, size_t alignmentA, size_t alignmentB) {
-  return (std::is_same<LayoutA, layout::RowMajor>::value && (problem_size.k() % alignmentA) == 0) ||
-         (std::is_same<LayoutB, layout::ColumnMajor>::value && (problem_size.k() % alignmentB) == 0);
+  return (platform::is_same<LayoutA, layout::RowMajor>::value && (problem_size.k() % alignmentA) == 0) ||
+         (platform::is_same<LayoutB, layout::ColumnMajor>::value && (problem_size.k() % alignmentB) == 0);
 }
 
 }  // namespace util
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Argument structure
 struct UniversalArgumentsBase
 {
   //
   // Data members
   //
 
-  GemmUniversalMode mode;
-  GemmCoord problem_size;
-  int batch_count;
-
-  int64_t batch_stride_D;
+  GemmUniversalMode mode = cutlass::gemm::GemmUniversalMode::kGemm;
+  GemmCoord problem_size{};
+  int batch_count{1};
+  int64_t batch_stride_D{0};
 
   //
   // Methods
   //
 
-  UniversalArgumentsBase() :
-    mode(GemmUniversalMode::kGemm),
-    batch_count(1),
-    batch_stride_D(0)
-  {}
+  UniversalArgumentsBase() = default;
 
   /// constructs an arguments structure
   UniversalArgumentsBase(
     GemmUniversalMode mode,
     GemmCoord problem_size,
     int batch_count,
     int64_t batch_stride_D)
@@ -113,35 +108,31 @@
   typename LayoutB>
 struct UniversalParamsBase
 {
   //
   // Data members
   //
 
-  GemmCoord problem_size;
-  GemmCoord grid_tiled_shape;
-  int swizzle_log_tile;
-
-  GemmUniversalMode mode;
-  int batch_count;
-  int gemm_k_size;
-
-  int64_t batch_stride_D;
-
-  int *semaphore;
+  GemmCoord problem_size{};
+  GemmCoord grid_tiled_shape{};
+  int swizzle_log_tile{0};
+  GemmUniversalMode mode = cutlass::gemm::GemmUniversalMode::kGemm;
+  int batch_count {0};
+  int gemm_k_size {0};
+  int64_t batch_stride_D {0};
+  int *semaphore = nullptr;
 
 
   //
   // Host dispatch API
   //
 
   /// Default constructor
   UniversalParamsBase() = default;
 
-
   /// Constructor
   UniversalParamsBase(
     UniversalArgumentsBase const &args, /// GEMM application arguments
     int device_sms,                     /// Number of SMs on the device
     int sm_occupancy)                   /// Kernel SM occupancy (in thread blocks)
   :
     problem_size(args.problem_size),
```

## cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_grouped.h

```diff
@@ -193,59 +193,42 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord *problem_sizes;
-    int problem_count;
-    int threadblock_count;
+    GemmUniversalMode mode = GemmUniversalMode::kGemm;
+    GemmCoord *problem_sizes = nullptr;
+    int problem_count{0};
+    int threadblock_count{0};
 
     typename EpilogueOutputOp::Params epilogue;
 
-    ElementA ** ptr_A;
-    ElementB ** ptr_B;
-    ElementC ** ptr_C;
-    ElementC ** ptr_D;
-
-    typename LayoutA::Stride::LongIndex *lda;
-    typename LayoutB::Stride::LongIndex *ldb;
-    typename LayoutC::Stride::LongIndex *ldc;
-    typename LayoutC::Stride::LongIndex *ldd;
+    ElementA ** ptr_A = nullptr;
+    ElementB ** ptr_B = nullptr;
+    ElementC ** ptr_C = nullptr;
+    ElementC ** ptr_D = nullptr;
+
+    typename LayoutA::Stride::LongIndex *lda = nullptr;
+    typename LayoutB::Stride::LongIndex *ldb = nullptr;
+    typename LayoutC::Stride::LongIndex *ldc = nullptr;
+    typename LayoutC::Stride::LongIndex *ldd = nullptr;
 
     // Only used by device-level operator
-    GemmCoord *host_problem_sizes;
+    GemmCoord *host_problem_sizes = nullptr;
 
-    bool allow_early_exit;
+    bool allow_early_exit = false;
 
     //
     // Methods
     //
 
     /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Arguments():
-      mode(GemmUniversalMode::kGemm),
-      problem_count(0),
-      threadblock_count(0),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      lda(nullptr),
-      ldb(nullptr),
-      ldc(nullptr),
-      ldd(nullptr),
-      host_problem_sizes(nullptr),
-      allow_early_exit(false)
-    {
-
-    }
+    Arguments() = default;
 
     /// Ctor
     CUTLASS_HOST_DEVICE
     Arguments(
       GemmUniversalMode mode,
       GemmCoord *problem_sizes,
       int problem_count,
@@ -286,51 +269,39 @@
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
   struct Params {
 
-    typename ProblemVisitor::Params problem_visitor;
-    int threadblock_count;
+    typename ProblemVisitor::Params problem_visitor{};
+    int threadblock_count = 0;
 
-    typename EpilogueOutputOp::Params output_op;
+    typename EpilogueOutputOp::Params output_op{};
 
-    GemmUniversalMode mode;
-    int batch_count;
+    GemmUniversalMode mode = cutlass::gemm::GemmUniversalMode::kGemm;
+    int batch_count = 0;
 
-    ElementA ** ptr_A;
-    ElementB ** ptr_B;
-    ElementC ** ptr_C;
-    ElementC ** ptr_D;
+    ElementA** ptr_A = nullptr;
+    ElementB** ptr_B = nullptr;
+    ElementC** ptr_C = nullptr;
+    ElementC** ptr_D = nullptr;
 
-    typename LayoutA::Stride::LongIndex *lda;
-    typename LayoutB::Stride::LongIndex *ldb;
-    typename LayoutC::Stride::LongIndex *ldc;
-    typename LayoutC::Stride::LongIndex *ldd;
+    typename LayoutA::Stride::LongIndex* lda = nullptr;
+    typename LayoutB::Stride::LongIndex* ldb = nullptr;
+    typename LayoutC::Stride::LongIndex* ldc = nullptr;
+    typename LayoutC::Stride::LongIndex* ldd = nullptr;
 
-    bool allow_early_exit;
+    bool allow_early_exit = false;
 
     //
     // Methods
     //
 
-    CUTLASS_HOST_DEVICE
-    Params():
-      mode(cutlass::gemm::GemmUniversalMode::kGemm),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      lda(nullptr),
-      ldb(nullptr),
-      ldc(nullptr),
-      ldd(nullptr),
-      allow_early_exit(false)
-    { }
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(Arguments const &args, void *workspace = nullptr, int tile_count = 0):
       problem_visitor(args.problem_sizes, args.problem_count, workspace, tile_count),
       threadblock_count(args.threadblock_count),
       output_op(args.epilogue),
       ptr_A(args.ptr_A),
@@ -376,16 +347,15 @@
 
 public:
 
   //
   // Methods
   //
 
-  CUTLASS_DEVICE
-  Rank2KGrouped() { }
+  Rank2KGrouped() = default;
 
   /// Determines whether kernel satisfies alignment
   static Status can_implement(cutlass::gemm::GemmCoord const & problem_size) {
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
```

## cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_universal.h

```diff
@@ -115,46 +115,42 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;
-
-    typename EpilogueOutputOp::Params epilogue;
-
-    void const * ptr_A;
-    void const * ptr_B;
-    void const * ptr_C;
-    void * ptr_D;
-
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C;
-    int64_t batch_stride_D;
-
-    typename LayoutA::Stride::Index lda;
-    typename LayoutB::Stride::Index ldb;
-    typename LayoutC::Stride::Index ldc;
-    typename LayoutC::Stride::Index ldd;
+    GemmUniversalMode mode = cutlass::gemm::GemmUniversalMode::kGemm;
+    GemmCoord problem_size {};
+    int batch_count{1};
+
+    typename EpilogueOutputOp::Params epilogue{};
+
+    void const * ptr_A = nullptr;
+    void const * ptr_B = nullptr;
+    void const * ptr_C = nullptr;
+    void * ptr_D = nullptr;
+
+    int64_t batch_stride_A {0};
+    int64_t batch_stride_B {0};
+    int64_t batch_stride_C {0};
+    int64_t batch_stride_D {0};
+
+    typename LayoutA::Stride::Index lda{0};
+    typename LayoutB::Stride::Index ldb{0};
+    typename LayoutC::Stride::Index ldc{0};
+    typename LayoutC::Stride::Index ldd{0};
 
-    bool allow_early_exit;
+    bool allow_early_exit{false};
 
     //
     // Methods
     //
     
-    Arguments(): 
-      mode(GemmUniversalMode::kGemm), 
-      batch_count(1), 
-      ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr),
-      allow_early_exit(false) { }
+    Arguments() = default;
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -173,15 +169,16 @@
       bool allow_early_exit = false
     ):
       mode(mode), 
       problem_size(problem_size), 
       batch_count(batch_count),
       epilogue(epilogue), 
       ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
-      batch_stride_A(batch_stride_A), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
+      batch_stride_A(batch_stride_A), batch_stride_B(0),
+      batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
       lda(lda), ldb(ldb), ldc(ldc), ldd(ldd),
       allow_early_exit(allow_early_exit) {
 
       }
 
       /// Returns arguments for a the transposed problem
       Arguments transposed_problem() const {
@@ -199,75 +196,54 @@
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
   struct Params {
 
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
+    cutlass::gemm::GemmCoord problem_size{};
+    cutlass::gemm::GemmCoord grid_tiled_shape{};
+    int swizzle_log_tile{0};
     
     // Mma1 Iterator A and B params
-    typename Mma1::IteratorA::Params params_A;
-    typename Mma1::IteratorB::Params params_BT;
+    typename Mma1::IteratorA::Params params_A{};
+    typename Mma1::IteratorB::Params params_BT{};
 
     // Mma2 Iterator A and B params 
-    typename Mma2::IteratorA::Params params_B;
-    typename Mma2::IteratorB::Params params_AT;
+    typename Mma2::IteratorA::Params params_B{};
+    typename Mma2::IteratorB::Params params_AT{};
 
-    typename Epilogue::OutputTileIterator::Params params_C;
-    typename Epilogue::OutputTileIterator::Params params_D;
+    typename Epilogue::OutputTileIterator::Params params_C{};
+    typename Epilogue::OutputTileIterator::Params params_D{};
     
-    typename EpilogueOutputOp::Params output_op;
+    typename EpilogueOutputOp::Params output_op{};
 
-    GemmUniversalMode mode;
-    int batch_count;
-    int gemm_k_size;
+    GemmUniversalMode mode = cutlass::gemm::GemmUniversalMode::kGemm;
+    int batch_count{0};
+    int gemm_k_size{0};
 
-    void * ptr_A;
-    void * ptr_B;
-    void * ptr_C;
-    void * ptr_D;
+    void * ptr_A = nullptr;
+    void * ptr_B = nullptr;
+    void * ptr_C = nullptr;
+    void * ptr_D = nullptr;
 
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C;
-    int64_t batch_stride_D;
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_B{0};
+    int64_t batch_stride_C{0};
+    int64_t batch_stride_D{0};
 
-    int *semaphore;
+    int *semaphore = nullptr;
 
-    bool allow_early_exit;
+    bool allow_early_exit {false};
 
     //
     // Methods
     //
 
-    CUTLASS_HOST_DEVICE
-    Params():
-      swizzle_log_tile(0),
-      params_A(0),
-      params_BT(0),
-      params_B(0),
-      params_AT(0),
-      params_C(0),
-      params_D(0),
-      batch_count(0),
-      gemm_k_size(0),
-      mode(cutlass::gemm::GemmUniversalMode::kGemm),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      batch_stride_A(0),
-      batch_stride_B(0),
-      batch_stride_C(0),
-      batch_stride_D(0),
-      semaphore(nullptr),
-      allow_early_exit(false) { }
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(
       Arguments const &args,
       cutlass::gemm::GemmCoord const & grid_tiled_shape,
       int gemm_k_size,
       void *workspace = nullptr
```

## cutlass_library/source/include/cutlass/gemm/kernel/rank_k_universal.h

```diff
@@ -102,44 +102,40 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;
-
-    typename EpilogueOutputOp::Params epilogue;
-
-    void const * ptr_A;
-    void const * ptr_C;
-    void * ptr_D;
-
-    int64_t batch_stride_A;
-    int64_t batch_stride_C;
-    int64_t batch_stride_D;
-
-    typename LayoutA::Stride::Index lda;
-    typename LayoutB::Stride::Index ldb;
-    typename LayoutC::Stride::Index ldc;
-    typename LayoutC::Stride::Index ldd;
+    GemmUniversalMode mode{GemmUniversalMode::kGemm};
+    GemmCoord problem_size{};
+    int batch_count{1};
+
+    typename EpilogueOutputOp::Params epilogue{};
+
+    void const * ptr_A{nullptr};
+    void const * ptr_C{nullptr};
+    void * ptr_D{nullptr};
+
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_C{0};
+    int64_t batch_stride_D{0};
+
+    typename LayoutA::Stride::Index lda{};
+    typename LayoutB::Stride::Index ldb{};
+    typename LayoutC::Stride::Index ldc{};
+    typename LayoutC::Stride::Index ldd{};
 
-    bool allow_early_exit;
+    bool allow_early_exit{false};
 
     //
     // Methods
     //
     
-    Arguments(): 
-      mode(GemmUniversalMode::kGemm), 
-      batch_count(1), 
-      ptr_A(nullptr), ptr_C(nullptr), ptr_D(nullptr),
-      allow_early_exit(false) { }
+    Arguments() = default;
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -156,81 +152,61 @@
     ):
       mode(mode), 
       problem_size(problem_size), 
       batch_count(batch_count),
       epilogue(epilogue), 
       ptr_A(ptr_A), ptr_C(ptr_C), ptr_D(ptr_D), 
       batch_stride_A(batch_stride_A), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
-      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd),
+      lda(lda), ldb(0),
+      ldc(ldc), ldd(ldd),
       allow_early_exit(allow_early_exit) {
 
       }
 
   };
 
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
   struct Params {
 
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
+    cutlass::gemm::GemmCoord problem_size{};
+    cutlass::gemm::GemmCoord grid_tiled_shape{};
+    int swizzle_log_tile{0};
    
-    typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorB::Params params_B;
-    typename Epilogue::OutputTileIterator::Params params_C;
-    typename Epilogue::OutputTileIterator::Params params_D;
-    
-    typename EpilogueOutputOp::Params output_op;
-
-    GemmUniversalMode mode;
-    int batch_count;
-    int gemm_k_size;
-
-    void * ptr_A;
-    void * ptr_B;
-    void * ptr_C;
-    void * ptr_D;
+    typename Mma::IteratorA::Params params_A{};
+    typename Mma::IteratorB::Params params_B{};
+    typename Epilogue::OutputTileIterator::Params params_C{};
+    typename Epilogue::OutputTileIterator::Params params_D{};
+    typename EpilogueOutputOp::Params output_op{};
+
+    GemmUniversalMode mode = cutlass::gemm::GemmUniversalMode::kGemm;
+    int batch_count{0};
+    int gemm_k_size{0};
+
+    void * ptr_A{nullptr};
+    void * ptr_B{nullptr};
+    void * ptr_C{nullptr};
+    void * ptr_D{nullptr};
+
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_B{0};
+    int64_t batch_stride_C{0};
+    int64_t batch_stride_D{0};
 
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C;
-    int64_t batch_stride_D;
+    int *semaphore{nullptr};
 
-    int *semaphore;
-
-    bool allow_early_exit;
+    bool allow_early_exit{false};
 
     //
     // Methods
     //
-
-    CUTLASS_HOST_DEVICE
-    Params():
-      swizzle_log_tile(0),
-      params_A(0),
-      params_B(0),
-      params_C(0),
-      params_D(0),
-      batch_count(0),
-      gemm_k_size(0),
-      mode(cutlass::gemm::GemmUniversalMode::kGemm),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      batch_stride_A(0),
-      batch_stride_B(0),
-      batch_stride_C(0),
-      batch_stride_D(0),
-      semaphore(nullptr),
-      allow_early_exit(false) { }
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(
       Arguments const &args,
       cutlass::gemm::GemmCoord const & grid_tiled_shape,
       int gemm_k_size,
       void *workspace = nullptr
```

## cutlass_library/source/include/cutlass/gemm/kernel/sm70_gemm.hpp

```diff
@@ -112,18 +112,18 @@
     EpilogueArguments epilogue{};
     KernelHardwareInfo hw_info{};
     TileSchedulerArguments scheduler{};
   };
 
   // Kernel entry point API
   struct Params {
-    GemmUniversalMode mode;
-    ProblemShape problem_shape;
-    MainloopParams mainloop;
-    EpilogueParams epilogue;
+    GemmUniversalMode mode{};
+    ProblemShape problem_shape{};
+    MainloopParams mainloop{};
+    EpilogueParams epilogue{};
   };
 
   //
   // Methods
   //
 
   // Convert to underlying arguments. In this case, a simple copy for the aliased type.
@@ -146,23 +146,24 @@
   static bool
   can_implement(Arguments const& args) {
     bool mode_implementable = args.mode == GemmUniversalMode::kGemm or
           (args.mode == GemmUniversalMode::kBatched && rank(ProblemShape{}) == 4);
     return mode_implementable && TileScheduler::can_implement(args.scheduler);
   }
 
-  static int
+  static size_t
   get_workspace_size(Arguments const& args) {
-    int workspace_size = 0;
+    size_t workspace_size = 0;
     return workspace_size;
   }
 
   static
   cutlass::Status
-  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr) {
+  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr, 
+    CudaHostAdapter* cuda_adapter = nullptr) {
     cutlass::Status status = Status::kSuccess;
 
     return status;
   }
 
   static dim3
   get_grid_shape(Params const& params) {
@@ -246,15 +247,14 @@
       gB,
       accumulators,
       k_tile_iter, k_tile_count,
       residue_mnk,
       thread_idx,
       smem_buf
     );
-
     // Epilogue and write to gD
     CollectiveEpilogue epilogue{params.epilogue};
     epilogue(
       problem_shape_MNKL,
       blk_shape,
       blk_coord_mnkl,
       accumulators,
```

## cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_array_tma_warpspecialized_cooperative.hpp

```diff
@@ -164,21 +164,21 @@
     EpilogueArguments epilogue{};
     KernelHardwareInfo hw_info{};
     TileSchedulerArguments scheduler{};
   };
 
   // Kernel entry point API
   struct Params {
-    GemmUniversalMode mode;
-    ProblemShape problem_shape;
-    MainloopParams mainloop;
-    EpilogueParams epilogue;
-    KernelHardwareInfo hw_info;
-    TileSchedulerParams scheduler;
-    void* workspace;
+    GemmUniversalMode mode{};
+    ProblemShape problem_shape{};
+    MainloopParams mainloop{};
+    EpilogueParams epilogue{};
+    KernelHardwareInfo hw_info{};
+    TileSchedulerParams scheduler{};
+    void* workspace{nullptr};
   };
 
   //
   // Methods
   //
 
   // Convert to underlying arguments. In this case, a simple copy for the aliased type.
@@ -286,30 +286,31 @@
     workspace_size += CollectiveMainloop::get_workspace_size(args.problem_shape, args.mainloop, sm_count);
     workspace_size = round_nearest(workspace_size,  MinWorkspaceAlignment);
 
     return workspace_size;
   }
 
   static cutlass::Status
-  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr) {
+  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     Status status = Status::kSuccess;
     uint8_t* workspace_ptr = reinterpret_cast<uint8_t*>(workspace);
     size_t workspace_offset = 0;
     constexpr uint32_t NumEpilogueSubTiles = CollectiveEpilogue::get_store_pipe_increment(TileShape{});
 
     status = TileScheduler::template initialize_workspace<typename ProblemShape::UnderlyingProblemShape, ElementAccumulator>(
       args.scheduler, workspace_ptr + workspace_offset, stream, typename ProblemShape::UnderlyingProblemShape{}, args.hw_info, NumMmaWarpGroups, NumEpilogueSubTiles);
     workspace_offset += TileScheduler::template get_workspace_size<typename ProblemShape::UnderlyingProblemShape, ElementAccumulator>(
       args.scheduler, typename ProblemShape::UnderlyingProblemShape{}, args.hw_info, NumMmaWarpGroups, NumEpilogueSubTiles);
     workspace_offset = round_nearest(workspace_offset,  MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
-    status = CollectiveEpilogue::initialize_workspace(args.problem_shape, args.epilogue, workspace_ptr + workspace_offset, stream);
+    status = CollectiveEpilogue::initialize_workspace(args.problem_shape, args.epilogue, workspace_ptr + workspace_offset, stream, cuda_adapter);
     workspace_offset += CollectiveEpilogue::get_workspace_size(args.problem_shape, args.epilogue);
     workspace_offset = round_nearest(workspace_offset,  MinWorkspaceAlignment);
 
     status = CollectiveMainloop::initialize_workspace(args.problem_shape, args.mainloop, workspace_ptr + workspace_offset, stream);
     workspace_offset += CollectiveMainloop::get_workspace_size(args.problem_shape, args.mainloop, args.hw_info.sm_count);
     workspace_offset = round_nearest(workspace_offset,  MinWorkspaceAlignment);
```

## cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma.hpp

```diff
@@ -131,18 +131,18 @@
     EpilogueArguments epilogue{};
     KernelHardwareInfo hw_info{};
     TileSchedulerArguments scheduler{};
   };
 
   // Kernel entry point API
   struct Params {
-    GemmUniversalMode mode;
-    ProblemShape problem_shape;
-    MainloopParams mainloop;
-    EpilogueParams epilogue;
+    GemmUniversalMode mode{};
+    ProblemShape problem_shape{};
+    MainloopParams mainloop{};
+    EpilogueParams epilogue{};
   };
 
   //
   // Methods
   //
 
   // Convert to underlying arguments. In this case, a simple copy for the aliased type.
@@ -176,21 +176,22 @@
     implementable &= CollectiveMainloop::can_implement(args.problem_shape, args.mainloop);
     implementable &= CollectiveEpilogue::can_implement(args.problem_shape, args.epilogue);
     implementable &= TileScheduler::can_implement(args.scheduler);
 
     return implementable;
   }
 
-  static int
+  static size_t
   get_workspace_size(Arguments const& args) {
     return 0;
   }
 
   static cutlass::Status
-  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr) {
+  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return Status::kSuccess;
   }
 
   // Computes the kernel launch grid shape based on runtime parameters
   static dim3
   get_grid_shape(Params const& params) {
     auto cluster_shape = ClusterShape{};
```

## cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized.hpp

```diff
@@ -137,18 +137,18 @@
     EpilogueArguments epilogue{};
     KernelHardwareInfo hw_info{};
     TileSchedulerArguments scheduler{};
   };
 
   // Kernel entry point API
   struct Params {
-    GemmUniversalMode mode;
-    ProblemShape problem_shape;
-    MainloopParams mainloop;
-    EpilogueParams epilogue;
+    GemmUniversalMode mode{};
+    ProblemShape problem_shape{};
+    MainloopParams mainloop{};
+    EpilogueParams epilogue{};
   };
 
   //
   // Methods
   //
 
   // Convert to underlying arguments. In this case, a simple copy for the aliased type.
@@ -183,22 +183,23 @@
     implementable &= CollectiveEpilogue::can_implement(args.problem_shape, args.epilogue);
     implementable &= TileScheduler::can_implement(args.scheduler);
 
     return implementable;
   }
 
   static
-  int
+  size_t
   get_workspace_size(Arguments const& args) {
     return 0;
   }
 
   static
   cutlass::Status
-  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr) {
+  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return Status::kSuccess;
   }
 
   // Computes the kernel launch grid shape based on runtime parameters
   static dim3
   get_grid_shape(Params const& params) {
     auto cluster_shape = ClusterShape{};
```

## cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_cooperative.hpp

```diff
@@ -144,21 +144,21 @@
     EpilogueArguments epilogue{};
     KernelHardwareInfo hw_info{};
     TileSchedulerArguments scheduler{};
   };
 
   // Kernel entry point API
   struct Params {
-    GemmUniversalMode mode;
-    ProblemShape problem_shape;
-    MainloopParams mainloop;
-    EpilogueParams epilogue;
-    KernelHardwareInfo hw_info;
-    TileSchedulerParams scheduler;
-    void* workspace;
+    GemmUniversalMode mode{};
+    ProblemShape problem_shape{};
+    MainloopParams mainloop{};
+    EpilogueParams epilogue{};
+    KernelHardwareInfo hw_info{};
+    TileSchedulerParams scheduler{};
+    void* workspace{nullptr};
   };
 
   //
   // Methods
   //
 
   // Convert to underlying arguments. In this case, a simple copy for the aliased type.
@@ -246,30 +246,31 @@
     workspace_size += CollectiveEpilogue::get_workspace_size(args.problem_shape, args.epilogue);
     workspace_size = round_nearest(workspace_size,  MinWorkspaceAlignment);
 
     return workspace_size;
   }
 
   static cutlass::Status
-  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr) {
+  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr, 
+    CudaHostAdapter* cuda_adapter = nullptr) {
     Status status = Status::kSuccess;
     uint8_t* workspace_ptr = reinterpret_cast<uint8_t*>(workspace);
     size_t workspace_offset = 0;
     constexpr uint32_t NumEpilogueSubTiles = CollectiveEpilogue::get_store_pipe_increment(TileShape{});
 
     status = TileScheduler::template initialize_workspace<ProblemShape, ElementAccumulator>(
       args.scheduler, workspace_ptr + workspace_offset, stream, args.problem_shape, args.hw_info, NumMmaWarpGroups, NumEpilogueSubTiles);
     workspace_offset += TileScheduler::template get_workspace_size<ProblemShape, ElementAccumulator>(
       args.scheduler, args.problem_shape, args.hw_info, NumMmaWarpGroups, NumEpilogueSubTiles);
     workspace_offset = round_nearest(workspace_offset,  MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
-    status = CollectiveEpilogue::initialize_workspace(args.problem_shape, args.epilogue, workspace_ptr + workspace_offset, stream);
+    status = CollectiveEpilogue::initialize_workspace(args.problem_shape, args.epilogue, workspace_ptr + workspace_offset, stream, cuda_adapter);
     workspace_offset += CollectiveEpilogue::get_workspace_size(args.problem_shape, args.epilogue);
     workspace_offset = round_nearest(workspace_offset,  MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
     return status;
```

## cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_pingpong.hpp

```diff
@@ -152,20 +152,20 @@
     EpilogueArguments epilogue{};
     KernelHardwareInfo hw_info{};
     TileSchedulerArguments scheduler{};
   };
 
   // Kernel entry point API
   struct Params {
-    GemmUniversalMode mode;
-    ProblemShape problem_shape;
-    MainloopParams mainloop;
-    EpilogueParams epilogue;
-    KernelHardwareInfo hw_info;
-    TileSchedulerParams scheduler;
+    GemmUniversalMode mode{};
+    ProblemShape problem_shape{};
+    MainloopParams mainloop{};
+    EpilogueParams epilogue{};
+    KernelHardwareInfo hw_info{};
+    TileSchedulerParams scheduler{};
   };
 
   //
   // Methods
   //
 
   // Convert to underlying arguments. In this case, a simple copy for the aliased type.
@@ -245,29 +245,30 @@
     workspace_size += CollectiveEpilogue::get_workspace_size(args.problem_shape, args.epilogue);
     workspace_size = round_nearest(workspace_size,  MinWorkspaceAlignment);
 
     return workspace_size;
   }
 
   static cutlass::Status
-  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr) {
+  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     Status status = Status::kSuccess;
     uint8_t* workspace_ptr = reinterpret_cast<uint8_t*>(workspace);
     size_t workspace_offset = 0;
 
     status = TileScheduler::template initialize_workspace<ProblemShape, ElementAccumulator>(
       args.scheduler, workspace_ptr + workspace_offset, stream, args.problem_shape, args.hw_info, NumMmaWarpGroups);
     workspace_offset += TileScheduler::template get_workspace_size<ProblemShape, ElementAccumulator>(
       args.scheduler, args.problem_shape, args.hw_info, NumMmaWarpGroups);
     workspace_offset = round_nearest(workspace_offset,  MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
-    status = CollectiveEpilogue::initialize_workspace(args.problem_shape, args.epilogue, workspace_ptr + workspace_offset, stream);
+    status = CollectiveEpilogue::initialize_workspace(args.problem_shape, args.epilogue, workspace_ptr + workspace_offset, stream, cuda_adapter);
     workspace_offset += CollectiveEpilogue::get_workspace_size(args.problem_shape, args.epilogue);
     workspace_offset = round_nearest(workspace_offset,  MinWorkspaceAlignment);
     if (status != Status::kSuccess) {
       return status;
     }
 
     return status;
```

## cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized.hpp

```diff
@@ -141,18 +141,18 @@
     EpilogueArguments epilogue{};
     KernelHardwareInfo hw_info{};
     TileSchedulerArguments scheduler{};
   };
 
   // Kernel entry point API
   struct Params {
-    GemmUniversalMode mode;
-    ProblemShape problem_shape;
-    MainloopParams mainloop;
-    EpilogueParams epilogue;
+    GemmUniversalMode mode{};
+    ProblemShape problem_shape{};
+    MainloopParams mainloop{};
+    EpilogueParams epilogue{};
   };
 
   //
   // Methods
   //
 
   // Convert to underlying arguments. In this case, a simple copy for the aliased type.
@@ -187,22 +187,23 @@
     implementable &= CollectiveEpilogue::can_implement(args.problem_shape, args.epilogue);
     implementable &= TileScheduler::can_implement(args.scheduler);
 
     return implementable;
   }
 
   static
-  int
+  size_t
   get_workspace_size(Arguments const& args) {
     return 0;
   }
 
   static
   cutlass::Status
-  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr) {
+  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return Status::kSuccess;
   }
 
   // Computes the kernel launch grid shape based on runtime parameters
   static dim3
   get_grid_shape(Params const& params) {
     auto cluster_shape = Shape<_1,_1,_1>{};
```

## cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized_cooperative.hpp

```diff
@@ -65,15 +65,14 @@
 public:
   //
   // Type Aliases
   //
   using ProblemShape = ProblemShape_;
   static_assert(cute::rank(ProblemShape{}) == 3 or cute::rank(ProblemShape{}) == 4,
     "ProblemShape{} should be <M,N,K> or <M,N,K,L>");
-
   // Mainloop derived types
   using CollectiveMainloop = CollectiveMainloop_;
   using TileShape = typename CollectiveMainloop::TileShape;
   using TiledMma  = typename CollectiveMainloop::TiledMma;
   using ArchTag   = typename CollectiveMainloop::ArchTag;
   using ElementA  = typename CollectiveMainloop::ElementA;
   using StrideA   = typename CollectiveMainloop::StrideA;
@@ -142,20 +141,20 @@
     EpilogueArguments epilogue{};
     KernelHardwareInfo hw_info{};
     TileSchedulerArguments scheduler{};
   };
 
   // Kernel entry point API
   struct Params {
-    GemmUniversalMode mode;
-    ProblemShape problem_shape;
-    MainloopParams mainloop;
-    EpilogueParams epilogue;
-    KernelHardwareInfo hw_info;
-    TileSchedulerParams scheduler;
+    GemmUniversalMode mode{};
+    ProblemShape problem_shape{};
+    MainloopParams mainloop{};
+    EpilogueParams epilogue{};
+    KernelHardwareInfo hw_info{};
+    TileSchedulerParams scheduler{};
   };
 
   //
   // Methods
   //
 
   // Convert to underlying arguments. In this case, a simple copy for the aliased type.
@@ -209,24 +208,25 @@
     implementable &= CollectiveEpilogue::can_implement(args.problem_shape, args.epilogue);
     implementable &= TileScheduler::can_implement(args.scheduler);
 
     return implementable;
   }
 
   static
-  int
+  size_t
   get_workspace_size(Arguments const& args) {
     TileScheduler t;
     return t.template get_workspace_size<ProblemShape, ElementAccumulator>(
       args.scheduler, args.problem_shape, args.hw_info, NumMmaWarpGroups);
   }
 
   static
   cutlass::Status
-  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr) {
+  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     TileScheduler t;
     return t.template initialize_workspace<ProblemShape, ElementAccumulator>(
       args.scheduler, workspace, stream, args.problem_shape, args.hw_info, NumMmaWarpGroups);
   }
 
   // Computes the kernel launch grid shape based on runtime parameters
   static dim3
```

## cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized_pingpong.hpp

```diff
@@ -66,15 +66,14 @@
 public:
   //
   // Type Aliases
   //
   using ProblemShape = ProblemShape_;
   static_assert(cute::rank(ProblemShape{}) == 3 or cute::rank(ProblemShape{}) == 4,
     "ProblemShape{} should be <M,N,K> or <M,N,K,L>");
-
   // Mainloop derived types
   using CollectiveMainloop = CollectiveMainloop_;
   using TileShape = typename CollectiveMainloop::TileShape;
   using TiledMma  = typename CollectiveMainloop::TiledMma;
   using ArchTag   = typename CollectiveMainloop::ArchTag;
   using ElementA  = typename CollectiveMainloop::ElementA;
   using StrideA   = typename CollectiveMainloop::StrideA;
@@ -152,20 +151,20 @@
     EpilogueArguments epilogue{};
     KernelHardwareInfo hw_info{};
     TileSchedulerArguments scheduler{};
   };
 
   // Kernel entry point API
   struct Params {
-    GemmUniversalMode mode;
-    ProblemShape problem_shape;
-    MainloopParams mainloop;
-    EpilogueParams epilogue;
-    KernelHardwareInfo hw_info;
-    TileSchedulerParams scheduler;
+    GemmUniversalMode mode{};
+    ProblemShape problem_shape{};
+    MainloopParams mainloop{};
+    EpilogueParams epilogue{};
+    KernelHardwareInfo hw_info{};
+    TileSchedulerParams scheduler{};
   };
 
   //
   // Methods
   //
 
   // Convert to underlying arguments. In this case, a simple copy for the aliased type.
@@ -220,22 +219,23 @@
     implementable &= CollectiveEpilogue::can_implement(args.problem_shape, args.epilogue);
     implementable &= TileScheduler::can_implement(args.scheduler);
 
     return implementable;
   }
 
   static
-  int
+  size_t
   get_workspace_size(Arguments const& args) {
     return 0;
   }
 
   static
   cutlass::Status
-  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr) {
+  initialize_workspace(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr,
+    CudaHostAdapter* cuda_adapter = nullptr) {
     return Status::kSuccess;
   }
 
   // Computes the kernel launch grid shape based on runtime parameters
   static dim3
   get_grid_shape(Params const& params) {
     // Given device SM count, set grid size s.t. we do not launch more thread blocks than we can run concurrently
```

## cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler.hpp

```diff
@@ -54,19 +54,42 @@
   get_work_idx_m_and_n(
       uint64_t blk_per_grid_dim,
       FastDivmodU64Pow2 const& divmod_cluster_shape_major,
       FastDivmodU64Pow2 const& divmod_cluster_shape_minor,
       FastDivmodU64 const& divmod_cluster_blk_major,
       int32_t log_swizzle_size,
       RasterOrder raster_order) {
+    auto [cta_m_in_cluster, cta_n_in_cluster, _] = cute::block_id_in_cluster();
+    return get_work_idx_m_and_n(
+      blk_per_grid_dim,
+      divmod_cluster_shape_major,
+      divmod_cluster_shape_minor,
+      divmod_cluster_blk_major,
+      log_swizzle_size,
+      raster_order,
+      cta_m_in_cluster,
+      cta_n_in_cluster
+    );
+  }
+
+  static CUTLASS_DEVICE
+  cute::tuple<int32_t, int32_t>
+  get_work_idx_m_and_n(
+      uint64_t blk_per_grid_dim,
+      FastDivmodU64Pow2 const& divmod_cluster_shape_major,
+      FastDivmodU64Pow2 const& divmod_cluster_shape_minor,
+      FastDivmodU64 const& divmod_cluster_blk_major,
+      int32_t log_swizzle_size,
+      RasterOrder raster_order,
+      uint64_t cta_m_in_cluster,
+      uint64_t cta_n_in_cluster) {
 
     uint64_t cluster_id, cluster_major_offset = 0, cluster_minor_offset = 0;
     divmod_cluster_shape_major(cluster_id, cluster_major_offset, blk_per_grid_dim);
 
-    auto [cta_m_in_cluster, cta_n_in_cluster, _] = cute::block_id_in_cluster();
     if (raster_order == RasterOrder::AlongN) {
       cluster_minor_offset = cta_m_in_cluster;
     }
     else {
       cluster_minor_offset = cta_n_in_cluster;
     }
 
@@ -93,15 +116,15 @@
       return {major_work_idx, minor_work_idx};
     }
 
   }
 
   // The basic tile scheduler does not require any additional workspace
   template <class ProblemShape, class ElementAccumulator>
-  static int
+  static size_t
   get_workspace_size(Arguments const&, ProblemShape, KernelHardwareInfo const&, uint32_t, const uint32_t = 1) {
     return 0;
   }
 
   template <class ProblemShape, class ElementAccumulator>
   static cutlass::Status
   initialize_workspace(Arguments const&, void*, cudaStream_t, ProblemShape, KernelHardwareInfo const&,
```

## cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler_group.hpp

```diff
@@ -396,15 +396,15 @@
   static bool
   continue_current_work(WorkTileInfo&) {
     return false;
   }
 
   // The basic tile scheduler does not require any additional workspace
   template <class ProblemShape, class ElementAccumulator>
-  static int
+  static size_t
   get_workspace_size(Arguments const&, ProblemShape, KernelHardwareInfo const&, uint32_t, const uint32_t = 1) {
     return 0;
   }
 
   template <class ProblemShape, class ElementAccumulator>
   static cutlass::Status
   initialize_workspace(Arguments const&, void*, cudaStream_t, ProblemShape, KernelHardwareInfo const&,
```

## cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler_stream_k.hpp

```diff
@@ -382,15 +382,16 @@
   CUTLASS_DEVICE
   static void
   fixup_helper(
     Params const& params,
     WorkTileInfo const& work_tile_info,
     FrgTensorC& accumulators,
     uint32_t num_barriers,
-    uint32_t barrier_idx) {
+    uint32_t barrier_idx,
+    uint32_t num_accumulator_mtxs = 1) {
 
     using ElementAccumulator = typename FrgTensorC::value_type;
 
     if (!requires_fixup(params, work_tile_info)) {
       return;
     }
     auto tile_idx = output_tile_index(params, work_tile_info);
@@ -408,15 +409,15 @@
       reduction_tile_idx *= Params::max_peers_per_tile(params.sk_units_, params.sk_tiles_);
       reduction_peer_offset = my_peer_id * cute::size<0>(TileShape{}) * cute::size<1>(TileShape{});
     }
 
     // Reductions use BlockStripedReduce with a width of BarrierManager::ThreadCount under the hood.
     // Thus, the start of the reduction space is the same across all threads in a warp group.
     int reduction_offset =
-      (cute::size<0>(TileShape{}) * cute::size<1>(TileShape{}) * reduction_tile_idx) +
+      (cute::size<0>(TileShape{}) * cute::size<1>(TileShape{}) * reduction_tile_idx * num_accumulator_mtxs) +
       reduction_peer_offset +
       (size(accumulators) * barrier_idx * BarrierManager::ThreadCount);
 
     ElementAccumulator* group_reduction_workspace = reinterpret_cast<ElementAccumulator*>(params.reduction_workspace_) + reduction_offset;
 
     using AccumulatorArrayT = Array<typename FrgTensorC::value_type, size(FrgTensorC{})>;
     using BlockStripedReduceT = BlockStripedReduce<BarrierManager::ThreadCount, AccumulatorArrayT>;
@@ -440,15 +441,15 @@
       reduction_tiles = params.sk_tiles_ * Params::max_peers_per_tile(params.sk_units_, params.sk_tiles_);
     }
     else {
       reduction_tiles = params.sk_tiles_;
     }
 
     auto reduction_workspace_size = Params::get_reduction_workspace_size(
-      reduction_tiles, to_gemm_coord(TileShape{}), sizeof_bits<ElementAccumulator>::value);
+      reduction_tiles, to_gemm_coord(TileShape{}), sizeof_bits<ElementAccumulator>::value, num_accumulator_mtxs);
     BarrierType* lock_workspace = reinterpret_cast<BarrierType*>(
       reinterpret_cast<uint8_t*>(params.reduction_workspace_) + reduction_workspace_size);
 
     if (work_tile_info.is_reduction_unit()) {
       plus<AccumulatorArrayT> add_fragments;
       auto peer_offset = size(accumulators) * num_barriers * BarrierManager::ThreadCount;
 
@@ -536,15 +537,15 @@
     );
 
     uint64_t tiles_mn = params.divmod_batch_.divisor;
     return tiles_mn * work_tile_info.L_idx + linear_idx_in_batch;
   }
 
   template <class ProblemShape, class ElementAccumulator>
-  static int
+  static size_t
   get_workspace_size(
     Arguments const& args,
     ProblemShape problem_shape,
     KernelHardwareInfo const& hw_info,
     uint32_t mma_warp_groups,
     const uint32_t epilogue_subtile = 1) {
 
@@ -832,15 +833,16 @@
 
     auto [work_idx_m, work_idx_n] = UnderlyingScheduler::get_work_idx_m_and_n(
                                           cta_per_grid_dim,
                                           params.divmod_cluster_shape_major_,
                                           params.divmod_cluster_shape_minor_,
                                           params.divmod_cluster_blk_major_,
                                           params.log_swizzle_size_,
-                                          params.raster_order_);
+                                          params.raster_order_
+                                        );
 
     // Set the M, N, and L block offsets
     work_tile_info.M_idx = work_idx_m;
     work_tile_info.N_idx = work_idx_n;
     work_tile_info.L_idx = static_cast<int32_t>(work_idx_l);
   }
```

## cutlass_library/source/include/cutlass/gemm/kernel/static_tile_scheduler.hpp

```diff
@@ -330,16 +330,16 @@
       args.raster_order,
       /* truncate_by_problem_size = */true
     );
   }
 
   // Convert CTA-level work tile info to cluster-level tile coord
   CUTLASS_DEVICE
-  cute::Coord<int,int,int,int>
-  tile_info_to_coord_mnkl(WorkTileInfo work_tile_info) const {
+  auto
+  work_tile_to_cluster_coord_mnkl(WorkTileInfo work_tile_info) const {
     // TileScheduler works at CTA-level, kernel works at cluster-level
     int m_coord = idx2crd(work_tile_info.M_idx / scheduler_params.cluster_shape_m_,
                           scheduler_params.problem_tiles_m_);
     int n_coord = idx2crd(work_tile_info.N_idx / scheduler_params.cluster_shape_n_,
                           scheduler_params.problem_tiles_n_);
     int l_coord = idx2crd(work_tile_info.L_idx,
                           scheduler_params.problem_tiles_l_);
```

## cutlass_library/source/include/cutlass/gemm/kernel/symm_universal.h

```diff
@@ -115,43 +115,40 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;
-
-    typename EpilogueOutputOp::Params epilogue;
-
-    void const * ptr_A;
-    void const * ptr_B;
-    void const * ptr_C;
-    void * ptr_D;
-
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C;
-    int64_t batch_stride_D;
-
-    typename LayoutA::Stride::Index lda;
-    typename LayoutB::Stride::Index ldb;
-    typename LayoutC::Stride::Index ldc;
-    typename LayoutC::Stride::Index ldd;
+    GemmUniversalMode mode = GemmUniversalMode::kGemm;
+    GemmCoord problem_size{};
+    int batch_count{1};
+
+    typename EpilogueOutputOp::Params epilogue{};
+
+    void const * ptr_A{nullptr};
+    void const * ptr_B{nullptr};
+    void const * ptr_C{nullptr};
+    void * ptr_D{nullptr};
+
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_B{0};
+    int64_t batch_stride_C{0};
+    int64_t batch_stride_D{0};
+
+    typename LayoutA::Stride::Index lda{0};
+    typename LayoutB::Stride::Index ldb{0};
+    typename LayoutC::Stride::Index ldc{0};
+    typename LayoutC::Stride::Index ldd{0};
 
     //
     // Methods
     //
     
-    Arguments(): 
-      mode(GemmUniversalMode::kGemm), 
-      batch_count(1), 
-      ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr) { }
+    Arguments() = default;
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -169,15 +166,16 @@
       typename LayoutC::Stride::Index ldd
     ):
       mode(mode), 
       problem_size(problem_size), 
       batch_count(batch_count),
       epilogue(epilogue), 
       ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
-      batch_stride_A(batch_stride_A), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
+      batch_stride_A(batch_stride_A), batch_stride_B(0),
+      batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
       lda(lda), ldb(ldb), ldc(ldc), ldd(ldd) {
 
       }
 
     /// Returns arguments for the transposed problem sizes
     Arguments transposed_problem_size() const {
       Arguments args(*this);
@@ -202,72 +200,51 @@
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
   struct Params {
 
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
+    cutlass::gemm::GemmCoord problem_size{};
+    cutlass::gemm::GemmCoord grid_tiled_shape{};
+    int swizzle_log_tile{0};
     
     // Mma1 Iterator A and B params
-    typename Mma1::IteratorA::Params params_A_mma1;
-    typename Mma1::IteratorB::Params params_B_mma1;
+    typename Mma1::IteratorA::Params params_A_mma1{};
+    typename Mma1::IteratorB::Params params_B_mma1{};
 
     // Mma2 Iterator A and B params 
-    typename Mma2::IteratorA::Params params_A_mma2;
-    typename Mma2::IteratorB::Params params_B_mma2;
+    typename Mma2::IteratorA::Params params_A_mma2{};
+    typename Mma2::IteratorB::Params params_B_mma2{};
 
-    typename Epilogue::OutputTileIterator::Params params_C;
-    typename Epilogue::OutputTileIterator::Params params_D;
+    typename Epilogue::OutputTileIterator::Params params_C{};
+    typename Epilogue::OutputTileIterator::Params params_D{};
     
-    typename EpilogueOutputOp::Params output_op;
+    typename EpilogueOutputOp::Params output_op{};
 
-    GemmUniversalMode mode;
-    int batch_count;
-    int gemm_k_size;
-
-    void * ptr_A;
-    void * ptr_B;
-    void * ptr_C;
-    void * ptr_D;
-
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C;
-    int64_t batch_stride_D;
+    GemmUniversalMode mode = cutlass::gemm::GemmUniversalMode::kGemm;
+    int batch_count {0};
+    int gemm_k_size {0};
+
+    void * ptr_A{nullptr};
+    void * ptr_B{nullptr};
+    void * ptr_C{nullptr};
+    void * ptr_D{nullptr};
+
+    int64_t batch_stride_A {0};
+    int64_t batch_stride_B {0};
+    int64_t batch_stride_C {0};
+    int64_t batch_stride_D {0};
 
-    int *semaphore;
+    int *semaphore{nullptr};
 
     //
     // Methods
     //
-
-    CUTLASS_HOST_DEVICE
-    Params():
-      swizzle_log_tile(0),
-      params_A_mma1(0),
-      params_B_mma1(0),
-      params_A_mma2(0),
-      params_B_mma2(0),
-      params_C(0),
-      params_D(0),
-      batch_count(0),
-      gemm_k_size(0),
-      mode(cutlass::gemm::GemmUniversalMode::kGemm),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      batch_stride_A(0),
-      batch_stride_B(0),
-      batch_stride_C(0),
-      batch_stride_D(0),
-      semaphore(nullptr) { }
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(
       Arguments const &args,
       cutlass::gemm::GemmCoord const & grid_tiled_shape,
       int gemm_k_size,
       void *workspace = nullptr
```

## cutlass_library/source/include/cutlass/gemm/kernel/tile_scheduler_params.h

```diff
@@ -31,24 +31,14 @@
 
 #pragma once
 
 /*! \file
     \brief Parameters structures for persistent tile schedulers
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by this unit test: `cutlass_test_unit_core_cpp11`.
-*/
-
 #include "cutlass/coord.h"
 #include "cutlass/kernel_hardware_info.h"
 #include "cutlass/workspace.h"
 #include "cutlass/platform/platform.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm_coord.h"
 ////////////////////////////////////////////////////////////////////////////////
@@ -121,17 +111,17 @@
   initialize(
     dim3 problem_blocks,
     GemmCoord cluster_shape,
     KernelHardwareInfo const& hw_info,
     int max_swizzle_size,
     RasterOrderOptions raster_order_option
   ) {
-    
+
     CUTLASS_UNUSED(hw_info);
-    
+
     // Round up to nearest multiple of swizzle_size along each mode
     auto log_swizzle_size = get_log_swizzle_size(problem_blocks.x, problem_blocks.y, max_swizzle_size);
     auto problem_blocks_m = round_up(problem_blocks.x, (1 << log_swizzle_size) * cluster_shape.m());
     auto problem_blocks_n = round_up(problem_blocks.y, (1 << log_swizzle_size) * cluster_shape.n());
 
     problem_tiles_m_ = problem_blocks_m / cluster_shape.m();
     problem_tiles_n_ = problem_blocks_n / cluster_shape.n();
@@ -615,47 +605,37 @@
       cluster_shape,
       hw_info,
       max_swizzle,
       raster_order_option
     );
 
     uint64_t ctas_per_wave = grid.x * grid.y;
-
+    auto cluster_size = cluster_shape.m() * cluster_shape.n();
     // The number of output tiles to be computed in stream-K and data-parallel fashion, respectively.
-    uint32_t sk_tiles = get_num_sk_tiles(output_tiles, ctas_per_wave, k_tiles_per_output_tile, decomposition_mode);
+    uint32_t sk_tiles = get_num_sk_tiles(
+      output_tiles,
+      ctas_per_wave,
+      cluster_size,
+      k_tiles_per_output_tile,
+      decomposition_mode
+    );
     uint64_t dp_tiles = output_tiles - sk_tiles;
 
     // Calculate the number of work units covering the data-parallel and stream-K tiles.
     // A "work unit" is a single index in the linearized ID space used by the scheduler.
     // We distinguish it from a "block," which is typically tied to a hardware unit
     // (e.g., the callers into this scheduler will be persistent thread blocks).
     // A work unit can encompass multiple output tiles worth of work (as will be the
     // case for stream-K blocks).
     // Since splitting is not required for data-parallel tiles, only one data-parallel unit
     // is needed per data-parallel tile.
     uint64_t dp_units = dp_tiles;
 
-    // Number of k iterations computed by the stream-K units as a whole
-    uint64_t k_tiles_sk_total = k_tiles_per_output_tile * sk_tiles;
-
-    // If there are stream-K tiles to compute and a sufficiently large number of k iterations
-    // across them, they will be covered by a single wave of persistent threadblocks. Thus, there
-    // will be as many work units as there are threadblocks in a single wave.
-    //
-    // When the total k iterations across stream-K tiles is too small to justify distributing
-    // across an entire wave of blocks, we instead distribute the iterations over a smaller
-    // set of blocks.
-
-    // Calculate the number of stream-K units that would be needed if each stream-K unit
-    // computed the minimum allowable k iterations. Truncate this to be in units of clusters.
-    auto cluster_size = cluster_shape.m() * cluster_shape.n();
-    uint64_t min_sized_sk_units = (k_tiles_sk_total / min_iters_per_sk_unit_);
-    min_sized_sk_units = (min_sized_sk_units / cluster_size) * cluster_size;
-
-    uint64_t sk_units = platform::min(ctas_per_wave, min_sized_sk_units);
+    uint64_t ctas_per_sk_wave = ctas_per_wave;
+    uint64_t sk_units = get_num_sk_units(cluster_shape, ctas_per_sk_wave, sk_tiles, k_tiles_per_output_tile);
 
     if (decomposition_mode == DecompositionMode::DataParallel ||
         (decomposition_mode == DecompositionMode::Heuristic && sk_tiles == 0) ||
         sk_units == 0) {
       // Short circuit to basic data-parallel decomposition
       set_params_basic(
         underlying_params,
@@ -865,82 +845,100 @@
       /* truncate_by_problem_size = */false
     );
   }
 
   // Returns the number of stream-K tiles that will be computed amongst `output_tiles` total
   // output tiles on a device with `ctas_per_wave` CTAs in each wave.
   static uint32_t
-  get_num_sk_tiles(uint64_t output_tiles, uint64_t ctas_per_wave, uint32_t k_tiles_per_output_tile, DecompositionMode decomposition_mode) {
+  get_num_sk_tiles(
+    uint64_t output_tiles,
+    uint64_t ctas_per_wave,
+    uint64_t cluster_size,
+    uint32_t k_tiles_per_output_tile,
+    DecompositionMode decomposition_mode
+  ) {
     uint32_t full_waves = static_cast<uint32_t>(output_tiles / ctas_per_wave);
     uint32_t total_waves = static_cast<uint32_t>((output_tiles + ctas_per_wave - 1) / ctas_per_wave);
 
     if (decomposition_mode == DecompositionMode::DataParallel ||
         decomposition_mode == DecompositionMode::SplitK) {
       return 0;
     }
 
+    // If there is wave quantization, assign the first two waves worth of tiles to be
+    // covered by stream-K work and the remainder to be data-parallel. Since we know
+    // that full_waves == total_waves - 1 in this case, the number of data-parallel
+    // waves is simply full_waves-1 (unless full_waves == 0).
+    uint32_t dp_waves = full_waves > 1 ? full_waves - 1 : 0;
+    uint64_t dp_tiles = dp_waves * ctas_per_wave;
+    uint64_t sk_tiles = output_tiles - dp_tiles;
+
     if (decomposition_mode == DecompositionMode::Heuristic) {
       if (full_waves == total_waves || k_tiles_per_output_tile <= min_iters_per_sk_unit_) {
         // All tiles will be data-parallel tiles if there is either no quantization
         // or if there is no work to be split.
         return 0;
       }
 
       //
       // The final wave is not full. Perform some stream-K work.
       //
 
       // Rudimentary heuristic: prefer data-parallel decomposition if we have more than
       // one wave and the tail wave is more than half full. This is subject to change.
       uint64_t tail_tiles = output_tiles - (full_waves * ctas_per_wave);
-      if (tail_tiles >= (ctas_per_wave / 2)) {
+      if (2 * tail_tiles >= ctas_per_wave) {
         return 0;
       }
     }
 
-    // If there is wave quantization, assign the first two waves worth of tiles to be
-    // covered by stream-K work and the remainder to be data-parallel. Since we know
-    // that full_waves == total_waves - 1 in this case, the number of data-parallel
-    // waves is simply full_waves-1 (unless full_waves == 0).
-    uint32_t dp_waves = full_waves > 0 ? full_waves - 1 : 0;
-
-    uint64_t dp_tiles = dp_waves * ctas_per_wave;
-    return static_cast<uint32_t>(output_tiles - dp_tiles);
+    return static_cast<uint32_t>(sk_tiles);
   }
 
   CUTLASS_HOST_DEVICE
   static uint64_t
-  get_num_sk_units(GemmCoord cluster_shape, uint64_t ctas_per_wave, uint32_t sk_tiles, uint32_t k_tiles_per_output_tile) {
+  get_num_sk_units(GemmCoord cluster_shape, uint64_t ctas_per_sk_wave, uint32_t sk_tiles, uint32_t k_tiles_per_output_tile) {
+    // If there are stream-K tiles to compute and a sufficiently large number of k iterations
+    // across them, they will be covered by a single wave of persistent threadblocks. Thus, there
+    // will be as many work units as there are threadblocks in a single wave.
+    //
+    // When the total k iterations across stream-K tiles is too small to justify distributing
+    // across an entire wave of blocks, we instead distribute the iterations over a smaller
+    // set of blocks.
+
+    // Calculate the number of stream-K units that would be needed if each stream-K unit
+    // computed the minimum allowable k iterations. Truncate this to be in units of clusters.
+
     // Number of k iterations computed by the stream-K units as a whole
     uint64_t k_tiles_sk_total = k_tiles_per_output_tile * sk_tiles;
 
     // Calculate the number of stream-K units that would be needed if each stream-K unit
     // computed the minimum allowable k iterations. Truncate this to be in units of clusters.
     auto cluster_size = cluster_shape.m() * cluster_shape.n();
     uint64_t min_sized_sk_units = (k_tiles_sk_total / min_iters_per_sk_unit_);
     min_sized_sk_units = (min_sized_sk_units / cluster_size) * cluster_size;
 
-    uint64_t sk_units = platform::min(ctas_per_wave, min_sized_sk_units);
+    uint64_t sk_units = platform::min(ctas_per_sk_wave, min_sized_sk_units);
     return sk_units;
   }
 
   // Calculates the size of the workspace needed for holding reduction barriers
   CUTLASS_HOST_DEVICE
   static int
   get_barrier_workspace_size(uint64_t num_tiles, uint32_t mma_warp_groups, uint32_t barrier_bits) {
     auto workspace_bits = num_tiles * mma_warp_groups * barrier_bits;
     return round_up_to_l2_alignment(bits_to_bytes(static_cast<int>(workspace_bits)));
   }
 
   // Calculates the size of the workspace needed for holding partial outputs from splits
   CUTLASS_HOST_DEVICE
   static int
-  get_reduction_workspace_size(uint64_t num_tiles, GemmCoord tile_shape, uint32_t accumulator_bits) {
+  get_reduction_workspace_size(uint64_t num_tiles, GemmCoord tile_shape, uint32_t accumulator_bits, uint32_t num_accumulator_mtxs = 1) {
     auto output_tile_size = tile_shape.m() * tile_shape.n();
-    auto workspace_bits = accumulator_bits * output_tile_size * num_tiles;
+    auto workspace_bits = accumulator_bits * output_tile_size * num_tiles * num_accumulator_mtxs;
     return round_up_to_l2_alignment(bits_to_bytes(static_cast<int>(workspace_bits)));
   }
 
   #if !defined(__CUDACC_RTC__)
   static void
   get_workspace_component_sizes(
     dim3 problem_blocks,
@@ -953,15 +951,16 @@
     int splits,
     int max_swizzle,
     RasterOrderOptions raster_order_option,
     DecompositionMode decomposition_mode,
     uint32_t mma_warp_groups,
     uint32_t barrier_bits,
     uint32_t accumulator_bits,
-    uint32_t epilogue_subtile = 1) {
+    uint32_t epilogue_subtile = 1,
+    uint32_t num_accumulator_mtxs = 1) {
 
     auto log_swizzle_size = UnderlyingParams::get_log_swizzle_size(problem_blocks.x, problem_blocks.y, max_swizzle);
     problem_blocks.x = round_up(problem_blocks.x, (1 << log_swizzle_size) * cluster_shape.m());
     problem_blocks.y = round_up(problem_blocks.y, (1 << log_swizzle_size) * cluster_shape.n());
 
     // Workspace is needed only for output tiles that will be split. Thus, we first determine the number
     // of output tiles that will be split, and then calculate the workspace needed to cover these.
@@ -971,15 +970,15 @@
       barrier_workspace_size = 0;
       reduction_workspace_size = 0;
     }
     else if (decomposition_mode == DecompositionMode::SplitK ||
         (decomposition_mode == DecompositionMode::Heuristic && splits > 1)) {
       // Basic split-K variant requires workspace for all output tiles
       barrier_workspace_size = get_barrier_workspace_size(output_tiles, mma_warp_groups, barrier_bits);
-      reduction_workspace_size = get_reduction_workspace_size(output_tiles, tile_shape, accumulator_bits);
+      reduction_workspace_size = get_reduction_workspace_size(output_tiles, tile_shape, accumulator_bits, num_accumulator_mtxs);
     }
     else {
       KernelHardwareInfo new_hw_info;
       new_hw_info.device_id = hw_info.device_id;
       new_hw_info.sm_count = hw_info.sm_count;
       if (new_hw_info.sm_count <= 0) {
         CUTLASS_TRACE_HOST("  WARNING: Arguments do not include a valid SM count.\n"
@@ -991,16 +990,24 @@
         problem_blocks,
         cluster_shape,
         new_hw_info,
         max_swizzle,
         raster_order_option
       );
       uint64_t ctas_per_wave = grid.x * grid.y;
-      uint32_t sk_tiles = get_num_sk_tiles(output_tiles, ctas_per_wave, static_cast<uint32_t>(k_tiles_per_output_tile), decomposition_mode);
-      uint64_t sk_units = get_num_sk_units(cluster_shape, ctas_per_wave, sk_tiles, k_tiles_per_output_tile);
+      uint64_t cluster_size = cluster_shape.m() * cluster_shape.n();
+      uint32_t sk_tiles = get_num_sk_tiles(
+        output_tiles,
+        ctas_per_wave,
+        cluster_size,
+        static_cast<uint32_t>(k_tiles_per_output_tile),
+        decomposition_mode
+      );
+      uint64_t ctas_per_sk_wave = ctas_per_wave;
+      uint64_t sk_units = get_num_sk_units(cluster_shape, ctas_per_sk_wave, sk_tiles, k_tiles_per_output_tile);
       uint64_t dp_tiles = output_tiles - sk_tiles;
 
       uint64_t reduction_tiles = sk_tiles;
       if (should_perform_separate_reduction(epilogue_subtile, sk_units, sk_tiles, dp_tiles, ctas_per_wave)) {
         // In separate reduction, each peer writes to its own location in scratch space.
         // Thus, for separate reduction, we need as many reduction tiles per output tile
         // as there are the maximum number of peers that can collaborate on an output tile.
@@ -1008,15 +1015,15 @@
       }
 
       // Though separate reduction requires a larger reduction workspace, only one barrier
       // is needed per output tile. Each peer will increment the barrier by one once the peer has
       // written its accumulator to scratch space. The separate reduction unit will only begin
       // performing the reduction when the barrier has reached the number of peers for the output tile.
       barrier_workspace_size = get_barrier_workspace_size(sk_tiles, mma_warp_groups, barrier_bits);
-      reduction_workspace_size = get_reduction_workspace_size(reduction_tiles, tile_shape, accumulator_bits);
+      reduction_workspace_size = get_reduction_workspace_size(reduction_tiles, tile_shape, accumulator_bits, num_accumulator_mtxs);
     }
   }
   #endif // !defined(__CUDACC_RTC__)
 
   // Returns whether the kernel is configured in a manner for which separate reduction should be used
   CUTLASS_HOST_DEVICE
   static bool
@@ -1026,28 +1033,29 @@
     // multiple of sk_tiles, will choose basic split-k path instead of separate reduction for now.
     return (epilogue_subtile != 1) && (dp_tiles == 0) && (sk_units > 2u * sk_tiles) &&
            (sk_units + sk_tiles * epilogue_subtile <= ctas_per_wave);
   }
 
   // Get the amount of scratch workspace needed for the kernel. This variant of the method should only be used when
   // problem_shape and tile_shape contain modes of only rank 1.
-  static int
+  static size_t
   get_workspace_size(
     BatchedGemmCoord problem_shape,
     GemmCoord tile_shape,
     GemmCoord cluster_shape,
     KernelHardwareInfo const& hw_info,
     int splits,
     int max_swizzle,
     RasterOrderOptions raster_order_option,
     DecompositionMode decomposition_mode,
     uint32_t mma_warp_groups,
     uint32_t barrier_bits,
     uint32_t element_accumulator_bits,
-    uint32_t epilogue_subtile) {
+    uint32_t epilogue_subtile,
+    uint32_t num_accumulator_mtxs) {
 
     dim3 problem_blocks = UnderlyingParams::get_tiled_cta_shape_mnl(problem_shape, tile_shape, cluster_shape);
     uint32_t k_tiles_per_output_tile = (problem_shape.k() + tile_shape.k() - 1) / tile_shape.k();
 
     return get_workspace_size(
       problem_blocks,
       k_tiles_per_output_tile,
@@ -1057,36 +1065,38 @@
       splits,
       max_swizzle,
       raster_order_option,
       decomposition_mode,
       mma_warp_groups,
       barrier_bits,
       element_accumulator_bits,
-      epilogue_subtile
+      epilogue_subtile,
+      num_accumulator_mtxs
     );
   }
 
   // Version of get_workspace_size that takes in as input the number of CTAs in the M and N dimensions.
   // This is useful for calculating the tiled shape when a mode of problem and/or CTA shape has rank > 1,
   // for which using CuTe algebra for calculating tile shapes is easiest.
-  static int
+  static size_t
   get_workspace_size(
     dim3 problem_blocks,
     uint32_t k_tiles_per_output_tile,
     GemmCoord tile_shape,
     GemmCoord cluster_shape,
     KernelHardwareInfo const& hw_info,
     int splits,
     int max_swizzle,
     RasterOrderOptions raster_order_option,
     DecompositionMode decomposition_mode,
     uint32_t mma_warp_groups,
     uint32_t barrier_bits,
     uint32_t element_accumulator_bits,
-    uint32_t epilogue_subtile = 1) {
+    uint32_t epilogue_subtile = 1,
+    uint32_t num_accumulator_mtxs = 1) {
 
     int barrier_workspace_size = 0;
     int reduction_workspace_size = 0;
 
     #if !defined(__CUDACC_RTC__)
       get_workspace_component_sizes(
         problem_blocks,
@@ -1099,15 +1109,16 @@
         splits,
         max_swizzle,
         raster_order_option,
         decomposition_mode,
         mma_warp_groups,
         barrier_bits,
         element_accumulator_bits,
-        epilogue_subtile
+        epilogue_subtile,
+        num_accumulator_mtxs
       );
     #endif
 
     return barrier_workspace_size + reduction_workspace_size;
   }
 
   // Initialize the workspace to be used for the kernel. This variant of the method should only be used when
@@ -1166,15 +1177,16 @@
     int splits,
     int max_swizzle,
     RasterOrderOptions raster_order_option,
     DecompositionMode decomposition_mode,
     uint32_t mma_warp_groups,
     uint32_t barrier_bits,
     uint32_t element_accumulator_bits,
-    uint32_t epilogue_subtile = 1) {
+    uint32_t epilogue_subtile = 1,
+    uint32_t num_accumulator_mtxs = 1) {
 
     #if !defined(__CUDACC_RTC__)
       int barrier_workspace_size = 0;
       int reduction_workspace_size = 0;
 
       get_workspace_component_sizes(
         problem_blocks,
@@ -1187,15 +1199,16 @@
         splits,
         max_swizzle,
         raster_order_option,
         decomposition_mode,
         mma_warp_groups,
         barrier_bits,
         element_accumulator_bits,
-        epilogue_subtile
+        epilogue_subtile,
+        num_accumulator_mtxs
       );
 
       if (barrier_workspace_size > 0) {
         if (workspace == nullptr) {
           return Status::kErrorWorkspaceNull;
         }
 
@@ -1297,17 +1310,17 @@
     ProblemShape const* host_problem_shapes,
     GemmCoord cta_shape,
     GemmCoord cluster_shape,
     KernelHardwareInfo const& hw_info,
     int max_swizzle_size,
     RasterOrderOptions raster_order_option
   ) {
-    
+
     CUTLASS_UNUSED(hw_info);
-    
+
     // Round up to nearest multiple of swizzle_size along each mode
     auto log_swizzle_size = get_log_swizzle_size(problem_blocks.x, problem_blocks.y, max_swizzle_size);
     auto problem_blocks_m = round_up(problem_blocks.x, (1 << log_swizzle_size) * cluster_shape.m());
     auto problem_blocks_n = round_up(problem_blocks.y, (1 << log_swizzle_size) * cluster_shape.n());
 
     RasterOrder raster_order = get_rasterization_order(
       problem_blocks_m,
```

## cutlass_library/source/include/cutlass/gemm/kernel/trmm_universal.h

```diff
@@ -106,40 +106,37 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;
-
-    typename EpilogueOutputOp::Params epilogue;
-
-    void const * ptr_A;
-    void const * ptr_B;
-    void * ptr_D;
-
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_D;
-
-    typename LayoutA::Stride::Index lda;
-    typename LayoutB::Stride::Index ldb;
-    typename LayoutC::Stride::Index ldd;
+    GemmUniversalMode mode{GemmUniversalMode::kGemm};
+    GemmCoord problem_size{};
+    int batch_count{1};
+
+    typename EpilogueOutputOp::Params epilogue{};
+
+    void const * ptr_A{nullptr};
+    void const * ptr_B{nullptr};
+    void * ptr_D{nullptr};
+
+    int64_t batch_stride_A{0};
+    int64_t batch_stride_B{0};
+    int64_t batch_stride_D{0};
+
+    typename LayoutA::Stride::Index lda{0};
+    typename LayoutB::Stride::Index ldb{0};
+    typename LayoutC::Stride::Index ldd{0};
 
     //
     // Methods
     //
-    
-    Arguments(): 
-      mode(GemmUniversalMode::kGemm), 
-      batch_count(1), 
-      ptr_A(nullptr), ptr_B(nullptr), ptr_D(nullptr) { }
+
+    Arguments() = default;
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -157,15 +154,15 @@
       problem_size(problem_size),
       batch_count(batch_count),
       epilogue(epilogue), 
       ptr_A(ptr_A), ptr_B(ptr_B), ptr_D(ptr_D), 
       batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_D(batch_stride_D), 
       lda(lda), ldb(ldb), ldd(ldd) {
       }
-
+    
     /// Returns arguments for the transposed problem sizes
     Arguments transposed_problem_size() const {
       Arguments args(*this);
 
       std::swap(args.problem_size.m(), args.problem_size.n());
 
       return args;
@@ -186,58 +183,42 @@
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
   struct Params {
 
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
+    cutlass::gemm::GemmCoord problem_size{};
+    cutlass::gemm::GemmCoord grid_tiled_shape{};
+    int swizzle_log_tile{0};
    
-    typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorB::Params params_B;
-    typename Epilogue::OutputTileIterator::Params params_D;
+    typename Mma::IteratorA::Params params_A{};
+    typename Mma::IteratorB::Params params_B{};
+    typename Epilogue::OutputTileIterator::Params params_D{};
     
-    typename EpilogueOutputOp::Params output_op;
+    typename EpilogueOutputOp::Params output_op{};
 
-    GemmUniversalMode mode;
-    int batch_count;
-    int gemm_k_size;
+    GemmUniversalMode mode = cutlass::gemm::GemmUniversalMode::kGemm;
+    int batch_count {0};
+    int gemm_k_size {0};
 
-    void * ptr_A;
-    void * ptr_B;
-    void * ptr_D;
+    void * ptr_A{nullptr};
+    void * ptr_B{nullptr};
+    void * ptr_D{nullptr};
 
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_D;
+    int64_t batch_stride_A {0};
+    int64_t batch_stride_B {0};
+    int64_t batch_stride_D {0};
 
-    int *semaphore;
+    int *semaphore{nullptr};
 
     //
     // Methods
     //
-
-    CUTLASS_HOST_DEVICE
-    Params():
-      swizzle_log_tile(0),
-      params_A(0),
-      params_B(0),
-      params_D(0),
-      batch_count(0),
-      gemm_k_size(0),
-      mode(cutlass::gemm::GemmUniversalMode::kGemm),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_D(nullptr),
-      batch_stride_A(0),
-      batch_stride_B(0),
-      batch_stride_D(0),
-      semaphore(nullptr) { }
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(
       Arguments const &args,
       cutlass::gemm::GemmCoord const & grid_tiled_shape,
       int gemm_k_size,
       void *workspace = nullptr
```

## cutlass_library/source/include/cutlass/gemm/thread/mma_sm60.h

```diff
@@ -143,25 +143,23 @@
 
       CUTLASS_PRAGMA_UNROLL
       for(auto m=0; m < Shape::kM / Mma::Shape::kM; m++){
 
         CUTLASS_PRAGMA_UNROLL
         for(auto n=0; n < Shape::kN / Mma::Shape::kN; n++){
 
-            Array<half_t, 2> tmp;
-            Array<half_t, 2> *ptr_tmp = &tmp;
-            ptr_tmp[0] = ptr_D[n*Shape::kM/2 + m];
+            Array<half_t, 2> tmp { ptr_D[n*Shape::kM/2 + m] };
 
             mma(
                 tmp,
                 ptr_A[k*Shape::kM/2 + m],
                 ptr_B[n*Shape::kK + k],
                 tmp);
 
-            ptr_D[n*Shape::kM/2 + m] = ptr_tmp[0];
+            ptr_D[n*Shape::kM/2 + m] = tmp;
         }
       }
     }
   }
 };
 
 /////////////////////////////
@@ -235,29 +233,27 @@
 
         CUTLASS_PRAGMA_UNROLL
         for(auto n=0; n < Shape::kN / Mma::Shape::kN; n++){
 
           CUTLASS_PRAGMA_UNROLL
           for(auto m=0; m < Shape::kM / Mma::Shape::kM; m++){
 
-            Array<half_t, 2> tmp;
-            Array<half_t, 2> *ptr_tmp = &tmp;
-            ptr_tmp[0] = ptr_D[m*Shape::kN/2 + n];
+            Array<half_t, 2> tmp { ptr_D[m*Shape::kN/2 + n] };
 
             Array<half_t, 2> tmp_B;
             tmp_B[0] = ptr_B->at(2*n*Shape::kK + k);
             tmp_B[1] = ptr_B->at((2*n+1)*Shape::kK + k);
 
             mma(
                 tmp,
                 ptr_A[k*Shape::kM + m],
                 tmp_B,
                 tmp);
 
-            ptr_D[m*Shape::kN/2 + n] = ptr_tmp[0];
+            ptr_D[m*Shape::kN/2 + n] = tmp;
         }
       }
     }
   }
 };
 
 
@@ -331,26 +327,23 @@
 
         CUTLASS_PRAGMA_UNROLL
         for (int m = 0; m < Shape::kM / Mma::Shape::kM; ++m) {
 
           CUTLASS_PRAGMA_UNROLL
           for (int n = 0; n < Shape::kN / Mma::Shape::kN; ++n) {
 
-          Array<half_t, 2> tmp;
-          Array<half_t, 2> *ptr_tmp = &tmp;
-
-          ptr_tmp[0] = ptr_D[m + n * Shape::kM/2];
+          Array<half_t, 2> tmp { ptr_D[m + n * Shape::kM/2] };
 
           mma(
             tmp,
             ptr_A[m + k * Shape::kM/2],
             ptr_B[k * Shape::kN + n],
             tmp);
 
-          ptr_D[m + n * Shape::kM/2] = ptr_tmp[0];
+          ptr_D[m + n * Shape::kM/2] = tmp;
         }
       }
     }
   }
 };
 
 /////////////////////////////
@@ -424,25 +417,23 @@
 
         CUTLASS_PRAGMA_UNROLL
         for(auto n=0; n < Shape::kN / Mma::Shape::kN; n++){
 
           CUTLASS_PRAGMA_UNROLL
           for(auto m=0; m < Shape::kM / Mma::Shape::kM; m++){
 
-            Array<half_t, 2> tmp;
-            Array<half_t, 2> *ptr_tmp = &tmp;
-            ptr_tmp[0] = ptr_D[m*Shape::kN/2 + n];
+            Array<half_t, 2> tmp { ptr_D[m*Shape::kN/2 + n] };
 
             mma(
                 tmp,
                 ptr_A[k*Shape::kM + m],
                 ptr_B[k*Shape::kN/2 + n],
                 tmp);
 
-            ptr_D[m*Shape::kN/2 + n] = ptr_tmp[0];
+            ptr_D[m*Shape::kN/2 + n] = tmp;
         }
       }
     }
   }
 };
 
 
@@ -517,29 +508,27 @@
 
       CUTLASS_PRAGMA_UNROLL
       for(auto m=0; m < Shape::kM / Mma::Shape::kM; m++){
 
         CUTLASS_PRAGMA_UNROLL
         for(auto n=0; n < Shape::kN / Mma::Shape::kN; n++){
 
-            Array<half_t, 2> tmp;
-            Array<half_t, 2> *ptr_tmp = &tmp;
-            ptr_tmp[0] = ptr_D[n*Shape::kM/2 + m];
+            Array<half_t, 2> tmp { ptr_D[n*Shape::kM/2 + m] };
 
             Array<half_t, 2> tmp_A;
             tmp_A[0] = ptr_A->at(2*m*Shape::kK + k);
             tmp_A[1] = ptr_A->at((2*m+1)*Shape::kK + k);
 
             mma(
                 tmp,
                 tmp_A,
                 ptr_B[n*Shape::kK + k],
                 tmp);
 
-            ptr_D[n*Shape::kM/2 + m] = ptr_tmp[0];
+            ptr_D[n*Shape::kM/2 + m] = tmp;
         }
       }
     }
   }
 };
 
 /////////////////////////////
@@ -613,29 +602,27 @@
 
         CUTLASS_PRAGMA_UNROLL
         for(auto n=0; n < Shape::kN / Mma::Shape::kN; n++){
 
           CUTLASS_PRAGMA_UNROLL
           for(auto m=0; m < Shape::kM / Mma::Shape::kM; m++){
 
-            Array<half_t, 2> tmp;
-            Array<half_t, 2> *ptr_tmp = &tmp;
-            ptr_tmp[0] = ptr_D[m*Shape::kN/2 + n];
+            Array<half_t, 2> tmp { ptr_D[m*Shape::kN/2 + n] };
 
             Array<half_t, 2> tmp_B;
             tmp_B[0] = ptr_B->at(2*n*Shape::kK + k);
             tmp_B[1] = ptr_B->at((2*n+1)*Shape::kK + k);
 
             mma(
                 tmp,
                 ptr_A[m*Shape::kK + k],
                 tmp_B,
                 tmp);
 
-            ptr_D[m*Shape::kN/2 + n] = ptr_tmp[0];
+            ptr_D[m*Shape::kN/2 + n] = tmp;
         }
       }
     }
   }
 };
 
 /////////////////////////////
@@ -709,29 +696,27 @@
 
       CUTLASS_PRAGMA_UNROLL
       for(auto m=0; m < Shape::kM / Mma::Shape::kM; m++){
 
         CUTLASS_PRAGMA_UNROLL
         for(auto n=0; n < Shape::kN / Mma::Shape::kN; n++){
 
-            Array<half_t, 2> tmp;
-            Array<half_t, 2> *ptr_tmp = &tmp;
-            ptr_tmp[0] = ptr_D[n*Shape::kM/2 + m];
+            Array<half_t, 2> tmp { ptr_D[n*Shape::kM/2 + m] };
 
             Array<half_t, 2> tmp_A;
             tmp_A[0] = ptr_A->at(2*m*Shape::kK + k);
             tmp_A[1] = ptr_A->at((2*m+1)*Shape::kK + k);
 
             mma(
                 tmp,
                 tmp_A,
                 ptr_B[k*Shape::kN + n],
                 tmp);
 
-            ptr_D[n*Shape::kM/2 + m] = ptr_tmp[0];
+            ptr_D[n*Shape::kM/2 + m] = tmp;
         }
       }
     }
   }
 };
 
 
@@ -806,25 +791,23 @@
 
         CUTLASS_PRAGMA_UNROLL
         for(auto n=0; n < Shape::kN / Mma::Shape::kN; n++){
 
           CUTLASS_PRAGMA_UNROLL
           for(auto m=0; m < Shape::kM / Mma::Shape::kM; m++){
 
-            Array<half_t, 2> tmp;
-            Array<half_t, 2> *ptr_tmp = &tmp;
-            ptr_tmp[0] = ptr_D[m*Shape::kN/2 + n];
+            Array<half_t, 2> tmp { ptr_D[m*Shape::kN/2 + n] };
 
             mma(
                 tmp,
                 ptr_A[m*Shape::kK + k],
                 ptr_B[k*Shape::kN/2 + n],
                 tmp);
 
-            ptr_D[m*Shape::kN/2 + n] = ptr_tmp[0];
+            ptr_D[m*Shape::kN/2 + n] = tmp;
         }
       }
     }
   }
 };
 
 /////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sm75.h

```diff
@@ -121,35 +121,52 @@
 
   /// Size of a threadblock-scoped access
   static int const kAccessSizeInBits = 128;
 
   /// Default Operator
   using Operator = Operator_;
 
+  // Warp thread arrangement
+  static int const kWarpThreadArrangementContiguousA =
+      platform::min(Shape::kM / (kAccessSizeInBits / sizeof_bits<ElementA>::value), 8);
+
+  static int const kWarpThreadArrangementStridedA =
+      kWarpSize / kWarpThreadArrangementContiguousA;
+
+  static int const kWarpThreadArrangementContiguousB =
+      platform::min(Shape::kN / (kAccessSizeInBits / sizeof_bits<ElementB>::value), 8);
+
+  static int const kWarpThreadArrangementStridedB =
+      kWarpSize / kWarpThreadArrangementContiguousB;
+
   //
   // Shared memory layouts
   //
-
+  static int const Crosswise_A = platform::min(int(128 / sizeof(ElementA)),
+                                               Shape::kM);
   using SmemLayoutA = 
     layout::ColumnMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<ElementA>::value, int(128 / sizeof(ElementA))>;
+      sizeof_bits<ElementA>::value, Crosswise_A>;
 
   // Shared memory layout
+  static int const Crosswise_B = platform::min(int(128 / sizeof(ElementB)),
+                                               Shape::kN);
   using SmemLayoutB = layout::RowMajorTensorOpMultiplicandCongruous<
-    sizeof_bits<ElementB>::value, int(128 / sizeof(ElementB))>;
+    sizeof_bits<ElementB>::value, Crosswise_B>;
 
   //
   // Iterators to write to shared memory
   //
 
   /// ThreadMap of iterator A
   using IteratorThreadMapA = transform::PitchLinearWarpRakedThreadMap<
     layout::PitchLinearShape<Shape::kM, Shape::kK>,
     kThreads,
-    layout::PitchLinearShape<8, 4>,
+    layout::PitchLinearShape<kWarpThreadArrangementContiguousA,
+                             kWarpThreadArrangementStridedA>,
     kAccessSizeInBits / sizeof_bits<ElementA>::value
   >;
 
   /// Shared memory iterator to A operand
   using SmemIteratorA = transform::threadblock::RegularTileIterator<
     MatrixShape<Shape::kM, Shape::kK>, 
     ElementA, 
@@ -158,15 +175,16 @@
     IteratorThreadMapA
   >;
 
   /// ThreadMap of iterator B
   using IteratorThreadMapB = transform::PitchLinearWarpRakedThreadMap<
     layout::PitchLinearShape<Shape::kN, Shape::kK>,
     kThreads,
-    layout::PitchLinearShape<8, 4>,
+    layout::PitchLinearShape<kWarpThreadArrangementContiguousB,
+                             kWarpThreadArrangementStridedB>,
     kAccessSizeInBits / sizeof_bits<ElementB>::value
   >;
 
   /// Shared memory iterator to B operand
   using SmemIteratorB = transform::threadblock::RegularTileIterator<
     MatrixShape<Shape::kK, Shape::kN>, 
     ElementB, 
@@ -410,24 +428,33 @@
   // Warp thread arrangement 
   static int const kWarpThreadArrangementContiguousA =
       Shape::kK / (kAccessSizeInBits / sizeof_bits<ElementA>::value);
 
   static int const kWarpThreadArrangementStridedA =
       kWarpSize / kWarpThreadArrangementContiguousA;
 
+  static int const kWarpThreadArrangementContiguousB =
+      platform::min(Shape::kN / (kAccessSizeInBits / sizeof_bits<ElementB>::value), 8);
+
+  static int const kWarpThreadArrangementStridedB =
+      kWarpSize / kWarpThreadArrangementContiguousB;
+
   //
   // Shared memory layouts
   //
 
   using SmemLayoutA = layout::RowMajorTensorOpMultiplicandCrosswise<
       sizeof_bits<ElementA>::value, Shape::kK>;
 
   // Shared memory layout
+  static int const Crosswise_B = platform::min(int(128 / sizeof(ElementB)),
+                                               Shape::kN);
+
   using SmemLayoutB = layout::RowMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<ElementB>::value, int(128 / sizeof(ElementB))>;
+      sizeof_bits<ElementB>::value, Crosswise_B>;
 
   //
   // Iterators to write to shared memory
   //
 
   /// ThreadMap of iterator A
   using IteratorThreadMapA = transform::PitchLinearWarpRakedThreadMap<
@@ -445,15 +472,16 @@
     IteratorThreadMapA
   >;
 
   /// ThreadMap of iterator B
   using IteratorThreadMapB = transform::PitchLinearWarpRakedThreadMap<
     layout::PitchLinearShape<Shape::kN, Shape::kK>,
     kThreads,
-    layout::PitchLinearShape<8, 4>,
+    layout::PitchLinearShape<kWarpThreadArrangementContiguousB,
+                             kWarpThreadArrangementStridedB>,
     kAccessSizeInBits / sizeof_bits<ElementB>::value
   >;
 
   /// Shared memory iterator to B operand
   using SmemIteratorB = transform::threadblock::RegularTileIterator<
     MatrixShape<Shape::kK, Shape::kN>, 
     ElementB, 
@@ -541,39 +569,47 @@
   /// Size of a threadblock-scoped access
   static int const kAccessSizeInBits = 128;
 
   /// Default Operator
   using Operator = Operator_; 
 
   // Warp thread arrangement 
+  static int const kWarpThreadArrangementContiguousA =
+      platform::min(Shape::kM / (kAccessSizeInBits / sizeof_bits<ElementA>::value), 8);
+
+  static int const kWarpThreadArrangementStridedA =
+      kWarpSize / kWarpThreadArrangementContiguousA;
+
   static int const kWarpThreadArrangementContiguousB =
       Shape::kK / (kAccessSizeInBits / sizeof_bits<ElementA>::value);
 
   static int const kWarpThreadArrangementStridedB =
       kWarpSize / kWarpThreadArrangementContiguousB;
 
   //
   // Shared memory layouts
   //
-
+  static int const Crosswise_A = platform::min(int(128 / sizeof(ElementA)),
+                                               Shape::kM);
   using SmemLayoutA = layout::ColumnMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<ElementA>::value, int(128 / sizeof(ElementA))>;
+      sizeof_bits<ElementA>::value, Crosswise_A>;
 
   // Shared memory layout
   using SmemLayoutB = layout::ColumnMajorTensorOpMultiplicandCrosswise<
       sizeof_bits<ElementB>::value, Shape::kK>;
 
   //
   // Iterators to write to shared memory
   //
 
   /// ThreadMap of iterator A
   using IteratorThreadMapA = transform::PitchLinearWarpRakedThreadMap<
       layout::PitchLinearShape<Shape::kM, Shape::kK>, kThreads,
-      layout::PitchLinearShape<8, 4>,
+      layout::PitchLinearShape<kWarpThreadArrangementContiguousA,
+                               kWarpThreadArrangementStridedA>,
       kAccessSizeInBits / sizeof_bits<ElementA>::value>;
 
   /// Shared memory iterator to A operand
   using SmemIteratorA = transform::threadblock::RegularTileIterator<
       MatrixShape<Shape::kM, Shape::kK>, ElementA, SmemLayoutA, 1,
       IteratorThreadMapA>;
```

## cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sm80.h

```diff
@@ -1281,44 +1281,62 @@
 
   /// Size of a threadblock-scoped access
   static int const kAccessSizeInBits = 128;
 
   /// Default Operator
   using Operator = Operator_;
 
+  // Warp thread arrangement
+  static int const kWarpThreadArrangementContiguousA =
+      platform::min(Shape::kM / (kAccessSizeInBits / sizeof_bits<ElementA>::value), 8);
+
+  static int const kWarpThreadArrangementStridedA =
+      kWarpSize / kWarpThreadArrangementContiguousA;
+
+  static int const kWarpThreadArrangementContiguousB =
+      platform::min(Shape::kN / (kAccessSizeInBits / sizeof_bits<ElementB>::value), 8);
+
+  static int const kWarpThreadArrangementStridedB =
+      kWarpSize / kWarpThreadArrangementContiguousB;
+
   //
   // Shared memory layouts
   //
-
+  static int const Crosswise_A = platform::min(int(128 / sizeof(ElementA)),
+                                               Shape::kM);
   using SmemLayoutA = layout::ColumnMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<ElementA>::value, int(128 / sizeof(ElementA))>;
+      sizeof_bits<ElementA>::value, Crosswise_A>;
 
   // Shared memory layout
+  static int const Crosswise_B = platform::min(int(128 / sizeof(ElementB)),
+                                               Shape::kN);
   using SmemLayoutB = layout::RowMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<ElementB>::value, int(128 / sizeof(ElementB))>;
+      sizeof_bits<ElementB>::value, Crosswise_B>;
 
   //
   // Iterators to write to shared memory
   //
 
   /// ThreadMap of iterator A
   using IteratorThreadMapA = transform::PitchLinearWarpRakedThreadMap<
       layout::PitchLinearShape<Shape::kM, Shape::kK>, kThreads,
-      layout::PitchLinearShape<8, 4>,
+      layout::PitchLinearShape<kWarpThreadArrangementContiguousA,
+                               kWarpThreadArrangementStridedA>,
       kAccessSizeInBits / sizeof_bits<ElementA>::value>;
 
   /// Shared memory iterator to A operand
   using SmemIteratorA = transform::threadblock::RegularTileAccessIterator<
       MatrixShape<Shape::kM, Shape::kK>, ElementA, SmemLayoutA, 1,
       IteratorThreadMapA>;
 
   /// ThreadMap of iterator B
   using IteratorThreadMapB = transform::PitchLinearWarpRakedThreadMap<
       layout::PitchLinearShape<Shape::kN, Shape::kK>, kThreads,
-      layout::PitchLinearShape<8, 4>,
+      layout::PitchLinearShape<kWarpThreadArrangementContiguousB,
+                               kWarpThreadArrangementStridedB>,
       kAccessSizeInBits / sizeof_bits<ElementB>::value>;
 
   /// Shared memory iterator to B operand
   using SmemIteratorB = transform::threadblock::RegularTileAccessIterator<
       MatrixShape<Shape::kK, Shape::kN>, ElementB, SmemLayoutB, 0,
       IteratorThreadMapB>;
 
@@ -1545,39 +1563,47 @@
   /// Size of a threadblock-scoped access
   static int const kAccessSizeInBits = 128;
 
   /// Default Operator
   using Operator = Operator_;
 
   // Warp thread arrangement
+  static int const kWarpThreadArrangementContiguousA =
+      platform::min(Shape::kM / (kAccessSizeInBits / sizeof_bits<ElementA>::value), 8);
+
+  static int const kWarpThreadArrangementStridedA =
+      kWarpSize / kWarpThreadArrangementContiguousA;
+
   static int const kWarpThreadArrangementContiguousB =
       Shape::kK / (kAccessSizeInBits / sizeof_bits<ElementA>::value);
 
   static int const kWarpThreadArrangementStridedB =
       kWarpSize / kWarpThreadArrangementContiguousB;
 
   //
   // Shared memory layouts
   //
-
+  static int const Crosswise_A = platform::min(int(128 / sizeof(ElementA)),
+                                               Shape::kM);
   using SmemLayoutA = layout::ColumnMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<ElementA>::value, int(128 / sizeof(ElementA))>;
+      sizeof_bits<ElementA>::value, Crosswise_A>;
 
   // Shared memory layout
   using SmemLayoutB = layout::ColumnMajorTensorOpMultiplicandCrosswise<
       sizeof_bits<ElementB>::value, Shape::kK>;
 
   //
   // Iterators to write to shared memory
   //
 
   /// ThreadMap of iterator A
   using IteratorThreadMapA = transform::PitchLinearWarpRakedThreadMap<
       layout::PitchLinearShape<Shape::kM, Shape::kK>, kThreads,
-      layout::PitchLinearShape<8, 4>,
+      layout::PitchLinearShape<kWarpThreadArrangementContiguousA,
+                               kWarpThreadArrangementStridedA>,
       kAccessSizeInBits / sizeof_bits<ElementA>::value>;
 
   /// Shared memory iterator to A operand
   using SmemIteratorA = transform::threadblock::RegularTileAccessIterator<
       MatrixShape<Shape::kM, Shape::kK>, ElementA, SmemLayoutA, 1,
       IteratorThreadMapA>;
 
@@ -1682,24 +1708,32 @@
   // Warp thread arrangement
   static int const kWarpThreadArrangementContiguousA =
       Shape::kK / (kAccessSizeInBits / sizeof_bits<ElementA>::value);
 
   static int const kWarpThreadArrangementStridedA =
       kWarpSize / kWarpThreadArrangementContiguousA;
 
+  static int const kWarpThreadArrangementContiguousB =
+      platform::min(Shape::kN / (kAccessSizeInBits / sizeof_bits<ElementB>::value), 8);
+
+  static int const kWarpThreadArrangementStridedB =
+      kWarpSize / kWarpThreadArrangementContiguousB;
+
   //
   // Shared memory layouts
   //
 
   using SmemLayoutA = layout::RowMajorTensorOpMultiplicandCrosswise<
       sizeof_bits<ElementA>::value, Shape::kK>;
 
   // Shared memory layout
+  static int const Crosswise_B = platform::min(int(128 / sizeof(ElementB)),
+                                               Shape::kN);
   using SmemLayoutB = layout::RowMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<ElementB>::value, int(128 / sizeof(ElementB))>;
+      sizeof_bits<ElementB>::value, Crosswise_B>;
 
   //
   // Iterators to write to shared memory
   //
 
   /// ThreadMap of iterator A
   using IteratorThreadMapA = transform::PitchLinearWarpRakedThreadMap<
@@ -1712,15 +1746,16 @@
   using SmemIteratorA = transform::threadblock::RegularTileAccessIterator<
       MatrixShape<Shape::kM, Shape::kK>, ElementA, SmemLayoutA, 0,
       IteratorThreadMapA>;
 
   /// ThreadMap of iterator B
   using IteratorThreadMapB = transform::PitchLinearWarpRakedThreadMap<
       layout::PitchLinearShape<Shape::kN, Shape::kK>, kThreads,
-      layout::PitchLinearShape<8, 4>,
+      layout::PitchLinearShape<kWarpThreadArrangementContiguousB,
+                               kWarpThreadArrangementStridedB>,
       kAccessSizeInBits / sizeof_bits<ElementB>::value>;
 
   /// Shared memory iterator to B operand
   using SmemIteratorB = transform::threadblock::RegularTileAccessIterator<
       MatrixShape<Shape::kK, Shape::kN>, ElementB, SmemLayoutB, 0,
       IteratorThreadMapB>;
```

## cutlass_library/source/include/cutlass/gemm/threadblock/mma_multistage.h

```diff
@@ -153,15 +153,15 @@
     static int const kAccessesPerGroupB =
         (AsyncCopyIterationsPerStageB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
 
     // Optional staged-accumulation (e.g., tf32x3 kernels) for improved numerical
     // accuracy, where each mainloop iteration first accumulates into a temporary
     // set of freshly-cleared accumulators, which are subsequently added to the
     // final accumulator set.
-    static bool const kStagedAccumulation = arch::UseStagedAccumulation<typename Operator::MathOperator>::value;
+    static bool const kStagedAccumulation = arch::detail::UseStagedAccumulation<Operator>::value;
   };
 
  private:
 
 
   // Structure encapsulating pipeline state live from one iteration to the next
   struct PipeState {
```

## cutlass_library/source/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h

```diff
@@ -28,24 +28,14 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Implements streamk threadblock mapping blockIdx to GEMM problems.
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/platform/platform.h"
 #include "cutlass/gemm/gemm_enumerated_types.h"
@@ -170,16 +160,15 @@
 
 
   //
   // Host+device interface
   //
 
   /// Constructor
-  CUTLASS_HOST_DEVICE
-  ThreadblockSwizzleStreamK() {}
+  ThreadblockSwizzleStreamK() = default;
 
   /// Returns the GEMM volume in thread block tiles
   CUTLASS_HOST_DEVICE
   GemmCoord tiled_shape() const
   {
     return GemmCoord(
         static_cast<int>(div_mod_tiled_shape_m),
```

## cutlass_library/source/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h

```diff
@@ -268,25 +268,25 @@
     "DefaultMmaTensorOp with arch::OpMultiplyAddMixedInputUpcast ElementA and ElementB cannot be of the same data type");
 
   // Data type used for internal computation - use the wider of the two data types for mma.sync operands
   using ElementOperand = typename platform::conditional<(sizeof(ElementA) > sizeof(ElementB)), 
                                                     ElementA, ElementB>::type;
 
   // Operand datatypes in the internal MMA instruction - use the wider of the two data types
-  using MmaElementA = ElementOperand;
-  using MmaElementB = ElementOperand;
+  using ElementAMma = ElementOperand;
+  using ElementBMma = ElementOperand;
   using MmaElementC = ElementC;
 
   // Uses 
   using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
       cutlass::arch::Mma<
         GemmShape<16, 8, 16>, 
         32, 
-        MmaElementA, cutlass::layout::RowMajor, 
-        MmaElementB, cutlass::layout::ColumnMajor,
+        ElementAMma, cutlass::layout::RowMajor, 
+        ElementBMma, cutlass::layout::ColumnMajor,
         MmaElementC, cutlass::layout::RowMajor, 
         arch::OpMultiplyAdd
       >,
       cutlass::MatrixShape<1, 1> >;
 
   // Define the warp-level tensor op
   using Type = cutlass::gemm::warp::MmaMixedInputTensorOp<
```

## cutlass_library/source/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h

```diff
@@ -370,18 +370,18 @@
   /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
   using Policy = Policy_;
 
   /// Underlying matrix multiply operator (concept: arch::Mma)
   using ArchMmaOperator = typename Policy::Operator;
 
   /// Underlying arch::Mma instruction datatype for A operand
-  using MmaElementA = typename ArchMmaOperator::ElementA;
+  using ElementAMma = typename ArchMmaOperator::ElementA;
 
   /// Underlying arch::Mma instruction datatype for B operand
-  using MmaElementB = typename ArchMmaOperator::ElementB;
+  using ElementBMma = typename ArchMmaOperator::ElementB;
 
   /// Underlying arch::Mma instruction datatype for C operand
   using MmaElementC = typename ArchMmaOperator::ElementC;
 
   /// Indicates math operator 
   using MathOperator = typename ArchMmaOperator::Operator;
 
@@ -404,30 +404,30 @@
   static int const kThreadCount = 32;
 
   /// Number of partitions along K dimension
   static int const kPartitionsK = PartitionsK_;
 
   /// 
   // static int const kLoadShapeK = InstructionShape::kK * 
-  //  (sizeof_bits<MmaElementA>::value / sizeof_bits<ElementB>::value);
+  //  (sizeof_bits<ElementAMma>::value / sizeof_bits<ElementB>::value);
 
 public:
 
   /// Iterates over the A operand in Shared Memory
   using IteratorA = MmaTensorOpMultiplicandTileIterator<
      MatrixShape<Shape::kM, Shape::kK>, Operand::kA, ElementA, LayoutA,
      MatrixShape<ArchMmaOperator::Shape::kM, ArchMmaOperator::Shape::kK>,
      Policy::OpDelta::kRow, kThreadCount, kPartitionsK>;
 
   /// Storage for A tile in registers (loaded from Shared Memory)
   using FragmentA = typename IteratorA::Fragment;
 
   /// Storage for transformed A tile in registers (for use in Mma instruction)
   using TransformedFragmentA =
-      Array<MmaElementA, FragmentA::kElements>;
+      Array<ElementAMma, FragmentA::kElements>;
 
   /// Underlying arch::Mma instruction operand fragement for matrix A
   using MmaOperandA = typename ArchMmaOperator::FragmentA;
 
   /// Iterates over the B operand in Shared Memory
   using IteratorB = MmaTensorOpMultiplicandTileIterator<
       MatrixShape<Shape::kK, Shape::kN>, Operand::kB, ElementB, LayoutB,
@@ -435,15 +435,15 @@
       Policy::OpDelta::kRow, kThreadCount, kPartitionsK>;
 
   /// Storage for B tile in registers (loaded from Shared Memory)
   using FragmentB = typename IteratorB::Fragment;
 
   /// Storage for transformed B tile in registers (for use in Mma instruction)
   using TransformedFragmentB =
-      Array<MmaElementB, FragmentB::kElements>;
+      Array<ElementBMma, FragmentB::kElements>;
 
   /// Underlying arch::Mma instruction operand fragement for matrix B
   using MmaOperandB = typename ArchMmaOperator::FragmentB;
 
   /// Iterates over the C operand in memory
   using IteratorC = MmaTensorOpAccumulatorTileIterator<
      MatrixShape<Shape::kM, Shape::kN>, ElementC, LayoutC,
@@ -519,38 +519,38 @@
   /// Transform the operand warp fragment register to the required data types and layout 
   /// for the `cultass::arch::Mma`
   CUTLASS_DEVICE
   void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
                  FragmentA const &A, FragmentB const &B) const {
 
     // Shuffle data within warp to obtain the mma.sync operand layout
-    detail::FragmentShuffler<MmaElementB, ElementB, MmaIterations::kColumn, 
+    detail::FragmentShuffler<ElementBMma, ElementB, MmaIterations::kColumn, 
              FragmentB::kElements, MmaOperandB::kElements, Operand::kB> shuffler_B;
     FragmentB tmp_B; 
     tmp_B = shuffler_B(B);
 
     // Convert the B operand to the Mma Instruction operand type
-    detail::FragmentConverter<MmaElementB, ElementB, FragmentB::kElements> convert_B;
+    detail::FragmentConverter<ElementBMma, ElementB, FragmentB::kElements> convert_B;
     dst_B = convert_B(tmp_B);
 
     FragmentA tmp_A;
 
     Array<ElementA, FragmentA::kElements / 2> *
         ptr_tmp_A = reinterpret_cast<Array<ElementA,
                                              FragmentA::kElements / 2> *>(&tmp_A);
-    Array<MmaElementA, FragmentA::kElements / 2> *
-        ptr_dst_A = reinterpret_cast<Array<MmaElementA,
+    Array<ElementAMma, FragmentA::kElements / 2> *
+        ptr_dst_A = reinterpret_cast<Array<ElementAMma,
                                              FragmentA::kElements / 2> *>(&dst_A);
 
     // Shuffle data within warp to obtain the mma.sync operand layout
-    detail::FragmentShuffler<MmaElementA, ElementA, MmaIterations::kRow,
+    detail::FragmentShuffler<ElementAMma, ElementA, MmaIterations::kRow,
              FragmentA::kElements, MmaOperandA::kElements, Operand::kA> shuffler_A;
 
     // Convert the A operand to the Mma Instruction operand type
-    detail::FragmentConverter<MmaElementA, ElementA, FragmentA::kElements / 2> convert_A;
+    detail::FragmentConverter<ElementAMma, ElementA, FragmentA::kElements / 2> convert_A;
 
     tmp_A = shuffler_A(A);
     ptr_dst_A[0] = convert_A(ptr_tmp_A[0]);
 
     ptr_dst_A[1] = convert_A(ptr_tmp_A[1]);
   }
 };
```

## cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h

```diff
@@ -230,15 +230,16 @@
 
   /// Constructor from TensorRef
   CUTLASS_DEVICE
   MmaTensorOpMultiplicandTileIterator(
     TensorRef const &ref, 
     int lane_id
   ):
-    stride_(ref.stride(0) / Layout::kElementsPerAccess), byte_offset_(0),
+    stride_(ref.stride(0) / Layout::kElementsPerAccess),
+    byte_offset_(0),
     k_group_idx_(0) {
       
     int quad_pair = (lane_id >> 3);
     int quad_quad = (lane_id >> 4);
     int lane_in_quad = (lane_id & 3);
     int lane_in_quad_pair = (lane_id & 7);
     int lane_in_quad_quad = (lane_id & 15);
@@ -252,16 +253,15 @@
       if (Policy::LdsmShape::kContiguous == 4) {
         // Matrix multiply 1688 A/B
         // Q0 Q1 Q2 Q3 (Q stands for 1 8x128bit block).
         // Four blocks are next to each other in the contiguous dimension.
         partition_contiguous_idx = ((lane_in_quad_pair >> 2) ^ i);
         access_contiguous_idx = (quad_pair ^ lane_in_quad);
         access_strided_idx = lane_in_quad_pair;
-      }
-      else if (Policy::LdsmShape::kContiguous == 2 &&
+      } else if (Policy::LdsmShape::kContiguous == 2 &&
                  kOperand == Operand::kA) {
         // Matrix multiply 16816 A
         // Q0 Q1
         // Q2 Q3
         partition_contiguous_idx = ((lane_in_quad_pair >> 2) ^ (i >> 1));
         access_contiguous_idx =
             (((quad_pair & 1) + ((i & 1) << 1)) ^ lane_in_quad);
@@ -276,17 +276,17 @@
         access_strided_idx = lane_in_quad_quad;
       } else if (Policy::LdsmShape::kContiguous == 1) {
         // Matrix multiply 16832.SP B
         // Q0
         // Q1
         // Q2
         // Q3
-        partition_contiguous_idx = ((lane_in_quad_pair >> 2) ^ (i >> 2)); 
-        access_contiguous_idx = ((i & 3) ^ lane_in_quad); 
-        access_strided_idx = lane_id; 
+        partition_contiguous_idx = ((lane_in_quad_pair >> 2) ^ (i >> 2));
+        access_contiguous_idx = ((i & 3) ^ lane_in_quad);
+        access_strided_idx = lane_id;
       }
 
       int access_contiguous =
           partition_contiguous_idx * Layout::PartitionShape::kContiguous +
           access_contiguous_idx;
 
       int access_strided = access_strided_idx;
@@ -838,14 +838,801 @@
   void set_kgroup_index(int k_group) {
     // no op
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
+/// This tile iterator is specialized for 32-thread TensorOps with 64B warp tile
+/// the contiguous dimension. This assumes Threadblock contiguous dimension has
+/// the same size as the warp tile.  It uses LDSM to load from shared
+/// memory and therefore must be initialized with a TensorRef to shared memory.
+///
+/// This specialization can be merged into the general one.  Most code is the same.
+///
+/// Satisfies:
+///   ReadableRandomAccessContiguousTileIteratorConcept
+///
+template <
+    /// Size of the matrix to load (concept: PitchLinearShape)
+    typename Shape_,
+    /// Identifies A or B multiplicand
+    Operand Operand_,
+    /// Data type of elements
+    typename Element_,
+    /// Shape of one matrix product operation (concept: PitchLinearShape)
+    typename InstructionShape_,
+    /// Interval between adjacent *MMA instructions (in units of MMA
+    /// instructions)
+    int OpDelta_,
+    /// Number of partitions along K dimension
+    int PartitionsK_>
+class MmaTensorOpMultiplicandTileIterator<
+    Shape_, Operand_, Element_,
+    cutlass::layout::TensorOpMultiplicandCongruous<16, 32>,
+    InstructionShape_, OpDelta_, 32, PartitionsK_> {
+ public:
+
+  /// Shape of tile to load (concept: PitchLinearShape)
+  using Shape = Shape_;
+
+  /// Operand tag
+  static Operand const kOperand = Operand_;
+
+  static_assert(kOperand == Operand::kA || kOperand== Operand::kB,
+    "MmaTensorOpMultiplicandIterator may only be instantiated for A or B operands to warp-level Mma.");
+
+  /// Element type
+  using Element = Element_;
+
+  /// Element number when the layout crosses
+  static int const kCrosswise = 32;
+
+  /// Layout of source tile
+  using Layout = cutlass::layout::TensorOpMultiplicandCongruous<
+      sizeof_bits<Element_>::value, kCrosswise>;
+
+  /// Shape of one matrix product operation (concept: GemmShape)
+  using InstructionShape = InstructionShape_;
+
+  /// Delta between *MMA operations (in units of *MMA operations, concept: MatrixShape)
+  static int const kOpDelta = OpDelta_;
+
+  /// Number of participating threads
+  static int const kThreads = 32;
+
+  /// Number of partitions along K dimension
+  static int const kPartitionsK = PartitionsK_;
+
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = TensorRef<Element, Layout>;
+
+  /// Index type
+  using Index = typename TensorRef::Index;
+
+  /// Long Index type
+  using LongIndex = typename TensorRef::LongIndex;
+
+  /// Long Index type
+  using StrideIndex = typename TensorRef::Layout::Stride::Index;
+
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
+
+  /// Internal structure of iterator - made public to enable introspection
+  struct Policy {
+    static_assert(
+        !(Shape::kContiguous % InstructionShape::kContiguous),
+        "Shape of warp-level Mma must be divisible by operator shape.");
+
+    // Determine number of elements along outer dimension per individual LDSM op
+    static int const kLdsmOpOuter = Layout::kElementsPerAccess;
+    static int const kLdsmOpInner = 8;
+
+    static_assert(!(Shape::kContiguous % kLdsmOpOuter),
+      "Shape of warp-level mma must be divisible by LDSM's fundamental tile size.");
+
+    static_assert(!(Shape::kStrided % kLdsmOpInner),
+      "Shape of warp-level mma must be divisible by LDSM's fundamental tile size.");
+
+    /// Shape of one individual LDSM instruction
+    static int const LdsmShapeStrided =
+        InstructionShape::kStrided / kLdsmOpInner;
+    static int const LdsmShapeContiguous = 4 / LdsmShapeStrided;
+    using LdsmShape =
+        layout::PitchLinearShape<LdsmShapeContiguous, LdsmShapeStrided>;
+
+    /// Number and arrangement of LDSM instructions
+    using LdsmIterations = layout::PitchLinearShape<
+        Shape::kContiguous / Layout::kElementsPerAccess / LdsmShapeContiguous,
+        1>;
+
+    /// Number of groups for each tile
+    static int const kGroupsPerTile =
+        Shape::kStrided / InstructionShape::kStrided;
+  };
+
+private:
+
+  /// Not working on this feature at the moment.
+  static_assert(kOpDelta == 1,
+    "Alternative arrangements not supported at present.");
+
+  /// Number of internal pointers needed to reference shared memory
+  static int const kPointerCount =
+      Layout::TileShape::kContiguous / Policy::LdsmShape::kContiguous / Layout::kFactor;
+
+  /// Pointer type used for accesses
+  using AccessType = Array<Element, Layout::kElementsPerAccess>;
+
+  /// Internal counter used to jump to next K partition
+  int k_group_idx_;
+
+public:
+
+  //
+  // Derived quantities
+  //
+
+  /// Fragment object holding a thread's part of a tile
+ using Fragment =
+     Array<Element, Shape::kContiguous * InstructionShape::kStrided / kThreads>;
+
+private:
+
+  /// Layout object storing stride values
+  StrideIndex stride_;
+
+  /// Shared memory base pointers - not advanced
+  AccessType const *pointer_[kPointerCount];
+
+  /// Byte offset incremented as iterator advances
+  Index byte_offset_;
+
+public:
+  
+  /// Default ctor constructs null iterator
+  CUTLASS_HOST_DEVICE
+  MmaTensorOpMultiplicandTileIterator(): stride_(0), byte_offset_(0) { }
+
+  /// Constructor from TensorRef
+  CUTLASS_DEVICE
+  MmaTensorOpMultiplicandTileIterator(
+    TensorRef const &ref, 
+    int lane_id
+  ):
+    stride_(ref.stride(0) * Layout::kFactor / Layout::kElementsPerAccess),
+    byte_offset_(0),
+    k_group_idx_(0) {
+      
+    int quad_pair = (lane_id >> 3);
+    int quad_quad = (lane_id >> 4);
+    //int lane_in_quad = (lane_id & 3);
+    int lane_in_quad_pair = (lane_id & 7);
+    int lane_in_quad_quad = (lane_id & 15);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < kPointerCount; ++i) {
+      int partition_contiguous_idx = -1;
+      int access_contiguous_idx = -1;
+      int access_strided_idx = -1;
+
+      if (Policy::LdsmShape::kContiguous == 4) {
+        // Matrix multiply 1688 A/B
+        // Q0 Q1 Q2 Q3 (Q stands for 1 8x128bit block).
+        // Four blocks are next to each other in the contiguous dimension.
+        partition_contiguous_idx = (lane_id % Layout::kFactor);
+        access_contiguous_idx = quad_pair ^ (lane_in_quad_pair / Layout::kFactor);
+        access_strided_idx = lane_in_quad_pair / Layout::kFactor;
+      } else if (Policy::LdsmShape::kContiguous == 2 &&
+          kOperand == Operand::kA) {
+        // Matrix multiply 16816 A
+        // Q0 Q1
+        // Q2 Q3
+        partition_contiguous_idx = (lane_id % Layout::kFactor);
+        access_contiguous_idx =
+            (((quad_pair & 1) + i * 2) ^ (lane_in_quad_pair / Layout::kFactor));
+        access_strided_idx = (lane_in_quad_pair + (lane_id >> 4 << 3)) / 2;
+      } else if (Policy::LdsmShape::kContiguous == 2 &&
+                 kOperand == Operand::kB) {
+        // Matrix multiply 16816 B
+        // Q0 Q2
+        // Q1 Q3
+        partition_contiguous_idx = (lane_id % Layout::kFactor);
+        access_contiguous_idx = (quad_quad + i * 2) ^ (lane_in_quad_pair / Layout::kFactor);
+        access_strided_idx = (lane_in_quad_quad / Layout::kFactor);
+      }
+
+      int access_contiguous =
+          partition_contiguous_idx * Layout::PartitionShape::kContiguous +
+          access_contiguous_idx;
+
+      int access_strided = access_strided_idx;
+
+      pointer_[i] = reinterpret_cast<AccessType const *>(ref.data()) +
+                    access_contiguous + access_strided * stride_;
+    }
+  }
+
+  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  CUTLASS_DEVICE
+  MmaTensorOpMultiplicandTileIterator &add_pointer_offset(LongIndex offset) {
+
+    byte_offset_ += offset * sizeof(Element);
+
+    return *this;
+  }
+
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
+  CUTLASS_HOST_DEVICE
+  MmaTensorOpMultiplicandTileIterator &add_tile_offset(TensorCoord const &tile_offset) {
+
+    int contiguous_offset = tile_offset.contiguous();
+    if (Shape::kContiguous ==
+        Layout::PartitionShape::kContiguous * Layout::kElementsPerAccess) {
+      if (tile_offset.contiguous() % 2) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int i = 0; i < kPointerCount / 2; ++i) {
+          AccessType const *tmp_pointer = pointer_[i];
+          pointer_[i] = pointer_[i + kPointerCount / 2];
+          pointer_[i + kPointerCount / 2] = tmp_pointer;
+        }
+      }
+      contiguous_offset = (tile_offset.contiguous() >> 1) << 1;
+    }
+
+    int offset = (tile_offset.strided() * InstructionShape::kStrided) *
+                     stride_ * Layout::kElementsPerAccess / Layout::kFactor +
+                 contiguous_offset * Shape::kContiguous;
+
+    add_pointer_offset(offset);
+
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_DEVICE
+  MmaTensorOpMultiplicandTileIterator & operator++() {
+
+    add_tile_offset({0, 1});
+
+    if (kPartitionsK > 1) {
+      ++k_group_idx_;
+      // Jump to next stage
+      if (k_group_idx_ == Policy::kGroupsPerTile) {
+        k_group_idx_ = 0;
+        add_tile_offset(
+            {0, ((kPartitionsK - 1) * Policy::kGroupsPerTile)});
+      }
+    }
+
+    return *this;
+  }
+
+  /// Advances the iterator along the opposite of the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaTensorOpMultiplicandTileIterator & operator--() {
+    byte_offset_ -= stride_ * InstructionShape::kStrided * sizeof(Element) *
+                    Layout::kElementsPerAccess;
+
+    return *this;
+  }
+
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  CUTLASS_DEVICE
+  MmaTensorOpMultiplicandTileIterator & operator+=(TensorCoord const &tile_offset) {
+    add_tile_offset(tile_offset);
+    return *this;
+  }
+
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  CUTLASS_DEVICE
+  MmaTensorOpMultiplicandTileIterator & operator-=(TensorCoord const &tile_offset) {
+    add_tile_offset(-tile_offset);
+    return *this;
+  }
+
+  /// Loads a fragment from memory at the location pointed to by the iterator.
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
+
+    load_with_byte_offset(frag, 0);
+  }
+
+  /// Loads a fragment from memory with additional logical offset
+  CUTLASS_DEVICE
+  void load_with_byte_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a linear offset in units of bytes
+      Index byte_offset) const {
+
+    Array<unsigned, Policy::LdsmShape::kCount> *fetch_ptr = 
+      reinterpret_cast<Array<unsigned, Policy::LdsmShape::kCount> *>(&frag);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int s = 0; s < Policy::LdsmIterations::kStrided; ++s) {
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < Policy::LdsmIterations::kContiguous; ++c) {
+
+        int access_idx = c + s * Policy::LdsmIterations::kContiguous;
+
+        AccessType const *source_ptr =
+            pointer_[c % kPointerCount] +
+            Layout::TileShape::kContiguous * (c / kPointerCount) +
+            Policy::kLdsmOpInner * Policy::LdsmShape::kStrided * s * stride_ / Layout::kFactor;
+
+        char const *source_byte_ptr = reinterpret_cast<char const *>(source_ptr) + byte_offset + byte_offset_;
+
+        cutlass::arch::ldsm<layout::ColumnMajor, Policy::LdsmShape::kCount>(
+          fetch_ptr[access_idx],
+          source_byte_ptr
+        );
+      }
+    }
+  }
+
+  /// Loads a fragment from memory with additional logical offset
+  CUTLASS_DEVICE
+  void load_with_pointer_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a linear offset
+      Index pointer_offset) const {
+    load_with_byte_offset(frag, pointer_offset * sizeof(Element));
+  }
+
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void load(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset) const {
+    load_with_byte_offset(frag, tile_offset, 0);
+  }
+
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void load(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset,
+      /// loads a tile with a logical offset AND a pointer offset
+      Index pointer_offset) const {
+    load_with_byte_offset(frag, tile_offset, pointer_offset * sizeof(Element));
+  }
+
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void load_with_byte_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset,
+      /// loads a tile with a logical offset AND a pointer offset
+      Index byte_offset) const {
+    Index pointer_offset = 
+      tile_offset.contiguous() * Shape::kContiguous / Layout::kElementsPerAccess + 
+      tile_offset.strided() * InstructionShape::kStrided * stride_ / Layout::kFactor;
+
+    byte_offset += sizeof(AccessType) * pointer_offset;
+
+    load_with_byte_offset(frag, byte_offset);
+  }
+
+  /// Notify the iterator which k-group it is currently pointing to.
+  ///
+  /// This does not advance the iterator. Rather, it overrides its internal
+  /// tracking with constant-valued k-group index to enable the compiler to
+  /// fold constants and achieve more efficient code.
+  ///
+  /// This is used by some nontrivial permuted layouts.
+  CUTLASS_DEVICE
+  void set_kgroup_index(int k_group) {
+    // no op
+  }
+};
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// This tile iterator is specialized for 32-thread TensorOps with 32B warp tile
+/// the contiguous dimension. This assumes Threadblock contiguous dimension has
+/// the same size as the warp tile.  It uses LDSM to load from shared
+/// memory and therefore must be initialized with a TensorRef to shared memory.
+///
+/// This specialization can be merged into the general one.  Most code is the same.
+///
+/// Satisfies:
+///   ReadableRandomAccessContiguousTileIteratorConcept
+///
+template <
+    /// Size of the matrix to load (concept: PitchLinearShape)
+    typename Shape_,
+    /// Identifies A or B multiplicand
+    Operand Operand_,
+    /// Data type of elements
+    typename Element_,
+    /// Shape of one matrix product operation (concept: PitchLinearShape)
+    typename InstructionShape_,
+    /// Interval between adjacent *MMA instructions (in units of MMA
+    /// instructions)
+    int OpDelta_,
+    /// Number of partitions along K dimension
+    int PartitionsK_>
+class MmaTensorOpMultiplicandTileIterator<
+    Shape_, Operand_, Element_,
+    cutlass::layout::TensorOpMultiplicandCongruous<16, 16>,
+    InstructionShape_, OpDelta_, 32, PartitionsK_> {
+ public:
+
+  /// Shape of tile to load (concept: PitchLinearShape)
+  using Shape = Shape_;
+
+  /// Operand tag
+  static Operand const kOperand = Operand_;
+
+  static_assert(kOperand == Operand::kA || kOperand== Operand::kB,
+    "MmaTensorOpMultiplicandIterator may only be instantiated for A or B operands to warp-level Mma.");
+
+  /// Element type
+  using Element = Element_;
+
+  /// Element number when the layout crosses
+  static int const kCrosswise = 16;
+
+  /// Layout of source tile
+  using Layout = cutlass::layout::TensorOpMultiplicandCongruous<
+      sizeof_bits<Element_>::value, kCrosswise>;
+
+  /// Shape of one matrix product operation (concept: GemmShape)
+  using InstructionShape = InstructionShape_;
+
+  /// Delta between *MMA operations (in units of *MMA operations, concept: MatrixShape)
+  static int const kOpDelta = OpDelta_;
+
+  /// Number of participating threads
+  static int const kThreads = 32;
+
+  /// Number of partitions along K dimension
+  static int const kPartitionsK = PartitionsK_;
+
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = TensorRef<Element, Layout>;
+
+  /// Index type
+  using Index = typename TensorRef::Index;
+
+  /// Long Index type
+  using LongIndex = typename TensorRef::LongIndex;
+
+  /// Long Index type
+  using StrideIndex = typename TensorRef::Layout::Stride::Index;
+
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
+
+  /// Internal structure of iterator - made public to enable introspection
+  struct Policy {
+    static_assert(
+        !(Shape::kContiguous % InstructionShape::kContiguous),
+        "Shape of warp-level Mma must be divisible by operator shape.");
+
+    // Determine number of elements along outer dimension per individual LDSM op
+    static int const kLdsmOpOuter = Layout::kElementsPerAccess;
+    static int const kLdsmOpInner = 8;
+
+    static_assert(!(Shape::kContiguous % kLdsmOpOuter),
+      "Shape of warp-level mma must be divisible by LDSM's fundamental tile size.");
+
+    static_assert(!(Shape::kStrided % kLdsmOpInner),
+      "Shape of warp-level mma must be divisible by LDSM's fundamental tile size.");
+
+    /// Shape of one individual LDSM instruction
+    static int const LdsmShapeStrided =
+        InstructionShape::kStrided / kLdsmOpInner;
+    static int const LdsmShapeContiguous = 4 / LdsmShapeStrided;
+    using LdsmShape =
+        layout::PitchLinearShape<LdsmShapeContiguous, LdsmShapeStrided>;
+
+    /// Number and arrangement of LDSM instructions
+    using LdsmIterations = layout::PitchLinearShape<
+        Shape::kContiguous / Layout::kElementsPerAccess / LdsmShapeContiguous,
+        1>;
+
+    /// Number of groups for each tile
+    static int const kGroupsPerTile =
+        Shape::kStrided / InstructionShape::kStrided;
+  };
+
+private:
+
+  /// Not working on this feature at the moment.
+  static_assert(kOpDelta == 1,
+    "Alternative arrangements not supported at present.");
+
+  /// Number of internal pointers needed to reference shared memory
+  static int const kPointerCount =
+      Layout::TileShape::kContiguous / Policy::LdsmShape::kContiguous / Layout::kFactor;
+
+  /// Pointer type used for accesses
+  using AccessType = Array<Element, Layout::kElementsPerAccess>;
+
+  /// Internal counter used to jump to next K partition
+  int k_group_idx_;
+
+public:
+
+  //
+  // Derived quantities
+  //
+
+  /// Fragment object holding a thread's part of a tile
+ using Fragment =
+     Array<Element, Shape::kContiguous * InstructionShape::kStrided / kThreads>;
+
+private:
+
+  /// Layout object storing stride values
+  StrideIndex stride_;
+
+  /// Shared memory base pointers - not advanced
+  AccessType const *pointer_[kPointerCount];
+
+  /// Byte offset incremented as iterator advances
+  Index byte_offset_;
+
+public:
+
+  /// Default ctor constructs null iterator
+  CUTLASS_HOST_DEVICE
+  MmaTensorOpMultiplicandTileIterator(): stride_(0), byte_offset_(0) { }
+
+  /// Constructor from TensorRef
+  CUTLASS_DEVICE
+  MmaTensorOpMultiplicandTileIterator(
+    TensorRef const &ref,
+    int lane_id
+  ):
+    stride_(ref.stride(0) * Layout::kFactor / Layout::kElementsPerAccess),
+    byte_offset_(0),
+    k_group_idx_(0) {
+
+    //int quad_pair = (lane_id >> 3);
+    int quad_quad = (lane_id >> 4);
+    int lane_in_pair = (lane_id & 1);
+    int lane_in_quad = (lane_id & 3);
+    int lane_in_quad_pair = (lane_id & 7);
+    int lane_in_quad_quad = (lane_id & 15);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < kPointerCount; ++i) {
+      int partition_contiguous_idx = -1;
+      int access_contiguous_idx = -1;
+      int access_strided_idx = -1;
+
+      if (Policy::LdsmShape::kContiguous == 2 &&
+          kOperand == Operand::kA) {
+        // Matrix multiply 16816 A
+        // Q0 Q1
+        // Q2 Q3
+        partition_contiguous_idx = lane_in_quad / 2;
+        access_strided_idx = lane_in_quad_pair / Layout::kFactor + quad_quad * 2;
+        access_contiguous_idx =
+            ((lane_in_pair * 2 + ((lane_id & 8) >> 3)) ^
+             access_strided_idx);
+      } else if (Policy::LdsmShape::kContiguous == 2 &&
+                 kOperand == Operand::kB) {
+        // Matrix multiply 16816 B
+        // Q0 Q2
+        // Q1 Q3
+        partition_contiguous_idx = lane_in_quad / 2;
+        access_strided_idx = lane_in_quad_quad / Layout::kFactor;
+        access_contiguous_idx =
+            ((lane_in_pair * 2 + quad_quad) ^
+             access_strided_idx);
+      }
+
+      int access_contiguous =
+          partition_contiguous_idx * Layout::PartitionShape::kContiguous +
+          access_contiguous_idx;
+
+      int access_strided = access_strided_idx;
+
+      pointer_[i] = reinterpret_cast<AccessType const *>(ref.data()) +
+                    access_contiguous + access_strided * stride_;
+    }
+  }
+
+  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  CUTLASS_DEVICE
+  MmaTensorOpMultiplicandTileIterator &add_pointer_offset(LongIndex offset) {
+
+    byte_offset_ += offset * sizeof(Element);
+
+    return *this;
+  }
+
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
+  CUTLASS_HOST_DEVICE
+  MmaTensorOpMultiplicandTileIterator &add_tile_offset(TensorCoord const &tile_offset) {
+
+    int contiguous_offset = tile_offset.contiguous();
+    if (Shape::kContiguous ==
+        Layout::PartitionShape::kContiguous * Layout::kElementsPerAccess) {
+      if (tile_offset.contiguous() % 2) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int i = 0; i < kPointerCount / 2; ++i) {
+          AccessType const *tmp_pointer = pointer_[i];
+          pointer_[i] = pointer_[i + kPointerCount / 2];
+          pointer_[i + kPointerCount / 2] = tmp_pointer;
+        }
+      }
+      contiguous_offset = (tile_offset.contiguous() >> 1) << 1;
+    }
+
+    int offset = (tile_offset.strided() * InstructionShape::kStrided) *
+                     stride_ * Layout::kElementsPerAccess / Layout::kFactor +
+                 contiguous_offset * Shape::kContiguous;
+
+    add_pointer_offset(offset);
+
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_DEVICE
+  MmaTensorOpMultiplicandTileIterator & operator++() {
+
+    add_tile_offset({0, 1});
+
+    if (kPartitionsK > 1) {
+      ++k_group_idx_;
+      // Jump to next stage
+      if (k_group_idx_ == Policy::kGroupsPerTile) {
+        k_group_idx_ = 0;
+        add_tile_offset(
+            {0, ((kPartitionsK - 1) * Policy::kGroupsPerTile)});
+      }
+    }
+
+    return *this;
+  }
+
+  /// Advances the iterator along the opposite of the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaTensorOpMultiplicandTileIterator & operator--() {
+    byte_offset_ -= stride_ * InstructionShape::kStrided * sizeof(Element) *
+                    Layout::kElementsPerAccess;
+
+    return *this;
+  }
+
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  CUTLASS_DEVICE
+  MmaTensorOpMultiplicandTileIterator & operator+=(TensorCoord const &tile_offset) {
+    add_tile_offset(tile_offset);
+    return *this;
+  }
+
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  CUTLASS_DEVICE
+  MmaTensorOpMultiplicandTileIterator & operator-=(TensorCoord const &tile_offset) {
+    add_tile_offset(-tile_offset);
+    return *this;
+  }
+
+  /// Loads a fragment from memory at the location pointed to by the iterator.
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
+
+    load_with_byte_offset(frag, 0);
+  }
+
+  /// Loads a fragment from memory with additional logical offset
+  CUTLASS_DEVICE
+  void load_with_byte_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a linear offset in units of bytes
+      Index byte_offset) const {
+
+    Array<unsigned, Policy::LdsmShape::kCount> *fetch_ptr =
+      reinterpret_cast<Array<unsigned, Policy::LdsmShape::kCount> *>(&frag);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int s = 0; s < Policy::LdsmIterations::kStrided; ++s) {
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < Policy::LdsmIterations::kContiguous; ++c) {
+
+        int access_idx = c + s * Policy::LdsmIterations::kContiguous;
+
+        AccessType const *source_ptr =
+            pointer_[c % kPointerCount] +
+            Layout::TileShape::kContiguous * (c / kPointerCount) +
+            Policy::kLdsmOpInner * Policy::LdsmShape::kStrided * s * stride_ / Layout::kFactor;
+
+        char const *source_byte_ptr = reinterpret_cast<char const *>(source_ptr) + byte_offset + byte_offset_;
+
+        cutlass::arch::ldsm<layout::ColumnMajor, Policy::LdsmShape::kCount>(
+          fetch_ptr[access_idx],
+          source_byte_ptr
+        );
+      }
+    }
+  }
+
+  /// Loads a fragment from memory with additional logical offset
+  CUTLASS_DEVICE
+  void load_with_pointer_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a linear offset
+      Index pointer_offset) const {
+    load_with_byte_offset(frag, pointer_offset * sizeof(Element));
+  }
+
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void load(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset) const {
+    load_with_byte_offset(frag, tile_offset, 0);
+  }
+
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void load(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset,
+      /// loads a tile with a logical offset AND a pointer offset
+      Index pointer_offset) const {
+    load_with_byte_offset(frag, tile_offset, pointer_offset * sizeof(Element));
+  }
+
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void load_with_byte_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset,
+      /// loads a tile with a logical offset AND a pointer offset
+      Index byte_offset) const {
+    Index pointer_offset =
+      tile_offset.contiguous() * Shape::kContiguous / Layout::kElementsPerAccess +
+      tile_offset.strided() * InstructionShape::kStrided * stride_ / Layout::kFactor;
+
+    byte_offset += sizeof(AccessType) * pointer_offset;
+
+    load_with_byte_offset(frag, byte_offset);
+  }
+
+  /// Notify the iterator which k-group it is currently pointing to.
+  ///
+  /// This does not advance the iterator. Rather, it overrides its internal
+  /// tracking with constant-valued k-group index to enable the compiler to
+  /// fold constants and achieve more efficient code.
+  ///
+  /// This is used by some nontrivial permuted layouts.
+  CUTLASS_DEVICE
+  void set_kgroup_index(int k_group) {
+    // no op
+  }
+};
+
+////////////////////////////////////////////////////////////////////////////////
+
 /// This tile iterator is specialized for 32-thread TensorOps. It uses LDSM to load from shared
 /// memory and therefore must be initialized with a TensorRef to shared memory. 
 ///
 /// Satisfies:
 ///   ReadableRandomAccessContiguousTileIteratorConcept
 ///
 template <
@@ -856,20 +1643,22 @@
     /// Data type of elements
     typename Element_,
     /// Shape of one matrix product operation (concept: MatrixShape)
     typename InstructionShape_,
     /// Interval between adjacent *MMA instructions (in units of MMA
     /// instructions)
     int OpDelta_,
+    /// Element number when the layout crosses (in units of elements)
+    int Crosswise,
     /// Number of partitions along K dimension
     int PartitionsK_>
 class MmaTensorOpMultiplicandTileIterator<
     Shape_, Operand_, Element_,
     cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous<
-        sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>,
+        sizeof_bits<Element_>::value, Crosswise>,
     InstructionShape_, OpDelta_, 32, PartitionsK_> {
  public:
 
   /// Shape of tile to load (concept: PitchLinearShape)
   using Shape = Shape_;
 
   /// Operand tag
@@ -878,17 +1667,20 @@
   static_assert(kOperand == Operand::kA,
                 "MmaTensorOpMultiplicandIterator for ColumnMajor Congruous may "
                 "only be instantiated for A operand to warp-level Mma.");
 
   /// Element type
   using Element = Element_;
 
+  /// MBlock or NBlock size
+  static int const kCrosswise = Crosswise;
+
   /// Layout of source tile
   using Layout = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>;
+      sizeof_bits<Element_>::value, kCrosswise>;
 
   /// Shape of one matrix product operation (concept: MatrixShape)
   using InstructionShape = InstructionShape_;
 
   /// Delta between *MMA operations (in units of *MMA operations, concept: MatrixShape)
   static int const kOpDelta = OpDelta_;
 
@@ -910,15 +1702,15 @@
   /// Coordinate for an element in the tensor
   using TensorCoord = typename TensorRef::TensorCoord;
 
   /// Underlying tile iterator implementation
   using Base = MmaTensorOpMultiplicandTileIterator<
       layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, kOperand, Element,
       layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                            int(128 / sizeof(Element_))>,
+                                            kCrosswise>,
       layout::PitchLinearShape<InstructionShape::kRow,
                                InstructionShape::kColumn>,
       kOpDelta, kThreads, PartitionsK_>;
 
  public:
 
   //
@@ -1088,20 +1880,22 @@
     /// Data type of elements
     typename Element_,
     /// Shape of one matrix product operation (concept: MatrixShape)
     typename InstructionShape_,
     /// Interval between adjacent *MMA instructions (in units of MMA
     /// instructions)
     int OpDelta_,
+    /// Element number when the layout crosses (in units of elements)
+    int Crosswise,
     /// Number of partitions along K dimension
     int PartitionsK_>
 class MmaTensorOpMultiplicandTileIterator<
     Shape_, Operand_, Element_,
     cutlass::layout::RowMajorTensorOpMultiplicandCongruous<
-        sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>,
+        sizeof_bits<Element_>::value, Crosswise>,
     InstructionShape_, OpDelta_, 32, PartitionsK_> {
  public:
 
   /// Shape of tile to load (concept: PitchLinearShape)
   using Shape = Shape_;
 
   /// Operand tag
@@ -1110,17 +1904,20 @@
   static_assert(kOperand == Operand::kB,
                 "MmaTensorOpMultiplicandIterator for RowMajor Congruous may "
                 "only be instantiated for B operand to warp-level Mma.");
 
   /// Element type
   using Element = Element_;
 
+  /// Element number when the layout crosses
+  static int const kCrosswise = Crosswise;
+
   /// Layout of source tile
   using Layout = cutlass::layout::RowMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>;
+      sizeof_bits<Element_>::value, kCrosswise>;
 
   /// Shape of one matrix product operation (concept: MatrixShape)
   using InstructionShape = InstructionShape_;
 
   /// Delta between *MMA operations (in units of *MMA operations, concept: MatrixShape)
   static int const kOpDelta = OpDelta_;
 
@@ -1139,15 +1936,15 @@
   /// Coordinate for an element in the tensor
   using TensorCoord = typename TensorRef::TensorCoord;
 
   /// Underlying tile iterator implementation
   using Base = MmaTensorOpMultiplicandTileIterator<
       layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, kOperand, Element,
       layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                            int(128 / sizeof(Element_))>,
+                                            kCrosswise>,
       layout::PitchLinearShape<InstructionShape::kColumn,
                                InstructionShape::kRow>,
       kOpDelta, kThreads, PartitionsK_>;
 
  public:
 
   //
@@ -1538,16 +2335,15 @@
         // Q1
         // Q2
         // Q3
         // Four blocks are next to each other in the strided dimension.
         partition_contiguous_idx = (lane_id % Layout::kFactor);
         access_contiguous_idx = (lane_in_quad_pair / Layout::kFactor);
         access_strided_idx = lane_id / Layout::kFactor;
-      }
-      else if (Policy::LdsmShape::kStrided ==
+      } else if (Policy::LdsmShape::kStrided ==
                      (Policy::LdsmShape::kCount / 2) &&
                  kOperand == Operand::kA) {
         // Matrix multiply 16816|1688.TF32 A
         // Q0 Q2
         // Q1 Q3
         partition_contiguous_idx = (lane_id % Layout::kFactor);
         access_contiguous_idx =
@@ -3831,26 +4627,26 @@
     LongIndex pq_rem;
 
     unsigned int pq_mul, pq_shr;
     find_divisor(pq_mul, pq_shr, pq);
 
     if(beta_ == 0.0f) {
       CUTLASS_PRAGMA_UNROLL
-      for(int i = 0; i < frag.size(); ++i) {
+      for(int i = 0; i < int(frag.size()); ++i) {
         output_frag_f[i] = frag[i];
       }
 
       if(InstructionShape::kM == Policy::kStridedPerSTG) {
         CUTLASS_PRAGMA_UNROLL
-        for(int i = 0; i < frag.size(); ++i) {
+        for(int i = 0; i < int(frag.size()); ++i) {
           output_frag[i] = (Element)(output_frag_f[i] * alpha_);
         }
       } else {
         CUTLASS_PRAGMA_UNROLL
-        for(int i = 0; i < frag.size(); ++i) {
+        for(int i = 0; i < int(frag.size()); ++i) {
           int map_i = (i / (16 * Policy::kPackedFactor)) * (16 * Policy::kPackedFactor)
                     + (i % (8 * Policy::kPackedFactor)) / 2 * 4
                     + (i % (8 * Policy::kPackedFactor)) % 2
                     + (i / (8 * Policy::kPackedFactor)) % 2 * 2;
           output_frag[i] = (Element)(output_frag_f[map_i] * alpha_);
         }
       }
@@ -3878,20 +4674,20 @@
             access_ptr[0] = frag_ptr[idx];
           }
         }
       }
     } else {
       if(InstructionShape::kM == Policy::kStridedPerSTG) {
         CUTLASS_PRAGMA_UNROLL
-        for(int i = 0; i < frag.size(); ++i) {
+        for(int i = 0; i < int(frag.size()); ++i) {
           output_frag_f[i] = frag[i];
         }
       } else {
         CUTLASS_PRAGMA_UNROLL
-        for(int i = 0; i < frag.size(); ++i) {
+        for(int i = 0; i < int(frag.size()); ++i) {
           int map_i = (i / (16 * Policy::kPackedFactor)) * (16 * Policy::kPackedFactor)
                     + (i % (8 * Policy::kPackedFactor)) / 2 * 4
                     + (i % (8 * Policy::kPackedFactor)) % 2
                     + (i / (8 * Policy::kPackedFactor)) % 2 * 2;
           output_frag_f[i] = frag[map_i];
         }
       }
```

## cutlass_library/source/include/cutlass/layout/matrix.h

```diff
@@ -34,24 +34,14 @@
     Layout functions map logical coordinates to linear memory. They often require additional
     data to describe strides between elements.
 
     Layout functions must implement all members in the public interface of IdentityTensorLayout<>
     defined in cutlass/tensor_ref.h.
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by this unit test: `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/pitch_linear_coord.h"
```

## cutlass_library/source/include/cutlass/layout/pitch_linear.h

```diff
@@ -28,24 +28,14 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Defines layout functions used by TensorRef and derived classes for pitch-linear memory.
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by this unit test: `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/coord.h"
 #include "cutlass/pitch_linear_coord.h"
 
 namespace cutlass {
```

## cutlass_library/source/include/cutlass/layout/tensor.h

```diff
@@ -56,14 +56,22 @@
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
 // Defines data layouts of various tensor formats usable by TensorRef and other classes.
 //
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+/// Tag used for 3-D NWC tensors for 1D conv, only used in 3.x API
+class TensorNWC {};
+
+/// Tag used for n-D KCSRT tensors for nD conv, only used in 3.x API for wgrad output layouts
+class TensorKCS {};
+class TensorKCSR {};
+class TensorKCSRT {};
+
 /// Mapping function for 4-D NHWC tensors.
 class TensorNHWC {
 public:
   /// Logical rank of tensor
   static int const kRank = 4;
 
   /// Rank of stride vector
```

## cutlass_library/source/include/cutlass/layout/tensor_op_multiplicand_sm75.h

```diff
@@ -256,14 +256,16 @@
 
   //
   // Static constants
   //
 
   static int const kElementSize = Base::kElementSize;
   static int const kElementsPerAccess = Base::kElementsPerAccess;
+  static int const kCrosswise = Base::kCrosswise;
+  static int const kFactor = Base::kFactor;
   using PartitionCount =  typename Base::PartitionCount;
   using AccessCount = typename Base::AccessCount;
 
  private:
   //
   // Data members
   //
@@ -365,14 +367,16 @@
       PitchLinearShape<PartitionShape::kContiguous, PartitionShape::kStrided>;
 
   //
   // Static constants
   //
   static int const kElementSize = 32;
   static int const kElementsPerAccess = kAccessSize / kElementSize;
+  static int const kCrosswise = Crosswise;
+  static int const kFactor = 1;
 
  private:
   //
   // Data members
   //
 
   /// Stride data member.
@@ -467,14 +471,16 @@
 
   //
   // Static constants
   //
 
   static int const kElementSize = Base::kElementSize;
   static int const kElementsPerAccess = Base::kElementsPerAccess;
+  static int const kCrosswise = Base::kCrosswise;
+  static int const kFactor = Base::kFactor;
   using PartitionCount =  typename Base::PartitionCount;
   using AccessCount = typename Base::AccessCount;
 
 private:
 
   //
   // Data members
@@ -572,14 +578,16 @@
 
   //
   // Static constants
   //
 
   static int const kElementSize = Base::kElementSize;
   static int const kElementsPerAccess = Base::kElementsPerAccess;
+  static int const kCrosswise = Base::kCrosswise;
+  static int const kFactor = Base::kFactor;
   using PartitionCount =  typename Base::PartitionCount;
   using AccessCount = typename Base::AccessCount;
 
 private:
 
   //
   // Data members
```

## cutlass_library/source/include/cutlass/pipeline/sm90_pipeline.hpp

```diff
@@ -146,23 +146,28 @@
         index_ = 0;
         phase_ ^= 1;
       }
     }
   }
 
   CUTLASS_DEVICE
-  PipelineState& operator=(const PipelineState& other) {
+  PipelineState& operator+=(uint32_t num_iterations) {
+    return advance(num_iterations);
+  }
+
+  CUTLASS_DEVICE
+  PipelineState& operator=(PipelineState const& other) {
     index_ = other.index();
     phase_ = other.phase();
     count_ = other.count();
     return *this;
   }
 
   CUTLASS_DEVICE
-  PipelineState advance(uint32_t num_iterations) {
+  PipelineState& advance(uint32_t num_iterations) {
     if constexpr (Stages > 0) {
       // Number of iterations cross over the stage boundary => flipped phase
       if ((num_iterations < Stages) && (index_ + num_iterations) >= Stages ) {
         phase_ ^= 1;
       }
       // How many times number of iterations cross over the stage boundary and
       // end up on a odd number => flipped phase
@@ -177,15 +182,15 @@
 
   CUTLASS_DEVICE
   static PipelineState make_pipeline_state(PipelineState start_state, uint32_t num_iterations) {
     return start_state.advance(num_iterations);
   }
 };
 
-template<class Pipeline>  
+template<class Pipeline>
 CUTLASS_DEVICE
 PipelineState<Pipeline::Stages> make_producer_start_state() {
   // Producer starts with an opposite phase as the buffers are initially empty
   constexpr int InitialProducerStage = 0;
   constexpr uint32_t InitialProducerPhase = 1;
   constexpr uint32_t InitialProducerCount = 0;
   return {InitialProducerStage, InitialProducerPhase, InitialProducerCount};
@@ -255,15 +260,14 @@
     cutlass::arch::fence_barrier_init();
 
     // Logic to optimally schedule Empty Arrives
     // Goal : To divide SYNCS Empty Arrival duty equally amongst the Warp-Group (128 threads)
     dim3 block_id = cute::block_id_in_cluster();
     auto cluster_size = cute::size(cluster_shape);
     static constexpr int MaxClusterSize = 16;
-    static_assert(cluster_size <= MaxClusterSize, "ERROR : Cluster size too large !" );
 
     // STEP 1 : Use Cute Layout function to generate an optimal dst block-id (0-15)
     if (params_.num_consumers % NumThreadsPerWarpGroup == 0) {
       int thread_idx = threadIdx.x % NumThreadsPerWarpGroup;
       is_signalling_thread_ = (thread_idx % (NumThreadsPerWarpGroup / MaxClusterSize)) == 0;
       auto layout = cute::composition(Swizzle<2,0,-2>{},
                                       Layout<Shape<_4,_4>,Stride<_4,_1>>{});
@@ -286,15 +290,15 @@
       #endif
     }
 
     // STEP 2: Find if this dst block-id needs an arrival for this problem
     is_signalling_thread_ &= dst_blockid_ < cluster_size;
     is_signalling_thread_ &= is_same_row_or_col(dst_blockid_, block_id, cluster_shape);
   }
-  
+
   template <typename ClusterShape>
   CUTLASS_DEVICE
   bool is_same_row_or_col(int dst_block_id, dim3 block_id, ClusterShape cluster_shape) {
     return (((dst_block_id % cute::size<0>(cluster_shape)) == block_id.x) ||
             (
               ((dst_block_id / cute::size<0>(cluster_shape)) == block_id.y)
             ));
@@ -302,15 +306,15 @@
 
   ////////////////////
   // Producer APIs
   ////////////////////
   // Four member functions are always used in pairs:
   //
   // * producer_try_acquire and producer_acquire, and
-  // * consumer_try_wait and consumer_wait. 
+  // * consumer_try_wait and consumer_wait.
   //
   // The two functions with "try" in their names are called "try" functions,
   // and the other two are conceptually "finalize" functions.
   // The "try" function in each pair starts the process of waiting on the barrier to flip.
   // It opportunistically waits for an implementation-dependent timeout.
   // Whether or not the barrier has flipped yet, the try function will return a token.
   // If the token indicates that the barrier has not flipped,
@@ -336,15 +340,15 @@
   }
 
   // Prevents early exit of producer blocks in Cluster.
   // This should be called once before kernel exits.
   CUTLASS_DEVICE
   void producer_tail(PipelineState state) {
     for (int count = 0; count < Stages; ++count) {
-      producer_acquire(state, {BarrierStatus::WaitOnly});  
+      producer_acquire(state, {BarrierStatus::WaitOnly});
       ++state;
     }
   }
 
   CUTLASS_DEVICE
   ProducerBarrierType* producer_get_barrier(PipelineState state) {
     return producer_get_barrier(state.index());
@@ -358,15 +362,15 @@
     return consumer_try_wait(state.index(), state.phase(), skip_wait);
   }
 
   CUTLASS_DEVICE
   ConsumerToken consumer_test_wait(PipelineState state, uint32_t skip_wait = false) {
     return consumer_test_wait(state.index(), state.phase(), skip_wait);
   }
-  
+
   CUTLASS_DEVICE
   void consumer_wait(PipelineState state) {
     consumer_wait(state.index(), state.phase());
   }
 
   CUTLASS_DEVICE
   void consumer_wait(PipelineState state, ConsumerToken barrier_token) {
@@ -460,15 +464,15 @@
   ConsumerToken consumer_test_wait(uint32_t stage, uint32_t phase, uint32_t skip_wait) {
     if (skip_wait) {
       return {BarrierStatus::WaitDone};
     }
     uint32_t barrier_status = full_barrier_ptr_[stage].test_wait(phase);
     return {static_cast<BarrierStatus>(barrier_status)};
   }
-  
+
   // Wait for producer to commit transactions (done by TMA)
   CUTLASS_DEVICE
   void consumer_wait(uint32_t stage, uint32_t phase) {
     full_barrier_ptr_[stage].wait(phase);
   }
 
   // Wait for producer to commit transactions (done by TMA)
@@ -671,15 +675,15 @@
 
   ////////////////////
   // Producer APIs
   ////////////////////
   // Four member functions are always used in pairs:
   //
   // * producer_try_acquire and producer_acquire, and
-  // * consumer_try_wait and consumer_wait. 
+  // * consumer_try_wait and consumer_wait.
   //
   // The two functions with "try" in their names are called "try" functions,
   // and the other two are conceptually "finalize" functions.
   // The "try" function in each pair starts the process of waiting on the barrier to flip.
   // It opportunistically waits for an implementation-dependent timeout.
   // Whether or not the barrier has flipped yet, the try function will return a token.
   // If the token indicates that the barrier has not flipped,
@@ -710,15 +714,15 @@
   }
 
   // Prevents early exit of producer blocks in Cluster.
   // This should be called once before kernel exits.
   CUTLASS_DEVICE
   void producer_tail(PipelineState state) {
     for (int count = 0; count < Stages; ++count) {
-      producer_acquire(state);  
+      producer_acquire(state);
       ++state;
     }
   }
 
   CUTLASS_DEVICE
   ProducerBarrierType* producer_get_barrier(PipelineState state) {
     return producer_get_barrier(state.index());
@@ -732,15 +736,15 @@
     return consumer_try_wait(state.index(), state.phase(), skip_wait);
   }
 
   CUTLASS_DEVICE
   ConsumerToken consumer_test_wait(PipelineState state, uint32_t skip_wait = false) {
     return consumer_test_wait(state.index(), state.phase(), skip_wait);
   }
-  
+
   CUTLASS_DEVICE
   void consumer_wait(PipelineState state, ConsumerToken barrier_token = {BarrierStatus::WaitAgain}) {
     consumer_wait(state.index(), state.phase(), barrier_token);
   }
 
   CUTLASS_DEVICE
   void consumer_release(PipelineState state) {
@@ -797,15 +801,15 @@
   ConsumerToken consumer_test_wait(uint32_t stage, uint32_t phase, uint32_t skip_wait) {
     if (skip_wait) {
       return {BarrierStatus::WaitDone};
     }
     uint32_t barrier_status = full_barrier_ptr_[stage].test_wait(phase);
     return {static_cast<BarrierStatus>(barrier_status)};
   }
-  
+
   CUTLASS_DEVICE
   void consumer_wait(uint32_t stage, uint32_t phase, ConsumerToken barrier_token) {
     if (barrier_token == BarrierStatus::WaitAgain) {
       full_barrier_ptr_[stage].wait(phase);
     }
   }
 
@@ -879,15 +883,15 @@
 
   ////////////////////
   // Producer APIs
   ////////////////////
   // Four member functions are always used in pairs:
   //
   // * producer_try_acquire and producer_acquire, and
-  // * consumer_try_wait and consumer_wait. 
+  // * consumer_try_wait and consumer_wait.
   //
   // The two functions with "try" in their names are called "try" functions,
   // and the other two are conceptually "finalize" functions.
   // The "try" function in each pair starts the process of waiting on the barrier to flip.
   // It opportunistically waits for an implementation-dependent timeout.
   // Whether or not the barrier has flipped yet, the try function will return a token.
   // If the token indicates that the barrier has not flipped,
@@ -919,15 +923,15 @@
   }
 
   // Prevents early exit of producer blocks in Cluster.
   // This should be called once before kernel exits.
   CUTLASS_DEVICE
   void producer_tail(PipelineState state) {
     for (int count = 0; count < Stages; ++count) {
-      producer_acquire(state);  
+      producer_acquire(state);
       ++state;
     }
   }
 
   CUTLASS_DEVICE
   ProducerBarrierType* producer_get_barrier(PipelineState state) {
     return producer_get_barrier(state.index());
@@ -941,15 +945,15 @@
     return consumer_try_wait(state.index(), state.phase(), skip_wait);
   }
 
   CUTLASS_DEVICE
   ConsumerToken consumer_test_wait(PipelineState state, uint32_t skip_wait = false) {
     return consumer_test_wait(state.index(), state.phase(), skip_wait);
   }
-  
+
   CUTLASS_DEVICE
   void consumer_wait(PipelineState state, ConsumerToken barrier_token = {BarrierStatus::WaitAgain}) {
     consumer_wait(state.index(), state.phase(), barrier_token);
   }
 
   CUTLASS_DEVICE
   void consumer_release(PipelineState state) {
@@ -1000,15 +1004,15 @@
   ConsumerToken consumer_test_wait(uint32_t stage, uint32_t phase, uint32_t skip_wait) {
     if (skip_wait) {
       return {BarrierStatus::WaitDone};
     }
     uint32_t barrier_status = full_barrier_ptr_[stage].test_wait(phase);
     return {static_cast<BarrierStatus>(barrier_status)};
   }
-  
+
   CUTLASS_DEVICE
   void consumer_wait(uint32_t stage, uint32_t phase) {
     uint32_t done = full_barrier_ptr_[stage].test_wait(phase);
     if (!done) {
       full_barrier_ptr_[stage].wait(phase);
     }
   }
```

## cutlass_library/source/include/cutlass/platform/platform.h

```diff
@@ -91,24 +91,14 @@
  *           - \p aligned_storage
  *
  * The idea is that, as we drop support for older compilers, we can simply #define
  * the \p __NV_STD_XYZ macros and \p platform namespace to alias their C++
  * counterparts (or trivially find-and-replace their occurrences in code text).
  */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by `cutlass_test_unit_core_cpp11`.
-*/
-
 //-----------------------------------------------------------------------------
 // Dependencies
 //-----------------------------------------------------------------------------
 
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/type_traits>
 #include <cuda/std/utility>
@@ -155,15 +145,15 @@
 #else
 #define CUTLASS_STL_NAMESPACE std
 #endif
 #endif
 
 /// builtin_unreachable
 #if !defined(CUTLASS_GCC_UNREACHABLE)
-#  if defined(__clang__) || defined(__GNUC__)
+#  if defined(__GNUC__)
 #    define CUTLASS_GCC_UNREACHABLE __builtin_unreachable()
 #  else
 #    define CUTLASS_GCC_UNREACHABLE
 #  endif
 #endif
 
 //-----------------------------------------------------------------------------
@@ -946,23 +936,21 @@
   CUTLASS_HOST_DEVICE
   static constexpr uint8_t lowest() noexcept { return 0;}
   CUTLASS_HOST_DEVICE
   static constexpr uint8_t max() noexcept { return 255U;}
   static constexpr bool is_integer = true;
 };
 
-#if !defined(__CUDACC_RTC__)
 template <>
 struct numeric_limits<float> {
   CUTLASS_HOST_DEVICE
   static constexpr float infinity() noexcept { return bit_cast<float, int32_t>(0x7f800000);}
   static constexpr bool is_integer = false;
   static constexpr bool has_infinity = true;
 };
-#endif
 
 /// std::float_round_style
 using CUTLASS_STL_NAMESPACE::float_round_style;
 using CUTLASS_STL_NAMESPACE::round_indeterminate;
 using CUTLASS_STL_NAMESPACE::round_toward_zero;
 using CUTLASS_STL_NAMESPACE::round_to_nearest;
 using CUTLASS_STL_NAMESPACE::round_toward_infinity;
```

## cutlass_library/source/include/cutlass/reduction/device/reduce_split_k.h

```diff
@@ -67,33 +67,29 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    MatrixCoord problem_size;
-    int partitions;
-    size_t partition_stride;
-    WorkspaceTensorRef workspace;
-    OutputTensorRef destination;
-    OutputTensorRef source;
-    typename OutputOp::Params output;
-    typename ReductionOp::Params reduction;
+    MatrixCoord problem_size{0,0};
+    int partitions{1};
+    size_t partition_stride{0};
+    WorkspaceTensorRef workspace{};
+    OutputTensorRef destination{};
+    OutputTensorRef source{};
+    typename OutputOp::Params output{};
+    typename ReductionOp::Params reduction{};
 
     //
     // Methods
     //
 
     /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Arguments() : 
-      problem_size(0, 0), 
-      partitions(1), 
-      partition_stride(0) { }
+    Arguments() = default;
    
     CUTLASS_HOST_DEVICE 
     Arguments(
       MatrixCoord const & problem_size
     ):
       problem_size(problem_size) { }
```

## cutlass_library/source/include/cutlass/reduction/kernel/reduce_softmax_final.h

```diff
@@ -67,22 +67,22 @@
 
   //
   // Arguments
   //
 
   struct Arguments {
 
-    cutlass::gemm::GemmCoord*  problem_sizes;
-    cutlass::gemm::GemmCoord   problem_size;
-    ElementNorm*               block_Norm;
-    ElementSum*                block_Sum;
-    int64_t*                   offset_Norm_Device;
-    int64_t*                   offset_Sum_Device;
-    int64_t                    batch_stride_Max;
-    int64_t                    batch_stride_Sum;
+    cutlass::gemm::GemmCoord*  problem_sizes{nullptr};
+    cutlass::gemm::GemmCoord   problem_size{};
+    ElementNorm*               block_Norm{nullptr};
+    ElementSum*                block_Sum{nullptr};
+    int64_t*                   offset_Norm_Device{nullptr};
+    int64_t*                   offset_Sum_Device{nullptr};
+    int64_t                    batch_stride_Max{0};
+    int64_t                    batch_stride_Sum{0};
 
     //
     // Methods
     //
     Arguments() { }
 
     // Non-grouped constructor without batching
```

## cutlass_library/source/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h

```diff
@@ -258,15 +258,15 @@
     if (!params.inner_count) {
       return params.reduction_identity;
     }
 
     ComputeFragment accumulator;
     
     CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < accumulator.size(); ++i) {
+    for (int i = 0; i < int(accumulator.size()); ++i) {
       accumulator[i] = params.reduction_identity;
     }
     
     // Compute the coordinate of the first access    
     int64_t src_byte_offset = 0;
     Coord<kInnerRank> coord;
```

## cutlass_library/source/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h

```diff
@@ -254,15 +254,15 @@
     NumericArrayConverter<ElementCompute, ElementSource, VectorLength> convert_source;
     ReductionOp reduction_op(params.reduction_op);
 
     // Accumulated output
     ComputeFragment identity_frag;
 
     CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < identity_frag.size(); ++i) {
+    for (int i = 0; i < int(identity_frag.size()); ++i) {
       identity_frag[i] = params.reduction_identity;
     }
 
     if (!params.inner_count) {
       return identity_frag;
     }
     
@@ -532,15 +532,15 @@
 
     ReductionOp reduction_op(params.reduction_op);
 
     // Accumulated output
     ComputeFragment identity_frag;
     
     CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < identity_frag.size(); ++i) {
+    for (int i = 0; i < int(identity_frag.size()); ++i) {
       identity_frag[i] = params.reduction_identity;
     }
 
     ComputeFragment accumulator = identity_frag;
     ComputeFragment workspace_fragments[kBatchSize];
 
     // Partially unrolled loop
```

## cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h

```diff
@@ -28,24 +28,14 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
   \brief 
 */
 
-/*
-  Note:  CUTLASS 3x increases the host compiler requirements to C++17. However, certain
-         existing integrations of CUTLASS require C++11 host compilers.
-
-         Until this requirement can be lifted, certain headers with this annotation are required
-         to be remain consistent with C++11 syntax.
-
-         C++11 compatibility is enforced by this unit test: `cutlass_test_unit_core_cpp11`.
-*/
-
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
 #include "cutlass/detail/helper_macros.hpp"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/layout/pitch_linear.h"
```

## cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h

```diff
@@ -56,33 +56,34 @@
 ///
 ///
 /// Satisfies: ForwardTileIteratorConcept |
 ///            ReadableContiguousTileIteratorConcept |
 ///            WriteableContiguousTileIteratorConcept
 ///
 template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
+          typename ThreadMap_, int Alignment, int Crosswise>
 class RegularTileAccessIterator<
     Shape_, Element_,
     layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                          int(128 / sizeof(Element_))>,
+                                          Crosswise>,
     AdvanceRank, ThreadMap_, Alignment> {
  public:
   static_assert(
       AdvanceRank == 0 || AdvanceRank == 1,
       "Specialization for pitch-linear iterator may along advance along the "
       "contiguous(rank=0) or strided(rank=1) dimension.");
 
   using Shape = Shape_;
   using Element = Element_;
   using Layout =
       layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                            int(128 / sizeof(Element_))>;
+                                            Crosswise>;
   static int const kAdvanceRank = AdvanceRank;
   static int const kAlignment = Alignment;
+  static int const kCrosswise = Crosswise;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
   using StrideIndex = typename Layout::Stride::Index;
 
   using TensorRef = TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
@@ -130,15 +131,15 @@
 
  public:
   /// Construct a TileIterator with zero threadblock offset
   CUTLASS_HOST_DEVICE
   RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
                             int thread_id   ///< ID of each participating thread
                             )
-      : stride_(ref.stride(0) / Layout::kElementsPerAccess),
+      : stride_(ref.stride(0) * Layout::kFactor / Layout::kElementsPerAccess),
         byte_offset_(0) {
     layout::PitchLinearCoord thread_offset_base =
         ThreadMap::initial_offset(thread_id);
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < Detail::kPointerCount; ++i) {
       // This is the offset of a thread within a threadblock tile for a specific
@@ -171,15 +172,15 @@
 
   /// Returns a pointer
   CUTLASS_HOST_DEVICE
   AccessType *get() const {
     AccessType *access_ptr = pointer_[iteration_strided_ & 1];
     int stride_idx = (iteration_strided_ & ~1);
 
-    int access_offset = stride_idx * ThreadMap::Delta::kStrided * stride_ +
+    int access_offset = stride_idx * ThreadMap::Delta::kStrided * stride_ / Layout::kFactor +
                         iteration_contiguous_ * ThreadMap::Delta::kContiguous /
                             ThreadMap::kElementsPerAccess;
 
     char *access_byte_ptr =
         reinterpret_cast<char *>(access_ptr + access_offset);
     return reinterpret_cast<AccessType *>(access_byte_ptr + byte_offset_);
   }
@@ -216,46 +217,46 @@
 
     return prev;
   }
 
   /// Adds a tile offset
   CUTLASS_DEVICE
   void add_tile_offset(TensorCoord const &coord) {
-    add_pointer_offset(coord.contiguous() * Shape::kContiguous +
+    add_pointer_offset(coord.contiguous() * Shape::kContiguous * Layout::kFactor +
                        coord.strided() * Shape::kStrided * stride_ *
-                           Layout::kElementsPerAccess);
+                           Layout::kElementsPerAccess / Layout::kFactor);
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Tile Iterator specialized for column-major congruous TensorOp formats.
 ///
 ///
 /// Satisfies: ForwardTileIteratorConcept |
 ///            ReadableContiguousTileIteratorConcept |
 ///            WriteableContiguousTileIteratorConcept
 ///
 template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
+          typename ThreadMap_, int Alignment, int Crosswise>
 class RegularTileAccessIterator<
     Shape_, Element_,
     layout::ColumnMajorTensorOpMultiplicandCongruous<
-        sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>,
+        sizeof_bits<Element_>::value, Crosswise>,
     AdvanceRank, ThreadMap_, Alignment> {
  public:
   static_assert(
       AdvanceRank == 0 || AdvanceRank == 1,
       "Specialization for column-major iterator may along advance along the "
       "columns(rank=0) or rows(rank=1) dimension.");
 
   using Shape = Shape_;
   using Element = Element_;
   using Layout = layout::ColumnMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>;
+      sizeof_bits<Element_>::value, Crosswise>;
   static int const kAdvanceRank = AdvanceRank;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
@@ -263,15 +264,15 @@
 
   using ThreadMap = ThreadMap_;
 
   /// Underlying iterator type
   using UnderlyingIterator = RegularTileAccessIterator<
       layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, Element,
       layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                            int(128 / sizeof(Element_))>,
+                                            Crosswise>,
       (kAdvanceRank == 0 ? 0 : 1), ThreadMap_>;
 
   using AccessType = typename UnderlyingIterator::AccessType;
 
  private:
   /// Underlying iterator
   UnderlyingIterator iterator_;
@@ -329,30 +330,30 @@
 ///
 ///
 /// Satisfies: ForwardTileIteratorConcept |
 ///            ReadableContiguousTileIteratorConcept |
 ///            WriteableContiguousTileIteratorConcept
 ///
 template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
+          typename ThreadMap_, int Alignment, int Crosswise>
 class RegularTileAccessIterator<
     Shape_, Element_,
     layout::RowMajorTensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                                  int(128 / sizeof(Element_))>,
+                                                  Crosswise>,
     AdvanceRank, ThreadMap_, Alignment> {
  public:
   static_assert(
       AdvanceRank == 0 || AdvanceRank == 1,
       "Specialization for row-major iterator may along advance along the "
       "columns(rank=0) or rows(rank=1) dimension.");
 
   using Shape = Shape_;
   using Element = Element_;
   using Layout = layout::RowMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>;
+      sizeof_bits<Element_>::value, Crosswise>;
   static int const kAdvanceRank = AdvanceRank;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
@@ -360,15 +361,15 @@
 
   using ThreadMap = ThreadMap_;
 
   /// Underlying iterator type
   using UnderlyingIterator = RegularTileAccessIterator<
       layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
       layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                            int(128 / sizeof(Element_))>,
+                                            Crosswise>,
       (kAdvanceRank == 0 ? 1 : 0), ThreadMap_>;
 
   using AccessType = typename UnderlyingIterator::AccessType;
 
  private:
   /// Underlying iterator
   UnderlyingIterator iterator_;
```

## cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h

```diff
@@ -49,31 +49,31 @@
 ///
 ///
 /// Satisfies: ForwardTileIteratorConcept | 
 ///            ReadableContiguousTileIteratorConcept | 
 ///            WriteableContiguousTileIteratorConcept
 ///
 template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
+          typename ThreadMap_, int Alignment, int Crosswise>
 class RegularTileIterator<
     Shape_, Element_,
     layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                          int(128 / sizeof(Element_))>,
+                                          Crosswise>,
     AdvanceRank, ThreadMap_, Alignment> {
  public:
 
   static_assert(AdvanceRank == 0 || AdvanceRank == 1, 
     "Specialization for pitch-linear iterator may along advance along the "
     "contiguous(rank=0) or strided(rank=1) dimension.");
 
   using Shape = Shape_;
   using Element = Element_;
   using Layout =
       layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                            int(128 / sizeof(Element))>;
+                                            Crosswise>;
   static int const kAdvanceRank = AdvanceRank;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
@@ -224,30 +224,30 @@
 ///
 ///
 /// Satisfies: ForwardTileIteratorConcept | 
 ///            ReadableContiguousTileIteratorConcept | 
 ///            WriteableContiguousTileIteratorConcept
 ///
 template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
+          typename ThreadMap_, int Alignment, int Crosswise>
 class RegularTileIterator<
     Shape_, Element_,
     layout::ColumnMajorTensorOpMultiplicandCongruous<
-        sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>,
+        sizeof_bits<Element_>::value, Crosswise>,
     AdvanceRank, ThreadMap_, Alignment> {
  public:
 
   static_assert(AdvanceRank == 0 || AdvanceRank == 1, 
     "Specialization for column-major iterator may along advance along the "
     "columns(rank=0) or rows(rank=1) dimension.");
 
   using Shape = Shape_;
   using Element = Element_;
   using Layout = layout::ColumnMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<Element_>::value, int(128 / sizeof(Element))>;
+      sizeof_bits<Element_>::value, Crosswise>;
   static int const kAdvanceRank = AdvanceRank;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
@@ -255,15 +255,15 @@
 
   using ThreadMap = ThreadMap_;
 
   /// Underlying iterator type
   using UnderlyingIterator = RegularTileIterator<
       layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, Element,
       layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                            int(128 / sizeof(Element))>,
+                                            Crosswise>,
       (kAdvanceRank == 0 ? 0 : 1), ThreadMap_>;
 
  public:
 
   /// Fragment object to be loaded or stored
   using Fragment = Array<Element, UnderlyingIterator::Fragment::kElements>;
 
@@ -345,30 +345,30 @@
 ///
 ///
 /// Satisfies: ForwardTileIteratorConcept | 
 ///            ReadableContiguousTileIteratorConcept | 
 ///            WriteableContiguousTileIteratorConcept
 ///
 template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
+          typename ThreadMap_, int Alignment, int Crosswise>
 class RegularTileIterator<
     Shape_, Element_,
     layout::RowMajorTensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                                  int(128 / sizeof(Element_))>,
+                                                  Crosswise>,
     AdvanceRank, ThreadMap_, Alignment> {
  public:
 
   static_assert(AdvanceRank == 0 || AdvanceRank == 1, 
     "Specialization for row-major iterator may along advance along the "
     "columns(rank=0) or rows(rank=1) dimension.");
 
   using Shape = Shape_;
   using Element = Element_;
   using Layout = layout::RowMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<Element_>::value, int(128 / sizeof(Element))>;
+      sizeof_bits<Element_>::value, Crosswise>;
   static int const kAdvanceRank = AdvanceRank;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
@@ -376,15 +376,15 @@
 
   using ThreadMap = ThreadMap_;
 
   /// Underlying iterator type
   using UnderlyingIterator = RegularTileIterator<
       layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
       layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                            int(128 / sizeof(Element))>,
+                                            Crosswise>,
       (kAdvanceRank == 0 ? 1 : 0), ThreadMap_>;
 
  public:
 
   /// Fragment object to be loaded or stored
   using Fragment = Array<Element, UnderlyingIterator::Fragment::kElements>;
```

## cutlass_library/source/tools/library/include/cutlass/library/arch_mappings.h

```diff
@@ -93,14 +93,19 @@
 };
 
 template <typename OperatorClass> struct ArchMap<arch::Sm86, OperatorClass> {
   static int const kMin = 86;
   static int const kMax = 1024;
 };
 
+template <typename OperatorClass> struct ArchMap<arch::Sm89, OperatorClass> {
+  static int const kMin = 89;
+  static int const kMax = 89;
+};
+
 template <typename OperatorClass> struct ArchMap<arch::Sm90, OperatorClass> {
   static int const kMin = 90;
   static int const kMax = 1024;
 };
 
 // Arch conditional WGMMA
 template <> struct ArchMap<arch::Sm90, arch::OpClassTensorOp> {
```

## cutlass_library/source/tools/library/include/cutlass/library/library.h

```diff
@@ -117,526 +117,506 @@
 //
 // OperationKind: Gemm
 // GemmKind:      Gemm
 //
 struct GemmConfiguration {
 
   /// GEMM problem size
-  gemm::GemmCoord problem_size;
+  gemm::GemmCoord problem_size{};
 
   /// Leading dimension of A matrix
-  int64_t lda;
+  int64_t lda{0};
 
   /// Leading dimension of B matrix
-  int64_t ldb;
+  int64_t ldb{0};
 
   /// Leading dimension of C matrix
-  int64_t ldc;
+  int64_t ldc{0};
 
   /// Leading dimension of D matrix
-  int64_t ldd;
+  int64_t ldd{0};
 
   /// Number of partitions of K dimension
-  int split_k_slices;
+  int split_k_slices{0};
 };
 
 /// Arguments for GEMM
 struct GemmArguments {
 
   /// Pointer to A matrix
-  void const *A;
+  void const *A{nullptr};
 
   /// Pointer to B matrix
-  void const *B;
+  void const *B{nullptr};
 
   /// Pointer to C matrix
-  void const *C;
+  void const *C{nullptr};
 
   /// Pointer to D matrix
-  void *D;
+  void *D{nullptr};
 
   /// Host or device pointer to alpha scalar
-  void const *alpha;
+  void const *alpha{nullptr};
 
   /// Host or device pointer to beta scalar
-  void const *beta;
+  void const *beta{nullptr};
 
   /// Enumerant indicating whether alpha/beta point to host or device memory
-  ScalarPointerMode pointer_mode;
+  ScalarPointerMode pointer_mode{};
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Configuration for batched GEMM in which multiple matrix products are computed
 //
 // OperationKind: Gemm
 // GemmKind:      Batched
 
 struct GemmBatchedConfiguration {
 
   /// GEMM problem size
-  gemm::GemmCoord problem_size;
+  gemm::GemmCoord problem_size{};
 
   /// Leading dimension of A matrix
-  int64_t lda;
+  int64_t lda{0};
 
   /// Leading dimension of B matrix
-  int64_t ldb;
+  int64_t ldb{0};
 
   /// Leading dimension of C matrix
-  int64_t ldc;
+  int64_t ldc{0};
 
   /// Leading dimension of D matrix
-  int64_t ldd;
+  int64_t ldd{0};
 
   /// Stride between instances of the A matrix in memory
-  int64_t batch_stride_A;
+  int64_t batch_stride_A{0};
 
   /// Stride between instances of the B matrix in memory
-  int64_t batch_stride_B;
+  int64_t batch_stride_B{0};
 
   /// Stride between instances of the C matrix in memory
-  int64_t batch_stride_C;
+  int64_t batch_stride_C{0};
 
   /// Stride between instances of the D matrix in memory
-  int64_t batch_stride_D;
+  int64_t batch_stride_D{0};
 
   /// Number of GEMMs in batch
-  int batch_count;
+  int batch_count{1};
 };
 
 /// Arguments to batched GEMM
 using GemmBatchedArguments = GemmArguments;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Configuration for batched GEMM in which multiple matrix products are computed
 //
 // OperationKind: Gemm
 // GemmKind:      Array
 
 struct GemmArrayConfiguration {
 
-  gemm::GemmCoord problem_size;
+  gemm::GemmCoord problem_size{};
 
   /// Leading dimension of A matrix
-  int64_t lda;
+  int64_t lda{0};
 
   /// Leading dimension of B matrix
-  int64_t ldb;
+  int64_t ldb{0};
 
   /// Leading dimension of C matrix
-  int64_t ldc;
+  int64_t ldc{0};
 
   /// Leading dimension of D matrix
-  int64_t ldd;
+  int64_t ldd{0};
 
-  int batch_count;
+  int batch_count{1};
 };
 
 /// Arguments for GEMM - used by all the GEMM operations
 struct GemmArrayArguments {
-  void const * const *A;
-  void const * const *B;
-  void const * const *C;
-  void * const *D;
-  void const *alpha;
-  void const *beta;
-  ScalarPointerMode pointer_mode;
+  void const * const *A{nullptr};
+  void const * const *B{nullptr};
+  void const * const *C{nullptr};
+  void * const *D{nullptr};
+  void const *alpha{nullptr};
+  void const *beta{nullptr};
+  ScalarPointerMode pointer_mode{};
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Universal GEMM supporting multiple split-K modes, multiple batched modes, real and complex
 //
 // OperationKind: Gemm
 // GemmKind:      Universal
 
 struct GemmUniversalConfiguration {
 
-  GemmUniversalMode mode;
-  gemm::GemmCoord problem_size;
-  int batch_count;
-
-  int64_t lda;
-  int64_t ldb;
-  int64_t ldc;
-  int64_t ldd;
+  GemmUniversalMode mode{GemmUniversalMode::kGemm};
+  gemm::GemmCoord problem_size{};
+  int batch_count{1};
+
+  int64_t lda{0};
+  int64_t ldb{0};
+  int64_t ldc{0};
+  int64_t ldd{0};
 };
 
 struct GemmUniversalArguments {
   // NOTE: these are replicated for 3.0 interfaces
-  gemm::GemmCoord problem_size;
-  int batch_count;
+  gemm::GemmCoord problem_size{};
+  int batch_count{1};
 
-  void const *A;
-  void const *B;
-  void const *C;
-  void *D;
-
-  void const *alpha;
-  void const *beta;
-  ScalarPointerMode pointer_mode;
+  void const *A{nullptr};
+  void const *B{nullptr};
+  void const *C{nullptr};
+  void *D{nullptr};
+
+  void const *alpha{nullptr};
+  void const *beta{nullptr};
+  ScalarPointerMode pointer_mode{};
 
   // NOTE: these are replicated for 3.0 interfaces
-  int64_t lda;
-  int64_t ldb;
-  int64_t ldc;
-  int64_t ldd;
-
-  int64_t batch_stride_A;
-  int64_t batch_stride_B;
-  int64_t batch_stride_C;
-  int64_t batch_stride_D;
+  int64_t lda{0};
+  int64_t ldb{0};
+  int64_t ldc{0};
+  int64_t ldd{0};
+
+  int64_t batch_stride_A{0};
+  int64_t batch_stride_B{0};
+  int64_t batch_stride_C{0};
+  int64_t batch_stride_D{0};
 
   // Needed for some 3.x kernels
-  int sm_count;
+  int sm_count{0};
 
-  library::RasterOrder raster_order;
+  library::RasterOrder raster_order{};
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Complex valued GEMM in which real and imaginary parts are separated by a stride
 //
 // OperationKind: Gemm
 // GemmKind:      Planar complex
 
 struct GemmPlanarComplexConfiguration {
 
-  GemmUniversalMode mode;
-  gemm::GemmCoord problem_size;
-  int batch_count;
-
-  int64_t lda_real;
-  int64_t lda_imag;
-
-  int64_t ldb_real;
-  int64_t ldb_imag;
-
-  int64_t ldc_real;
-  int64_t ldc_imag;
-
-  int64_t ldd_real;
-  int64_t ldd_imag;
+  GemmUniversalMode mode{GemmUniversalMode::kGemm};
+  gemm::GemmCoord problem_size{};
+  int batch_count{1};
+  int64_t lda_real{0};
+  int64_t lda_imag{0};
+  int64_t ldb_real{0};
+  int64_t ldb_imag{0};
+  int64_t ldc_real{0};
+  int64_t ldc_imag{0};
+  int64_t ldd_real{0};
+  int64_t ldd_imag{0};
 };
 
 /// Arguments for planar complex GEMMs
 struct GemmPlanarComplexArguments {
 
-  void const *A_real;
-  void const *A_imag;
-
-  void const *B_real;
-  void const *B_imag;
-
-  void const *C_real;
-  void const *C_imag;
-
-  void *D_real;
-  void *D_imag;
-
-  void const *alpha;
-  void const *beta;
-  ScalarPointerMode pointer_mode;
-
-  int64_t batch_stride_A_real;
-  int64_t batch_stride_A_imag;
-
-  int64_t batch_stride_B_real;
-  int64_t batch_stride_B_imag;
-
-  int64_t batch_stride_C_real;
-  int64_t batch_stride_C_imag;
-
-  int64_t batch_stride_D_real;
-  int64_t batch_stride_D_imag;
+  void const *A_real{nullptr};
+  void const *A_imag{nullptr};
+  void const *B_real{nullptr};
+  void const *B_imag{nullptr};
+  void const *C_real{nullptr};
+  void const *C_imag{nullptr};
+  void *D_real{nullptr};
+  void *D_imag{nullptr};
+  void const *alpha{nullptr};
+  void const *beta{nullptr};
+  ScalarPointerMode pointer_mode{};
+
+  int64_t batch_stride_A_real{0};
+  int64_t batch_stride_A_imag{0};
+  int64_t batch_stride_B_real{0};
+  int64_t batch_stride_B_imag{0};
+  int64_t batch_stride_C_real{0};
+  int64_t batch_stride_C_imag{0};
+  int64_t batch_stride_D_real{0};
+  int64_t batch_stride_D_imag{0};
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// This is a special form of planar complex which loads pointers and problem size
 /// from memory.
 struct GemmPlanarComplexArrayConfiguration {
 
-  gemm::GemmCoord problem_size;
-  int batch_count;
-
-  int64_t lda_real;
-  int64_t lda_imag;
-
-  int64_t ldb_real;
-  int64_t ldb_imag;
-
-  int64_t ldc_real;
-  int64_t ldc_imag;
+  gemm::GemmCoord problem_size{};
+  int batch_count{1};
 
-  int64_t ldd_real;
-  int64_t ldd_imag;
+  int64_t lda_real{0};
+  int64_t lda_imag{0};
+  int64_t ldb_real{0};
+  int64_t ldb_imag{0};
+  int64_t ldc_real{0};
+  int64_t ldc_imag{0};
+  int64_t ldd_real{0};
+  int64_t ldd_imag{0};
 };
 
 /// Arguments for planar complex GEMMs
 struct GemmPlanarComplexArrayArguments {
 
-  int const *M;
-  int const *N;
-  int const *K;
-
-  void const * const * A_real;
-  void const * const * A_imag;
-  void const * const * B_real;
-  void const * const * B_imag;
-  void const * const * C_real;
-  void const * const * C_imag;
-  void * const * D_real;
-  void * const * D_imag;
-
-  void const * alpha;
-  void const * beta;
-  ScalarPointerMode pointer_mode;
+  int const *M{nullptr};
+  int const *N{nullptr};
+  int const *K{nullptr};
+
+  void const * const * A_real{nullptr};
+  void const * const * A_imag{nullptr};
+  void const * const * B_real{nullptr};
+  void const * const * B_imag{nullptr};
+  void const * const * C_real{nullptr};
+  void const * const * C_imag{nullptr};
+  void * const * D_real{nullptr};
+  void * const * D_imag{nullptr};
+
+  void const * alpha{nullptr};
+  void const * beta{nullptr};
+  ScalarPointerMode pointer_mode{};
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Grouped GEMM supporting
 //
 // OperationKind: Gemm
 // GemmKind:      Grouped
 
 struct GemmGroupedConfiguration {
-
-  int problem_count;
-  int threadblock_count;
-
+  int problem_count{0};
+  int threadblock_count{0};
 };
 
 struct GemmGroupedArguments {
 
-  gemm::GemmCoord *problem_sizes;
+  gemm::GemmCoord *problem_sizes{nullptr};
 
-  void * ptr_A;
-  void * ptr_B;
-  void * ptr_C;
-  void * ptr_D;
-
-  int64_t *lda;
-  int64_t *ldb;
-  int64_t *ldc;
-  int64_t *ldd;
-
-  void const *alpha;
-  void const *beta;
-  ScalarPointerMode pointer_mode;
+  void * ptr_A{nullptr};
+  void * ptr_B{nullptr};
+  void * ptr_C{nullptr};
+  void * ptr_D{nullptr};
+
+  int64_t *lda{nullptr};
+  int64_t *ldb{nullptr};
+  int64_t *ldc{nullptr};
+  int64_t *ldd{nullptr};
+
+  void const *alpha{nullptr};
+  void const *beta{nullptr};
+  ScalarPointerMode pointer_mode{};
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
 // OperationKind: kSparseGemm
 //
 
 /// Computes GEMM assuming one of the inputs has 2:4 structured sparsity.
 struct SparseGemmConfiguration {
 
-  GemmUniversalMode mode;
-  gemm::GemmCoord problem_size;
-  int batch_count;                /// number of sparse matrix products in batch
-
-  int64_t lda;                    /// leading dimension of A operand
-  int64_t ldb;                    /// leading dimension of B operand
-  int64_t ldc;                    /// leading dimension of C operand
-  int64_t ldd;                    /// leading dimension of D operand
-  int64_t lde;                    /// leading dimension of E operand (metadata matrix)
-
-  int64_t batch_stride_A;         // stride between matrices
-  int64_t batch_stride_B;         // stride between matrices
-  int64_t batch_stride_C;         // stride between matrices
-  int64_t batch_stride_D;         // stride between matrices
-  int64_t batch_stride_E;         // stride between matrices
+  GemmUniversalMode mode{GemmUniversalMode::kGemm};
+  gemm::GemmCoord problem_size{};
+  int batch_count{1};         /// number of sparse matrix products in batch
+  int64_t lda{0};             /// leading dimension of A operand
+  int64_t ldb{0};             /// leading dimension of B operand
+  int64_t ldc{0};             /// leading dimension of C operand
+  int64_t ldd{0};             /// leading dimension of D operand
+  int64_t lde{0};             /// leading dimension of E operand (metadata matrix)
+  int64_t batch_stride_A{0};  // stride between matrices
+  int64_t batch_stride_B{0};  // stride between matrices
+  int64_t batch_stride_C{0};  // stride between matrices
+  int64_t batch_stride_D{0};  // stride between matrices
+  int64_t batch_stride_E{0};  // stride between matrices
 };
 
 /// Arguments for sparse GEMMs
 struct SparseGemmArguments {
-
-  void const *A;                    /// pointer to A matrix
-  void const *B;                    /// pointer to B matrix
-  void const *C;                    /// pointer to C matrix
-  void *D;                          /// pointer to D matrix
-  void const *E;                    /// pointer to E matrix (metadata)
-
-  void const *alpha;                /// pointer to alpha scalar
-  void const *beta;                 /// pointer to beta scalar
-  ScalarPointerMode pointer_mode;   /// enumerant indicating whether alpha/beta pointers are host
+  void const *A{nullptr};          /// pointer to A matrix
+  void const *B{nullptr};          /// pointer to B matrix
+  void const *C{nullptr};          /// pointer to C matrix
+  void *D{nullptr};                  /// pointer to D matrix
+  void const *E{nullptr};          /// pointer to E matrix (metadata)
+  void const *alpha{nullptr};      /// pointer to alpha scalar
+  void const *beta{nullptr};       /// pointer to beta scalar
+  ScalarPointerMode pointer_mode{}; /// enumerant indicating whether alpha/beta pointers are host
                                     ///   or device pointers.
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Configuration for basic Rank K update operations
 //
 // OperationKind: (Syrk, Herk, Syr2k, Her2k)
 // RankKKind:      Universal
 //
 struct RankKConfiguration {
 
   /// SYRK problem size
-  gemm::GemmCoord problem_size;
+  gemm::GemmCoord problem_size{};
 
   /// Leading dimension of A matrix
-  int64_t lda;
+  int64_t lda{0};
 
   /// Leading dimension of B matrix
-  int64_t ldb;
+  int64_t ldb{0};
 
   /// Leading dimension of C matrix
-  int64_t ldc;
+  int64_t ldc{0};
 
   /// Leading dimension of D matrix
-  int64_t ldd;
+  int64_t ldd{0};
 
   /// Batch Count
-  int batch_count;
+  int batch_count{1};
 };
 
 /// Arguments for (Syrk, Herk, Syr2k, Her2k)
 struct RankKArguments {
 
   /// Pointer to A matrix
-  void const *A;
+  void const *A{nullptr};
 
   /// Pointer to B matrix (used only for Syr2k and Her2k)
-  void const *B;
+  void const *B{nullptr};
 
   /// Pointer to C matrix
-  void const *C;
+  void const *C{nullptr};
 
   /// Pointer to D matrix
-  void *D;
+  void *D{nullptr};
 
   /// Host or device pointer to alpha scalar
-  void const *alpha;
+  void const *alpha{nullptr};
 
   /// Host or device pointer to beta scalar
-  void const *beta;
+  void const *beta{nullptr};
 
   /// Enumerant indicating whether alpha/beta point to host or device memory
-  ScalarPointerMode pointer_mode;
+  ScalarPointerMode pointer_mode{};
 
-  int64_t batch_stride_A;
-  int64_t batch_stride_B;
-  int64_t batch_stride_C;
-  int64_t batch_stride_D;
+  int64_t batch_stride_A{0};
+  int64_t batch_stride_B{0};
+  int64_t batch_stride_C{0};
+  int64_t batch_stride_D{0};
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Configuration for basic TRMM operations
 //
 // OperationKind: Trmm
 // TrmmKind:      Universal
 //
 struct TrmmConfiguration {
 
   /// TRMM problem size
-  gemm::GemmCoord problem_size;
+  gemm::GemmCoord problem_size{};
 
   /// Leading dimension of A matrix
-  int64_t lda;
+  int64_t lda{0};
 
   /// Leading dimension of B matrix
-  int64_t ldb;
+  int64_t ldb{0};
 
   /// Leading dimension of D matrix
-  int64_t ldd;
+  int64_t ldd{0};
 
   /// Batch Count
-  int batch_count;
+  int batch_count{1};
 };
 
 /// Arguments for TRMM
 struct TrmmArguments {
 
   /// Pointer to A matrix
-  void const *A;
+  void const *A{nullptr};
 
   /// Pointer to B matrix
-  void const *B;
+  void const *B{nullptr};
 
   /// Pointer to D matrix
-  void *D;
+  void *D{nullptr};
 
   /// Host or device pointer to alpha scalar
-  void const *alpha;
+  void const *alpha{nullptr};
 
   /// Host or device pointer to beta scalar
-  void const *beta;
+  void const *beta{nullptr};
 
   /// Enumerant indicating whether alpha/beta point to host or device memory
-  ScalarPointerMode pointer_mode;
+  ScalarPointerMode pointer_mode{};
 
-  int64_t batch_stride_A;
-  int64_t batch_stride_B;
-  int64_t batch_stride_D;
+  int64_t batch_stride_A{0};
+  int64_t batch_stride_B{0};
+  int64_t batch_stride_D{0};
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Configuration for basic SYMM/HEMM update operations
 //
 // OperationKind: (Symm, Hemm)
 // SymmKind:      Universal
 //
 struct SymmConfiguration {
 
   /// SYMM/HEMM problem size
-  gemm::GemmCoord problem_size;
+  gemm::GemmCoord problem_size{};
 
   /// Leading dimension of A matrix
-  int64_t lda;
+  int64_t lda{0};
 
   /// Leading dimension of B matrix
-  int64_t ldb;
+  int64_t ldb{0};
 
   /// Leading dimension of C matrix
-  int64_t ldc;
+  int64_t ldc{0};
 
   /// Leading dimension of D matrix
-  int64_t ldd;
+  int64_t ldd{0};
 
   /// Batch Count
-  int batch_count;
+  int batch_count{1};
 };
 
 /// Arguments for (Symm, Hemm)
 struct SymmArguments {
 
   /// Pointer to A matrix
-  void const *A;
+  void const *A{nullptr};
 
   /// Pointer to B matrix
-  void const *B;
+  void const *B{nullptr};
 
   /// Pointer to C matrix
-  void const *C;
+  void const *C{nullptr};
 
   /// Pointer to D matrix
-  void *D;
+  void *D{nullptr};
 
   /// Host or device pointer to alpha scalar
-  void const *alpha;
+  void const *alpha{nullptr};
 
   /// Host or device pointer to beta scalar
-  void const *beta;
+  void const *beta{nullptr};
 
   /// Enumerant indicating whether alpha/beta point to host or device memory
-  ScalarPointerMode pointer_mode;
+  ScalarPointerMode pointer_mode{};
 
-  int64_t batch_stride_A;
-  int64_t batch_stride_B;
-  int64_t batch_stride_C;
-  int64_t batch_stride_D;
+  int64_t batch_stride_A{0};
+  int64_t batch_stride_B{0};
+  int64_t batch_stride_C{0};
+  int64_t batch_stride_D{0};
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Two dimensional convolution
 //
@@ -645,51 +625,51 @@
 struct Conv2dConfiguration {
 
   conv::SplitKMode split_k_mode;
 
   /// Conv2d problem size
   //  contains strictly conv2d size (N,H,W,C,K,R,S,P,Q,padding,stride,dilation,mode)
   //  also includes (split_k_slices, groups)
-  conv::Conv2dProblemSize problem_size;
+  conv::Conv2dProblemSize problem_size{};
 
   // stride of operand A
-  std::vector<int64_t> stride_a;
+  std::vector<int64_t> stride_a{};
 
   // stride of operand B
-  std::vector<int64_t> stride_b;
+  std::vector<int64_t> stride_b{};
 
   // stride of operand C
-  std::vector<int64_t> stride_c;
+  std::vector<int64_t> stride_c{};
 };
 
 
 /// Three dimensional convolution
 //
 // OperationKind: Conv3d
 //
 struct Conv3dConfiguration {
 
-  conv::SplitKMode split_k_mode;
+  conv::SplitKMode split_k_mode{};
 
   /// Conv2d problem size
   //  contains strictly conv2d size (N,D,H,W,C,K,T,R,S,Z,P,Q,padding,stride,dilation,mode)
   //  also includes (split_k_slices, groups)
-  conv::Conv3dProblemSize problem_size;
+  conv::Conv3dProblemSize problem_size{};
 
   /// Layout object for activations tensor
-  layout::TensorNDHWC layout_activations;
+  layout::TensorNDHWC layout_activations{};
 
   /// Layout object for filters tensor
-  layout::TensorNDHWC layout_filters;
+  layout::TensorNDHWC layout_filters{};
 
   /// Layout object for source tensor
-  layout::TensorNDHWC layout_source;
+  layout::TensorNDHWC layout_source{};
 
   /// Layout object for output tensor
-  layout::TensorNDHWC layout_output;
+  layout::TensorNDHWC layout_output{};
 
   //
   // Methods
   //
 
   // Mapping functions (A,B,C -> activation,filter,output)
   layout::TensorNDHWC layout_a(library::ConvKind const &conv_kind) const {
@@ -723,88 +703,87 @@
 /// Arguments for CONV
 struct ConvArguments {
 
   /////////////////////////////////////////////////////////
   /// ImplicitGemm matrices A, B, C, D
   /////////////////////////////////////////////////////////
   /// pointer to implicit gemm matrix A
-  void const *A;
+  void const *A{nullptr};
 
   /// pointer to implicit gemm matrix B
-  void const *B;
+  void const *B{nullptr};
 
   /// pointer to reordered matrix B
-  void const *reordered_B;
+  void const *reordered_B{nullptr};
 
   /// pointer to implicit gemm matrix C
-  void const *C;
+  void const *C{nullptr};
 
   /// pointer to implicit gemm destination matrix D
-  void *D;
+  void *D{nullptr};
 
   /// Host or device pointer to alpha scalar
-  void const *alpha;
+  void const *alpha{nullptr};
 
   /// Host or device pointer to beta scalar
-  void const *beta;
+  void const *beta{nullptr};
 
   /// Enumerant indicating whether alpha/beta point to host or device memory
-  ScalarPointerMode pointer_mode;
-
+  ScalarPointerMode pointer_mode{};
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Configuration for Reduction operations
 //
 // OperationKind: Reduction
 //
 struct ReductionConfiguration {
 
   /// Reduction problem size
-  MatrixCoord problem_size;
+  MatrixCoord problem_size{};
 
   /// Number of partitions to reduce
-  int partitions;
+  int partitions{0};
 
   /// Number of elements between each partition
-  int64_t partition_stride;
+  int64_t partition_stride{0};
 
   /// leading dimension of 'w'orkspace operand
-  int64_t ldw;
+  int64_t ldw{0};
 
   /// leading dimension of 's'ource operand
-  int64_t lds;
+  int64_t lds{0};
 
   /// leading dimension of 'd'estination operand
-  int64_t ldd;
+  int64_t ldd{0};
 };
 
 /// Arguments for Reduction
 struct ReductionArguments {
 
   /// Pointer to workspace matrix
-  void const *workspace;
+  void const *workspace{nullptr};
 
   /// Pointer to source matrix
-  void const *source;
+  void const *source{nullptr};
 
   /// Pointer to destination matrix
-  void *destination;
+  void *destination{nullptr};
 
   /// pointer to reference matrix
-  void *reference;
+  void *reference{nullptr};
 
   /// Host or device pointer to alpha scalar
-  void const *alpha;
+  void const *alpha{nullptr};
 
   /// Host or device pointer to beta scalar
-  void const *beta;
+  void const *beta{nullptr};
 
   /// Enumerant indicating whether alpha/beta point to host or device memory
-  ScalarPointerMode pointer_mode;
+  ScalarPointerMode pointer_mode{};
 };
 
 } // namespace library
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/tools/library/include/cutlass/library/operation_table.h

```diff
@@ -105,15 +105,15 @@
     layout_C(layout_C),
     element_D(element_D),
     layout_D(layout_D)
   { }
 
   inline
   bool operator==(GemmFunctionalKey const &rhs) const {
-    return 
+    return
       (provider == rhs.provider) &&
       (gemm_kind == rhs.gemm_kind) &&
       (element_compute == rhs.element_compute) &&
       (element_scalar == rhs.element_scalar) &&
       (element_A == rhs.element_A) &&
       (layout_A == rhs.layout_A) &&
       (transform_A == rhs.transform_A) &&
@@ -161,24 +161,24 @@
 
 /// Hash function for GemmFunctionalKey
 struct GemmFunctionalKeyHasher {
   using IntHash = std::hash<int>;
 
   inline
   static size_t rotl(size_t key, int shl) {
-    return (key << shl) | (key >> (sizeof(key)*8 - shl));
+    return (key << shl) | (key >> (sizeof(key)*8u - static_cast<size_t>(shl)));
   }
 
   inline
   size_t operator()(GemmFunctionalKey const &key) const {
     IntHash hash;
 
     return
-      rotl(hash(int(key.provider)),        1) ^ 
-      rotl(hash(int(key.gemm_kind)),       2) ^ 
+      rotl(hash(int(key.provider)),        1) ^
+      rotl(hash(int(key.gemm_kind)),       2) ^
       rotl(hash(int(key.element_compute)), 3) ^
       rotl(hash(int(key.element_scalar)),  4) ^
       rotl(hash(int(key.element_A)),       5) ^
       rotl(hash(int(key.layout_A)),        6) ^
       rotl(hash(int(key.transform_A)),     7) ^
       rotl(hash(int(key.element_B)),       8) ^
       rotl(hash(int(key.layout_B)),        9) ^
@@ -203,15 +203,15 @@
   //
 
   GemmPreferenceKey(): compute_capability(), alignment() { }
 
   GemmPreferenceKey(int cc, int alignment): compute_capability(cc), alignment(alignment) { }
 
   bool operator<(GemmPreferenceKey const &rhs) const {
-    return (compute_capability < rhs.compute_capability) || 
+    return (compute_capability < rhs.compute_capability) ||
       ((compute_capability == rhs.compute_capability) && (alignment < rhs.alignment));
   }
 
   bool operator==(GemmPreferenceKey const &rhs) const {
     return compute_capability == rhs.compute_capability;
   }
 };
@@ -284,32 +284,32 @@
     layout_A(layout_A),
     element_B(element_B),
     layout_B(layout_B),
     element_C(element_C),
     layout_C(layout_C),
     element_accumulator(element_accumulator),
     element_compute(element_compute)
-  { } 
+  { }
 
-  inline 
+  inline
   bool operator==(ConvFunctionalKey const &rhs) const {
     return
       (provider == rhs.provider) &&
       (conv_kind == rhs.conv_kind) &&
       (element_A == rhs.element_A) &&
       (layout_A == rhs.layout_A) &&
       (element_B == rhs.element_B) &&
       (layout_B == rhs.layout_B) &&
       (element_C == rhs.element_C) &&
       (layout_C == rhs.layout_C) &&
       (element_accumulator == rhs.element_accumulator) &&
       (element_compute == rhs.element_compute);
   }
 
-  inline 
+  inline
   bool operator!=(ConvFunctionalKey const &rhs) const {
     return !(*this == rhs);
   }
 };
 /////////////////////////////////////////////////////////////////////////////////////////////////
 inline
 std::ostream& operator<< (std::ostream& out, const cutlass::library::ConvFunctionalKey& key) {
@@ -321,32 +321,32 @@
       << "element_B: " << to_string(key.element_B) << std::endl
       << "layout_B: " << to_string(key.layout_B) << std::endl
       << "element_C: " << to_string(key.element_C) << std::endl
       << "layout_C: " << to_string(key.layout_C) << std::endl
       << "element_accumulator: " << to_string(key.element_accumulator) << std::endl
       << "element_compute: " << to_string(key.element_compute) << std::endl
       << "}";
-  
+
   return out;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 struct ConvFunctionalKeyHasher {
   using IntHash = std::hash<int>;
 
   inline
   static size_t rotl(size_t key, int shl) {
-    return (key << shl) | (key >> (sizeof(key)*8 - shl));
+    return (key << shl) | (key >> (sizeof(key)*8u - static_cast<size_t>(shl)));
   }
 
   inline
   size_t operator()(ConvFunctionalKey const &key) const {
     IntHash hash;
 
-    return 
+    return
       rotl(hash(int(key.provider)), 1) ^
       rotl(hash(int(key.conv_kind)), 2) ^
       rotl(hash(int(key.element_A)), 3) ^
       rotl(hash(int(key.layout_A)), 4) ^
       rotl(hash(int(key.element_B)), 5) ^
       rotl(hash(int(key.layout_B)), 6) ^
       rotl(hash(int(key.element_C)), 7) ^
@@ -366,19 +366,19 @@
 
   //
   // Methods
   //
 
   ConvPreferenceKey(): compute_capability(), iterator_algorithm() { }
 
-  ConvPreferenceKey(int cc, IteratorAlgorithmID iterator_algorithm): 
+  ConvPreferenceKey(int cc, IteratorAlgorithmID iterator_algorithm):
     compute_capability(cc), iterator_algorithm(iterator_algorithm) { }
 
   bool operator<(ConvPreferenceKey const &rhs) const {
-    return (compute_capability < rhs.compute_capability) || 
+    return (compute_capability < rhs.compute_capability) ||
       ((compute_capability == rhs.compute_capability) && (iterator_algorithm < rhs.iterator_algorithm));
   }
 
   bool operator==(ConvPreferenceKey const &rhs) const {
     return (compute_capability == rhs.compute_capability) &&
           (iterator_algorithm == rhs.iterator_algorithm);
   }
@@ -429,48 +429,48 @@
     provider(provider),
     element_workspace(element_workspace),
     element_accumulator(element_accumulator),
     element_output(element_output),
     element_compute(element_compute),
     reduce_math_op(reduce_math_op),
     epilogue_math_op(epilogue_math_op)
-  { } 
+  { }
 
-  inline 
+  inline
   bool operator==(ReductionFunctionalKey const &rhs) const {
     return
       (provider == rhs.provider) &&
       (element_workspace == rhs.element_workspace) &&
       (element_accumulator == rhs.element_accumulator) &&
       (element_output == rhs.element_output) &&
       (element_compute == rhs.element_compute) &&
       (reduce_math_op == rhs.reduce_math_op) &&
       (epilogue_math_op == rhs.epilogue_math_op);
   }
 
-  inline 
+  inline
   bool operator!=(ReductionFunctionalKey const &rhs) const {
     return !(*this == rhs);
   }
 };
 
 
 struct ReductionFunctionalKeyHasher {
   using IntHash = std::hash<int>;
 
   inline
   static size_t rotl(size_t key, int shl) {
-    return (key << shl) | (key >> (sizeof(key)*8 - shl));
+    return (key << shl) | (key >> (sizeof(key)*8u - static_cast<size_t>(shl)));
   }
 
   inline
   size_t operator()(ReductionFunctionalKey const &key) const {
     IntHash hash;
 
-    return 
+    return
       rotl(hash(int(key.provider)), 1) ^
       rotl(hash(int(key.element_workspace)), 2) ^
       rotl(hash(int(key.element_accumulator)), 3) ^
       rotl(hash(int(key.element_output)), 4) ^
       rotl(hash(int(key.element_compute)), 5) ^
       rotl(hash(int(key.reduce_math_op)), 6) ^
       rotl(hash(int(key.epilogue_math_op)), 7);
@@ -501,27 +501,27 @@
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Table of cutlass::library::Operation instances
 class OperationTable {
 public:
 
-  /// Map of all operations of type kGemm 
+  /// Map of all operations of type kGemm
   // provider (kCUTLASS)
   GemmOperationFunctionalMap gemm_operations;
 
-  /// Map of all operations of type kConv2d 
+  /// Map of all operations of type kConv2d
   // provider (kCUTLASS, kReferenceHost, kReferenceDevice)
   ConvOperationFunctionalMap conv2d_operations;
 
-  /// Map of all operations of type kConv3d 
+  /// Map of all operations of type kConv3d
   // provider (kCUTLASS, kReferenceHost, kReferenceDevice)
   ConvOperationFunctionalMap conv3d_operations;
 
-  /// Map of all operations of type kConv2d 
+  /// Map of all operations of type kConv2d
   // provider (kCUTLASS)
   ReductionOperationFunctionalMap reduction_operations;
 
 public:
 
   void append(Manifest const &manifest);
```

## cutlass_library/source/tools/library/src/gemm_operation_3x.hpp

```diff
@@ -33,14 +33,16 @@
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/library/library.h"
 #include "library_internal.h"
+#include "cutlass/gemm/dispatch_policy.hpp"
+#include <unordered_map>
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass::library {
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -59,15 +61,14 @@
   using LayoutD = typename Operator::LayoutD;
   // assuming all tensors use same type for StrideIndex
   using StrideIndex = typename Operator::LayoutA::Index;
   using ElementAccumulator = typename Operator::ElementAccumulator;
   using ElementCompute = typename Operator::EpilogueOutputOp::ElementCompute;
 
 private:
-
   GemmDescription description_;
 
 public:
 
   /// Constructor
   GemmOperation3xBase(char const *name = "unknown_gemm", GemmKind gemm_kind_ = GemmKind::kGemm) {
 
@@ -211,15 +212,16 @@
         return Status::kErrorInvalidProblem;
       }
     }
   };
 
   /// Constructs the arguments structure given the configuration and arguments
   static Status update_arguments_(
-      OperatorArguments &operator_args, GemmUniversalArguments const *arguments) {
+      OperatorArguments &operator_args,
+      GemmUniversalArguments const *arguments) {
     Status status = Status::kSuccess;
 
     status = UpdateFusionArgs<decltype(operator_args.epilogue.thread)>::update_(
       operator_args.epilogue.thread, *arguments);
     if (status != Status::kSuccess) {
       return status;
     }
@@ -257,24 +259,23 @@
         case RasterOrder::kAlongM:
           operator_args.scheduler.raster_order = Enum_t::AlongM;
           break;
         default:
           operator_args.scheduler.raster_order = Enum_t::Heuristic;
       }
     }
-    
+
     return status;
   }
 
 public:
 
   /// Returns success if the operation can proceed
   Status can_implement(
       void const *configuration_ptr, void const *arguments_ptr) const override {
-
     GemmUniversalConfiguration const *configuration =
       static_cast<GemmUniversalConfiguration const *>(configuration_ptr);
     GemmUniversalArguments const *arguments =
       static_cast<GemmUniversalArguments const *>(arguments_ptr);
 
     OperatorArguments args;
     auto status = update_arguments_(args, arguments);
@@ -284,15 +285,14 @@
 
     // can_implement rules may need access to problem shape
     args.problem_shape = cute::make_shape(
       configuration->problem_size.m(),
       configuration->problem_size.n(),
       configuration->problem_size.k(),
       configuration->batch_count);
-
     return Operator::can_implement(args);
   }
 
   /// Gets the host-side workspace
   uint64_t get_host_workspace_size(void const *configuration) const override {
     return sizeof(Operator);
   }
```

## cutlass_library/source/tools/library/src/library_internal.h

```diff
@@ -148,14 +148,15 @@
   static NumericTypeID const kId = NumericTypeID::kBF16;
 };
 
 template <> struct NumericTypeMap<cutlass::tfloat32_t> {
   static NumericTypeID const kId = NumericTypeID::kTF32;
 };
 
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename T> struct MathOperationMap {
   static MathOperationID const kId = MathOperationID::kInvalid;
 };
 
 template <> struct MathOperationMap<cutlass::arch::OpMultiplyAdd> {
```

## cutlass_library/source/tools/library/src/util.cu

```diff
@@ -418,14 +418,16 @@
   }
 
   return Status::kInvalid;
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
+///////////////////////////////////////////////////////////////////////////////////////////////////
+
 static struct {
   char const *text;
   char const *pretty;
   NumericTypeID enumerant;
 }
 NumericTypeID_enumerants[] = {
   {"unknown", "<unknown>", NumericTypeID::kUnknown},
@@ -1198,15 +1200,15 @@
   ss << int_value;
   return ss.str();
 }
 
 /// Lexical cast TO a string FROM a byte array. Returns true if cast is successful or false if invalid.
 std::string lexical_cast(std::vector<uint8_t> &bytes, NumericTypeID type) {
 
-  int size_bytes = sizeof_bits(type) / 8;
+  size_t size_bytes = sizeof_bits(type) / 8;
 
   if (!size_bytes || size_bytes != bytes.size()) {
     return "<invalid>";
   }
 
   bytes.resize(size_bytes, 0);
```

## cutlass_library/source/tools/profiler/include/cutlass/profiler/enumerated_types.h

```diff
@@ -154,16 +154,16 @@
 using DispositionMap = std::map<library::Provider, Disposition>;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 // Print vector for the report
 template <typename T>
 std::ostream& operator<< (std::ostream& out, const std::vector<T>& v) {
-  for(int i = 0; i < v.size(); ++i) {
-    out << to_string(v[i], true) << (i+1 != v.size() ? "," : "");
+  for (size_t i = 0; i < v.size(); ++i) {
+    out << to_string(v[i], true) << (i + 1u != v.size() ? "," : "");
   }
   return out;
 }
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace profiler
 } // namespace cutlass
```

## cutlass_library/source/tools/profiler/include/cutlass/profiler/gemm_operation_profiler.h

```diff
@@ -25,15 +25,15 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-   \brief Defines a math function
+   \brief Gemm Profiler
 */
 
 #pragma once
 
 #include <vector>
 #include <string>
 #include <memory>
@@ -63,53 +63,41 @@
 /// Abstract base class for each math function
 class GemmOperationProfiler : public OperationProfiler {
 public:
 
   /// Problem structure obtained from problem space
   struct GemmProblem {
 
-    cutlass::library::GemmUniversalMode mode;
+    cutlass::library::GemmUniversalMode mode{library::GemmUniversalMode::kGemm};
 
-    int64_t m;
-    int64_t n;
-    int64_t k;
-    
-    int64_t lda;
-    int64_t ldb;
-    int64_t ldc;
+    int64_t m{16};
+    int64_t n{16};
+    int64_t k{16};
+
+    int64_t lda{0};
+    int64_t ldb{0};
+    int64_t ldc{0};
     std::vector<uint8_t> alpha;
     std::vector<uint8_t> beta;
 
-    cutlass::library::SplitKMode split_k_mode;
-    int split_k_slices;
-    int batch_count;
+    cutlass::library::SplitKMode split_k_mode{library::SplitKMode::kNone};
+    int split_k_slices{1};
+    int batch_count{1};
 
-    cutlass::library::RasterOrder raster_order;
+    cutlass::library::RasterOrder raster_order{cutlass::library::RasterOrder::kHeuristic};
     // gemm with parallel interleaved reduction
     // gemm epilogue (alpha, beta) = (1.0, 0.0)
     // reduction epilogue (alpha, beta) = (GemmProblem::alpha, GemmProblem::beta)
     std::vector<uint8_t> alpha_one;
     std::vector<uint8_t> beta_zero;
 
     //
     // Methods
     //
 
-    GemmProblem():
-      mode(library::GemmUniversalMode::kGemm),
-      m(16), 
-      n(16), 
-      k(16),
-      lda(0), 
-      ldb(0), 
-      ldc(0), 
-      split_k_slices(1), 
-      batch_count(1),
-      raster_order(cutlass::library::RasterOrder::kHeuristic){ }
-
     /// Parses the problem
     Status parse(
       library::GemmDescription const &operation_desc,
       ProblemSpace const &problem_space,
       ProblemSpace::Problem const &problem);
 
     /// Total number of bytes loaded
@@ -124,23 +112,23 @@
       library::GemmDescription const &operation_desc,
       ProblemSpace const &problem_space);
   };
 
   /// Workspace used
   struct GemmWorkspace {
 
-    DeviceAllocation *A;
-    DeviceAllocation *B;
-    DeviceAllocation *C;
-    DeviceAllocation *Computed;
-    DeviceAllocation *Reference;
+    DeviceAllocation *A{nullptr};
+    DeviceAllocation *B{nullptr};
+    DeviceAllocation *C{nullptr};
+    DeviceAllocation *Computed{nullptr};
+    DeviceAllocation *Reference{nullptr};
 
     /// Number of copies of the problem workspace which are visited sequentially during
     /// profiling to avoid camping in the last level cache.
-    int problem_count;
+    int problem_count{1};
 
     library::GemmUniversalConfiguration configuration;
     library::GemmUniversalArguments arguments;
 
     /// Buffer used for the operation's host workspace
     std::vector<uint8_t> host_workspace;
 
@@ -149,21 +137,14 @@
 
     /// Library configuration and arguments for reduction operator
     library::ReductionConfiguration reduction_configuration;
     library::ReductionArguments reduction_arguments;
 
     /// Buffer used for the cutlass reduction operations' host workspace
     std::vector<uint8_t> reduction_host_workspace;
-
-    //
-    // Methods
-    //
-
-    GemmWorkspace():
-      A(nullptr), B(nullptr), C(nullptr), Computed(nullptr), Reference(nullptr), problem_count(1) { }
   };
 
 protected:
 
   //
   // Data members
   //
@@ -253,15 +234,17 @@
   /// Verifies CUTLASS against host and device references
   bool verify_with_reference_(
     Options const &options,
     PerformanceReport &report,
     DeviceContext &device_context,
     library::Operation const *operation,
     ProblemSpace const &problem_space,
-    ProblemSpace::Problem const &problem);
+    ProblemSpace::Problem const &problem,
+    cutlass::library::NumericTypeID element_A,
+    cutlass::library::NumericTypeID element_B);
 
   /// Method to profile a CUTLASS Operation
   Status profile_cutlass_(
     double &runtime,
     Options const &options,
     library::Operation const *operation,
     void *arguments,
```

## cutlass_library/source/tools/profiler/src/cutlass_profiler.cu

```diff
@@ -38,28 +38,28 @@
 // Profiler includes
 #include "cutlass/profiler/cutlass_profiler.h"
 #include "cutlass/profiler/gemm_operation_profiler.h"
 #include "cutlass/profiler/rank_k_operation_profiler.h"
 #include "cutlass/profiler/rank_2k_operation_profiler.h"
 #include "cutlass/profiler/trmm_operation_profiler.h"
 #include "cutlass/profiler/symm_operation_profiler.h"
-#include "cutlass/profiler/conv2d_operation_profiler.h"          
-#include "cutlass/profiler/conv3d_operation_profiler.h"          
+#include "cutlass/profiler/conv2d_operation_profiler.h"
+#include "cutlass/profiler/conv3d_operation_profiler.h"
 #include "cutlass/profiler/sparse_gemm_operation_profiler.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace profiler {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 CutlassProfiler::CutlassProfiler(
   Options const &options
-): 
+):
   options_(options) {
 
   operation_profilers_.emplace_back(new GemmOperationProfiler(options));
 
   operation_profilers_.emplace_back(new SparseGemmOperationProfiler(options));
 
   operation_profilers_.emplace_back(new Conv2dOperationProfiler(options));
@@ -141,15 +141,14 @@
 }
 
 /// Profiles all operations
 int CutlassProfiler::profile_() {
 
   int result = 0;
   DeviceContext device_context;
-
   // For all profilers
   for (auto & profiler : operation_profilers_) {
 
     if (options_.operation_kind == library::OperationKind::kInvalid ||
       options_.operation_kind == profiler->kind()) {
 
       result = profiler->profile_all(options_, library::Singleton::get().manifest, device_context);
@@ -189,16 +188,16 @@
   }
 
   out << "\n\nFor details about a particular function, specify the function name with --help.\n\nExample:\n\n"
     << "  $ cutlass_profiler --operation=Gemm --help\n\n"
     << "  $ cutlass_profiler --operation=RankK --help\n\n"
     << "  $ cutlass_profiler --operation=Trmm --help\n\n"
     << "  $ cutlass_profiler --operation=Symm --help\n\n"
-    << "  $ cutlass_profiler --operation=Conv3d --help\n\n"         
-    << "  $ cutlass_profiler --operation=Conv2d --help\n\n"         
+    << "  $ cutlass_profiler --operation=Conv3d --help\n\n"
+    << "  $ cutlass_profiler --operation=Conv2d --help\n\n"
     << "  $ cutlass_profiler --operation=SparseGemm --help\n\n"
   ;
 }
 
 /// Prints usage
 void CutlassProfiler::print_options_(std::ostream &out) {
   options_.print_options(out);
```

## cutlass_library/source/tools/profiler/src/gemm_operation_profiler.cu

```diff
@@ -32,14 +32,15 @@
    \brief Execution environment
 */
 
 #include <iostream>
 #include <stdexcept>
 #include <iomanip>
 #include <ios>
+#include <vector>
 
 #include "cutlass/core_io.h"
 
 #include "cutlass/profiler/cublas_helpers.h"
 #include "cutlass/profiler/gemm_operation_profiler.h"
 #include "cutlass/profiler/gpu_timer.h"
 #include "cutlass/library/singleton.h"
@@ -163,15 +164,15 @@
     this->n = 1024;
   }
 
   if (!arg_as_int(this->k, "k", problem_space, problem)) {
     // default value
     this->k = 1024;
   }
-  
+
   if (!arg_as_SplitKModeID(this->split_k_mode, "split_k_mode", problem_space, problem)) {
     // default value
     this->split_k_mode = library::SplitKMode::kSerial;
   }
 
   this->mode = library::GemmUniversalMode::kGemm;
   if (this->split_k_mode == library::SplitKMode::kParallel) {
@@ -417,14 +418,15 @@
 
 }
 
 /// Initialize reduction problem dimensions and library::Operation
 bool GemmOperationProfiler::initialize_reduction_configuration_(
   library::Operation const *operation,
   ProblemSpace::Problem const &problem) {
+
   library::GemmDescription const &gemm_desc =
     static_cast<library::GemmDescription const&>(operation->description());
 
   if (!cast_from_double(problem_.alpha_one, gemm_desc.element_epilogue, 1)) {
     return false;
   }
 
@@ -573,15 +575,14 @@
   // Initialize the CUTLASS operation
   //
   Status status = Status::kSuccess;
 
   if (options.profiling.provider_enabled(library::Provider::kCUTLASS)) {
 
     if (options.execution_mode != ExecutionMode::kDryRun) {
-
       uint64_t workspace_size = underlying_operation->get_host_workspace_size(&gemm_workspace_.configuration);
       gemm_workspace_.host_workspace.resize(workspace_size, 0);
 
       workspace_size = underlying_operation->get_device_workspace_size(&gemm_workspace_.configuration,
                                                             &gemm_workspace_.arguments);
       gemm_workspace_.device_workspace.reset(library::NumericTypeID::kU8, workspace_size);
 
@@ -616,15 +617,14 @@
     results_.back().op_kind = library::OperationKind::kGemm;
     results_.back().disposition = Disposition::kNotRun;
 
     for (auto provider : verification_providers_) {
       results_.back().verification_map[provider] = Disposition::kNotRun;
     }
   }
-
   return status;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Verifies CUTLASS against references
 bool GemmOperationProfiler::verify_cutlass(
@@ -742,15 +742,21 @@
       else {
         // set verification map for cublas to not supported
         results_.back().verification_map[library::Provider::kCUBLAS] = Disposition::kNotSupported;
       }
     }
 #endif // #if CUTLASS_ENABLE_CUBLAS
 
-    bool verification_status = verify_with_reference_(options, report, device_context, operation, problem_space, problem);
+    library::GemmDescription const &gemm_desc =
+      static_cast<library::GemmDescription const &>(operation->description());
+
+
+    cutlass::library::NumericTypeID element_A = gemm_desc.A.element;
+    cutlass::library::NumericTypeID element_B = gemm_desc.B.element;
+    bool verification_status = verify_with_reference_(options, report, device_context, operation, problem_space, problem, element_A, element_B);
 
     // Update disposition to worst case verification outcome among all
     // verification providers which are supported
     bool is_any_verification_run_passed = false;
     for (auto &m : results_.back().verification_map) {
       if (m.second == Disposition::kFailed || m.second == Disposition::kIncorrect) {
         results_.back().disposition = m.second;
@@ -790,15 +796,14 @@
   Options const &options,
   PerformanceReport &report,
   DeviceContext &device_context,
   library::Operation const *operation,
   ProblemSpace const &problem_space,
   ProblemSpace::Problem const &problem) {
 
-
 #if CUTLASS_ENABLE_CUBLAS
 
   library::GemmDescription const &gemm_desc =
     static_cast<library::GemmDescription const &>(operation->description());
 
   //
   // Construct cuBLAS operators
@@ -909,16 +914,18 @@
 /// Verifies CUTLASS against host and device references
 bool GemmOperationProfiler::verify_with_reference_(
   Options const &options,
   PerformanceReport &report,
   DeviceContext &device_context,
   library::Operation const *operation,
   ProblemSpace const &problem_space,
-  ProblemSpace::Problem const &problem) {
-
+  ProblemSpace::Problem const &problem, 
+  cutlass::library::NumericTypeID element_A, 
+  cutlass::library::NumericTypeID element_B) 
+{
   library::GemmDescription const &gemm_desc =
     static_cast<library::GemmDescription const &>(operation->description());
 
   //
   // Initialize state
   //
 
@@ -973,21 +980,21 @@
       gemm_workspace_.configuration.problem_size.n(),
       gemm_workspace_.configuration.problem_size.k(),
       gemm_desc.tile_description.math_instruction.element_accumulator,
       gemm_desc.element_epilogue,
 
       problem_.alpha.data(),
 
-      gemm_desc.A.element,
+      element_A,
       gemm_desc.A.layout,
       gemm_desc.transform_A,
       ptr_A,
       int(gemm_workspace_.configuration.lda),
 
-      gemm_desc.B.element,
+      element_B,
       gemm_desc.B.layout,
       gemm_desc.transform_B,
       ptr_B,
       int(gemm_workspace_.configuration.ldb),
 
       problem_.beta.data(),
 
@@ -1007,15 +1014,14 @@
       gemm_workspace_.C->batch_stride(),
       gemm_workspace_.Reference->batch_stride());
 
     if (status != Status::kSuccess) {
       results_.back().verification_map[provider] = Disposition::kNotRun;
       continue;
     }
-
     results_.back().status = status;
 
     if (provider == library::Provider::kReferenceHost) {
       gemm_workspace_.Reference->copy_from_host(ptr_D);
     }
 
     //
```

## cutlass_library/source/tools/profiler/src/operation_profiler.cu

```diff
@@ -47,14 +47,16 @@
 // sleep not supported
 #endif
 
 #include "cutlass/profiler/options.h"
 #include "cutlass/profiler/operation_profiler.h"
 #include "cutlass/profiler/gpu_timer.h"
 
+#include "cutlass/trace.h"
+
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace profiler {
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -62,15 +64,15 @@
 
 /// Ctor
 OperationProfiler::OperationProfiler(
   Options const &options,
   library::OperationKind kind,
   ArgumentDescriptionVector const &arguments,
   ProviderVector const & verification_providers
-): 
+):
   kind_(kind), arguments_(arguments) {
 
   ArgumentDescriptionVector tile_description_arguments{
     {ArgumentTypeID::kEnumerated, {"op_class", "opcode-class"}, "Class of math instruction (simt, tensorop, wmmatensorop, wmma)"},
     {ArgumentTypeID::kEnumerated, {"accum", "accumulator-type"}, "Math instruction accumulator data type"},
     {ArgumentTypeID::kInteger, {"cta_m", "threadblock-shape::m"}, "Threadblock shape in the M dimension"},
     {ArgumentTypeID::kInteger, {"cta_n", "threadblock-shape::n"}, "Threadblock shape in the N dimension"},
@@ -89,16 +91,16 @@
     {ArgumentTypeID::kInteger, {"max_cc", "maximum-compute-capability"}, "Maximum device compute capability"}
   };
 
   arguments_.insert(arguments_.end(), tile_description_arguments.begin(), tile_description_arguments.end());
 
   for (auto provider : verification_providers) {
     if (std::find(
-      options.verification.providers.begin(), 
-      options.verification.providers.end(), 
+      options.verification.providers.begin(),
+      options.verification.providers.end(),
       provider) != options.verification.providers.end()) {
 
       verification_providers_.push_back(provider);
     }
   }
 
 }
@@ -114,22 +116,22 @@
 /// Prints usage statement for the math function
 void OperationProfiler::print_usage(std::ostream &out) const {
   for (auto const & desc : arguments_) {
 
     size_t const kAliasStart = 10;
 
     size_t columns = 0;
-    
+
     std::string type_str = to_string(desc.type);
     columns += type_str.size();
 
     out << "  [" << type_str << "]";
 
     if (columns < kAliasStart) {
-      out << std::string(kAliasStart - columns, ' ');  
+      out << std::string(kAliasStart - columns, ' ');
     }
 
     columns = 0;
 
     int j = 0;
     for (auto const & alias : desc.aliases) {
       columns += alias.size() + (j ? 1 : 0) + 2;
@@ -157,15 +159,14 @@
 
   library::OpcodeClassID opcode_class;
   if (arg_as_OpcodeClassID(opcode_class, "op_class", problem_space, problem)) {
     if (opcode_class != op_desc.tile_description.math_instruction.opcode_class) {
       return false;
     }
   }
-  
   int64_t int_value;
 
   if (arg_as_int(int_value, "inst_m", problem_space, problem)) {
     if (int64_t(op_desc.tile_description.math_instruction.instruction_shape.m()) != int_value) {
       return false;
     }
   }
@@ -248,22 +249,87 @@
       return false;
     }
   }
 
   return true;
 }
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
+#if defined(CUTLASS_DEBUG_TRACE_LEVEL) && (CUTLASS_DEBUG_TRACE_LEVEL > 1)
+
+std::ostream& operator<<(std::ostream& out, library::Provider provider) {
+  if (provider == library::Provider::kNone) {
+    out << "kNone";
+  }
+  else if (provider == library::Provider::kCUTLASS) {
+    out << "kCUTLASS";
+  }
+  else if (provider == library::Provider::kReferenceHost) {
+    out << "kReferenceHost";
+  }
+  else if (provider == library::Provider::kReferenceDevice) {
+    out << "kReferenceDevice";
+  }
+  else if (provider == library::Provider::kCUBLAS) {
+    out << "kCUBLAS";
+  }
+  else if (provider == library::Provider::kCUDNN) {
+    out << "kCUDNN";
+  }
+  else {
+    out << "kInvalid";
+  }
+
+  return out;
+}
+
+std::ostream& operator<<(std::ostream& out, library::OperationKind provider) {
+  if (provider == library::OperationKind::kGemm) {
+    out << "kGemm";
+  }
+  else if (provider == library::OperationKind::kRankK) {
+    out << "kRankK";
+  }
+  else if (provider == library::OperationKind::kRank2K) {
+    out << "kRank2K";
+  }
+  else if (provider == library::OperationKind::kTrmm) {
+    out << "kTrmm";
+  }
+  else if (provider == library::OperationKind::kSymm) {
+    out << "kSymm";
+  }
+  else if (provider == library::OperationKind::kConv2d) {
+    out << "kConv2d";
+  }
+  else if (provider == library::OperationKind::kConv3d) {
+    out << "kConv3d";
+  }
+  else if (provider == library::OperationKind::kEqGemm) {
+    out << "kEqGemm";
+  }
+  else if (provider == library::OperationKind::kSparseGemm) {
+    out << "kSparseGemm";
+  }
+  else if (provider == library::OperationKind::kReduction) {
+    out << "kReduction";
+  }
+  else {
+    out << "kInvalid";
+  }
+
+  return out;
+}
+
+#endif // defined(CUTLASS_DEBUG_TRACE_LEVEL) && (CUTLASS_DEBUG_TRACE_LEVEL > 1)
 
 /// Entry point to profile all operations in the manifest
 int OperationProfiler::profile_all(
-  Options const &options, 
-  library::Manifest const &manifest, 
+  Options const &options,
+  library::Manifest const &manifest,
   DeviceContext &device_context) {
-  
   ProblemSpace problem_space(arguments_, options.cmdline);
 
   // 1. Construct performance report
   PerformanceReport report(options, problem_space.argument_names(), kind_);
 
   // 2. For each problem in problem space
   ProblemSpace::Iterator problem_it = problem_space.begin();
@@ -278,39 +344,70 @@
     report.next_problem();
 
     // For each operation in manifest
     int matched_operation_count = 0;
     for (auto const& operation_ptr : manifest) {
 
       library::Operation const *operation = operation_ptr.get();
+#if defined(CUTLASS_DEBUG_TRACE_LEVEL) && (CUTLASS_DEBUG_TRACE_LEVEL > 1)
+      std::cerr << "  Operation: " << typeid(*operation).name() << "\n"
+                << "    name: " << operation->description().name << "\n"
+                << "    kind: " << operation->description().kind << "\n"
+                << "    provider: " << operation->description().provider << "\n";
+#endif // CUTLASS_DEBUG_TRACE_LEVEL
 
       auto min_cc = operation->description().tile_description.minimum_compute_capability;
       auto max_cc = operation->description().tile_description.maximum_compute_capability;
 
+#if defined(CUTLASS_DEBUG_TRACE_LEVEL) && (CUTLASS_DEBUG_TRACE_LEVEL > 1)
+      std::cerr << "    min_cc: " << min_cc << "\n";
+      std::cerr << "    max_cc: " << min_cc << "\n";
+#endif
+
       // Clear named allocations
       device_context.free();
 
+#if defined(CUTLASS_DEBUG_TRACE_LEVEL) && (CUTLASS_DEBUG_TRACE_LEVEL > 1)
+      if (operation->description().kind != kind_) {
+        std::cerr << "    @ kind " << operation->description().kind
+                  << " != kind_ " << kind_ << "\n";
+      }
+      if (operation->description().provider != library::Provider::kCUTLASS) {
+        std::cerr << "    @ provider " << operation->description().provider
+                  << " != library::Provider::kCUTLASS\n";
+      }
+      if (options.device.compute_capability() < min_cc) {
+        std::cerr << "    @ compute_capability "
+                  << options.device.compute_capability()
+                  << " < min_cc " << min_cc << "\n";
+      }
+      if (options.device.compute_capability() > max_cc) {
+        std::cerr << "    @ compute_capability "
+                  << options.device.compute_capability()
+                  << " > max_cc " << max_cc << "\n";
+      }
+#endif
+
       // Execute compatible cutlass operations if they satisfy the current device's compute capability
       if (operation->description().kind == kind_ &&
           operation->description().provider == library::Provider::kCUTLASS &&
           options.device.compute_capability() >= min_cc &&
           options.device.compute_capability() <= max_cc) {
 
         std::string operation_name(operation->description().name);
-
         // Filter kernels by name
         bool filtered_by_name = options.operation_names.empty();
         if (!filtered_by_name) {
-          
+
           for (auto const & op_name : options.operation_names) {
             if (find_string_matches_(op_name, operation_name)) {
               filtered_by_name = true;
               break;
             }
-          } 
+          }
         }
 
         for (auto const & op_name : options.excluded_operation_names) {
           if (find_string_matches_(op_name, operation_name)) {
             filtered_by_name = false;
             break;
           }
@@ -329,18 +426,18 @@
           report,
           device_context,
           operation,
           problem_space,
           problem);
 
         if (status == Status::kErrorInternal) {
-          
+
           // If there was an internal error, consume the CUDA error and move to the next operation.
           (void)cudaGetLastError();
-          
+
           report.append_results(results_);
           continue;
         }
         else if (status != Status::kSuccess) {
           // If the workspace could not be initialized for any other reason, continue to
           // the next operation.
           continue;
@@ -381,17 +478,17 @@
         //
 
         // B. Verify CUTLASS
         if (continue_profiling && options.profiling.provider_enabled(library::Provider::kCUTLASS)) {
 
           continue_profiling = this->verify_cutlass(
             options,
-            report, 
-            device_context, 
-            operation, 
+            report,
+            device_context,
+            operation,
             problem_space,
             problem);
 
           retval |= (not continue_profiling);
         }
 
         if (options.execution_mode == ExecutionMode::kDryRun) {
@@ -415,18 +512,18 @@
         //
         // D. Profile
         //
 
         if (continue_profiling && options.profiling.enabled) {
 
           continue_profiling = this->profile(
-            options, 
-            report, 
-            device_context, 
-            operation, 
+            options,
+            report,
+            device_context,
+            operation,
             problem_space,
             problem);
         }
 
         report.append_results(results_);
         results_.clear();
       }
@@ -455,15 +552,15 @@
   if (sleep_duration) {
     #ifdef __unix__
     usleep(sleep_duration * 1000);
     #elif defined(_WIN32) || defined(WIN32)
     SleepEx(sleep_duration, false);
     #else
     // sleep not supported
-    #endif 
+    #endif
   }
 }
 
 
 /// Compares tensors for equality
 Disposition OperationProfiler::compare_tensors(
   Options const &options,
@@ -481,24 +578,24 @@
     count = reference.capacity();
   }
 
   if (options.verification.epsilon == 0) {
 
     // bit-level equality
     passed = DeviceAllocation::block_compare_equal(
-      experimental.type(), 
+      experimental.type(),
       experimental.data(),
       reference.data(),
       count);
   }
   else {
 
     // relative error function
     passed = DeviceAllocation::block_compare_relatively_equal(
-      experimental.type(), 
+      experimental.type(),
       experimental.data(),
       reference.data(),
       count,
       options.verification.epsilon,
       options.verification.nonzero_floor);
   }
 
@@ -512,15 +609,15 @@
   library::OperationDescription const &desc,
   library::Provider provider,
   library::Provider verification_provider) {
 
   for (auto const & named_allocation : device_context) {
 
     DeviceAllocation *allocation = named_allocation.second;
-    
+
     std::stringstream filename;
 
     filename << desc.name << "_" << library::to_string(provider) << "_";
 
     if (verification_provider != library::Provider::kInvalid) {
       filename << "verified_by_" << library::to_string(verification_provider) << "_";
     }
@@ -531,15 +628,15 @@
 
     allocation->write_tensor_csv(out);
     out << "\n";
 
     if (options.report.verbose) {
       std::cout << "wrote '" << filename.str() << "'" << std::endl;
     }
-  } 
+  }
 }
 
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Method to profile a CUTLASS Operation
 Status OperationProfiler::profile_cutlass_(
@@ -571,30 +668,30 @@
       host_workspace,
       device_workspace);
 
     if (status != Status::kSuccess) {
       return status;
     }
   }
-  
+
   //
   // Initialize GPU timer
   //
 
   timer.start();
 
   //
   // Profiling loop
   //
 
   int Iterations = options.profiling.iterations;
 
   int iteration = 0;
   for (; iteration < Iterations; ++iteration) {
-    
+
     status = operation->run(
       arguments,
       host_workspace,
       device_workspace);
 
     if (status != Status::kSuccess) {
       return status;
@@ -606,23 +703,23 @@
   //
 
   timer.stop_and_wait();
 
   //
   // Update performance result
   //
-  
+
   runtime = timer.duration(iteration);
 
   return status;
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Sets operation description 
+/// Sets operation description
 void OperationProfiler::initialize_result_(
   PerformanceResult &result,
   library::OperationDescription const &operation_desc,
   ProblemSpace const &problem_space) {
 
   set_argument(result, "op_class", problem_space,
     library::to_string(operation_desc.tile_description.math_instruction.opcode_class));
@@ -653,32 +750,32 @@
   char const *name,
   ProblemSpace const &problem_space,
   std::string const &value) {
 
   result.arguments.at(problem_space.argument_index(name)) = make_pair(std::string(name), value);
 }
 
-void OperationProfiler::set_argument(  
+void OperationProfiler::set_argument(
   PerformanceResult &result,
   char const *name,
   ProblemSpace const &problem_space,
   int64_t value) {
 
   result.arguments.at(problem_space.argument_index(name)) = make_pair(std::string(name), library::lexical_cast(value));
 }
 
 
 /// finds string matches filter_string in operation_name
 bool OperationProfiler::find_string_matches_(
-  std::string const &filter_string, 
+  std::string const &filter_string,
   std::string const &operation_name) {
   // Returns true if all substrings appear in the operation_name in order
-  
+
   // Split filter_string of the format "gemm*f32*nt" to tokens ["gemm", "f32", "nt"]
-  std::string item;  
+  std::string item;
   std::istringstream iss(filter_string);
   std::vector<std::string> filter_tokens;
   while (std::getline(iss, item, '*')) {
     filter_tokens.push_back(item);
   }
 
   // Search filter_tokens in operation_name in order
@@ -688,15 +785,15 @@
     if (start < operation_name.length()) {
       // Find token in operation_name[start:]
       idx = operation_name.substr(start).find(token);
       if (idx == std::string::npos) {
         return false;
       }
     }
-    start += (idx + token.length()); 
+    start += (idx + token.length());
   }
 
   // All tokens in filter_string found in operation_name
   return true;
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/tools/profiler/src/symm_operation_profiler.cu

```diff
@@ -230,15 +230,15 @@
     operation_desc.C.layout, {int(this->m), int(this->n)}).front();
 
   return Status::kSuccess;
 }
 
 /// Total number of bytes loaded
 int64_t SymmOperationProfiler::SymmProblem::bytes(library::SymmDescription const &operation_desc) const {
-  int64_t bytes;
+  int64_t bytes = 0;
   // Input bytes read and Output bytes written for the gemm problem
   // Half matrix including the diagonal will have (X*(X+1))/2 elements
   if (operation_desc.side_mode == SideMode::kLeft) {
     bytes =
       int64_t(library::sizeof_bits(operation_desc.A.element) * m / 8) * (m + 1) / 2 +
       int64_t(library::sizeof_bits(operation_desc.B.element) * m / 8) * n + 
       int64_t(library::sizeof_bits(operation_desc.C.element) * m / 8) * n;
```

## cutlass_library/source/tools/util/include/cutlass/util/command_line.h

```diff
@@ -117,15 +117,15 @@
     }
   }
 
   /**
    * Returns the commandline parameter for a given index (not including flags)
    */
   template <typename value_t>
-  void get_cmd_line_argument(int index, value_t& val) const {
+  void get_cmd_line_argument(size_t index, value_t& val) const {
     using namespace std;
     if (index < args.size()) {
       istringstream str_stream(args[index]);
       str_stream >> val;
     }
   }
```

## cutlass_library/source/tools/util/include/cutlass/util/cublas_wrappers.hpp

```diff
@@ -59,15 +59,15 @@
 using ComplexDouble = cuda::std::complex<double>;
 }
 #endif // BLAM_COMPLEX_TYPES
 
 // User could potentially define Half instead of cute::
 #ifndef BLAM_HALF_TYPE
 #define BLAM_HALF_TYPE 1
-#include <cute/numeric/half.hpp>
+#include <cute/numeric/numeric_types.hpp>
 namespace blam {
 using Half = cute::half_t;
 }
 #endif // BLAM_HALF_TYPE
 
 namespace blam
 {
```

## cutlass_library/source/tools/util/include/cutlass/util/device_dump.h

```diff
@@ -65,15 +65,15 @@
              total_threads);
 
     __syncthreads();
 
     return;
   }
 
-  int total_elements = frag.size();
+  int total_elements = int(frag.size());
 
   if (M < 0 || M > total_elements) {
     if (thread_id == 0 && block_id == 0)
       printf("Element number M = %d should between [1, %d].\n", M,
              total_elements);
 
     __syncthreads();
```

## cutlass_library/source/tools/util/include/cutlass/util/device_rmsnorm.h

```diff
@@ -38,16 +38,16 @@
 #include "cutlass/tensor_ref.h"
 #include "cutlass/util/device_utils.h"
 #include <float.h>
 
 namespace cutlass {
 
 __global__ void rmsnorm_twoPassAlgo_e8(float4 *output, const float4 *input,
-				       const float4 *weight,
-				       const int m, const int n, float epsilon) {
+                                       const float4 *weight,
+                                       const int m, const int n, float epsilon) {
   const int m_idx = blockIdx.x;
   const int tid = threadIdx.x;
   const int bdimx = blockDim.x;
   __shared__ float s_mean;
   float local_sums[1] = {0.0f};
   const int n_8 = n / 8;
   int offset = m_idx * n_8;
@@ -111,17 +111,17 @@
 
     output[index] = tmp;
   }
 }
 
 template<typename T>
 __global__ void rmsnorm_twoPassAlgo_e1(T* output,
-				       const T* input,
-				       const T* weight,
-				       const int m, const int n,
+                                       const T* input,
+                                       const T* weight,
+                                       const int m, const int n,
                                        float epsilon)
 {
   const int m_idx = blockIdx.x;
   const int tid = threadIdx.x;
   const int bdimx = blockDim.x;
   __shared__ float s_mean;
   float local_sums[1] = {0.0f};
@@ -152,15 +152,15 @@
 }
 
 template <typename T>
 void rmsnorm(cutlass::MatrixCoord tensor_size,
              TensorRef<T, layout::RowMajor> ref_output,
              TensorRef<T, layout::RowMajor> ref_input,
              TensorRef<T, layout::RowMajor> ref_weight,
-             cudaStream_t stream, float epsilon = 1e-5){
+             cudaStream_t stream, float epsilon = 1e-5f){
   const int m = tensor_size.row();
   const int n = tensor_size.column();
   T* output = ref_output.data();
   const T* input = ref_input.data();
   const T* weight = ref_weight.data();
   dim3 grid(m);
```

## cutlass_library/source/tools/util/include/cutlass/util/host_tensor.h

```diff
@@ -108,17 +108,21 @@
   /// kBitsStoredVec          : The bits of store vec that could be divisiable by the element
   /// kElementsPerStoredVec   : The number of elements could be stored in per store vec
   /// kNumStoragePerStoredVec : How much storage(i.e. sizeof(element storage)) the store vec needs to consume.
   ///                           Usually the element storage of subbyte is uint8_t.
   /// Example
   ///  int2:  kBitsStoredVec = 8; kElementsPerStoredVec = 4; kNumStoragePerStoredVec = 1 uint8_t;
   ///  int4:  kBitsStoredVec = 8; kElementsPerStoredVec = 2; kNumStoragePerStoredVec = 1 uint8_t;
-  static int const kBitsStoredVec        = (sizeof_bits<Element>::value < 8) ? cutlass::lcm(static_cast<int>(sizeof_bits<Element>::value), 8) : sizeof_bits<Element>::value; 
-  static int const kElementsPerStoredVec = kBitsStoredVec / sizeof_bits<Element>::value;
-  static int const kNumStoragePerStoredVec = kBitsStoredVec / (sizeof(Element) * 8);
+  static constexpr int kBitsStoredVec        = (sizeof_bits<Element>::value < 8) ? cutlass::lcm(sizeof_bits<Element>::value, 8) : sizeof_bits<Element>::value; 
+  static constexpr int kElementsPerStoredVec = kBitsStoredVec / sizeof_bits<Element>::value;
+  static constexpr int kNumStoragePerStoredVec = kBitsStoredVec / (sizeof(Element) * 8);
+
+  static_assert(kBitsStoredVec != 0, "kBitsStoredVec can not be zero");
+  static_assert(kElementsPerStoredVec != 0, "kElementsPerStoredVec can not be zero");
+  static_assert(kNumStoragePerStoredVec != 0, "kNumStoragePerStoredVec can not be zero");
 
  private:
 
   //
   // Data members
   //
```

## cutlass_library/source/tools/util/include/cutlass/util/packed_stride.hpp

```diff
@@ -104,8 +104,382 @@
     cute::get<2>(s_copy) = static_cast<IntT>(0);
   }
   return s_copy;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+// Strides with group mode
+
+template <class StrideIntT>
+cute::Stride<StrideIntT, cute::Int<1>, cute::Int<0>>
+make_cute_packed_stride(cute::Stride<StrideIntT, cute::Int<1>, cute::Int<0>> s, cute::Shape<int,int,int> shape_MKL) {
+  static_assert(std::is_integral_v<StrideIntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+  auto s_copy = s;
+  cute::get<0>(s_copy) = static_cast<StrideIntT>(cute::get<1>(shape_MKL));
+  return s_copy;
+}
+
+template <class StrideIntT>
+cute::Stride<cute::Int<1>, StrideIntT, cute::Int<0>>
+make_cute_packed_stride(cute::Stride<cute::Int<1>, StrideIntT, cute::Int<0>> s, cute::Shape<int,int,int> shape_MKL) {
+  static_assert(std::is_integral_v<StrideIntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+  auto s_copy = s;
+  cute::get<1>(s_copy) = static_cast<StrideIntT>(cute::get<0>(shape_MKL));
+  return s_copy;
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+// Strides for convolutions
+
+// Output cutlass::layout::TensorNDHWC -> rank-3 stride (InT,_1,_0)
+// Note: For fprop/dgrad kernel, strides are assumed to be layout right in NZPQK/NDHWC order
+// and therefore can be coalesced to just q/w. For wgrad kernel, strides are assumed to be layout
+// right in KTRSC order and can be coalesced to just k.
+// We enforce this condition here with asserts.
+template <class IntT, size_t RankT_>
+cute::Stride<IntT, cute::Int<1>, cute::Int<0>>
+make_cute_packed_stride(
+    cute::Stride<IntT, cute::Int<1>, cute::Int<0>> s,
+    cute::array<int32_t, RankT_> shape_output,
+    cute::array<IntT, RankT_> stride_output,
+    cutlass::conv::Operator conv_op) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+  static_assert(RankT_ >= 3u);
+  constexpr static int RankT = static_cast<int>(RankT_);
+
+  assert(stride_output[RankT-1] == 1);
+  cute::for_each(cute::make_seq<RankT-2>{}, [&](auto i) {
+    assert(stride_output[i] == shape_output[i+1] * stride_output[i+1]);
+  });
+
+  auto s_copy = s;
+  cute::get<0>(s_copy) = (conv_op == cutlass::conv::Operator::kWgrad) ?
+      stride_output[0] :
+      stride_output[RankT-2];
+  return s_copy;
+}
+
+//
+// Activation tensor ((w, h, d, n), _1) for fprop kernel
+//
+
+// Activation cutlass::layout::TensorNWC -> rank-2 stride ((W,N),_1)
+template <class IntT>
+cute::Stride<cute::Stride<IntT, IntT>, cute::Int<1>>
+make_cute_packed_stride(
+    cute::Stride<cute::Stride<IntT, IntT>, cute::Int<1>> s,
+    cute::array<IntT, 3> stride_nwc,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+  assert(stride_nwc[2] == 1);
+  auto s_copy = s;
+  cute::get<0,0>(s_copy) = stride_nwc[1];
+  cute::get<0,1>(s_copy) = stride_nwc[0];
+  return s_copy;
+}
+
+// Activation cutlass::layout::TensorNHWC -> rank-2 stride ((W,H,N),_1)
+template <class IntT>
+cute::Stride<cute::Stride<IntT, IntT, IntT>, cute::Int<1>>
+make_cute_packed_stride(
+    cute::Stride<cute::Stride<IntT, IntT, IntT>, cute::Int<1>> s,
+    cute::array<IntT, 4> stride_nhwc,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+  assert(stride_nhwc[3] == 1);
+  auto s_copy = s;
+  cute::for_each(cute::make_seq<3>{}, [&](auto i) {
+    cute::get<0,i>(s_copy) = stride_nhwc[2-i];
+  });
+  return s_copy;
+}
+
+// Activation cutlass::layout::TensorNDHWC -> rank-2 stride ((W,H,D,N),_1)
+template <class IntT>
+cute::Stride<cute::Stride<IntT, IntT, IntT, IntT>, cute::Int<1>>
+make_cute_packed_stride(
+    cute::Stride<cute::Stride<IntT, IntT, IntT, IntT>, cute::Int<1>> s,
+    cute::array<IntT, 5> stride_ndhwc,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+
+  assert(stride_ndhwc[4] == 1);
+  auto s_copy = s;
+  cute::for_each(cute::make_seq<4>{}, [&](auto i) {
+    cute::get<0,i>(s_copy) = stride_ndhwc[3-i];
+  });
+  return s_copy;
+}
+
+//
+// Filter tensor (k, (_1, s, r, t)) for fprop kernel
+//
+
+// Filter cutlass::layout::TensorNWC -> rank-2 stride (k, (_1, s))
+template <class IntT>
+cute::Stride<IntT, cute::Stride<cute::Int<1>, IntT>>
+make_cute_packed_stride(
+    cute::Stride<IntT, cute::Stride<cute::Int<1>, IntT>> s,
+    cute::array<IntT, 3> stride_ksc,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+
+  assert(stride_ksc[2] == 1);
+  auto s_copy = s;
+  cute::get<0,0>(s_copy) = stride_ksc[0];
+  cute::get<1,1>(s_copy) = stride_ksc[1];
+  return s_copy;
+}
+
+// Filter cutlass::layout::TensorNHWC -> rank-2 stride (k, (_1, s, r))
+template <class IntT>
+cute::Stride<IntT, cute::Stride<cute::Int<1>, IntT, IntT>>
+make_cute_packed_stride(
+    cute::Stride<IntT, cute::Stride<cute::Int<1>, IntT, IntT>> s,
+    cute::array<IntT, 4> stride_krsc,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+
+  assert(stride_krsc[3] == 1);
+  auto s_copy = s;
+  cute::get<0,0>(s_copy) = stride_krsc[0];
+  cute::for_each(cute::make_seq<2>{}, [&](auto i) {
+    cute::get<1,2-i>(s_copy) = stride_krsc[i+1];
+  });
+  return s_copy;
+}
+
+// Filter cutlass::layout::TensorNDHWC -> rank-2 stride (k, (_1, s, r, t))
+template <class IntT>
+cute::Stride<IntT, cute::Stride<cute::Int<1>, IntT, IntT, IntT>>
+make_cute_packed_stride(
+    cute::Stride<IntT, cute::Stride<cute::Int<1>, IntT, IntT, IntT>> s,
+    cute::array<IntT, 5> stride_ktrsc,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+
+  assert(stride_ktrsc[4] == 1);
+  auto s_copy = s;
+  cute::get<0,0>(s_copy) = stride_ktrsc[0];
+  cute::for_each(cute::make_seq<3>{}, [&](auto i) {
+    cute::get<1,3-i>(s_copy) = stride_ktrsc[i+1];
+  });
+  return s_copy;
+}
+
+//
+// Activation tensor (_1, (w, h, d, n)) for wgrad kernel
+//
+// It is also Filter tensor ((_1), (k, s, r, t)) for dgrad kernel
+//
+
+// Activation cutlass::layout::TensorNWC -> rank-2 stride (_1, (W,N)) in wgrad
+// Filter cutlass::layout::TensorNWC -> rank-2 stride ((_1), (k, s)) in dgrad
+template <class IntT>
+cute::Stride<cute::Int<1>, cute::Stride<IntT, IntT>>
+make_cute_packed_stride(
+    cute::Stride<cute::Int<1>, cute::Stride<IntT, IntT>> s,
+    cute::array<IntT, 3> stride_nwc,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+
+  assert(stride_nwc[2] == 1);
+  auto s_copy = s;
+  if (ConvOp == cutlass::conv::Operator::kWgrad) {
+    cute::get<1,0>(s_copy) = stride_nwc[1];
+    cute::get<1,1>(s_copy) = stride_nwc[0];
+  }
+  else if (ConvOp == cutlass::conv::Operator::kDgrad) {
+    // stride_nwc in dgrad is ksc.
+    cute::get<1,0>(s_copy) = stride_nwc[0];
+    cute::get<1,1>(s_copy) = stride_nwc[1];
+  }
+  return s_copy;
+}
+
+// Activation cutlass::layout::TensorNHWC -> rank-2 stride (_1, (W,H,N)) in wgrad
+// Filter cutlass::layout::TensorNHWC -> rank-2 stride ((_1), (k, s, r)) in dgrad
+template <class IntT>
+cute::Stride<cute::Int<1>, cute::Stride<IntT, IntT, IntT>>
+make_cute_packed_stride(
+    cute::Stride<cute::Int<1>, cute::Stride<IntT, IntT, IntT>> s,
+    cute::array<IntT, 4> stride_nhwc,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+
+  assert(stride_nhwc[3] == 1);
+  auto s_copy = s;
+  if (ConvOp == cutlass::conv::Operator::kWgrad) {
+    cute::for_each(cute::make_seq<3>{}, [&](auto i) {
+      cute::get<1,i>(s_copy) = stride_nhwc[2-i];
+    });
+  }
+  else if (ConvOp == cutlass::conv::Operator::kDgrad) {
+    // stride_nhwc in dgrad is krsc.
+    cute::get<1,0>(s_copy) = stride_nhwc[0];
+    cute::for_each(cute::make_seq<2>{}, [&](auto i) {
+      cute::get<1,2-i>(s_copy) = stride_nhwc[i+1];
+    });
+  }
+  return s_copy;
+}
+
+// Activation cutlass::layout::TensorNDHWC -> rank-2 stride (_1, (W,H,D,N)) in wgrad
+// Filter cutlass::layout::TensorNDHWC -> rank-2 stride ((_1), (k, s, r, t)) in dgrad
+template <class IntT>
+cute::Stride<cute::Int<1>, cute::Stride<IntT, IntT, IntT, IntT>>
+make_cute_packed_stride(
+    cute::Stride<cute::Int<1>, cute::Stride<IntT, IntT, IntT, IntT>> s,
+    cute::array<IntT, 5> stride_ndhwc,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+
+  assert(stride_ndhwc[4] == 1);
+  auto s_copy = s;
+  if (ConvOp == cutlass::conv::Operator::kWgrad) {
+    cute::for_each(cute::make_seq<4>{}, [&](auto i) {
+      cute::get<1,i>(s_copy) = stride_ndhwc[3-i];
+    });
+  }
+  else if (ConvOp == cutlass::conv::Operator::kDgrad) {
+    // stride_ndhwc in dgrad is ktrsc.
+    cute::get<1,0>(s_copy) = stride_ndhwc[0];
+    cute::for_each(cute::make_seq<3>{}, [&](auto i) {
+      cute::get<1,3-i>(s_copy) = stride_ndhwc[i+1];
+    });
+  }
+  return s_copy;
+}
+
+//
+// NZPQ tensor (_1, nzpq) for wgrad kernel
+//
+
+// cutlass::layout::TensorNWC -> rank-2 stride (_1, nzpq)
+template <class IntT>
+cute::Stride<cute::Int<1>, IntT>
+make_cute_packed_stride(
+    cute::Stride<cute::Int<1>, IntT> s,
+    cute::array<IntT, 3> stride_nqk,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+
+  assert(stride_nqk[2] == 1);
+  auto s_copy = s;
+  cute::get<1>(s_copy) = stride_nqk[1];
+  return s_copy;
+}
+
+// cutlass::layout::TensorNHWC -> rank-2 stride (_1, nzpq)
+template <class IntT>
+cute::Stride<cute::Int<1>, IntT>
+make_cute_packed_stride(
+    cute::Stride<cute::Int<1>, IntT> s,
+    cute::array<IntT, 4> stride_npqk,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+
+  assert(stride_npqk[3] == 1);
+  auto s_copy = s;
+  cute::get<1>(s_copy) = stride_npqk[2];
+  return s_copy;
+}
+
+// cutlass::layout::TensorNDHWC -> rank-2 stride (_1, nzpq)
+template <class IntT>
+cute::Stride<cute::Int<1>, IntT>
+make_cute_packed_stride(
+    cute::Stride<cute::Int<1>, IntT> s,
+    cute::array<IntT, 5> stride_nzpqk,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+
+  assert(stride_nzpqk[4] == 1);
+  auto s_copy = s;
+  cute::get<1>(s_copy) = stride_nzpqk[3];
+  return s_copy;
+}
+
+
+
+//
+// Wgrad output tensor (k, (_1, s, r, t), _0)
+//
+
+// Filter cutlass::layout::TensorKCS -> rank-3 stride (k, (_1, s), _0)
+template <class IntT>
+cute::Stride<IntT, cute::Stride<cute::Int<1>, IntT>, cute::Int<0>>
+make_cute_packed_stride(
+    cute::Stride<IntT, cute::Stride<cute::Int<1>, IntT>, cute::Int<0>> s,
+    [[maybe_unused]] cute::array<int32_t, 3> shape_output,
+    cute::array<IntT, 3> stride_ksc,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+
+  assert(stride_ksc[2] == 1);
+  auto s_copy = s;
+  cute::get<0,0>(s_copy) = stride_ksc[0];
+  cute::get<1,1>(s_copy) = stride_ksc[1];
+  return s_copy;
+}
+
+// Filter cutlass::layout::TensorKCSR -> rank-3 stride (k, (_1, s, r), _0)
+template <class IntT>
+cute::Stride<IntT, cute::Stride<cute::Int<1>, IntT, IntT>, cute::Int<0>>
+make_cute_packed_stride(
+    cute::Stride<IntT, cute::Stride<cute::Int<1>, IntT, IntT>, cute::Int<0>> s,
+    [[maybe_unused]] cute::array<int32_t, 4> shape_output,
+    cute::array<IntT, 4> stride_krsc,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+
+  assert(stride_krsc[3] == 1);
+  auto s_copy = s;
+  cute::get<0,0>(s_copy) = stride_krsc[0];
+  cute::for_each(cute::make_seq<2>{}, [&](auto i) {
+    cute::get<1,2-i>(s_copy) = stride_krsc[i+1];
+  });
+  return s_copy;
+}
+
+// Filter cutlass::layout::TensorKCSRT -> rank-3 stride (k, (_1, s, r, t), _0)
+template <class IntT>
+cute::Stride<IntT, cute::Stride<cute::Int<1>, IntT, IntT, IntT>, cute::Int<0>>
+make_cute_packed_stride(
+    cute::Stride<IntT, cute::Stride<cute::Int<1>, IntT, IntT, IntT>, cute::Int<0>> s,
+    [[maybe_unused]] cute::array<int32_t, 5> shape_output,
+    cute::array<IntT, 5> stride_ktrsc,
+    conv::Operator ConvOp) {
+  static_assert(std::is_integral_v<IntT>,
+    "Stride must have an integral type so it can be set dynamically. Static strides not supported.");
+
+  assert(stride_ktrsc[4] == 1);
+  auto s_copy = s;
+  cute::get<0,0>(s_copy) = stride_ktrsc[0];
+  cute::for_each(cute::make_seq<3>{}, [&](auto i) {
+    cute::get<1,3-i>(s_copy) = stride_ktrsc[i+1];
+  });
+  return s_copy;
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 } // namespace cutlass
```

## cutlass_library/source/tools/util/include/cutlass/util/print_error.hpp

```diff
@@ -36,16 +36,17 @@
 #include <cmath>
 #include <iostream>
 #include <type_traits>
 
 #include <cute/util/type_traits.hpp>
 #include <cute/tensor.hpp>
 
-#include <cute/numeric/half.hpp>
+#include <cute/numeric/numeric_types.hpp>
 #include <cute/numeric/complex.hpp>
+
 #include <cutlass/layout/layout.h>
 
 // The computed infinity norm does not include
 // any NaN column absolute-value sums.
 struct matrix_inf_norm_result {
   // Accumulate errors in double, as this is generally
   // the highest precision that the examples use.
@@ -57,83 +58,103 @@
 // and thus passed by value (as std::span or std::string_view would be).
 // However, generic cute::Tensor are more like containers
 // and thus are best passed by reference or const reference.
 template <typename EngineType, typename LayoutType>
 matrix_inf_norm_result
 matrix_inf_norm(cute::Tensor<EngineType, LayoutType> const& host_matrix)
 {
-  using std::abs;
   using error_type = decltype(std::declval<matrix_inf_norm_result>().inf_norm);
   using element_type = typename EngineType::value_type;
 
   error_type inf_norm = 0.0;
   bool found_nan = false;
 
   // Computing the infinity norm requires that we be able
   // to treat the input as a matrix, with rows and columns.
   const int64_t num_rows = cute::size<0>(host_matrix);
   const int64_t num_cols = cute::size<1>(host_matrix);
 
-  for(int64_t i = 0; i < num_rows; ++i) {
+  auto abs_fn = [] (element_type A_ij) {
+    if constexpr (not std::is_unsigned_v<element_type>) {
+      using std::abs;
+      return abs(A_ij);
+    }
+    else {
+      return A_ij;
+    }
+  };
+
+  for (int64_t i = 0; i < num_rows; ++i) {
     error_type row_abs_sum = 0.0;
     for(int64_t j = 0; j < num_cols; ++j) {
-      row_abs_sum += abs(host_matrix(i, j));
+      row_abs_sum += abs_fn(host_matrix(i, j));
     }
-    if(std::isnan(row_abs_sum)) {
+    if (std::isnan(row_abs_sum)) {
       found_nan = true;
-    } else {
+    }
+    else {
       inf_norm = row_abs_sum > inf_norm ? row_abs_sum : inf_norm;
     }
   }
 
   return {inf_norm, found_nan};
 }
 
 // Infinity norm of (X - Y).
 template <typename EngineType, typename LayoutType>
 matrix_inf_norm_result
 matrix_diff_inf_norm(cute::Tensor<EngineType, LayoutType> const& X,
                      cute::Tensor<EngineType, LayoutType> const& Y)
 {
-  using std::abs;
   using error_type = decltype(std::declval<matrix_inf_norm_result>().inf_norm);
   using element_type = typename EngineType::value_type;
 
+  auto abs_fn = [] (element_type A_ij) {
+    if constexpr (not std::is_unsigned_v<element_type>) {
+      using std::abs;
+      return abs(A_ij);
+    }
+    else {
+      return A_ij;
+    }
+  };
+
   assert(cute::size<0>(X) == cute::size<0>(Y));
   assert(cute::size<1>(X) == cute::size<1>(Y));
 
   // Computing the infinity norm requires that we be able
   // to treat the input as a matrix, with rows and columns.
   const int64_t num_rows = cute::size<0>(X);
   const int64_t num_cols = cute::size<1>(X);
 
   error_type inf_norm = 0.0;
   bool found_nan = false;
 
-  for(int64_t i = 0; i < num_rows; ++i) {
+  for (int64_t i = 0; i < num_rows; ++i) {
     error_type row_abs_sum = 0.0;
-    for(int64_t j = 0; j < num_cols; ++j) {
-      row_abs_sum += error_type(abs(element_type(X(i,j)) - 
-                                    element_type(Y(i,j))));
+    for (int64_t j = 0; j < num_cols; ++j) {
+      row_abs_sum += error_type(abs_fn(element_type(X(i,j)) -
+                                       element_type(Y(i,j))));
     }
-    if(std::isnan(row_abs_sum)) {
+    if (std::isnan(row_abs_sum)) {
       found_nan = true;
-    } else {
+    }
+    else {
       inf_norm = row_abs_sum > inf_norm ? row_abs_sum : inf_norm;
     }
   }
 
   return {inf_norm, found_nan};
 }
 
 template <typename EngineType_A, typename LayoutType_A,
           typename EngineType_B, typename LayoutType_B,
           typename EngineType_C, typename LayoutType_C,
           typename EngineType_C_ref, typename LayoutType_C_ref>
-void
+auto
 print_matrix_multiply_mollified_relative_error(
   char const A_value_type_name[],
   cute::Tensor<EngineType_A, LayoutType_A> const& A,
   char const B_value_type_name[],
   cute::Tensor<EngineType_B, LayoutType_B> const& B,
   char const C_value_type_name[],
   cute::Tensor<EngineType_C, LayoutType_C> const& C,
@@ -153,46 +174,47 @@
   // Printing the infinity norm of C is a way to check
   // that both the function being tested (C)
   // and the reference implementation (C_ref)
   // don't just do nothing (or fill with zeros).
   using std::cout;
   using cute::shape;
   cout << "Matrix A: " << shape<0>(A) << "x" << shape<1>(A) << " of " << A_value_type_name << '\n'
-       << "Matrix B: " << shape<0>(B) << "x" << shape<1>(B) << " of " << B_value_type_name << '\n'
-       << "Matrix C: " << shape<0>(C) << "x" << shape<1>(C) << " of " << C_value_type_name << '\n'
-       << std::scientific
-       << "Infinity norm of A: " << A_norm << '\n'
-       << "Infinity norm of B: " << B_norm << '\n'
-       << "Infinity norm of C: " << C_norm << '\n'
-       << "Infinity norm of (C - C_ref): " << diff_norm << '\n';
+      << "Matrix B: " << shape<0>(B) << "x" << shape<1>(B) << " of " << B_value_type_name << '\n'
+      << "Matrix C: " << shape<0>(C) << "x" << shape<1>(C) << " of " << C_value_type_name << '\n'
+      << std::scientific
+      << "Infinity norm of A: " << A_norm << '\n'
+      << "Infinity norm of B: " << B_norm << '\n'
+      << "Infinity norm of C: " << C_norm << '\n'
+      << "Infinity norm of (C - C_ref): " << diff_norm << '\n';
 
   if(A_norm_times_B_norm == 0.0) {
     cout << "Mollified relative error: " << relative_error << '\n';
   } else {
     cout << "Relative error: " << relative_error << '\n';
   }
 
   if (A_has_nan || B_has_nan || C_has_nan || diff_has_nan) {
-    cout << "Did we encounter NaN in A? " << (A_has_nan ? "yes" : "no") << '\n' 
-         << "Did we encounter NaN in B? " << (B_has_nan ? "yes" : "no") << '\n'
-         << "Did we encounter NaN in C? " << (C_has_nan ? "yes" : "no") << '\n'
-         << "Did we encounter NaN in (C - C_ref)? " << (diff_has_nan ? "yes" : "no") << '\n';
+    cout << "Did we encounter NaN in A? " << (A_has_nan ? "yes" : "no") << '\n'
+        << "Did we encounter NaN in B? " << (B_has_nan ? "yes" : "no") << '\n'
+        << "Did we encounter NaN in C? " << (C_has_nan ? "yes" : "no") << '\n'
+        << "Did we encounter NaN in (C - C_ref)? " << (diff_has_nan ? "yes" : "no") << '\n';
   }
+  return relative_error;
 }
 
 template <typename EngineType, typename LayoutType>
-void
+auto
 print_matrix_multiply_mollified_relative_error(
   const char value_type_name[],
   const cute::Tensor<EngineType, LayoutType>& A,
   const cute::Tensor<EngineType, LayoutType>& B,
   const cute::Tensor<EngineType, LayoutType>& C_computed,
   const cute::Tensor<EngineType, LayoutType>& C_expected)
 {
-  print_matrix_multiply_mollified_relative_error(value_type_name, A, value_type_name, B,
+  return print_matrix_multiply_mollified_relative_error(value_type_name, A, value_type_name, B,
                                                  value_type_name, C_computed, C_expected);
 }
 
 // Take a CUTLASS HostTensor (or the like) as input,
 // and return a const CuTe Tensor.
 // This is useful for use with the above error printing functions.
 // This implicitly "transposes" if the layout is RowMajor.
@@ -229,15 +251,16 @@
 template <typename T1, typename T2>
 int
 print_relative_error(
     std::size_t n,
     T1 const& data,
     T2 const& reference,
     bool print_verbose = false,
-    bool print_error = true) {
+    bool print_error = true,
+    double error_margin = 0.00001) {
   using std::abs; using std::sqrt;
 
   // Use either double or complex<double> for error computation
   using value_type = cute::remove_cvref_t<decltype(reference[0])>;
   using error_type = std::conditional_t<cute::is_complex<value_type>::value,
                                         cute::complex<double>,
                                         double>;
@@ -248,46 +271,71 @@
 
   double eps = 1e-200;
 
   double tot_error_sq = 0;
   double tot_norm_sq = 0;
   double tot_ind_rel_err = 0;
   double max_ind_rel_err = 0;
-  for (std::size_t i = 0; i < n; ++i)
-  {
+  double max_diff = 0;
+  for (std::size_t i = 0; i < n; ++i) {
     error_type val = data[i];
     error_type ref = reference[i];
 
     double aref = abs(ref);
     double diff = abs(ref - val);
     double rel_error = diff / (aref + eps);
 
     // Individual relative error
     tot_ind_rel_err += rel_error;
 
     // Maximum relative error
     max_ind_rel_err  = std::max(max_ind_rel_err, rel_error);
 
+    // Maximum delta in value error
+    max_diff = std::max(max_diff, diff);
+
     // Total relative error
     tot_error_sq += diff * diff;
     tot_norm_sq  += aref * aref;
 
     if (print_verbose) {
       std::cout << i << ":\t" << val << "\t" << ref << "\t" << rel_error << std::endl;
     }
   }
 
-  printf("Vector reference  norm: [%.5e]\n", sqrt(tot_norm_sq));
+  double ave_rel_err = tot_ind_rel_err / double(n);
+  if (print_error) {
+    printf("Average relative error: %.3e\n", ave_rel_err);
+  }
+
+  if (print_error) {
+    printf("Maximum relative error: %.3e\n", max_ind_rel_err);
+  }
+
+  if (print_error) {
+    printf("Maximum difference    : %.3e\n", max_diff);
+  }
 
   double tot_rel_err = sqrt(tot_error_sq/(tot_norm_sq+eps));
-  if (print_error)
-    printf("Vector  relative error: [%.5e]\n", tot_rel_err);
+  if (print_error) {
+    printf("Vector relative error:  %.3e\n", tot_rel_err);
+  }
 
-  double ave_rel_err = tot_ind_rel_err / double(n);
-  if (print_error)
-    printf("Average relative error: [%.5e]\n", ave_rel_err);
+  printf("Vector reference  norm: %.3e\n", sqrt(tot_norm_sq));
 
-  if (print_error)
-    printf("Maximum relative error: [%.5e]\n", max_ind_rel_err);
+  return (tot_rel_err <= error_margin) ? EXIT_SUCCESS : EXIT_FAILURE;
+}
 
-  return (tot_rel_err == 0.0) ? EXIT_SUCCESS : EXIT_FAILURE;
+// Overload for cute::Tensor<>
+template <class Engine, class Layout>
+int
+print_relative_error(
+    cute::Tensor<Engine, Layout> data,
+    cute::Tensor<Engine, Layout> reference,
+    bool print_verbose = false,
+    bool print_error = true,
+    double error_margin = 0.00001) {
+  assert(size(data) == size(reference));
+  return print_relative_error(static_cast<std::size_t>(size(data)),
+                              data, reference,
+                              print_verbose, print_error, error_margin);
 }
```

## cutlass_library/source/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h

```diff
@@ -64,15 +64,15 @@
   }
 };
 
 template <int Rank>
 struct LinearToCoordinateHelper<Rank, 0> {
 
   CUTLASS_HOST_DEVICE
-  void operator()(Coord<Rank> &coord, int64_t idx, Coord<Rank> const &extent) const {
+  void operator()(Coord<Rank> &coord, int64_t idx, Coord<Rank> const &) const {
     coord[Rank - 1] = int(idx);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <int Rank>
```

## cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_fill.h

```diff
@@ -130,17 +130,16 @@
       int int_scale_ = -1
     ):
       seed(seed_), 
       mean(static_cast<FloatType>(mean_)), 
       stddev(static_cast<FloatType>(stddev_)), 
       int_scale(int_scale_) {
 
-      float_scale_up = FloatType(IntType(1) << int_scale);
-      float_scale_up += FloatType(0.5) * float_scale_up;
-      float_scale_down = FloatType(1) / FloatType(IntType(1) << int_scale);
+      float_scale_up = FloatType(IntType(2) << int_scale); // scale up to clamp low order bits
+      float_scale_down = FloatType(1) / FloatType(IntType(2) << int_scale);
     }
   };
 
   //
   // Data members
   //
 
@@ -168,16 +167,16 @@
   Element operator()() {
 
     FloatType rnd = random_normal_float<FloatType>(&rng_state);
     rnd = params.mean + params.stddev * rnd;
 
     Element result;
     if (params.int_scale >= 0) {
-      rnd = FloatType(IntType(rnd * params.float_scale_up));
-      result = Element(rnd * params.float_scale_down);
+      rnd = FloatType(IntType(std::llround(rnd * params.float_scale_up)));
+      result = Element(IntType(rnd * params.float_scale_down));
     }
     else {
       result = Element(rnd);
     }
 
     return result;
   }
@@ -444,17 +443,16 @@
       int int_scale_ = -1
     ):
       seed(seed_), 
       range(static_cast<FloatType>(max_ - min)), 
       max(static_cast<FloatType>(max_)),
       int_scale(int_scale_) {
 
-      float_scale_up = FloatType(IntType(1) << int_scale);
-      float_scale_up += FloatType(0.5) * float_scale_up;
-      float_scale_down = FloatType(1) / FloatType(IntType(1) << int_scale);
+      float_scale_up = FloatType(IntType(2) << int_scale); // scale up to clamp low order bits
+      float_scale_down = FloatType(1) / FloatType(IntType(2) << int_scale);
     }
   };
 
   //
   // Data members
   //
 
@@ -485,16 +483,16 @@
     rnd = params.max - params.range * rnd;
 
     // Random values are cast to integer after scaling by a power of two to facilitate error
     // testing
     Element result;
 
     if (params.int_scale >= 0) {
-      rnd = FloatType(IntType(rnd * params.float_scale_up));
-      result = Element(rnd * params.float_scale_down);
+      rnd = FloatType(IntType(std::llround(rnd * params.float_scale_up)));
+      result = Element(IntType(rnd * params.float_scale_down));
     }
     else {
       result = Element(rnd);
     }
 
     return result;
   }
@@ -770,17 +768,21 @@
       uint64_t seed_ = 0, 
       int MetaSizeInBits_ = 2 
     ):
       seed(seed_), 
       MetaSizeInBits(MetaSizeInBits_) {
       if (MetaSizeInBits_ == 2) {
         range = 6;
-      } else if (MetaSizeInBits_ == 4) {
+      }
+      else if (MetaSizeInBits_ == 4) {
         range = 2;
       }
+      else {
+        throw std::invalid_argument("Invalid MetaSizeInBits");
+      }
     }
   };
 
   //
   // Data members
   //
 
@@ -1157,42 +1159,18 @@
   typedef typename TensorView::TensorCoord TensorCoord;
 
   /// 
   static_assert((Layout::kRank == 2), "TensorClearPartial is only supported for matrices");
 
   /// Parameters structure
   struct Params {
-
-    //
-    // Data members
-    //
-
-    TensorView view;
-    Element element;
-    FillMode fill_mode;
-    int alignment;
-
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Params(): fill_mode(FillMode::kNone) { }
-
-    //
-    // Methods
-    //
-
-    /// Construction of Gaussian RNG functor.
-    Params(
-      TensorView view_,
-      Element element_,
-      FillMode fill_mode_,
-      int alignment_
-    ):
-      view(view_), element(element_), fill_mode(fill_mode_), alignment(alignment_) {
-
-    }
+    TensorView view{};
+    Element element{};
+    FillMode fill_mode{FillMode::kNone};
+    int alignment{0};
   };
 
   //
   // Data members
   //
 
   /// Parameters object
@@ -1303,15 +1281,15 @@
   cudaStream_t stream = nullptr) {
 
   typedef detail::TensorClearPartialFunc<Element, Layout> Func;
   typedef typename Func::Params Params;
 
   TensorForEach<Func, Layout::kRank, Params>(
     view.extent(),
-    Params(view, element, fill_mode, alignment),
+    Params{view, element, fill_mode, alignment},
     /*grid_size*/0, /*block_size*/0,
     stream
   );
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -1731,15 +1709,15 @@
   Element s = Element(0)) {
 
   using Layout = layout::PackedVectorLayout;
   Layout::TensorCoord size(static_cast<Layout::Index>(capacity)); // -Wconversion
   Layout layout = Layout::packed(size);
   TensorView<Element, Layout> view(ptr, layout, size);
 
-  Array<Element, Layout::kRank> c;
+  Array<Element, Layout::kRank> c{};
   c[0] = v;
 
   TensorFillLinear(view, c, s);
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 ///////////////////////////////////////////////////////////////////////////////////////////////////
```

## cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_reduce.h

```diff
@@ -116,15 +116,15 @@
   TensorView<Element, Layout> view_B,   /// View of the tensor to reduce over
   ComputeType identity,                 /// Identity element of the reduction operation
   ReduceOp reduce,                      /// Reduces an accumulated value with a transformed element: f(ComputeType, ComputeType) => ComputeType
   TransformOp transform,                /// Transforms the tensor element to ComputeType: g(Element) => ComputeType
   ComputeType *workspace) {             /// Device-side workspace for accumulating partial results. The reduced element is stored in workspace[0]
   
   int64_t idx = threadIdx.x + blockIdx.x * blockDim.x;
-  int64_t size = view_A.size();
+  auto size = static_cast<int64_t>(view_A.size());
 
   __shared__ ComputeType scratchpad[kBlockSize];
 
   for (; idx < size; idx += blockDim.x * gridDim.x) {
 
     // Map linear thread ID onto tensor coordinate
     typename Layout::TensorCoord coord;
```

## cutlass_library/source/tools/util/include/cutlass/util/reference/host/convolution.h

```diff
@@ -193,15 +193,15 @@
         }
       }
     }
   }
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
-/// Dgrad
+/// Dgrad / Deconv
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// dx = dgrad(dy, w)
 template <
   typename ElementA,
   typename LayoutA,
   typename ElementB,
@@ -217,15 +217,16 @@
 void Conv2dDgrad(
   cutlass::conv::Conv2dProblemSize problem_size,
   TensorRef<ElementA, LayoutA> tensor_dy,
   TensorRef<ElementB, LayoutB> tensor_w,
   TensorRef<ElementC, LayoutC> tensor_dx_in,
   TensorRef<ElementD, LayoutC> tensor_dx_out,
   ElementCompute alpha,
-  ElementCompute beta) {
+  ElementCompute beta,
+  bool is_deconv = false) {
 
   ConvertOp convert_op;
   InnerProductOp inner_product_op;
 
   // Apply MMA and accumulate ElementAccumulator
   for (int n = 0; n < problem_size.N; ++n) {
     for (int h = 0; h < problem_size.H; ++h) {
@@ -268,15 +269,16 @@
                   << s << ") [" 
                   << ((p < problem_size.P && q < problem_size.Q) ? "true":"false") << "]"        
                   << std::endl;
 #endif
                   if (p < problem_size.P && q < problem_size.Q) {
 
                     ElementA a = tensor_dy.at(cutlass::make_Coord(n, p, q, k));
-                    ElementB b = tensor_w.at(cutlass::make_Coord(k, r, s, c));
+                    ElementB b = is_deconv ? tensor_w.at(cutlass::make_Coord(c, r, s, k))
+                        : tensor_w.at(cutlass::make_Coord(k, r, s, c));
 
                     acc = inner_product_op(ElementAccumulator(a), ElementAccumulator(b), acc);
                   }
                 }
 
               } // for (K)
             } // for (S)
@@ -416,24 +418,25 @@
       ElementCompute,
       ElementAccumulator,
       ElementD,
       ConvertOp, InnerProductOp
     >(problem_size, tensor_A, tensor_B, tensor_C, tensor_D, alpha, beta);
     break;
 
+  case conv::Operator::kDeconv:
   case conv::Operator::kDgrad:
     Conv2dDgrad<
       ElementA, LayoutA,
       ElementB, LayoutB,
       ElementC, LayoutC,
       ElementCompute,
       ElementAccumulator,
       ElementD,
       ConvertOp, InnerProductOp
-    >(problem_size, tensor_A, tensor_B, tensor_C, tensor_D, alpha, beta);
+    >(problem_size, tensor_A, tensor_B, tensor_C, tensor_D, alpha, beta, (convolutional_operator == conv::Operator::kDeconv));
     break;
 
   case conv::Operator::kWgrad:
     Conv2dWgrad<
       ElementA, LayoutA,
       ElementB, LayoutB,
       ElementC, LayoutC,
@@ -533,15 +536,15 @@
         }
       }
     }
   }
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
-/// Dgrad
+/// Dgrad / Deconv
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// dx = dgrad(dy, w)
 template <
   typename ElementA,
   typename LayoutA,
   typename ElementB,
@@ -556,15 +559,16 @@
 void Conv3dDgrad(
   cutlass::conv::Conv3dProblemSize problem_size,
   TensorRef<ElementA, LayoutA> tensor_dy,
   TensorRef<ElementB, LayoutB> tensor_w,
   TensorRef<ElementC, LayoutC> tensor_dx_in,
   TensorRef<ElementC, LayoutC> tensor_dx_out,
   ElementCompute alpha,
-  ElementCompute beta) {
+  ElementCompute beta,
+  bool is_deconv = false) {
 
   ConvertOp convert_op;
   InnerProductOp inner_product_op;
 
   // Apply MMA and accumulate ElementAccumulator
   for (int n = 0; n < problem_size.N; ++n) {
     for (int d = 0; d < problem_size.D; ++d) {
@@ -600,16 +604,16 @@
                       z = z / problem_size.stride_d;
                       p = p / problem_size.stride_h;
                       q = q / problem_size.stride_w;
                       
                       if (z < problem_size.Z && p < problem_size.P && q < problem_size.Q) {
 
                         ElementA a = tensor_dy.at(cutlass::make_Coord(n, z, p, q, k));
-                        ElementB b = tensor_w.at(cutlass::make_Coord(k, t, r, s, c));
-
+                        ElementB b = is_deconv ? tensor_w.at(cutlass::make_Coord(c, t, r, s, k))
+                            : tensor_w.at(cutlass::make_Coord(k, t, r, s, c));
                         acc = inner_product_op(ElementAccumulator(a), ElementAccumulator(b), acc);
                       }
                     }
 
                   } // for (K)
                 } // for (S)
               } // for (R)
@@ -756,23 +760,24 @@
       ElementC, LayoutC,
       ElementCompute,
       ElementAccumulator,
       ConvertOp, InnerProductOp
     >(problem_size, tensor_A, tensor_B, tensor_C, tensor_D, alpha, beta);
     break;
 
+  case conv::Operator::kDeconv:
   case conv::Operator::kDgrad:
     Conv3dDgrad<
       ElementA, LayoutA,
       ElementB, LayoutB,
       ElementC, LayoutC,
       ElementCompute,
       ElementAccumulator, 
       ConvertOp, InnerProductOp
-    >(problem_size, tensor_A, tensor_B, tensor_C, tensor_D, alpha, beta);
+    >(problem_size, tensor_A, tensor_B, tensor_C, tensor_D, alpha, beta, (convolutional_operator == conv::Operator::kDeconv));
     break;
 
   case conv::Operator::kWgrad:
     Conv3dWgrad<
       ElementA, LayoutA,
       ElementB, LayoutB,
       ElementC, LayoutC,
```

## cutlass_library/source/tools/util/include/cutlass/util/reference/host/gett.hpp

```diff
@@ -31,18 +31,19 @@
 /*! \file
     \brief Reference implementation for GETT in host-side code.
 */
 
 #pragma once
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
+#include "cutlass/gemm/gemm.h"
 #include "cutlass/complex.h"
 #include "cutlass/numeric_conversion.h"
 #include "cutlass/epilogue/thread/activation.h"
+#include "cutlass/relatively_equal.h"
 
 #include "cute/tensor.hpp"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass::reference::host {
 
@@ -111,15 +112,14 @@
   using ActivationFunctor = ActivationFunctor_;
   using BiasBinaryOp = BiasBinaryOp_;
 
   using EngineC = typename TensorC::engine_type;
   using LayoutC = typename TensorC::layout_type;
   using EngineD =  typename TensorD::engine_type;
   using LayoutD = typename TensorD::layout_type;
-
   static constexpr bool PerColumnBias = PerColumnBias_;
 
   ElementScalar alpha = ElementScalar(1);
   ElementScalar beta = ElementScalar(0);
 
   TensorC C{};
   TensorD D{};
@@ -232,14 +232,15 @@
 
     // do compute
     for (int m_b = 0; m_b < kBlockM; ++m_b) {
       for (int n_b = 0; n_b < kBlockN; ++n_b) {
         acc[m_b][n_b] = fma_op(a_frag[m_b], b_frag[n_b], acc[m_b][n_b]);
       }
     }
+
   }
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// GETT - Epilogue
 template <class EpilogueParams, class ElementAccumulator, int kBlockM, int kBlockN>
```

## cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_fill.h

```diff
@@ -35,14 +35,15 @@
 #pragma once
 
 // Standard Library includes
 #include <utility>
 #include <cstdlib>
 #include <cmath>
 #include <random>
+#include <stdexcept>
 
 // Cutlass includes
 #include "cutlass/cutlass.h"
 #include "cutlass/complex.h"
 #include "cutlass/quaternion.h"
 #include "cutlass/array.h"
 #include "cutlass/numeric_types.h"
@@ -192,15 +193,15 @@
     std::mt19937 bernoulli_rnd(rnd_device());
     std::bernoulli_distribution bernoulli_dist(pnz / 100);
     bool bernoulli_result = bernoulli_dist(bernoulli_rnd);
 
     // Sample from the Gaussian distribution for a nonzero element
     if (bernoulli_result) {
       if (int_scale >= 0) {
-        rnd = double(int64_t(rnd * double(1 << int_scale))) / double(1 << int_scale);
+        rnd = double(std::llround(rnd * double(1 << int_scale))) / double(1 << int_scale);
         result = static_cast<Element>(rnd);
       }
       else {
         result = static_cast<Element>(rnd);
       }
     }
     else {
@@ -563,15 +564,15 @@
 
     rnd = min + range * rnd;
 
     // Random values are cast to integer after scaling by a power of two to facilitate error
     // testing
     Element result;
     if (int_scale >= 0) {
-      rnd = double(int64_t(rnd * double(1 << int_scale))) / double(1 << int_scale);
+      rnd = double(std::llround(rnd * double(1 << int_scale))) / double(1 << int_scale);
       result = static_cast<Element>(Real(rnd));
     }
     else {
       result = static_cast<Element>(Real(rnd));
     }
 
     return result;
@@ -1377,17 +1378,21 @@
     uint64_t seed_ = 0, 
     int MetaSizeInBits_ = 2
   ):
     seed(seed_), MetaSizeInBits(MetaSizeInBits_) {
       std::srand((unsigned)seed);
       if (MetaSizeInBits_ == 2) {
         range = 6;
-      } else if (MetaSizeInBits_ == 4) {
+      }
+      else if (MetaSizeInBits_ == 4) {
         range = 2;
       }
+      else {
+        throw std::invalid_argument("Invalid MetaSizeInBits");
+      }
     }
 
   /// Compute random value and update RNG state
   Element operator()() const {
     Element FourToTwoMeta[6] = {0x4, 0x8, 0x9, 0xc, 0xd, 0xe};
     Element TwoToOneMeta[2] = {0x4, 0xe};
```

## cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_reduce.h

```diff
@@ -57,15 +57,15 @@
 ComputeType TensorTransformReduce(
   TensorView<Element, Layout> view,
   ComputeType identity,
   ReduceOp reduce,
   TransformOp transform
 ) {
 
-  for (int64_t idx = 0; idx < view.size(); ++idx) {
+  for (int64_t idx = 0; idx < int64_t(view.size()); ++idx) {
     typename Layout::TensorCoord coord;
     cutlass::reference::detail::LinearToCoordinate<Layout::kRank>()(coord, idx, view.extent());
 
     if (view.contains(coord)) {
       Element x = view.at(coord);
       identity = reduce(identity, transform(x));
     }
@@ -90,15 +90,15 @@
   ReduceOp reduce,
   TransformOp transform) {
   
   if (view_A.extent() != view_B.extent()) {
     throw std::runtime_error("Tensor extents must match.");
   }
 
-  for (int64_t idx = 0; idx < view_A.size(); ++idx) {
+  for (int64_t idx = 0; idx < int64_t(view_A.size()); ++idx) {
 
     typename Layout::TensorCoord coord;
     cutlass::reference::detail::LinearToCoordinate<Layout::kRank>()(coord, idx, view_A.extent());
 
     if (view_A.contains(coord)) {
       Element a = view_A.at(coord);
       Element b = view_B.at(coord);
```

## Comparing `cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_tensor.hpp` & `cutlass_library/source/examples/common/gather_tensor.hpp`

 * *Files 2% similar despite different names*

```diff
@@ -28,14 +28,15 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
 #include "cute/layout.hpp"
 #include "cute/tensor.hpp"
+#include "cute/util/print.hpp"
 
 namespace example {
 
 using namespace cute;
 
 // Empty type used to disable gather/scatter for a GEMM argument
 struct NoGather
@@ -55,15 +56,15 @@
   CUTE_HOST_DEVICE constexpr
   Index
   operator()(I i) const { return indices_[i]; }
 
   CUTE_HOST_DEVICE friend
   void 
   print(IndexedGather const &s) {
-    print("Indexed");
+    cute::print("Indexed");
   }
 
   Index const *indices_;
 };
 
 /// Function object that applies a stride to its argument
 /// Example: StridedFunc<int,_2> gathers every other row/column
@@ -77,17 +78,17 @@
   CUTE_HOST_DEVICE constexpr
   auto
   operator()(I i) const { return i * stride_; }
 
   CUTE_HOST_DEVICE friend
   void 
   print(StridedGather const &s) {
-    print("Strided{");
+    cute::print("Strided{");
     print(s.stride_);
-    print("}");
+    cute::print("}");
   }
 
   Stride stride_;
 };
 
 /// Custom stride object that applies a function followed by a stride
 template <class Func, class Stride>
@@ -105,19 +106,19 @@
   CUTE_HOST_DEVICE constexpr friend
   auto
   operator*(CustomStride const &s, I i) { return s.func_(i) * s.stride_; }
 
   CUTE_HOST_DEVICE friend
   void
   print(CustomStride const & s) {
-    print("Custom{");
+    cute::print("Custom{");
     print(s.func_);
-    print(",");
+    cute::print(",");
     print(s.stride_);
-    print("}");
+    cute::print("}");
   }
 
   template<class Div>
   CUTE_HOST_DEVICE constexpr friend
   auto
   safe_div(CustomStride const &s, Div const &div)
   {
```

## Comparing `cutlass_library/source/include/cute/tile.hpp` & `cutlass_library/source/include/cutlass/conv/kernel/conv_universal.hpp`

 * *Files 24% similar despite different names*

```diff
@@ -26,33 +26,38 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
-#include <cute/config.hpp>
+#include "cutlass/detail/dependent_false.hpp"
 
-#include <cute/layout.hpp>
+////////////////////////////////////////////////////////////////////////////////
 
-namespace cute
-{
+namespace cutlass::conv::kernel {
 
-//
-// A Tile is not a Layout, it's a tuple of Layouts or Tiles or Underscores
-//
+////////////////////////////////////////////////////////////////////////////////
 
-template <class... Layouts>
-using Tile = tuple<Layouts...>;
+/*
+ * Stateless universal device CONV kernel type that treats CONV as
+ * a composition of a collective mainloop and a collective epilogue.
+**/
+template <
+  class CollectiveMainloop_,
+  class CollectiveEpilogue_,
+  class TileSchedulerTag_ = void,
+  class Enable = void
+>
+class ConvUniversal {
+  static_assert(cutlass::detail::dependent_false<Enable>,
+      "Could not find a valid specialization at the kernel layer to dispatch against.");
+};
 
-template <class Tile>
-using is_tile = is_tuple<Tile>;
+////////////////////////////////////////////////////////////////////////////////
 
-template <class... Layouts>
-CUTE_HOST_DEVICE constexpr
-auto
-make_tile(Layouts const&... layouts)
-{
-  return Tile<Layouts...>(layouts...);
-}
+} // namespace cutlass::conv::kernel
 
-} // end namespace cute
+////////////////////////////////////////////////////////////////////////////////
+
+#include "cutlass/conv/kernel/sm90_implicit_gemm_tma_warpspecialized.hpp"
+////////////////////////////////////////////////////////////////////////////////
```

## Comparing `cutlass_library/source/include/cute/numeric/bfloat.hpp` & `cutlass_library/source/include/cute/arch/copy_sm50.hpp`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2023 - 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2024 - 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -28,26 +28,45 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
 #include <cute/config.hpp>
 
-#include <vector_types.h>
-#include <cutlass/numeric_types.h>
+#include <cute/arch/copy.hpp>
 
-namespace cute {
-
-using cutlass::bfloat16_t;
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 500
+  #define CUTE_ARCH_WARP_SHUFFLE_ENABLED 1
+#endif
 
-//
-// Display utilities
-//
+namespace cute
+{
 
-#if !defined(__CUDACC_RTC__)
-CUTE_HOST std::ostream& operator<<(std::ostream& os, bfloat16_t const& v)
+struct SM50_Shuffle_U32_2x2Trans
 {
-  return os << float(v);
-}
+  using SRegisters = uint32_t[2];
+  using DRegisters = uint32_t[2];
+
+  CUTE_HOST_DEVICE static void
+  copy(uint32_t const& src0, uint32_t const& src1, uint32_t& dst0, uint32_t& dst1)
+  {
+#if defined(CUTE_ARCH_WARP_SHUFFLE_ENABLED)
+    uint32_t x0 = src0;
+    uint32_t y0 = __shfl_xor_sync(0xffffffff, x0, 1);
+
+    uint32_t x1 = src1;
+    uint32_t y1 = __shfl_xor_sync(0xffffffff, x1, 1);
+
+    if (threadIdx.x % 2 == 0) {
+      dst1 = y0;
+    } 
+    else {
+      dst0 = y1;
+    }
+#else 
+    CUTE_INVALID_CONTROL_PATH("Trying to use __shfl_xor_sync without CUTE_ARCH_WARP_SHUFFLE_ENABLED.");
 #endif
+  }
+};
+
 
 } // end namespace cute
```

## Comparing `cutlass_library/source/include/cute/numeric/float8.hpp` & `cutlass_library/source/include/cutlass/conv/collective/collective_conv.hpp`

 * *Files 12% similar despite different names*

```diff
@@ -26,18 +26,37 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
-#include <cute/config.hpp>
+#include "cutlass/detail/dependent_false.hpp"
+#include "cutlass/conv/collective/detail.hpp"
 
-#include <vector_types.h>
-#include <cutlass/numeric_types.h>
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-namespace cute {
+namespace cutlass::conv::collective {
 
-using cutlass::float_e4m3_t;
-using cutlass::float_e5m2_t;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // end namespace cute
+template <
+  class DispatchPolicy,
+  class TileShape,
+  class ElementA,
+  class ElementB,
+  class TiledMma,
+  class TileTraitsA,
+  class TileTraitsB
+>
+struct CollectiveConv {
+  static_assert(cutlass::detail::dependent_false<ElementA>, "Could not find a mainloop specialization.");
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+} // namespace cutlass::conv::collective
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+#include "sm90_implicit_gemm_gmma_ss_warpspecialized.hpp"
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

## Comparing `cutlass_library/source/include/cute/numeric/half.hpp` & `cutlass_library/source/include/cute/atom/copy_traits_sm50.hpp`

 * *Files 17% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2023 - 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2024 - 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,16 +26,33 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
-#include <cute/config.hpp>
-#include <vector_types.h>
-#include <cutlass/numeric_types.h>
+#include <cute/arch/copy_sm50.hpp>
+#include <cute/atom/copy_traits.hpp>
 
-namespace cute {
+#include <cute/layout.hpp>
 
-using cutlass::half_t;
+namespace cute
+{
+
+template <>
+struct Copy_Traits<SM50_Shuffle_U32_2x2Trans>
+{
+  // Logical thread id to thread idx (one-thread)
+  using ThrID = Layout<_32>;
+
+  // Map from (src-thr,src-val) to bit
+  using SrcLayout = Layout<Shape <_32,_64>,
+                           Stride<_64, _1>>;
+  // Map from (dst-thr,dst-val) to bit
+  using DstLayout = Layout<Shape <Shape < _2,  _16>,Shape <_32,  _2>>,
+                           Stride<Stride<_32, _128>,Stride< _1, _64>>>;
+
+  // Reference map from (thr,val) to bit
+  using RefLayout = SrcLayout;
+};
 
 } // end namespace cute
```

## Comparing `cutlass_library/source/include/cute/numeric/tfloat.hpp` & `cutlass_library/source/include/cute/numeric/numeric_types.hpp`

 * *Files 19% similar despite different names*

```diff
@@ -26,28 +26,50 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
-#include <cute/config.hpp>
-
 #include <vector_types.h>
 #include <cutlass/numeric_types.h>
+#include <cutlass/numeric_size.h>
+
+#include <cute/numeric/int.hpp>
+#include <cute/numeric/real.hpp>
 
 namespace cute {
 
+template <typename T>
+struct sizeof_bits : public cutlass::sizeof_bits<T> {};
+
+// DO NOT change auto to int, sizeof_bits<sparse_elem> use integral_ratio instead of int 
+template <class T>
+static constexpr auto sizeof_bits_v = sizeof_bits<T>::value;
+
+using cutlass::bits_to_bytes;
+
+using cutlass::is_subbyte;
+
+template <class T>
+static constexpr auto is_subbyte_v = is_subbyte<T>::value;
+
+using cutlass::half_t;
+using cutlass::bfloat16_t;
+
 using cutlass::tfloat32_t;
 
-//
-// Display utilities
-//
-
-#if !defined(__CUDACC_RTC__)
-CUTE_HOST std::ostream& operator<<(std::ostream& os, tfloat32_t const& v)
-{
-  return os << float(v);
-}
-#endif
+// Umbrella floating-point 8-bit data type : type_erased_dynamic_float8_t
+// This umbrella datatype can be enabled when a user provides a specific
+// datatype in runtime argument list.
+using cutlass::type_erased_dynamic_float8_t;
+using cutlass::float_e4m3_t;
+using cutlass::float_e5m2_t;
+
+using cutlass::uint1b_t;
+using cutlass::int2b_t;
+using cutlass::uint2b_t;
+using cutlass::int4b_t;
+using cutlass::uint4b_t;
+using cutlass::bin1_t;
 
 } // end namespace cute
```

## Comparing `cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h` & `cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_linear.h`

 * *Files 1% similar despite different names*

```diff
@@ -62,15 +62,15 @@
 /// Tile iterator used to load output tile from shared memory in epilogue.
 ///
 /// Satisfies: ReadableTileIterator
 ///
 template <typename ThreadMap_,  ///< Thread map (conept: PitchLinearThreadMap)
           typename Element_,    ///< Element data type
           int MaxAlignment = ThreadMap_::kElementsPerAccess *sizeof_bits<Element_>::value / 8>
-class SharedLoadIteratorPitchLiner {
+class SharedLoadIteratorPitchLinear {
  public:
   using ThreadMap = ThreadMap_;
   using Element = Element_;
 
   using Layout = layout::RowMajor;
   using TensorRef = TensorRef<Element, Layout>;
   using ConstTensorRef = typename TensorRef::ConstTensorRef;
@@ -119,15 +119,15 @@
  public:
   //
   // Methods
   //
 
   /// Constructor
   CUTLASS_DEVICE
-  SharedLoadIteratorPitchLiner(TensorRef ref, int thread_idx)
+  SharedLoadIteratorPitchLinear(TensorRef ref, int thread_idx)
       : byte_pointer_(reinterpret_cast<uint8_t *>(ref.data())),
         stride_((ref.stride(0) * sizeof_bits<Element>::value) / 8),
         base_smem_address_(0) {
     TensorCoord thread_offset = ThreadMap::initial_offset(thread_idx);
 
     // Initialize pointer
     // thread_offset.row() is contiguous dim
```

## Comparing `nvidia_cutlass-3.4.1.0.dist-info/LICENSE.txt` & `nvidia_cutlass-3.5.0.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `nvidia_cutlass-3.4.1.0.dist-info/METADATA` & `nvidia_cutlass-3.5.0.0.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: nvidia-cutlass
-Version: 3.4.1.0
+Version: 3.5.0.0
 Summary: CUTLASS
 License: BSD-3-Clause
 Project-URL: Homepage, https://github.com/nvidia/cutlass
 Project-URL: Bug Tracker, https://github.com/nvidia/cutlass/issues
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: BSD License
 Classifier: Operating System :: OS Independent
@@ -14,19 +14,19 @@
 Requires-Dist: cuda-python >=11.8.0
 Requires-Dist: networkx
 Requires-Dist: numpy
 Requires-Dist: pydot
 Requires-Dist: scipy
 Requires-Dist: treelib
 
-![ALT](/media/images/gemm-hierarchy-with-epilogue-no-labels.png "Complete CUDA GEMM decomposition")
+![ALT](./media/images/gemm-hierarchy-with-epilogue-no-labels.png "Complete CUDA GEMM decomposition")
 
-# CUTLASS 3.4
+# CUTLASS 3.5
 
-_CUTLASS 3.4 - February 2024_
+_CUTLASS 3.5 - April 2024_
 
 CUTLASS is a collection of CUDA C++ template abstractions for implementing
 high-performance matrix-matrix multiplication (GEMM) and related computations at all levels 
 and scales within CUDA. It incorporates strategies for hierarchical decomposition and 
 data movement similar to those used to implement cuBLAS and cuDNN.  CUTLASS decomposes 
 these "moving parts" into reusable, modular software components abstracted by C++ template 
 classes.  Primitives for different levels of a conceptual parallelization hierarchy
@@ -35,51 +35,56 @@
 as building blocks within custom kernels and applications.
 
 To support a wide variety of applications, CUTLASS provides extensive support for
 mixed-precision computations, providing specialized data-movement and
 multiply-accumulate abstractions for half-precision floating
 point (FP16), BFloat16 (BF16), Tensor Float 32 (TF32),
 single-precision floating point (FP32),
-[FP32 emulation via tensor core instruction](/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm),
+[FP32 emulation via tensor core instruction](./examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm),
 double-precision floating
 point (FP64) types, integer data types (4b and 8b), and binary data types (1b).
 CUTLASS demonstrates warp-synchronous matrix multiply operations
 targeting the programmable, high-throughput _Tensor Cores_ implemented by
 NVIDIA's Volta, Turing, Ampere, and Hopper architectures.
 
-See the [Quick Start Guide](/media/docs/quickstart.md) to get started quickly.
+See the [Quick Start Guide](./media/docs/quickstart.md) to get started quickly.
 
-See the [functionality listing](/media/docs/functionality.md) for the list of operations
+See the [functionality listing](./media/docs/functionality.md) for the list of operations
 supported at each level of the execution model hierarchy.
 
 CUTLASS 3.0 introduced a new core library, CuTe, to describe and manipulate tensors of threads and data.
 CuTe is a collection of C++ CUDA template abstractions for defining and operating on hierarchically multidimensional layouts of threads and data. CuTe provides `Layout` and `Tensor` objects that compactly package the type, shape, memory space, and layout of data, while performing the complicated indexing for the user. This lets programmers focus on the logical descriptions of their algorithms while CuTe does the mechanical bookkeeping for them. With these tools, we can quickly design, implement, and modify all dense linear algebra operations.
 
 The core abstractions of CuTe are hierarchically multidimensional layouts which can be composed with data arrays to represent tensors. The representation of layouts is powerful enough to represent nearly everything we need to implement efficient dense linear algebra. Layouts can also be combined and manipulated via functional composition, on which we build a large set of common operations such as tiling and partitioning.
 
 CUTLASS 3.0 and beyond adopts CuTe throughout the GEMM hierarchy in its templates. This greatly simplifies the design
-and improves code composability and readability. More documentation specific to CuTe can be found in its [dedicated documentation directory](/media/docs/cute/00_quickstart.md).
+and improves code composability and readability. More documentation specific to CuTe can be found in its [dedicated documentation directory](./media/docs/cute/00_quickstart.md).
 
 In addition to GEMMs, CUTLASS implements high-performance convolution via the implicit GEMM algorithm. Implicit GEMM is the formulation of a convolution operation as a GEMM thereby taking advantage of CUTLASS's modular GEMM pipeline. This allows CUTLASS to build convolutions by reusing highly-optimized GEMM components.
 
-# What's New in CUTLASS 3.4
+# What's New in CUTLASS 3.5
 
-CUTLASS 3.4.1 is an update to CUTLASS adding:
-- Statically available [CUTLASS Version macros](/include/cutlass/version.h) that allow for handling API changes between CUTLASS releases on the users' side.
-- Improvements for Hopper [Group-GEMM](/examples/57_hopper_grouped_gemm) and [Pointer-Array Batched GEMM](/examples/56_hopper_ptr_array_batched_gemm).
-- Updates and bugfixes from the community (thanks!).
-
-CUTLASS 3.4.0 is an update to CUTLASS adding:
-
-- Improved [Mixed-input Hopper GEMMs](/examples/55_hopper_mixed_dtype_gemm) supporting {16-bit, 8-bit} x {8-bit, 4-bit} input types with fast numerical converters and group scaling factors tuned for optimal performance on Hopper H100.
-- Beta release of [Pointer-Array Batched GEMMs](/examples/56_hopper_ptr_array_batched_gemm) utilizing TMA and Hopper H100 tensor cores now available. (Requires CUDA 12.3 or above)
-- Beta release of [Group-GEMM](/examples/57_hopper_grouped_gemm) - commonly used in optimization of Mixture-Of-Expert models, is now available on Hopper GPUs taking advantage of TMA and Hopper H100 tensor cores. (Requires CUDA 12.3 or above)
-- [Ampere Sparse GEMM](/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm_with_visitor.cu) supports Epilogue Visitor Tree (EVT) now.
-- Improvements to NamedBarriers including details of [ReservedNamedBarriers](/include/cutlass/arch/barrier.h) used within the CUTLASS library.
-- Improved [CuTe documentation](/media/docs/cute/) including improved clarity and depth of [Quickstart](/media/docs/cute/00_quickstart.md), [CuTe Layout](/media/docs/cute/01_layout.md), and [CuTe Layout Algebra](/media/docs/cute/02_layout_algebra.md). Associated code comments, post-conditions, and details in [CuTe Core Unit Tests](/test/unit/cute/core/) also improved.
+CUTLASS 3.5 is an update to CUTLASS adding:
+
+- Implicit GEMM Convolutions targeting Hopper SM90A via WGMMA + [TMA im2col](./include/cute/atom/copy_traits_sm90_im2col.hpp).
+  + Native implementation in CUTLASS 3.x using CuTe, mirroring the [same design hierarchy as that of GEMMs](./media/docs/gemm_api_3x.md).
+  + Support for 1D, 2D, and 3D convolutions in a [rank-agnostic fashion](./include/cutlass/conv/convnd_problem_shape.hpp).
+  + Support for [Fprop](./test/unit/conv/device_3x/fprop/sm90_conv3d_fprop_implicit_gemm_s8_s8_s32_tensorop_s32.cu), [Dgrad](./test/unit/conv/device_3x/dgrad/sm90_conv2d_dgrad_implicit_gemm_f16_f16_f32_tensorop_f16.cu), and [Wgrad](./test/unit/conv/device_3x/wgrad/sm90_conv1d_wgrad_implicit_gemm_f16_f16_f32_tensorop_f16.cu) algorithms.
+  + [CUTLASS profiler support](./python/cutlass_library/conv3x_emitter.py) for 2D and 3D convolutions implemented via the 3.x API.
+  + NOTE: this is a beta release. Further updates to CUTLASS will include major performance improvements, feature enablement, and possible breaking changes to the API until 3.7 release. Your feedback is welcome on the design!
+- Support for [Ada (SM89) FP8 tensor cores via the 2.x API](./examples/58_ada_fp8_gemm/ada_fp8_gemm.cu). Requires CUDA 12.4 or newer.
+- [Ampere gather/scatter convolution example](./examples/59_ampere_gather_scatter_gemm/README.md) in CuTe and CUTLASS 3.x.
+  + Showcasing how custom kernels can be written and optimized using CUTLASS 3.x and CuTe and the general strategy for implementing convolutions as specializations of GETTs.
+  + Implementation of a coarse grained sparse gather/scatter kernel achieving peak performance on Ampere class tensor cores.
+- 32x and 16x tile sizes are added to CUTLASS 2.x to improve the performance of narrow-tall and wide-short matrices.
+- Updates to CuTe documentation for [`cute::Tensor<>`](./media/docs/cute/03_tensor.md), [MMA atoms](./media/docs/cute/0t_mma_atom.md), and an overhauled [CuTe GEMM tutorial series](./examples/cute/tutorial).
+- Extensions to CuTe to support [L2 prefetching](./include/cute/algorithm/prefetch.hpp) and [TMA store+reductions](./include/cute/arch/copy_sm90_tma.hpp#L1337).
+- Remove C++11 requirement on a few CUTLASS 2.x API header files. All CUTLASS files now require C++17.
+- Fixes to greatly reduce build warnings.
+- Updates and bugfixes from the community (thanks!)
 
 Minimum requirements:
 
 - Architecture: Volta
 - Compiler: Must support at least C++17
 - CUDA Toolkit version: 11.4
 
@@ -114,15 +119,15 @@
 kernels, CUTLASS performance is also comparable to cuDNN when running Resnet-50 layers on an [NVIDIA A100](https://www.nvidia.com/en-us/data-center/a100/)
 as shown in the above figure.  Tensor Core operations are implemented using CUDA's
 [mma instruction](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma).
 
 # Compatibility
 
 CUTLASS requires a C++17 host compiler and 
-performs best when built with the [**CUDA 12.3.2 Toolkit**](https://developer.nvidia.com/cuda-downloads).
+performs best when built with the [**CUDA 12.4 Toolkit**](https://developer.nvidia.com/cuda-downloads).
 It is also compatible with CUDA 11.4, CUDA 11.5, CUDA 11.6, CUDA 11.7, CUDA 11.8, CUDA 12.0, CUDA 12.1, CUDA 12.2.2, CUDA 12.3.1 and CUDA 12.3.2.
 
 ## Operating Systems
 We have tested the following environments.
 
 |**Operating System** | **Compiler** |
 |-----------------|----------|
@@ -158,36 +163,36 @@
 
 The target architecture information is passed on to CUTLASS via the cmake flag `CUTLASS_NVCC_ARCHS`. In order to maximize performance on Hopper GH100, users are required to build CUTLASS with `90a` as the target architecture. If a user accidentally builds a kernel which uses SM90a features (e.g. Hopper Tensor Core Instructions), using the SM90 target (note the lack of "a"), with either CTK 12 or 11.8, the kernel is expected to fail with a runtime error.
 
 ```
 cmake .. -DCUTLASS_NVCC_ARCHS="90a" 
 ```
 
-Please refer to the [functionality documentation](media/docs/functionality.md) for details on which kernels require which target architectures.
+Please refer to the [functionality documentation](./media/docs/functionality.md) for details on which kernels require which target architectures.
 
 # Documentation
 
 CUTLASS is described in the following documents and the accompanying
 [Doxygen documentation](https://nvidia.github.io/cutlass).
 
-- [Quick Start Guide](/media/docs/quickstart.md) - build and run CUTLASS
-- [Functionality](/media/docs/functionality.md) - summarizes functionality available in CUTLASS
-- [Efficient GEMM in CUDA](media/docs/efficient_gemm.md) - describes how GEMM kernels may be implemented efficiently in CUDA
-- [CUTLASS 3.x Design](media/docs/cutlass_3x_design.md) - describes the CUTLASS 3.x design, its benefits, and how CuTe enables us to write much more composable components
-- [GEMM API 3.x](media/docs/gemm_api_3x.md) - describes the CUTLASS 3.x GEMM model and C++ template concepts
-- [GEMM API 2.x](media/docs/gemm_api.md) - describes the CUTLASS 2.x GEMM model and C++ template concepts
-- [Implicit GEMM Convolution](media/docs/implicit_gemm_convolution.md) - describes 2-D and 3-D convolution in CUTLASS
-- [Code Organization](media/docs/code_organization.md) - describes the organization and contents of the CUTLASS project
-- [Terminology](media/docs/terminology.md) - describes terms used in the code
-- [Programming Guidelines](media/docs/programming_guidelines.md) - guidelines for writing efficient modern CUDA C++
-- [Fundamental types](media/docs/fundamental_types.md) - describes basic C++ classes used in CUTLASS to represent numeric quantities and arrays
-- [Layouts](media/docs/layout.md) - describes layouts of matrices and tensors in memory
-- [Tile Iterators](media/docs/tile_iterator_concept.md) - describes C++ concepts for iterating over tiles of matrices in memory
-- [CUTLASS Profiler](media/docs/profiler.md) - command-line driven profiling application
-- [CUTLASS Utilities](media/docs/utilities.md) - additional templates used to facilate rapid development
+- [Quick Start Guide](./media/docs/quickstart.md) - build and run CUTLASS
+- [Functionality](./media/docs/functionality.md) - summarizes functionality available in CUTLASS
+- [Efficient GEMM in CUDA](./media/docs/efficient_gemm.md) - describes how GEMM kernels may be implemented efficiently in CUDA
+- [CUTLASS 3.x Design](./media/docs/cutlass_3x_design.md) - describes the CUTLASS 3.x design, its benefits, and how CuTe enables us to write much more composable components
+- [GEMM API 3.x](./media/docs/gemm_api_3x.md) - describes the CUTLASS 3.x GEMM model and C++ template concepts
+- [GEMM API 2.x](./media/docs/gemm_api.md) - describes the CUTLASS 2.x GEMM model and C++ template concepts
+- [Implicit GEMM Convolution](./media/docs/implicit_gemm_convolution.md) - describes 2-D and 3-D convolution in CUTLASS
+- [Code Organization](./media/docs/code_organization.md) - describes the organization and contents of the CUTLASS project
+- [Terminology](./media/docs/terminology.md) - describes terms used in the code
+- [Programming Guidelines](./media/docs/programming_guidelines.md) - guidelines for writing efficient modern CUDA C++
+- [Fundamental types](./media/docs/fundamental_types.md) - describes basic C++ classes used in CUTLASS to represent numeric quantities and arrays
+- [Layouts](./media/docs/layout.md) - describes layouts of matrices and tensors in memory
+- [Tile Iterators](./media/docs/tile_iterator_concept.md) - describes C++ concepts for iterating over tiles of matrices in memory
+- [CUTLASS Profiler](./media/docs/profiler.md) - command-line driven profiling application
+- [CUTLASS Utilities](./media/docs/utilities.md) - additional templates used to facilate rapid development
 
 # Resources
 We have also described the structure of an efficient GEMM in our talk at the
 [GPU Technology Conference 2018](http://on-demand.gputechconf.com/gtc/2018/presentation/s8854-cutlass-software-primitives-for-dense-linear-algebra-at-all-levels-and-scales-within-cuda.pdf).
 
  - [CUTLASS: Software Primitives for Dense Linear Algebra at All Levels and Scales within CUDA](https://www.nvidia.com/en-us/on-demand/session/gtcsiliconvalley2018-s8854/)
  - [Developing CUDA Kernels to Push Tensor Cores to the Absolute Limit on NVIDIA A100](https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s21745/)
@@ -198,15 +203,15 @@
 # Building CUTLASS
 
 CUTLASS is a header-only template library and does not need to be built to be used by other
 projects. Client applications should target CUTLASS's `include/` directory in their include
 paths.
 
 CUTLASS unit tests, examples, and utilities can be build with CMake.
-The minimum version of CMake is given in the [Quickstart guide](media/docs/quickstart.md).
+The minimum version of CMake is given in the [Quickstart guide](./media/docs/quickstart.md).
 Make sure the `CUDACXX` environment  variable points to NVCC in the CUDA Toolkit installed
 on your system.
 
 ```bash
 $ export CUDACXX=${CUDA_INSTALL_PATH}/bin/nvcc
 ```
 
@@ -243,15 +248,15 @@
 # Project Structure
 
 CUTLASS is arranged as a header-only library along with Utilities, Tools, Examples, and unit tests. 
 [Doxygen documentation](https://nvidia.github.io/cutlass) provides a complete list of files, classes, 
 and template concepts defined in the CUTLASS project.
 
 A detailed explanation of the source code organization may be found in the 
-[CUTLASS documentation](media/docs/code_organization.md), but several main components are summarized below.
+[CUTLASS documentation](./media/docs/code_organization.md), but several main components are summarized below.
 
 ## CUTLASS Template Library
 
 ```
 include/                     # client applications should target this directory in their build's include paths
 
   cutlass/                   # CUDA Templates for Linear Algebra Subroutines and Solvers - headers only
@@ -292,15 +297,15 @@
 
     *                        # Core library types such as Shape, Stride, Layout, Tensor, and associated operations
 
 ```
 
 ### CUTLASS SDK Examples
 
-[CUTLASS SDK examples](/examples) apply CUTLASS templates to implement basic computations.
+[CUTLASS SDK examples](./examples) apply CUTLASS templates to implement basic computations.
 
 ### Tools
 
 ```
 tools/
   library/                   # CUTLASS Instance Library - contains instantiations of all supported CUTLASS templates
     include/
@@ -317,15 +322,15 @@
 ```
 
 ### Test
 
 The `test/unit/` directory consist of unit tests implemented with Google Test that demonstrate
 basic usage of Core API components and complete tests of the CUTLASS GEMM computations.
 
-Instructions for building and running the Unit tests are described in the [Quickstart guide](media/docs/quickstart.md).
+Instructions for building and running the Unit tests are described in the [Quickstart guide](./media/docs/quickstart.md).
 
 # Performance Profiling
 
 The `tools/profiler/` directory contains a command-line utility for launching each of the GEMM kernels.
 It can be built as follows:
 
 ```bash
@@ -533,17 +538,17 @@
 
 =============================
 
 ```
 
 ## More Details on Compiling CUTLASS Kernels and CUTLASS Profiler
 - Please follow the links for more CMake examples on selectively compiling CUTLASS kernels:
-  - [GEMM CMake Examples](media/docs/quickstart.md#gemm-cmake-examples) 
-  - [Implicit GEMM convolution CMake Examples](media/docs/quickstart.md#convolution-cmake-examples)
-- [Further details about the CUTLASS Profiler are described here.](media/docs/profiler.md)
+  - [GEMM CMake Examples](./media/docs/quickstart.md#gemm-cmake-examples) 
+  - [Implicit GEMM convolution CMake Examples](./media/docs/quickstart.md#convolution-cmake-examples)
+- [Further details about the CUTLASS Profiler are described here.](./media/docs/profiler.md)
 
 
 # About
 
 CUTLASS is released by NVIDIA Corporation as Open Source software under the 
 [3-clause "New" BSD license](LICENSE.txt).
```

## Comparing `nvidia_cutlass-3.4.1.0.dist-info/RECORD` & `nvidia_cutlass-3.5.0.0.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,32 +1,32 @@
-cutlass/__init__.py,sha256=DePSWfTkKXUAnrAPnWwds50cVy8pTjiNvZAxJSOuYLY,6220
-cutlass/library_defaults.py,sha256=wNXzl5MXHl82kxyr2b6vUZjhsyv2CkWDFQcTik75Lv4,26509
+cutlass/__init__.py,sha256=EYWMzhrPc81Z8cUL_fSjanYjCai2DxgadLCNdHfrf-k,6220
+cutlass/library_defaults.py,sha256=o7i47Ch93OY-ycqCvzzIp9l4WV9MwHz547JhE488N1Y,26526
 cutlass/shape.py,sha256=7fh_vNa2F8hResb8UXIRN-vZhcwhBrqQ8ZQiFvbj6Kw,5715
 cutlass/swizzle.py,sha256=Y3q4Bo1s1wipmtkbwDIo_ysSXMfygbfLiscih_23ggc,2718
 cutlass/backend/__init__.py,sha256=kB9hHPriwJa8BotmU6WYnrgIGr08XO-tmfKb4BCno00,2464
 cutlass/backend/arguments.py,sha256=FYpbOZklD65r5GKlQTW68dacAf59egcjDs0doVh6tzM,5678
-cutlass/backend/c_types.py,sha256=YOgp-pCrVMOxf6vL9BOYNGzSAqmtBjVBVaQsdH35MtA,21870
+cutlass/backend/c_types.py,sha256=7pW_2wOEu-KoeGXYinLDEVBgZ0EcDu1eLWJA7CiAdP8,21871
 cutlass/backend/compiler.py,sha256=OpQKY6IMD9emY7eXbvpvJQIaL6WwHonsiUtyTR5F2x0,18206
 cutlass/backend/conv2d_operation.py,sha256=RWT_icTO_HifOUWAKrGZAt63_dSb1p6TEhVeA5Z44jk,26627
-cutlass/backend/epilogue.py,sha256=g-e5OLJ5_KPFR7PMDqnlStJOZLF_V2fdjqoF0qJ2dg0,17649
+cutlass/backend/epilogue.py,sha256=0AcEUq0zyLNwpFN4_kA63XmOm8vxdSibPs73z9764SI,17657
 cutlass/backend/frontend.py,sha256=A_GiB6KlqOQ8WfjbdOa6bZDNmBBVhPqLcZyUssdZpUA,3813
-cutlass/backend/gemm_operation.py,sha256=gorIAOpxBT8hbgKW7CXQn-t57eToXjVLdxLh0PAYKvw,85184
+cutlass/backend/gemm_operation.py,sha256=62Txt6rGDEDyxEccAriP-vvjup3LF4xAEtZeILRUAiQ,85190
 cutlass/backend/library.py,sha256=6-xQs5kDusqtgsLr0lAE4A2u7pe-c7Xl0jrqm7EaEyM,17280
 cutlass/backend/memory_manager.py,sha256=S__kXNTkB9E3FhFOwaBZuZsq7JGupLgeLPDUNV6eurE,4332
 cutlass/backend/operation.py,sha256=JZh5dxLAZmwi4MwM6q81KW3pLhJQ8dxbz00TaCBC3AE,5523
 cutlass/backend/reduction_operation.py,sha256=dPn6-hcs8vXLGzT1Luz2V_tAMGhgKQgPokQjMlSKsYw,15701
 cutlass/backend/type_hint.py,sha256=kMpamxIAJv9L5wSfaneZleJVvUPRM7OCcGyNOGnU_EY,1907
 cutlass/backend/evt/__init__.py,sha256=U2phdUwzV4HvH4qhl69nkVHGVZPOMfiRZP6dEeBb7w0,1920
 cutlass/backend/evt/epilogue.py,sha256=wRieODTg7UIMUtO8K5Chzi4WKtM-OmqBAAoTUkWs7M8,7027
 cutlass/backend/evt/backend/__init__.py,sha256=AatXET7OXG5vT9Gl2Yg7W8Yj4u6CZlDBdHzHW6DnWHU,2047
 cutlass/backend/evt/backend/emitter_base.py,sha256=7N5D8H98C8Wc0npYJTXekrR7rI30Qx2ilKhgimIsL4I,6453
 cutlass/backend/evt/backend/sm80_emitter.py,sha256=Bf0INdUWtshBOIpm07bMplRJIwFoIl7VNGdM00ACMtQ,2252
 cutlass/backend/evt/backend/sm80_nodes.py,sha256=XRo3QzFg1oVsWvBmlidMmcTutmyTxYeaTzyE5JwV4IQ,7581
 cutlass/backend/evt/backend/sm90_emitter.py,sha256=AUktGc0ZbOcMuW9lf0pM7cgPigcvrQ17sxsdThlfAKk,3828
-cutlass/backend/evt/backend/sm90_nodes.py,sha256=0ZrcxsFXIbM5egroSJPRjFzaj-gYyrVG_UUEVPMIb3Y,10925
+cutlass/backend/evt/backend/sm90_nodes.py,sha256=f2n5hrqRbyrpmXWqI7vAmk-15ksSBCnum89RyCIsllY,10985
 cutlass/backend/evt/frontend/__init__.py,sha256=F6aHjIbQLkrAa8G9iOqWf0kDoGvkl6EXaYbfaKa6WmM,1867
 cutlass/backend/evt/frontend/frontend_base.py,sha256=IqW1_mRYbq_EfkeFnJDPRsEZ7Lbvu6ZDAuPpMZLDK3k,8856
 cutlass/backend/evt/frontend/python_ast.py,sha256=b-QZxDsursswl4A3N4FuaWJch0ClFUcBbiW8q7Nm8RE,6746
 cutlass/backend/evt/ir/__init__.py,sha256=Mai0-ZwVnkPl6Nl_WwUupQiwGQdLJhV5pwvy3PTvYA0,2405
 cutlass/backend/evt/ir/compute_nodes.py,sha256=fQoO_556EFvZtBnKOFnaAzrQOcmNUzIQAu9VKm9HKk8,3447
 cutlass/backend/evt/ir/dag_ir.py,sha256=dTlrqjQeevxYibEYu_NVVxU5_U6qmJ_iZ99S0SmAcI8,7524
 cutlass/backend/evt/ir/layout_algorithm.py,sha256=gyOLj3QsEQFaX4eNaYQJ1naZEephafAeJjHFS2Tex68,13261
@@ -47,56 +47,57 @@
 cutlass/backend/evt/passes/pass_preprocess_red.py,sha256=iec2gln_WBc8l8tlhMuu2y-utnFUX8BFBhhm7mBjFpM,4456
 cutlass/backend/evt/passes/pass_shape_type_propagation.py,sha256=pGoHXcHrWLNCGni_wrr5_GGB6xRB4C6T2kmSJFUjFbY,2817
 cutlass/backend/evt/passes/smem_size_calculator.py,sha256=SO6W6nAg62QC_o_zbE5yxXRiXpYO0Czi0R0l4Q4B6SY,8126
 cutlass/backend/evt/passes/util.py,sha256=5xYfKK-Kdq0lAYzFZJQjZRbLhXi3LA_9eWxDL6gVUHo,1966
 cutlass/backend/utils/__init__.py,sha256=ArCLhegZ62oDQrDzm9VA_HVR2nST96tDzGmGIhnbH38,1833
 cutlass/backend/utils/device.py,sha256=m2w4udS6BjCqHwqGMLqLNTs7jdwAmPoQbsYXPCw_7Is,4461
 cutlass/emit/__init__.py,sha256=RS5Hu8xkJHrlTPdm5iIjin2eFAfYmKcrFBhlH3aXl_A,1838
-cutlass/emit/common.py,sha256=NxCL0Awrb6DpOPnsuefYSOzC-y_uU4ebQvYzCb4ZAJA,10566
-cutlass/emit/pytorch.py,sha256=gkoHPRT96io8qr580RCSYa-8-dIDyk6Ti368xCkG_KY,37350
+cutlass/emit/common.py,sha256=tUt7QGTXsbq42RAUcQxIlsOI8qBFL9VfNfRzRs57ZSM,10591
+cutlass/emit/pytorch.py,sha256=GfZRno5e5NfCx-QFgC5GPGoHk0eVr6RjvVqF2HfLv-g,37822
 cutlass/epilogue/__init__.py,sha256=RhQImU7uW0JkmJ7CjAW2yjYcT9FTYCkSq4MKZ6xZ9MA,2100
 cutlass/epilogue/epilogue.py,sha256=sejujJTuBWWlqsIajbh1OYLtS_OL4DWbYiRK1PQFWdQ,5562
 cutlass/epilogue/evt_ops.py,sha256=e9IrTz97HoHtGicCjNLfSlAbwJYOq0z88r_TI1ZfyTY,2909
 cutlass/op/__init__.py,sha256=74WUa90KdLB_aKPKPi7LPcvmBtLcaAnF_dBqkMeHo6M,1992
 cutlass/op/conv.py,sha256=vJWvksvhKdcf0xyZiII2ZovS0dnGvNrXFD_k9MvrF8o,43314
-cutlass/op/gemm.py,sha256=HOD2W_r-vtqV6bRanxviCq7uN7W5Xf9VFD2aVhbvmKc,32082
+cutlass/op/gemm.py,sha256=5ku2DNA8crbObmh8BrZ_kj9TOt3Vfq5cVNroz-WAVgk,32080
 cutlass/op/gemm_grouped.py,sha256=27LjwYg2mUfEUWo5QRVzhnO0HhfvQW3RqN5hUnIZRqA,12455
 cutlass/op/op.py,sha256=XQphFQmbgkpWh8Q0Ps00tuLo5R60Z_S8v4q8XQ8lTa4,18566
 cutlass/utils/__init__.py,sha256=AmwJZ9nyvaFf6Y6yiGNJ93LwMHr8sJG3gj-tE9B2Uhc,2011
 cutlass/utils/check.py,sha256=qRoImE3UAw2QdaoLMA66SMg1_sf8Jh9lTgxs4gEB8-U,12125
 cutlass/utils/datatypes.py,sha256=ZCGfbGMaaAZfIXeQuSnzQ5a_SYS49VDn4kINu5VUxNg,12115
 cutlass/utils/profiler.py,sha256=v_YQuCmAlD5b_V1EnOeiRIsEbSXpd8d7Mc2bOwCQG8M,6905
 cutlass_library/__init__.py,sha256=qVV5DmDtEfZ4VmxETjJx5tcCq4nmTWmVrR4eFGgz6ts,2779
-cutlass_library/conv2d_operation.py,sha256=slMNzhdJGkmuVVGHz5K1enDTvHzRkMDrTBXyHsnA_XY,19821
-cutlass_library/conv3d_operation.py,sha256=D6VZQJXdegek0gZEKeCcsCtSPK9GA0giAXFP_tyxK0s,14390
-cutlass_library/gemm_operation.py,sha256=tOVLQLELApPAf4XW_qTUZ8Lvhf7XJ6Oe_54eJ-rLfA8,52367
-cutlass_library/generator.py,sha256=gmlyD5mpmfK9mWTaPPpKu3GnJISFr_XVb4SuhVENMFs,249537
-cutlass_library/library.py,sha256=JshtZ4snQpB-IohR_ldbWBBsWkkllkHWlJ-LRLnBTNc,33582
-cutlass_library/manifest.py,sha256=itvdG5Q933dj0hVP_zlGD0HDkfD8dLE4Ur1VQmQF2sc,25597
+cutlass_library/conv2d_operation.py,sha256=aIOXbDP5d8dA4KzRdVyAA__pqTpFTgoi2PStqjRKt5M,24882
+cutlass_library/conv3d_operation.py,sha256=XKqhhZ0yuQNdHF7yAZ6sYkxbJ9T6SqiqP6a9qnYhNaA,19586
+cutlass_library/conv3x_emitter.py,sha256=18sD4MBVmSi7UNu5NxI_Z0XS1aRnjnyDRNJPCdbtfqg,9839
+cutlass_library/gemm_operation.py,sha256=3ay-IHkc3konFKWjFnIPa2YdnO_FyNw4127hor2VNpw,53775
+cutlass_library/generator.py,sha256=bBrcX6yLOTRRmsCy6kPvPR3jYX_n9Vkabqr7sF48RU4,277676
+cutlass_library/library.py,sha256=t4YUEnNwXuamd4g7_c3RdlMu2yXUZRMKwMyUCeqtUNs,34549
+cutlass_library/manifest.py,sha256=OqIm2sFfRIxCJpUVi6QCE7tip4cPfMJitM7aoofuIpo,31128
 cutlass_library/rank_2k_operation.py,sha256=W2G67DpIDxKh7uPjEqb69yRwvRU8F5N6oilNpIOSVz0,16216
 cutlass_library/rank_k_operation.py,sha256=2X7brew3f-pkOXrEIz4MxmZYEby12c9UYBbqnHtc7Ic,15775
 cutlass_library/symm_operation.py,sha256=u6Jjd9M_pzRa0EH2Ez10fWotMBwnoROtLKVNGsWUISo,16164
 cutlass_library/trmm_operation.py,sha256=n9qM9nzZTHUgouvI8NdRHK2tZSTYicf1pwwIvY5Vy4Y,16650
-cutlass_library/source/examples/CMakeLists.txt,sha256=Yh3rmN-8QlGD3lyzKc4nABYP6vwHaOkJG03MBpSDvoM,4477
+cutlass_library/source/examples/CMakeLists.txt,sha256=ebMc1LFgjmRwFsJf3GFOPL-V875xXS845Awwnzfk6tA,4527
 cutlass_library/source/examples/00_basic_gemm/CMakeLists.txt,sha256=xZWAVDb9avpsrbk9SbYLQFhw6en_gCHslz5tqLrfMkk,1668
 cutlass_library/source/examples/00_basic_gemm/basic_gemm.cu,sha256=S0czC-CyZvDmLFhMkZdjuAVvIppr36KJVISZuyGMtgc,14698
 cutlass_library/source/examples/01_cutlass_utilities/CMakeLists.txt,sha256=zw4mHvortU-364oMdZ6Ev5WNbfAEkueZ4uFpbvURoug,1681
 cutlass_library/source/examples/01_cutlass_utilities/cutlass_utilities.cu,sha256=yd32Yt3wDh2X32UXkF64iTVMbk4XveuoyzJsSvEkPhI,13255
 cutlass_library/source/examples/02_dump_reg_shmem/CMakeLists.txt,sha256=qm-erstgVloRNE2_x8u3Z5V4L2mTqfPvn0ywfsNkZH0,1693
 cutlass_library/source/examples/02_dump_reg_shmem/dump_reg_shmem.cu,sha256=AramH3SKfehNzz2P9ts1ZBICMk_sgOuEKQuF4cBHr6A,7157
 cutlass_library/source/examples/03_visualize_layout/CMakeLists.txt,sha256=uzk6bA3zKEEe3Odb-faGB-g987zs_72djR0eyfSeTjs,1789
 cutlass_library/source/examples/03_visualize_layout/options.h,sha256=h68xhMWpelTOSIwZYcEh0zVR8tQ52_u7FGeCdBkNYwk,4478
 cutlass_library/source/examples/03_visualize_layout/register_layout.cu,sha256=klwjfNt1w-0t91xjY6lbf2A3WSCpCG4MccT0sR_36CQ,7081
 cutlass_library/source/examples/03_visualize_layout/register_layout.h,sha256=t4EkrV1VSeDF1kEu7jtpyhJh8r4932W4cNOZk6RD-Cg,2691
 cutlass_library/source/examples/03_visualize_layout/visualize_layout.cpp,sha256=Codmgx5-agaO4NNw4VwBdvrj5AkTJ3W4hgfGyYQGLRI,5819
-cutlass_library/source/examples/03_visualize_layout/visualize_layout.h,sha256=7hwnYvyngJeWSo-f3h3iaFgthn9oyN0c3RMUIaYdsxs,11415
+cutlass_library/source/examples/03_visualize_layout/visualize_layout.h,sha256=eKxtzmxr06oB955gJcSGypENiUqaiNd7BskW5uZUa5M,11425
 cutlass_library/source/examples/04_tile_iterator/CMakeLists.txt,sha256=gTEglouAqhXLjx2QAjG5lbB2WE8A-JeoqjekOvWQVog,1673
-cutlass_library/source/examples/04_tile_iterator/tile_iterator.cu,sha256=99UsX1Qlwu-tbD7eHm7ipjH8WxRisdDzak_6KSz_My4,8226
+cutlass_library/source/examples/04_tile_iterator/tile_iterator.cu,sha256=_gcQmSm_U_-Bp2oJ1VXo2qv5SDtRFRq9hrZgJ7DZKsg,8229
 cutlass_library/source/examples/05_batched_gemm/CMakeLists.txt,sha256=vlX_z9JODdn7MtDUCuwxNhTaWA5n7G--WVdobosw7qc,1672
-cutlass_library/source/examples/05_batched_gemm/batched_gemm.cu,sha256=FEogB3PH_vJh8pSlL2tv4iHefDsLT84TdP8T5Oahp-c,15161
+cutlass_library/source/examples/05_batched_gemm/batched_gemm.cu,sha256=8qADrrFmSYa9tlOTl0sAWs48_5wfoez8XIbWakGiD0g,15185
 cutlass_library/source/examples/06_splitK_gemm/CMakeLists.txt,sha256=CsNgWdHOG4sMIzUUmxiLZyr_c8m8_QBjUkp8W-HAwUk,1670
 cutlass_library/source/examples/06_splitK_gemm/splitk_gemm.cu,sha256=iLgwFW7DBtmbQOnWcot087dS59vEhI89n_PDen4M2Lk,17570
 cutlass_library/source/examples/07_volta_tensorop_gemm/CMakeLists.txt,sha256=b4wubfB3Jsq407iBHsm-jbZHHQ5FviYO366g86jOi00,1686
 cutlass_library/source/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu,sha256=ozOlGvY-P_-b3TykHdM84PFOom_zknPG_oQZAMFl0-I,18283
 cutlass_library/source/examples/08_turing_tensorop_gemm/CMakeLists.txt,sha256=rNovFar014STyVk1682AcSUAbmKoWxtbmIcf7kiP3OM,1688
 cutlass_library/source/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu,sha256=uK2lrG4K1ENANeng61eK3YpCU0Jvyya51ctascEfF6Y,18228
 cutlass_library/source/examples/09_turing_tensorop_conv2dfprop/CMakeLists.txt,sha256=MakVTGcAIRQiVj0BebqUGbD8pL7wh2W9qWQxZoqwZ28,1703
@@ -107,15 +108,15 @@
 cutlass_library/source/examples/11_planar_complex_array/planar_complex_array.cu,sha256=nuGqgL6N34_1ENUTgad3RkFYthejzzB1xhUDQ7CKF18,23244
 cutlass_library/source/examples/12_gemm_bias_relu/CMakeLists.txt,sha256=6r6jxtZZtHEpzTaWGanruoK4zwsueHhTrEuq5jEe7ug,1676
 cutlass_library/source/examples/12_gemm_bias_relu/gemm_bias_relu.cu,sha256=lsi5jfrTRyY8aLL_jEy_CipskZJf2zM446f3q7QVmDU,13152
 cutlass_library/source/examples/13_two_tensor_op_fusion/CMakeLists.txt,sha256=bYiggkUQMiDQDKl-oEbhlhN5sZmaSSSxkiQ9Y3fwef4,2757
 cutlass_library/source/examples/13_two_tensor_op_fusion/README.md,sha256=lL3dCfT4XUOYsyvx8pDTsrzyw40F69vq22tvZdH3WRM,5893
 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h,sha256=AzZKnx0wsROzZQcjyUMB6j5qPRXf5OVLOM7JQ3b12zE,26102
 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_gemm_run.h,sha256=EfWYmucTU1sFQC1OZC4g6Zk624PEV7PcrlVYvM6-A3A,24557
-cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_grouped_gemm_run.h,sha256=MxC1c2muF6LtJtenqLzzqxtvlfwS_KJb04k3QRHfuA4,18662
+cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_grouped_gemm_run.h,sha256=i8s7f71W8h-mPyab3ph_1-hlBbVMCSMf3nFtfsJ_hqU,18662
 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h,sha256=bqQd6TnlUooa6AhmjkzMWGym3bd_th1gxUG6jnrEWBg,28268
 cutlass_library/source/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h,sha256=MyunWMJ02bbXWoFRaDwFySkIv-dlOp4LLGM_U8wkNqw,26174
 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu,sha256=104xDBGXZRmC6UufeS0c159aZ1myGzvPRPLgbG6EaQM,8756
 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu,sha256=gi_P8lmYHV7kUcnXnnaSnYK9Yzlg80D_1DW8-RgyJvc,8759
 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu,sha256=ci9GcorawIQ1x78AbEczIJ2NxrTkqb9QFnk-IFjIGtA,8712
 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu,sha256=u5tJns7PqhWQjfgTAsQkN64TVmDQWoDpTefPVD40N9o,8762
 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu,sha256=_KUrrKkesfUt9QhpXSpVEQtS7uaW6ELZ8ahkir-lFYo,8782
@@ -130,15 +131,15 @@
 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu,sha256=sdakdSqlZWedAz-UJMO0lJiXvpYDfolrvNqgRQm-Xig,7358
 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu,sha256=MunJUQV8Rhx6gKH5lK1wv0pA-Qb2-zShQ5GN99HH6qI,7424
 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu,sha256=rHC9tTq2lGgTVCB9LZ81HUuQ2ZHmL5RY0HGezeRnN5U,11029
 cutlass_library/source/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu,sha256=pPKMMlP3WHIRQz0MTKkWklrH_34sWg6qjW3HPAA4VFo,7619
 cutlass_library/source/examples/13_two_tensor_op_fusion/test_run.h,sha256=NhpgGHUvtvxzQ3LInRkcuZlqB7Ekgymd-gwELj0ygqw,3577
 cutlass_library/source/examples/13_two_tensor_op_fusion/device/b2b_gemm.h,sha256=SW6QkbqcTicQ72DctgATKFeutbyrmbQYsD6KsQ1-ing,12711
 cutlass_library/source/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h,sha256=VZ_ixeUC-ePxmLpybctagVW0-_Y_pKxQJBRJZPmydjA,11520
-cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h,sha256=SbL_kpSZxCGUicvsIunM6NYzYJs6p1OWZLOxkBmhUi8,30457
+cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h,sha256=azj6c_mqHGpo9bX6JYeLzqGKNaPMKqrM1URm_bzPY8E,30434
 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/b2b_gemm_grouped_problem_visitor.h,sha256=sL4PQjgqFyAssLTS6YPv5cEV6pSR2yn_pYylxfQalhw,6120
 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h,sha256=yvflTWmNJKQ1IYZJ_virZQ5LNCt2wztsL9Cd0TNs9NI,18151
 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h,sha256=x5OQixUXro0vHIooSFmqcCkuup9GuZRIMncKM1g_R90,3973
 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h,sha256=UPdP1ArvmEfbD9uikYSv8aTH2EivR2PvnEUe2ZfSK0o,26762
 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h,sha256=OcsybSDrG-vA47nWaFfqUznUzaJ2XJEJUOnafnl28iA,26775
 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h,sha256=MydEaruoSJXP8HvMLooJLztC2PV1JDWYcZ4W5pnpZcc,28422
 cutlass_library/source/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h,sha256=fNvxr3aXcADObfUv2E2QD0UigDVpxzCkudSM8aIbWU0,28073
@@ -161,88 +162,88 @@
 cutlass_library/source/examples/13_two_tensor_op_fusion/threadblock/grouped_threadblock_swizzle.h,sha256=TpHVsGjxGXxKDKjHSJzY6KgsRbQYEtjoHSTwqp3RXys,5719
 cutlass_library/source/examples/14_ampere_tf32_tensorop_gemm/CMakeLists.txt,sha256=xubqAJi7BtAarld0tx03MijYFoQYU6U3c5CFkgHeDAM,1698
 cutlass_library/source/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu,sha256=MbWGpOsIeCgs5p4yHKconZrnWGf8HV-f_KhpUbkGj5s,18020
 cutlass_library/source/examples/15_ampere_sparse_tensorop_gemm/CMakeLists.txt,sha256=BvpdxnlswK8w-5evfCHWaAU5ULyeNOGmWDU_fUQJZSY,1830
 cutlass_library/source/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu,sha256=JvMYW24ilguIAegnIo4LBMt430tkZ9RnCtnvfyeIx1c,15042
 cutlass_library/source/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm_with_visitor.cu,sha256=skYwiG9VlcfibS11nrAgRs_6q5gBS2gIEcbAasXMHrc,15957
 cutlass_library/source/examples/16_ampere_tensorop_conv2dfprop/CMakeLists.txt,sha256=jVW7lAmF_kDp93hSBBc3pQqqPpVZ2IHSkQtIgDgjxP4,1703
-cutlass_library/source/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu,sha256=YMRZI6cV7y0fg9n_k-rjOP_Q7BTLVueeZLlwTvWzjpo,27844
+cutlass_library/source/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu,sha256=QWGFDhZmRUItSTQ0Y77Epw73tvdexwLVV8QV_5MIxGM,28030
 cutlass_library/source/examples/17_fprop_per_channel_bias/CMakeLists.txt,sha256=6JMEuZw4ixrOcDZqa8qfxH6jkeiTkc5IS8f2GNvzxUI,1694
 cutlass_library/source/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu,sha256=29hZmtW0wzNO-VvNISd42pk116-bq6XGyFX3m8tDgdE,12580
 cutlass_library/source/examples/18_ampere_fp64_tensorop_affine2_gemm/CMakeLists.txt,sha256=xmhYWCwE555lIkzZOoI8AjcPvhryMyqxGFPcBjfEplk,1715
 cutlass_library/source/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu,sha256=rdXlN-YTIU7Y-unZffbHQphq2MbRQvbfRXVFytevhQg,14007
 cutlass_library/source/examples/19_tensorop_canonical/CMakeLists.txt,sha256=ZkN7rKp0ZjryLKrbkOU8cZ39-ignoj1kEIwCBj9iI7c,1682
 cutlass_library/source/examples/19_tensorop_canonical/tensorop_canonical.cu,sha256=gXiTuRNiqnh1fp_s8JzJT0-KjEIkG2hwUoYubqWsjZE,13401
 cutlass_library/source/examples/20_simt_canonical/CMakeLists.txt,sha256=DkE5QP6WxBI9mPupQa2OV25Zk997QB3Yasjl8Ppt3vs,1674
 cutlass_library/source/examples/20_simt_canonical/simt_canonical.cu,sha256=kzm-jYKEI1xC07_Csx3e0ljlM1P8QkNXUIkwItqCcEg,12556
 cutlass_library/source/examples/21_quaternion_gemm/CMakeLists.txt,sha256=CrmQ6umikdvAj7wMZtxJGFXhVuTb26NaOT7bw9KwUmc,1678
 cutlass_library/source/examples/21_quaternion_gemm/quaternion_gemm.cu,sha256=tBBNvNGHRmljyIWqDV6rnHJkqSrqwQfeHzr29KGHqE4,17319
 cutlass_library/source/examples/22_quaternion_conv/CMakeLists.txt,sha256=esdY8mauWHi2lHSfvbXInd6UFdVRMhNRVuCnNWPC2GA,1680
 cutlass_library/source/examples/22_quaternion_conv/quaternion_conv.cu,sha256=yTMD7O3dxf3DZ5BxHzq4NqoXwlzzjHJy3EnfJGED_PM,21423
-cutlass_library/source/examples/23_ampere_gemm_operand_reduction_fusion/CMakeLists.txt,sha256=jBswNmEpH1YWB0dHJl4asQF7-bqMiwA_Tm1BeKyt97s,1721
-cutlass_library/source/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu,sha256=zRswvKXjf9IVAtham8I9kSmDLw8M_bwql2WuzCgUCzw,27556
+cutlass_library/source/examples/23_ampere_gemm_operand_reduction_fusion/CMakeLists.txt,sha256=OydleWcbnrxWHSMZ2A-G99hLbZ_cgDqwpqNqfvxCvX0,1894
+cutlass_library/source/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu,sha256=aG3DkoMfCJ-5pUm4oOibdAXYoo851_NAVqP_GJDNps0,27556
 cutlass_library/source/examples/24_gemm_grouped/CMakeLists.txt,sha256=QnZGQk1CI-0hMb6831FaXPpyjIB9SAdI7F6gQkFxXDg,1674
-cutlass_library/source/examples/24_gemm_grouped/gemm_grouped.cu,sha256=sxyPvMN1O1hHu0uP4B_YjpolVn41L3d5MAfLB2Ona3w,50890
+cutlass_library/source/examples/24_gemm_grouped/gemm_grouped.cu,sha256=w0WPNhVfeWmqfqoGUjT3cby5CJD5CrQGMzu9qfuCvXA,50898
 cutlass_library/source/examples/25_ampere_fprop_mainloop_fusion/CMakeLists.txt,sha256=OZJBfDCUiX3t93OxGipV-aRlDSBPLj_RMDX483ccd4g,1816
 cutlass_library/source/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu,sha256=LtEFWk3hxQy025xczjcO9R5QOub_nl8vXRan4DpCuMQ,26547
 cutlass_library/source/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu,sha256=XNhHGbCjF7WOC7QHJG43UClLMOs5ytpOkqfBJ6xTArU,25628
 cutlass_library/source/examples/26_ampere_wgrad_mainloop_fusion/CMakeLists.txt,sha256=vJIlm4umjAMQIWQuziRyiT1lEDzfMtOEnEwehSswUkU,1705
 cutlass_library/source/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu,sha256=0RbULfX9TmFzCmr7D3pgENT9T5CQj_yzFlpStQC3n_c,25538
-cutlass_library/source/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu,sha256=xIBF9P_c0O4ZY7O2iyyyiyqKoKnet5LqXBqtGUTYD6k,30446
+cutlass_library/source/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu,sha256=JP6xMVUdidO5WnCp-iHWwmcKyB29T48xmr_47w6RiJg,30398
 cutlass_library/source/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/CMakeLists.txt,sha256=ZzHtHXsqE-2bko6VpSpLIHmSwI87raRDdACv9EweLRE,1733
 cutlass_library/source/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/CMakeLists.txt,sha256=ZGmXEOhsRABC47oB0TpQgu9cm7goELhF5dA03iPwd0Y,1732
 cutlass_library/source/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu,sha256=BDHyBT5rJBcvTkwL88TEwWUQpXvJe8uKP_8HXtFU1UE,28159
-cutlass_library/source/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm.cu,sha256=yPkEYF8cn0gqUWM7-e_Uk3vwTSOj8rYATJSDSOvqgZo,28382
+cutlass_library/source/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm.cu,sha256=XA3n4kYuhxtomQMKmTKBeiy7u6WStul1aLAqyelCDQI,28338
 cutlass_library/source/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/CMakeLists.txt,sha256=EKm0GyvNatIH0EXZaMweZFsMYOet-1zsgsSslpsPwVw,1754
 cutlass_library/source/examples/30_wgrad_split_k/30_wgrad_split_k.cu,sha256=Kxmn_ZOywhS9gKpjNyQKslC9ASL3wXsDLBbHBx_B5q8,27304
 cutlass_library/source/examples/30_wgrad_split_k/CMakeLists.txt,sha256=3E8IaVHJm_Aou5P8K55R2wOwoMQrQmFmbbtfkg4hVB4,1677
 cutlass_library/source/examples/31_basic_syrk/CMakeLists.txt,sha256=3nMk8XJSCLgniYPp_N5ySANCiZygGNeK9WxfQmjL8YU,1668
-cutlass_library/source/examples/31_basic_syrk/basic_syrk.cu,sha256=6zZwIy_11i31kR7UdI29ADmHr2Vfb0sp1JrnfUPiFbU,15205
+cutlass_library/source/examples/31_basic_syrk/basic_syrk.cu,sha256=tH0tnKJNmVKUQU_VBiGa-unk5y78Ji6xg2NSzNZaOT4,15202
 cutlass_library/source/examples/32_basic_trmm/CMakeLists.txt,sha256=LjvzYUBF9UPAPQd6ZejaFQhxtIeFrp8qEj1QSKzIEYQ,1668
 cutlass_library/source/examples/32_basic_trmm/basic_trmm.cu,sha256=IGdvibXfRdJ963Z1yhhs7gA--cTHlcSnEowHSdweX5U,15906
 cutlass_library/source/examples/33_ampere_3xtf32_tensorop_symm/CMakeLists.txt,sha256=vIVXD8AWg7Qvads5bur2WvBJlxDXbUBZM-wjO6piIW8,1702
-cutlass_library/source/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu,sha256=VoxavrdS9zEAVpbyHEHSAr_LuV8d6cv5NYq5wMFgPHM,31803
+cutlass_library/source/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu,sha256=rsyDF6TvaLTOz0hwAciFmRYO7zIRXUyszvK9OM3hJ1Q,31765
 cutlass_library/source/examples/34_transposed_conv2d/34_transposed_conv2d.cu,sha256=96ipX-R4kiSy-vUw2_r_fxDQKNtxlgkkz6YO4hQ_QHA,22378
 cutlass_library/source/examples/34_transposed_conv2d/CMakeLists.txt,sha256=PquQJu1AdrLtxM3rDkym_QV1rs_dzEJRYSyeyeJ9fwI,1684
 cutlass_library/source/examples/35_gemm_softmax/CMakeLists.txt,sha256=w08fYtV_PGmw7n30YDpN2HgBDuF_xH6nJaNfHdoaAaw,1673
-cutlass_library/source/examples/35_gemm_softmax/gemm_softmax.cu,sha256=k4PtBldTEr7LQTH7LErqYBt9_jTaic7jlSM_yC26pLQ,23114
+cutlass_library/source/examples/35_gemm_softmax/gemm_softmax.cu,sha256=_9ebprfEFUtvE2ctrQGUGHJ5gWevdC8ONzPB8wnNjAw,23120
 cutlass_library/source/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h,sha256=w3F7NIu8FIZMePD9sNqSZruiHxkM5IeRO0BTlJ11QNw,16723
 cutlass_library/source/examples/35_gemm_softmax/gemm_with_softmax.h,sha256=furJszO_C50GrP8Bgr8m5aTI-WpJ8so66MBRiaV5YsU,19055
 cutlass_library/source/examples/36_gather_scatter_fusion/CMakeLists.txt,sha256=_5PlZf-_EKJAUOt30OtwZvgFu_3j-5vXbrejBJOvv_I,1687
 cutlass_library/source/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu,sha256=LWaAe7xdvUIbLBNcfuHjuzn17QU_CAf8Xemgf64L8GA,21008
 cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/CMakeLists.txt,sha256=-hLWr1dxT3xN07OqzeOLs35nfxbjf_GBldWi1lxZ4DI,1689
-cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu,sha256=0nz9JSsyvGNo4u7kOE9SJ3JLYhaYr7cfwO5LraAY6uA,31111
+cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu,sha256=Ci1jnlepf5VQUZ6-UO7cWIh3uWa8D3tc48mkqbSyp2I,31111
 cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h,sha256=fCys6pezkynv5zXMY1kY3gDiPTtGf_kBe_dGuprXhac,13982
 cutlass_library/source/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h,sha256=c_JaI87ySPtK9b-XftsT8AIUZWbMr6ZU-TZsFmGuNY8,33900
 cutlass_library/source/examples/38_syr2k_grouped/CMakeLists.txt,sha256=4TstiMGFjOOtMqq0zWUJBURXJsmPrvmy4ihIvrQMyuw,1675
-cutlass_library/source/examples/38_syr2k_grouped/syr2k_grouped.cu,sha256=AY2xgZUuvjYGxGBcpsgKutxSYfe9WfWAUWnzT_YTRbo,47457
+cutlass_library/source/examples/38_syr2k_grouped/syr2k_grouped.cu,sha256=N3lOp3_LFa110wzXgOkn-oUpZoBSA4vGwgo2jH813Rk,47468
 cutlass_library/source/examples/39_gemm_permute/CMakeLists.txt,sha256=BMFNKdTd4qCCZdXJwWAD31xE82JRC06ZYPSZNF5MSJU,1674
 cutlass_library/source/examples/39_gemm_permute/gemm_permute.cu,sha256=XV8yyAoQAN2PH6YiALjK8o_heu77tiZYIoJMPFMJ-9U,48551
 cutlass_library/source/examples/39_gemm_permute/layouts.h,sha256=bCHDgs8phHOYoRhYuIuQ4DZfCMfD8FcyodTmVz_BMic,15309
 cutlass_library/source/examples/39_gemm_permute/permute_info.h,sha256=oL3An_W-lkX5AUNMn94BGPxKz74n_ojD3IkXjTTe6dY,11985
 cutlass_library/source/examples/40_cutlass_py/README.md,sha256=ldwEmscgdcq25t_nyIg0jkaxEdvxBqy7Un4Ru3wrmP0,253
 cutlass_library/source/examples/40_cutlass_py/customizable/README.md,sha256=X5y74GFjW0Zdmv9vMnF_gxIcU9lldb5XD7WNEorAXog,15590
 cutlass_library/source/examples/40_cutlass_py/customizable/grouped_gemm_problem_size.csv,sha256=k_EdKnOiWmQBu0zBbj6J4MautnOID7fXNPJDuaJP1ZQ,36
 cutlass_library/source/examples/41_fused_multi_head_attention/CMakeLists.txt,sha256=ApeEV1lIWd3-_FpmSWv8-91je6GojWHRs4eOYZ6llFc,2390
-cutlass_library/source/examples/41_fused_multi_head_attention/debug_utils.h,sha256=INxoxzErAzs5UyKNDSnZVgYKRTk6chJOIXjEzXL9S84,11865
+cutlass_library/source/examples/41_fused_multi_head_attention/debug_utils.h,sha256=98U_eiutCsCPdNATlAwS5jOfC3yU4vLNM_MQSmKA6SM,11871
 cutlass_library/source/examples/41_fused_multi_head_attention/default_fmha_grouped.h,sha256=H7S44BP3Tc7kv3p2uZP-GNzbBIBdoOKiWW382VyC0rY,10832
-cutlass_library/source/examples/41_fused_multi_head_attention/fmha_grouped.h,sha256=3d9hQAuw2gzgG_73yw-O1zzD_nr0n18votLjlJLysj8,37522
+cutlass_library/source/examples/41_fused_multi_head_attention/fmha_grouped.h,sha256=IxplnBe4M4mABuU2eQg_uVDMeQAmnKtAwksisqAP7ME,37291
 cutlass_library/source/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h,sha256=B0-8QT9qvM72jkmsElvy9PHFYo6RGcE4L7V1AdIEfYA,6666
 cutlass_library/source/examples/41_fused_multi_head_attention/fused_multi_head_attention_backward.cu,sha256=iMUakladCKFQaQkx0JMLmIizi4h9N_LOjNmlSkg-9DI,11208
-cutlass_library/source/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu,sha256=3aWqeMsqJj4E7HVTHYC0exfuEtLeE-hWCW0TrvN3sv8,38218
-cutlass_library/source/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu,sha256=QQaUN0hlhreUKbbFR8glWzAIKk_nrJ2LMm0tSVqu7y4,40003
+cutlass_library/source/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu,sha256=mIncfl2uWJWPgk_FKD5NtdusK-BDpBtHNLi1w80PLcE,38221
+cutlass_library/source/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu,sha256=ADNjtJqC2v4UvpL-NMfrqzqpMFJBOe2wmgcng6NdPCw,40006
 cutlass_library/source/examples/41_fused_multi_head_attention/gemm_kernel_utils.h,sha256=jOuZRTXHVEQhxoXNsCGvwVxhvLJcxdVv0iVrISLljJE,11075
-cutlass_library/source/examples/41_fused_multi_head_attention/kernel_backward.h,sha256=B3jgE-VTNIm-ic-oR63dEQ-tHI7CRcom6QQiA_7WBPk,97621
-cutlass_library/source/examples/41_fused_multi_head_attention/kernel_forward.h,sha256=APcpnktBd_-F0WCbg-HkMeBQHI7o3VWs_HWTlSsQT3s,52615
+cutlass_library/source/examples/41_fused_multi_head_attention/kernel_backward.h,sha256=s2RT96LjhOHQXCmEqWdOG078Tzq07roNMbXB8abcOhw,97640
+cutlass_library/source/examples/41_fused_multi_head_attention/kernel_forward.h,sha256=jzC87GziauRbe5i19anALoCAAlQgWnMTWbVI3NIpgZQ,52585
 cutlass_library/source/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h,sha256=Y_U9LwsPUP3KdyQztcR9YzOxODMulK37egG5Jha03ko,22356
 cutlass_library/source/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h,sha256=FEM1zJXwWAAj-MOyczvFOZU9nd1oyB00BVgO_Yjdpo0,9162
 cutlass_library/source/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h,sha256=wh0vJ6a5CdaW4Z7Q2mXgASanJzS5RldtbGXC8ySvKjU,6118
 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma.h,sha256=XQGRjgW5m5kGEHWwEbN41qg9eEZBygbxJzD8bN8vdjw,3994
 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h,sha256=ZfydQcrs96AZEocDz7rdkw1bkGimIYx1nzZFfm7VmXo,6248
-cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h,sha256=kkqUTCwrIqgyDbZplqvSKWFUNaJxYnJeL3pxzXkQLeg,26933
+cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h,sha256=szVNApSX40NYcDIont44bjmxPKgP-kjEbVs0cYNqFpM,26967
 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h,sha256=3CUIjth_LTKu_EgnvtCtxM27O_FUmU6yA7I0gezSmQY,14105
 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/find_default_mma.h,sha256=6H-ZGyK7no3qt9PWfqaXIPgI7Zg8sWuCg6BYTFWvR5g,6782
 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h,sha256=c_WGWzePvERdsLzVMmTXTnI9g57XB9ZdvWVjt3Lc_Xc,13959
 cutlass_library/source/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h,sha256=yyksL5KmWbarQEHIS77yUjv7GCS8gJoKnBTbLP6U-I0,68653
 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/default_warp_iterator_from_smem.h,sha256=AEHjSPNTlefNxlkg8XtjRweQuRo9_aF_o_9E_ExTmw4,5776
 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h,sha256=TmdGak6YYGAT2tnO1CNYHp4XipCOEH_hinARli8DiM4,23862
 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/make_residual_last.h,sha256=ROjADTKxgBPkDz69dr4JjDnObkffiXxre0MT_msMUlU,3142
@@ -250,15 +251,15 @@
 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h,sha256=gEXnghkhWHdGdyAKnySDl0TLXh5K9l-7jNFbzWLxMxY,64507
 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h,sha256=nWvY8fS-dhKrlfGlUbb7falgEIopcIL8-f1rliGsmFk,2512
 cutlass_library/source/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h,sha256=3Ygky9l7M9GRmgdTHpzI7HY95Q4EC0zpOp92w3KksPo,10063
 cutlass_library/source/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h,sha256=V-1TeutpdMDCk0k7xgAtfImpyzuvYztB67a6XQsP9Xk,3761
 cutlass_library/source/examples/42_ampere_tensorop_group_conv/CMakeLists.txt,sha256=5rl7oelzJC4ic7KnxsBlrwczVtLskuUwGhjgTva-iB0,1701
 cutlass_library/source/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu,sha256=8bBYPs8YU3-aBzW9V9Ijp2Lv591SjJLQUT_3QYpcqRs,23901
 cutlass_library/source/examples/43_ell_block_sparse_gemm/CMakeLists.txt,sha256=6KBEffBBtMcngrf07hh4waHF4pfobVyfIKvOg_1Nvks,1690
-cutlass_library/source/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu,sha256=CB6yLgsENf3N4B5ScJM34LqOLqCZHzK7Pbr9rFJ-BdU,23867
+cutlass_library/source/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu,sha256=dquFbQcQTqoALL8RX7UgulKeiqxUskzbg8wQm1EvETg,23867
 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/README.md,sha256=RjKLKPbAV06QXvbvIb0YAIjtKh5E8XNnfql8IEuZmh0,2589
 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/config.json,sha256=ejjHm3L8Jqx9EV2K8PD8PKQZgQ7KiCqGdmW3x_qRpTU,1105
 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h,sha256=o2oaZZlBAB_Sbx7dvGBN8gQQMJIpR-hdEG9Fb-xCTSc,10231
 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/utils.h,sha256=I-tN19gktZG9Swq--X9vvvWyYp6xmL9myFmPyPJDBTU,3745
 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h,sha256=q0mW7GyEUyuJ-WxPl2XyzJksWma3qUxlKUl5nw0SMY0,6370
 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h,sha256=E3k49QsW723Mp-cURTkCkYEJtUwv2eh9iqTsZ4ZFQNc,4099
 cutlass_library/source/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h,sha256=f-kQxixdRt9J8aZYpA4_yigWk3IVM6Gsw_CEAsxtrdg,8285
@@ -286,335 +287,366 @@
 cutlass_library/source/examples/48_hopper_warp_specialized_gemm/CMakeLists.txt,sha256=cLQ1WZZ1CA64K6R85j7RGQlPIQTj0iAj7F0m3fxTyb0,1707
 cutlass_library/source/examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu,sha256=iIYOqOPivfvGhV5OTiD4HZCzQXRMoC8ezlVVuFCSqtY,30447
 cutlass_library/source/examples/49_hopper_gemm_with_collective_builder/CMakeLists.txt,sha256=4y58JIduQ9cL5H1z2yRi0WHAnX2F4YNwuXEbiUyl0PA,1751
 cutlass_library/source/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu,sha256=CIyEBwdlH6aOEqqLpLuYCj95pLa1fNuwb-2i9r7IQwU,18806
 cutlass_library/source/examples/50_hopper_gemm_with_epilogue_swizzle/CMakeLists.txt,sha256=KwVAq3OyATyV1YdIXKzJZvT87wK9BAbX11gSO05xADs,1717
 cutlass_library/source/examples/51_hopper_gett/51_hopper_gett.cu,sha256=R4ayUjJWVf41BsyXbMQO_ly4D4Bwtf_fHlLOhbNIRBQ,17340
 cutlass_library/source/examples/51_hopper_gett/CMakeLists.txt,sha256=ilSMrCArBRCAlpNNtH8aFO6UoiTRI-BxsriRIcQ7jsE,1668
-cutlass_library/source/examples/51_hopper_gett/gett_kernel.cuh,sha256=tLQFiydtEWfPTV7_PzUx-pIzE37YBV8chKLI68nQ73w,5572
-cutlass_library/source/examples/52_hopper_gather_scatter_fusion/52_hopper_gather_scatter_fusion.cu,sha256=NNMtb-U2F-L_rxpAR4CTtUKKFoAR2CARG-jfGgjZHJk,27580
+cutlass_library/source/examples/51_hopper_gett/gett_kernel.cuh,sha256=mGLowo5EjQTaWb0h2OAo6P3E8EX52z7oHOdqmJBhZI0,5599
+cutlass_library/source/examples/52_hopper_gather_scatter_fusion/52_hopper_gather_scatter_fusion.cu,sha256=zmODHhtuLT6ibOiyEnbbhFSCYpXGbPS62UY2u5vjno8,27605
 cutlass_library/source/examples/52_hopper_gather_scatter_fusion/CMakeLists.txt,sha256=CKP0WToydU4YAAn5lsccaexfG-8m95qYkYrDZ9cA9yk,1704
-cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_gemm.hpp,sha256=byDCO6HbyzqGkp6_YGGkcZmTswcdiLmyyVrLPxG0C6M,17835
+cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_gemm.hpp,sha256=mZ3ZKykmb1g3U_Z6qDGbtkp-0bsOyB7yaAOE7rrDHzA,17965
 cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_kernel.cuh,sha256=yR_r3dfiX58pzDQfKrhcb0MBcNi9C0C796F6-4y6XuQ,5606
-cutlass_library/source/examples/52_hopper_gather_scatter_fusion/gather_tensor.hpp,sha256=yTky7XjtZu5ZnaQK2q_bKjF4n49KkQQ-Ecy_42vo3Uo,7018
-cutlass_library/source/examples/52_hopper_gather_scatter_fusion/scatter_epilogue.hpp,sha256=rbThw0WaBcIGuIumslR_id5kKPYUpvQ_3qb1cj5x7dQ,8533
-cutlass_library/source/examples/53_hopper_gemm_permute/53_hopper_gemm_permute.cu,sha256=FP5fVHhg9Evi7TQqfn_l9ylzzQ6luDZ-NQOtjY5mJdQ,45351
+cutlass_library/source/examples/52_hopper_gather_scatter_fusion/scatter_epilogue.hpp,sha256=JbenPFcLs7uvXAtuaOKPg_37EgbuD81cQzJpsJ_NK3s,8543
+cutlass_library/source/examples/53_hopper_gemm_permute/53_hopper_gemm_permute.cu,sha256=HG8rA_N7H01BdmtvX9Yn5g1Xnp2-YbEbDNQibvf8foA,45401
 cutlass_library/source/examples/53_hopper_gemm_permute/CMakeLists.txt,sha256=91JFY_uql8ii37OckMV6zrw9Bj1hj2mxqcg7PsH113c,1690
-cutlass_library/source/examples/53_hopper_gemm_permute/permute_kernel.cuh,sha256=tksr0WV99OKxy1F51XV0kTrYO_ItR-aNkVrKc_bSyMk,4078
+cutlass_library/source/examples/53_hopper_gemm_permute/permute_kernel.cuh,sha256=e2ARdMeNNzhmWWGsciyNlorfp3EvzZ8VTfMaitiED4g,4084
 cutlass_library/source/examples/53_hopper_gemm_permute/permute_traits.hpp,sha256=VJwuGxV8rSO2ghHhwG2Zr3K8kmiR5M_stoJ02R2ULE8,11441
 cutlass_library/source/examples/54_hopper_fp8_warp_specialized_gemm/54_hopper_fp8_warp_specialized_gemm.cu,sha256=asrNhA_S0d35eQXcB80saZR1_HvbP9gLruUazTn1vlk,22065
 cutlass_library/source/examples/54_hopper_fp8_warp_specialized_gemm/CMakeLists.txt,sha256=zFB9wnXS_rSoN_Z3DeJPXs3SmnwO4DWHjib_OLoXuvc,1712
 cutlass_library/source/examples/54_hopper_fp8_warp_specialized_gemm/hopper_fp8_commandline.hpp,sha256=hVhmfTyQ0IHv1neyOrDy2i7AeiCXAMDDW4WT8uM4SCE,5157
 cutlass_library/source/examples/55_hopper_mixed_dtype_gemm/55_hopper_mixed_dtype_gemm.cu,sha256=OC5diqF26FohTCTAlhdjaNPWc7STBjaEkddtKR7K08U,31597
 cutlass_library/source/examples/55_hopper_mixed_dtype_gemm/CMakeLists.txt,sha256=d4hlrNRwdtKlY36ltm6mHNBj6bgAHpRjz5BLQumh2kg,2978
 cutlass_library/source/examples/55_hopper_mixed_dtype_gemm/README.md,sha256=jbHYIiuzRS-xi3ktsCAX6CK1v8-do2TZ9M45uJip7-c,2587
 cutlass_library/source/examples/55_hopper_mixed_dtype_gemm/unfused_weight_dequantize.hpp,sha256=6OlR8abYj0rgWedSR5ioUUdiK_yKC0nG1LQIHxc7KCg,7838
 cutlass_library/source/examples/56_hopper_ptr_array_batched_gemm/56_hopper_ptr_array_batched_gemm.cu,sha256=aDvnnY4--hHeZQTaVAv8htJps4VuChHVNDbFlrxtW5Q,19510
 cutlass_library/source/examples/56_hopper_ptr_array_batched_gemm/CMakeLists.txt,sha256=BQkNmFueHpBUc4fTfldhaGWGVa2iRFkFYd97wFWRsXI,2802
 cutlass_library/source/examples/57_hopper_grouped_gemm/57_hopper_grouped_gemm.cu,sha256=xjwYShVBtTvkNa8QmsAvoIc1H_J-8eSLjItCKV4WsJc,26735
 cutlass_library/source/examples/57_hopper_grouped_gemm/CMakeLists.txt,sha256=upV8x4uHCwdcYa_-nwtzL6mfYBUbcC5CTjtR_TSSo9M,3411
+cutlass_library/source/examples/58_ada_fp8_gemm/CMakeLists.txt,sha256=AB5_AiGkSnewnnLTjR3QColSHEFJL3DrUP7doEy0Kf8,1664
+cutlass_library/source/examples/58_ada_fp8_gemm/ada_fp8_gemm.cu,sha256=ePfU44hFWm6yO7GwX5qiOKelVj0dgGhszXcilCkM1Lc,28799
+cutlass_library/source/examples/59_ampere_gather_scatter_conv/CMakeLists.txt,sha256=nWUy8JT2Gi66sCTzwrusmZJWU5BYk4jbW17wzpSwwVQ,1833
+cutlass_library/source/examples/59_ampere_gather_scatter_conv/README.md,sha256=6zk_sSmchZDoR6baPRiEf-PYEouRZaxrlN09WH6LHrU,11886
+cutlass_library/source/examples/59_ampere_gather_scatter_conv/ampere_conv_kernel.h,sha256=5veqJ4hRkNxsQiK5oRECHPptLErvVRPuasE2hoGClGA,12530
+cutlass_library/source/examples/59_ampere_gather_scatter_conv/ampere_gather_scatter_conv.cu,sha256=SwqFlOHfUe-2ruRh3iy7lkA3_LV7Msp5IRGZDFSZgZY,17356
 cutlass_library/source/examples/60_cutlass_import/CMakeLists.txt,sha256=bsFPLO2QCaLVjtcgsM9m3iITr76Eyl45H6fItHoQvOo,2738
 cutlass_library/source/examples/60_cutlass_import/main.cpp,sha256=znYwwrZxrLkmatucK-zu2G1GGOx4skaUf_HrzPEBnuo,2849
+cutlass_library/source/examples/common/gather_tensor.hpp,sha256=_f-FryVIVWXDjjiRuH1fGIynlc70YIe2x7uKkjSEBwY,7085
 cutlass_library/source/examples/common/helper.h,sha256=yQzAGVCkV9XnUZb0aLN122ShQL1oFLxz0YIbKAMm3Yw,4469
 cutlass_library/source/examples/cute/CMakeLists.txt,sha256=7EgIvGz9o8GBegsGcoSUk6X6tkx4iKSQQGKX3peWLXI,1625
-cutlass_library/source/examples/cute/tutorial/CMakeLists.txt,sha256=OozaRGiJsu1NGF6mIpj9jm8KoWyTiUSqSU9_K1ecSBk,1727
-cutlass_library/source/examples/cute/tutorial/sgemm_nt_1.cu,sha256=_Fs7nnBGl_bFjTZFvd8n6RzdVbROo1VnpTY7BFiwyE8,14342
-cutlass_library/source/examples/cute/tutorial/tiled_copy.cu,sha256=7LvXwpWOg1n5NCi9ZsnISEsZqyLo_rYyENVac3IvT7k,9550
+cutlass_library/source/examples/cute/tutorial/CMakeLists.txt,sha256=gskQ8wIoyCvvCHssnYyj-QreSsX0JptYHPEucsCI-m8,1907
+cutlass_library/source/examples/cute/tutorial/sgemm_1.cu,sha256=ZPt2v6pJL6YUOhrLTaJpCST1BM5Ntul16RGjen1P22w,17292
+cutlass_library/source/examples/cute/tutorial/sgemm_2.cu,sha256=BytdocHnhLGiwtw5X03YX9rU-ugCcgO3Gouipw6_748,19286
+cutlass_library/source/examples/cute/tutorial/sgemm_sm70.cu,sha256=mQQLCCOvCH7FbkLK_OvWyxvgS4Xbz2MTPOnKnlfQKIc,18881
+cutlass_library/source/examples/cute/tutorial/sgemm_sm80.cu,sha256=zHqezutuYvOlmuj1MRv_M0S_cFpvrQfYARrSzYljK70,19981
+cutlass_library/source/examples/cute/tutorial/tiled_copy.cu,sha256=Z0Ap1NEWKpi9I2MbzG7AuGel3ewnw1yK5VJyXoaCdH4,9890
 cutlass_library/source/examples/python/00_basic_gemm.ipynb,sha256=QgiyxtED1YJJjW--a9x9mmuAMacyGi-bdhhsPzGKEqw,16234
 cutlass_library/source/examples/python/01_epilogue.ipynb,sha256=4jbKS02AmF9T67aTBrtPpGf6EcpMPrIaf-wb90SWjSc,7350
 cutlass_library/source/examples/python/02_pytorch_extension_grouped_gemm.ipynb,sha256=OmsQHeT2ioT79TbvGpJ-Eui3QZeHJJIbQm_gGVRa8s8,10053
 cutlass_library/source/examples/python/03_basic_conv2d.ipynb,sha256=OxPTR-kjWtL9y-RXz3Td2I-p40o0MIAXxpu5gzkJ0fQ,18123
 cutlass_library/source/examples/python/04_epilogue_visitor.ipynb,sha256=xV0_vTaqqSSN7hQuR4Kd61QPeGzMMzMKtnS9Fd1Babs,8815
 cutlass_library/source/examples/python/README.md,sha256=c9sxxBDR5VTwpA717YYxRb5YKW_kr1QwL9S8i78rTT4,961
-cutlass_library/source/include/cute/config.hpp,sha256=1CIMpAXnABqQShgkRCyj7PmVF61-Ug40kmm5ZfEg_wE,5419
-cutlass_library/source/include/cute/int_tuple.hpp,sha256=Q4Vxz8o7ehNChJ4-x3wzyCXZrPJS4_w-oEadPw4FzB8,28088
-cutlass_library/source/include/cute/layout.hpp,sha256=HU3U6w-fARp4Z6K9TRBRnyjAj8iqLOGHlvLKB0-B_-M,58037
-cutlass_library/source/include/cute/layout_composed.hpp,sha256=mm4D-anb8MRDqB0EZupWH1_PVs_GQzdIgHiBqDGqQTI,17749
-cutlass_library/source/include/cute/pointer.hpp,sha256=3E64iHhNY6XIeV1TAhNeoaZcXIBijEWj0QeBvOCCYJM,8403
-cutlass_library/source/include/cute/pointer_base.hpp,sha256=hfh7GJCgMfnr3WdM-qfW_1YNtttTCsv5PM4Xc1lxEuQ,8326
-cutlass_library/source/include/cute/pointer_flagged.hpp,sha256=QgtAKLYo68HxB07FgZz1Vy1o7DI_uwCYHvbcZsb56AM,5510
+cutlass_library/source/include/cute/config.hpp,sha256=IljaUhrOCkyQajKoK3ujs04iAbHtjGli0I7Smk-DYpA,5368
+cutlass_library/source/include/cute/int_tuple.hpp,sha256=dlQSFBJlixd8XcudtENaHKFaQTtoVel_wOopu0bCb_c,28072
+cutlass_library/source/include/cute/layout.hpp,sha256=1adpgWj-daj-cl41FxtNzWLIDTq_y4a1SpTcEAa-h1Q,61599
+cutlass_library/source/include/cute/layout_composed.hpp,sha256=A0cOLNMF7gShigumlNElL5DCldmBNWAt25RaUgK8Z1g,18072
+cutlass_library/source/include/cute/pointer.hpp,sha256=vl3ait3E7w6K_IAiN2LEBGsQeXB22Ln8Eb4S402KNbI,8583
+cutlass_library/source/include/cute/pointer_base.hpp,sha256=Itb2t6r842J8TIF5Tjt8uq-BGqCb8D702shoDM0kgeQ,8336
+cutlass_library/source/include/cute/pointer_flagged.hpp,sha256=DzqYXMQT57C1Gzj4JgFg4Xy-njC8n3VZ1G12-Y4hk_4,5725
 cutlass_library/source/include/cute/pointer_swizzle.hpp,sha256=EV49T_o-mLx1fZMK8dY8c3fVb_L3-WrwW4O6PyC2qho,6136
-cutlass_library/source/include/cute/stride.hpp,sha256=jW-0o0ZTB9CmE8WW3CffzV9d-NQzV0H3oCk8WENF9uI,15939
-cutlass_library/source/include/cute/swizzle.hpp,sha256=STwxDs-7zbk4raBfgh2gLCkDIldLxj9etPfivLGqIzs,15341
+cutlass_library/source/include/cute/stride.hpp,sha256=P4bKK_Rizl761wZFRMPjQQ6EbOEQxN3IOxDYn2bxIsU,16790
+cutlass_library/source/include/cute/swizzle.hpp,sha256=C8Lczw4uQTjGpoVBOdYHfBSxPjmDX4OTEJDsWpHCP4A,15513
 cutlass_library/source/include/cute/swizzle_layout.hpp,sha256=x_Komt2SRDOMicalVwtytZyrEp8dSVjtP8hdyPaRkJw,21354
-cutlass_library/source/include/cute/tensor.hpp,sha256=i2AZhFPMgE4kBJgAmXE53HnV5Mzm0390HmA1yQSYlA8,35169
+cutlass_library/source/include/cute/tensor.hpp,sha256=hSoCkSjwkde7gZ-hduYf4DcTENcSkQnpgc09p4gA7EM,35447
 cutlass_library/source/include/cute/tensor_predicate.hpp,sha256=NGSvD6L45ZHDmG3GfbfPqkxe7wbPPtKUqtnDMZBQM_g,2595
-cutlass_library/source/include/cute/tile.hpp,sha256=eWP_OY2gjXB4xlJ_SbfthgcjmM1hF6oJLIfRokLXhhw,2279
-cutlass_library/source/include/cute/underscore.hpp,sha256=p4ggZkGaIPOFYlNb7UmY1BRgixIcNJwwuM9AGQ5xdQ0,6203
-cutlass_library/source/include/cute/algorithm/axpby.hpp,sha256=qp7CIHIJcQZZGiuUDAf6x1b8439GlgU1qLRRDLGQyDs,3026
+cutlass_library/source/include/cute/underscore.hpp,sha256=1d_a3jx_ox2qCiHjrAm7R44WnBHk-qsyz17bzxB24cg,6246
+cutlass_library/source/include/cute/algorithm/axpby.hpp,sha256=m81DMoLdQ-lrttHqfhw5ZwyHg3w3PzVmG1kjbd8DqBI,3276
 cutlass_library/source/include/cute/algorithm/clear.hpp,sha256=9Hah3Gza3Z7z2FHSIa7UZtn6h_Evv-LvVM3vYwH4AUg,2351
-cutlass_library/source/include/cute/algorithm/copy.hpp,sha256=p6P0whFO_GuMUJM0GhJss_dEYuODHXrMgyRlaz6jH7k,13582
+cutlass_library/source/include/cute/algorithm/cooperative_copy.hpp,sha256=fKw1J22LXNwFzjhLY5RcQxpQAfVhP9kpKdxrR7C0CVQ,9711
+cutlass_library/source/include/cute/algorithm/cooperative_gemm.hpp,sha256=JfV6HZcPDZNmY5sq-H-Jnl6C8_bM2tkcKEcgDEc0Dt8,22774
+cutlass_library/source/include/cute/algorithm/copy.hpp,sha256=ne1VwYVUes0iFrBsr21uEYrQaGRHTCL9_FekfBxzn-k,13575
 cutlass_library/source/include/cute/algorithm/fill.hpp,sha256=oLTvuWcH0-wgW9KLYIJkig_69QVbkJigbgd-kVU3J80,2906
-cutlass_library/source/include/cute/algorithm/functional.hpp,sha256=BX2QHrLqan3wsUZGNh_pheTaK1H9HhdmtiTNdY8QP4M,10843
-cutlass_library/source/include/cute/algorithm/gemm.hpp,sha256=o0U2DEcjSWMM6ZvCLIpmeE0e61jyQdvfHCxv-mOWqQM,26851
+cutlass_library/source/include/cute/algorithm/functional.hpp,sha256=ixPbF5qhV3ngqeptlJxZERXbGug3vh-1F-9gsHO5I_0,10890
+cutlass_library/source/include/cute/algorithm/gemm.hpp,sha256=e5pJZ2FIbcnnXoVJb6Y2LkQWnrq88wpsKz5P3_-DKi8,18275
 cutlass_library/source/include/cute/algorithm/prefer.hpp,sha256=8GAXcjivHX9M1bzVIHXKSVsApI7swaW6mZLUSDsTkiA,2124
-cutlass_library/source/include/cute/algorithm/tensor_algorithms.hpp,sha256=0F9p0g-t6CQFzYp77BdP3cYOom3HM9i95-OCtvwHWYU,5248
-cutlass_library/source/include/cute/algorithm/tuple_algorithms.hpp,sha256=8sgCW0sYRSkFovhUd1Yg62hvVjylguNVg6Zwmj24Io0,28067
-cutlass_library/source/include/cute/arch/cluster_sm90.hpp,sha256=e4Lpw0paS-fHJgBPyMpKxw0WyvnnD9DNSo5UJy-1N9Q,7606
-cutlass_library/source/include/cute/arch/copy.hpp,sha256=hMuRnmSbP9m48STTM8IoTYgH6TTOdhZtM8tk2sH_h_E,3248
-cutlass_library/source/include/cute/arch/copy_sm75.hpp,sha256=TQ2MZl8OSBdge4SkUzB_qASg2cRLPAwMB3pNcSmmtNw,7703
-cutlass_library/source/include/cute/arch/copy_sm80.hpp,sha256=7TY6D56bjwdCO5hzAPY6WIWUn_OJr4-mDCZaEnZ5Mj0,6832
-cutlass_library/source/include/cute/arch/copy_sm90.hpp,sha256=Wz_7-TfJyLeqFhBMzaXlSn9fy0-yY1tHVoI7PDoIkIs,7530
-cutlass_library/source/include/cute/arch/copy_sm90_desc.hpp,sha256=l-3d8Noke6fDJfWN6mOt5FxkTRpGvWSBn83JsPv2jok,14108
-cutlass_library/source/include/cute/arch/copy_sm90_tma.hpp,sha256=hpOeUQcENXRy_QCILWhfYedQNgpR15f4OXl8IT-3tPc,36138
+cutlass_library/source/include/cute/algorithm/prefetch.hpp,sha256=UabxPJpvqgbGOp1RfVOyrM6vRlVCW2YCVkMCMWxGJd0,5742
+cutlass_library/source/include/cute/algorithm/tensor_algorithms.hpp,sha256=wXrd9CIBjIgyUqBlWuT-8E0kzeYMFwwSy2n5eUVZek0,5238
+cutlass_library/source/include/cute/algorithm/tuple_algorithms.hpp,sha256=uiPmOAmZoyea392hujSQ0Cd8J7wufIYeV2u229FSl04,27396
+cutlass_library/source/include/cute/arch/cluster_sm90.hpp,sha256=qjSQ4-7Fd9V3zbVA9XQYv75w82_UGgcdhmO6NuKKiiw,7642
+cutlass_library/source/include/cute/arch/copy.hpp,sha256=8sn8DzYddQaov7aOZdYwqiX5Qq4EqIJt6IddX8IxXEY,3474
+cutlass_library/source/include/cute/arch/copy_sm50.hpp,sha256=nM1i-fUG8nOUtVsU4yX7NVvpyR0xo_Ds4Ww8h4_Bvr4,2686
+cutlass_library/source/include/cute/arch/copy_sm75.hpp,sha256=fuso20JVqy54l3jTWsiIMFinZYg0P9UXO6Vu3GISsiI,7739
+cutlass_library/source/include/cute/arch/copy_sm80.hpp,sha256=5BR9-eipTYIwK5A9XbS23AdwYZKxNKOWagKurvBos48,6856
+cutlass_library/source/include/cute/arch/copy_sm90.hpp,sha256=Lwrr6beXPL9jl7dK8WqvhTmpQ4NI8EE0nhs00C8WwTY,7566
+cutlass_library/source/include/cute/arch/copy_sm90_desc.hpp,sha256=WyQqQK1k7Skp1NgQXrylYHy61M3B9w-TlYvqtuhbOsA,14193
+cutlass_library/source/include/cute/arch/copy_sm90_tma.hpp,sha256=FaTMx1dQh-ajF7oIzR8RivGaMXR3nTxYCOUD48PHxLE,49750
 cutlass_library/source/include/cute/arch/mma.hpp,sha256=-grJ4VHhdp4m5FUDB-bJQtkEDRWbx8oynsZNgFD2dFY,2393
-cutlass_library/source/include/cute/arch/mma_sm61.hpp,sha256=PAoS4_nzdRTkCrSFV2A06B1_fQbKWD0dknszPirnpoU,3160
-cutlass_library/source/include/cute/arch/mma_sm70.hpp,sha256=76iXJ5GzybB7BzQlLb5Bc81207oYscSXyVhMI40f7lI,12452
-cutlass_library/source/include/cute/arch/mma_sm75.hpp,sha256=pyr7VJSGRdt9pnXXRxjFcyJ1EraIb_cDQzYdshu3bf0,4262
-cutlass_library/source/include/cute/arch/mma_sm80.hpp,sha256=JIG1Yxqvf6_bCD55bYiiX9oQo60eh0DLP1ga0ACiv8E,68426
-cutlass_library/source/include/cute/arch/mma_sm90.hpp,sha256=PdwhY0UGrp-6KWIUE9fA2NlaA1e6SCfT-iaeKEg0fbY,47771
+cutlass_library/source/include/cute/arch/mma_sm61.hpp,sha256=lX0QHIUhYLl-7vhf9RqSUlpGXNMzecWB5LbnePvqrk0,3172
+cutlass_library/source/include/cute/arch/mma_sm70.hpp,sha256=3vgVxPwbF_HlMisVDd0z6OjsZsxl_yQGOxuZ447FfCA,12500
+cutlass_library/source/include/cute/arch/mma_sm75.hpp,sha256=cWCdccIEyuiNrUUKpAtj_4pGf_uRKeC9ptzYN_JNuPA,4274
+cutlass_library/source/include/cute/arch/mma_sm80.hpp,sha256=bISKdUEqipvG_y0b0hU4MG07jX70TyROFWZ75NYu5fA,68808
+cutlass_library/source/include/cute/arch/mma_sm90.hpp,sha256=nAXzj6sablDNq2VrSb8GDnVp0NFJfb8jAofmvrsvS9w,53627
 cutlass_library/source/include/cute/arch/mma_sm90_desc.hpp,sha256=c-w4sqdT8je-_8W_xgnQCZFPvk5r23vZFYWnwo4EbOo,6172
-cutlass_library/source/include/cute/arch/mma_sm90_gmma.hpp,sha256=9sprOUIdOjYRVXIdjZH9S9pUyZWxtQHa62CsAo_O3EE,944978
-cutlass_library/source/include/cute/arch/util.hpp,sha256=2dX5q0wZSfnYQBI7VtzWadHqczv7FD8Wvt4dir24w4U,8212
-cutlass_library/source/include/cute/atom/copy_atom.hpp,sha256=V8_50rn7meCQ0xjDiAuPPjlunV-ClKhAOvCPQowbblw,27553
-cutlass_library/source/include/cute/atom/copy_traits.hpp,sha256=KLFFYglOuGGDtdFR1DHvrAYd7-U4zh512ykAJSSPFFY,5975
+cutlass_library/source/include/cute/arch/mma_sm90_gmma.hpp,sha256=0t9Xl7rcv1M3Iz8aHFBriJxMA-dPTnHkeGJJq7gSSjM,946916
+cutlass_library/source/include/cute/arch/util.hpp,sha256=HmT572mhR8hcT6Z1shp644Ws-_0s1OG5eGhK2x32Pn4,8810
+cutlass_library/source/include/cute/atom/copy_atom.hpp,sha256=hxTzxIjp8sPxxiWw2n_59sN7Iji33Rw9PGMi7TlM1ww,27595
+cutlass_library/source/include/cute/atom/copy_traits.hpp,sha256=U73ybORb8MvAGi691uFD9JT55hdlcegBlxnP5bzoQis,5684
+cutlass_library/source/include/cute/atom/copy_traits_sm50.hpp,sha256=6Ohpi5Fl6nkyYxjrRwxO1b1_L1zczYN-guwy3Kt3W6U,2512
 cutlass_library/source/include/cute/atom/copy_traits_sm75.hpp,sha256=higj3n55ckW1RLt50IWenphETViyesTp046ZI6tTD5s,5087
 cutlass_library/source/include/cute/atom/copy_traits_sm80.hpp,sha256=mMSfRMxO-OJehANfkgYKEBiRKAAzBC2VLEG-Y8cnZcA,7107
 cutlass_library/source/include/cute/atom/copy_traits_sm90.hpp,sha256=qvsk-zbLgdSU3DYKF6pDGtXeErsvjA_EPMRtRVTXA-Y,4589
-cutlass_library/source/include/cute/atom/copy_traits_sm90_tma.hpp,sha256=iMI-YPXjema9cfVdqszuF97X-ifpct8C6MAzogh_oQc,51919
+cutlass_library/source/include/cute/atom/copy_traits_sm90_im2col.hpp,sha256=-1cedT-6L-trKEWhSUdDceYVian_IHhuotXNUv6A_tM,37568
+cutlass_library/source/include/cute/atom/copy_traits_sm90_tma.hpp,sha256=3uth2j-qgJbcl8cJyyaTpZnaK2dFfg35jnXy3HQuXKw,56618
 cutlass_library/source/include/cute/atom/copy_traits_sm90_tma_swizzle.hpp,sha256=W7pxjid95zsjluASqsQFJzn7IUyNK_bzWcNe8DkoWhQ,2860
-cutlass_library/source/include/cute/atom/mma_atom.hpp,sha256=VTnyRKzEG5A_u8sD2vecrfFM1vGgkVS0gSysITOJmB8,33411
-cutlass_library/source/include/cute/atom/mma_traits.hpp,sha256=yPNZ6tAGvzJnZt9wvksjSOO6VdV4pB7Kr0PylempRd8,8677
+cutlass_library/source/include/cute/atom/mma_atom.hpp,sha256=1Pru5Cg-wT7iTUAKOWOHPDq1WGhz1aiG6nYEaprarV4,33597
+cutlass_library/source/include/cute/atom/mma_traits.hpp,sha256=cnT-2Gv4Uli0zqyMqZG7I9cUc9We1xFezAs25IfT260,8749
 cutlass_library/source/include/cute/atom/mma_traits_sm61.hpp,sha256=jnUQoveeyIiodmgY0vnfwryX5oxSKLAPc7EDgI2nLKY,2773
 cutlass_library/source/include/cute/atom/mma_traits_sm70.hpp,sha256=Q3ufO4dvZK405_K8Pz-5gxvDshsUw-8kgcGxBKzR7z4,6092
 cutlass_library/source/include/cute/atom/mma_traits_sm75.hpp,sha256=NcnLJPa11o2hYWRcpuoLjH5mv6z4SNuP9RkbJVjuTQA,3303
-cutlass_library/source/include/cute/atom/mma_traits_sm80.hpp,sha256=-3NsMj1HpO2994Ojur0Zm4_8hXaX7Q3AZ2ZdDDkVeQk,14128
+cutlass_library/source/include/cute/atom/mma_traits_sm80.hpp,sha256=tNTYG_wsI-YU-WMqMqFFZgMmysn0nH0iJ_tYz7xETyo,14088
 cutlass_library/source/include/cute/atom/mma_traits_sm90.hpp,sha256=TYdPsKdgRbei_w0pvAEqgQPZ6n6cqgrqVLem1ib0M20,5049
-cutlass_library/source/include/cute/atom/mma_traits_sm90_gmma.hpp,sha256=UEcxklyQNsvHv9Zz6DrjyG2f-3VbgZDwnv1Je6orWZY,189975
-cutlass_library/source/include/cute/container/alignment.hpp,sha256=07vQAj6LWl4yvARgZhD-_WbgAQk3JB8rZLDNuQt-UJk,2982
-cutlass_library/source/include/cute/container/array.hpp,sha256=xhKvCwSlBeNUjQaGEBHIK1FapDyTwVWQIvR44v93h2g,9557
+cutlass_library/source/include/cute/atom/mma_traits_sm90_gmma.hpp,sha256=wkJcBUbpMo8-1sXoDmDrjsI1DHmbVyERm6zPspldfQY,189998
+cutlass_library/source/include/cute/container/alignment.hpp,sha256=J5IqW_tPt28gr8H4kAayNp_405jN_1OIE7p3mw-TWC0,2992
+cutlass_library/source/include/cute/container/array.hpp,sha256=axGa-rutiwuf-tyqP50Y958ucDhCdwejqx1_D4WJAus,9549
 cutlass_library/source/include/cute/container/array_aligned.hpp,sha256=0DOyvYsflG87KnsPnZXZM2My7BpFCxZWwYiVjrjMjsQ,2082
-cutlass_library/source/include/cute/container/array_subbyte.hpp,sha256=wfg7UVeqW2Adjy1-ImG7e73L6hOh9ROhWdtAVnM-QLc,18172
-cutlass_library/source/include/cute/container/bit_field.hpp,sha256=TQWU8OEgy1e8ymKBFMN3Egw3Ufrz47vSx3LgId_3OsI,5480
-cutlass_library/source/include/cute/container/cuda_types.hpp,sha256=Yn4HSplXTr98e_X8SSylP7bvn0R3ryJI8ZsenuDSVUM,4609
-cutlass_library/source/include/cute/container/tuple.hpp,sha256=r-JzB9I-fJAEcx0AIIBzCaOa-x5LgAdo2rJCjBwT5ow,21158
-cutlass_library/source/include/cute/container/type_list.hpp,sha256=ie1SDwWwjWwr3X8B9Y5HxdBCbGN8oyZXeNM4lAX3-Vg,4224
-cutlass_library/source/include/cute/numeric/arithmetic_tuple.hpp,sha256=MFZv3ONXTtKWj1RJxDrXbL9IypwWAQ_3xRyvznKKrAo,15563
-cutlass_library/source/include/cute/numeric/bfloat.hpp,sha256=jp1zm1f4e_ZhXrfRehmR8sr6JmyZSvI9E2HlxqY19X8,2170
-cutlass_library/source/include/cute/numeric/complex.hpp,sha256=jSlEdOPT4y5Aoj6JIuvEfOUwTcdDYX_0mj5hq9bgquM,2694
-cutlass_library/source/include/cute/numeric/float8.hpp,sha256=4JgfWvyyFwmqXFAmICkwLWdi3jWfB_Nf9vAN5qfRiUw,2033
-cutlass_library/source/include/cute/numeric/half.hpp,sha256=mlZxB2MVeZowZS0FgcNsN5ZJhpQxJgzt7u1vgBW7BL8,1997
-cutlass_library/source/include/cute/numeric/int.hpp,sha256=GIy-iy84aa9a_jTR6ZcAlLjLnA7_PgSOouc5g_d3jhQ,4871
+cutlass_library/source/include/cute/container/array_subbyte.hpp,sha256=fcr7aDpqptYMzKivPsyd1GCBTXbzKWa4SwIhKByZN2g,18091
+cutlass_library/source/include/cute/container/bit_field.hpp,sha256=FaLcnalZouHywPIDCgKcnEzQ8SwZD73nL1-OolrsyhE,5490
+cutlass_library/source/include/cute/container/cuda_types.hpp,sha256=fyAtxaD8yq4B0GGcysAHcp-8T5o6qGqNKYmPu8kUvTQ,4615
+cutlass_library/source/include/cute/container/tuple.hpp,sha256=deDRiALmz0JkYng8vxlHNq7Zq7eGtxzNfTA_PeV6pWY,20607
+cutlass_library/source/include/cute/container/type_list.hpp,sha256=FXpL2l7ktlSDBu1kHUHnL4sBjwxQkqtLXMIfT9P4Cj4,4225
+cutlass_library/source/include/cute/numeric/arithmetic_tuple.hpp,sha256=zUMkErRISymWV2KBcTlHgk8FPjwwIgaaw-INC7H7GBU,14952
+cutlass_library/source/include/cute/numeric/complex.hpp,sha256=Mu_3BF6PqDcnrMMBN7_2qvXgyq9XQxEVAj31OxmQ3t0,2796
+cutlass_library/source/include/cute/numeric/int.hpp,sha256=B0AXJfJf43J6FrlTmTGQG9jDsc2k68onwxR765R9ieE,3880
 cutlass_library/source/include/cute/numeric/integer_sequence.hpp,sha256=NVorwXasOV9TYdaIJ3zR1vO39B4XTppdfBS322VWoJI,4671
-cutlass_library/source/include/cute/numeric/integer_subbyte.hpp,sha256=u8kygnwpafdlKs3Kwm9_eJ4lohwl1xFg9z-zO1cyjKw,7216
-cutlass_library/source/include/cute/numeric/integral_constant.hpp,sha256=XDpvDuQQRdFfKwA293PkrR0Xixj6-u8hJtXzg0BZaAc,13004
-cutlass_library/source/include/cute/numeric/integral_ratio.hpp,sha256=prHtOOHl-5bwPPa9XHzBYFLFfvVPOSjfKWh4Ek__yIA,7214
-cutlass_library/source/include/cute/numeric/math.hpp,sha256=yoJic7QL8uryYgY-EZjhsTn-bSUJH_M_NoFtrLFO8GY,9013
+cutlass_library/source/include/cute/numeric/integral_constant.hpp,sha256=-71r_hrKyXM89CN3Q66xTKbOyTawCmcyk0RjXwC_AEI,13829
+cutlass_library/source/include/cute/numeric/integral_ratio.hpp,sha256=S4fA9qctCoKFxGCQOOAwDTVYwrqpX3RqULl3kmmduz4,7312
+cutlass_library/source/include/cute/numeric/math.hpp,sha256=jtjgQf1kQVhye54TZiACKZaQL6M90fqfA3KCPTFNKrA,9038
+cutlass_library/source/include/cute/numeric/numeric_types.hpp,sha256=mU4NA5eFixLaXO54iRBgHtHn5amgEr1pQNkR1-P8c_s,2949
 cutlass_library/source/include/cute/numeric/real.hpp,sha256=uGq-i1NGYvh4DTGkGeIqMpkwSvy3VRvtS2cyK6YWsck,2259
-cutlass_library/source/include/cute/numeric/tfloat.hpp,sha256=_DWtglRjUiWrL-rDE7b7-9hkuelE7G_NrfgI8xQSKpA,2170
-cutlass_library/source/include/cute/numeric/uint128.hpp,sha256=SVXO-1KvmlvCif1DmYalBT6VIEBszsi3oP5AdNdXknM,7531
 cutlass_library/source/include/cute/util/debug.hpp,sha256=kNIsqqk409AXT4OacFsfaYlSj0-PFi_b2i2fDwSVgF8,5010
 cutlass_library/source/include/cute/util/print.hpp,sha256=PgHh6c-cYP-ZS4--4RMbqLk-4ae8LGrzro8fkS-igYg,4310
-cutlass_library/source/include/cute/util/type_traits.hpp,sha256=lgg8ueBaIENogR7G0DMdrl2z4XJNFKaDoGkCTg09fAw,7775
+cutlass_library/source/include/cute/util/type_traits.hpp,sha256=TON1QhIV09iFrYGt6ZMygwoiBtRH3PvEATZGxxexm0g,8197
 cutlass_library/source/include/cutlass/aligned_buffer.h,sha256=riHSb49dAKr0fyadhB6HBkczjlZB33nNc8WNf4UbvNQ,3793
-cutlass_library/source/include/cutlass/array.h,sha256=orKBrDazE9uzabRqHpoNX7eY1hDawrqn81hOSVh9r74,66810
-cutlass_library/source/include/cutlass/array_planar_complex.h,sha256=JHt0MG8ZXRtHIlOc9J2vyuydsbHzJIoJDXpT4GG7kg4,3662
-cutlass_library/source/include/cutlass/array_subbyte.h,sha256=DjrtCMbiJVwiKDxacm72z_KjfINsa1EyH9zUukhZRSk,13552
-cutlass_library/source/include/cutlass/barrier.h,sha256=72fYuralrCxZIlj-M9w5bD88jlCLR6z0yjwD8eDRbwI,12443
-cutlass_library/source/include/cutlass/bfloat16.h,sha256=6yzuslrgFFJduyFbW8_K87jv50jwJDyAocjMSTfkmUs,14278
+cutlass_library/source/include/cutlass/array.h,sha256=4bmrYR84v_GLuAwtCJ-_-o059eQJlxDGiuIvDtorr2g,67892
+cutlass_library/source/include/cutlass/array_planar_complex.h,sha256=xMKMSCHba1qXxOjvUXPpdHKiebjA1FI4S5_Cmj46Ohg,3463
+cutlass_library/source/include/cutlass/array_subbyte.h,sha256=cA6XGVSWlfiGXm8jlK4XASu6od2_qIWP6SCNucXRpD8,13434
+cutlass_library/source/include/cutlass/barrier.h,sha256=vpOo94lCwLvBcGYxds8ZA7BxBBUPzc5i3tkZGmpCalY,12433
+cutlass_library/source/include/cutlass/bfloat16.h,sha256=iWyQkyoJs9P7OJpWeQxRX_aD2pE-W_ex3QY_qYP2Skg,13884
 cutlass_library/source/include/cutlass/blas3.h,sha256=RkBJtx90VjPh7pYD7atP3VwE68vT-vQA1dVO53uUDXE,5294
 cutlass_library/source/include/cutlass/blas3_types.h,sha256=XqbnvemR-yNIVnXm1Rwpn6sCVAhTYQVZ-xFCqLufu6c,3263
 cutlass_library/source/include/cutlass/block_striped.h,sha256=hub4xiRjaOsS6A4-0ORRFf8BMQt5cg7BwhzOHzZr7-o,9386
-cutlass_library/source/include/cutlass/cluster_launch.hpp,sha256=LWu66X3j9-83CYgaZSirKKxcZR5TYRvq8UlgAFFp11A,8853
-cutlass_library/source/include/cutlass/complex.h,sha256=83LmklS5ED9Qvu-t7yOyoSVZiO-40pVsOeQsvmIrV8g,20146
+cutlass_library/source/include/cutlass/cluster_launch.hpp,sha256=ohdd8Vyy4qE4afscJqRVbFlbS6GHS-xHK1wqO-qd5cA,9553
+cutlass_library/source/include/cutlass/complex.h,sha256=0f2gdV4am-P8xvuZJQD2I03m-p-BRUC_BiokZ1t2Hrs,19734
 cutlass_library/source/include/cutlass/constants.h,sha256=jS09QTP840pUVr2OaN0qAoKDkE0NxrQ0rFg8HIXKuGw,47943
-cutlass_library/source/include/cutlass/coord.h,sha256=1IFEVJJaMU1wkIGIT-6QZwMHyuHEliKkTf-OZtxQEhE,12221
-cutlass_library/source/include/cutlass/core_io.h,sha256=jdUenbqI1ifYlr1n0J__bVyE-485FMmuBWT_A1fVMD0,11385
-cutlass_library/source/include/cutlass/cuda_host_adapter.hpp,sha256=ekCi0o3a2CDldkMYU4-wRNRYNyCPOHs_kgm1yjJ5UIk,6322
-cutlass_library/source/include/cutlass/cutlass.h,sha256=s6_ucSBLealwyRhbZMLlMtL16Rl-hWhm824jaS9LSUo,6872
+cutlass_library/source/include/cutlass/coord.h,sha256=snPfwt8xrZuPDGfXgyR68YIYmzjH6UdL3o3sK-Jkgps,11827
+cutlass_library/source/include/cutlass/core_io.h,sha256=BJGwHUxuXKCOVrFqLcjRj8UoGWrLaRUClDC_A7siy08,10992
+cutlass_library/source/include/cutlass/cuda_host_adapter.hpp,sha256=uUOzdnczd1GdswEQB_ZkW-7PhWgosfo1vwXRLslseZ4,7249
+cutlass_library/source/include/cutlass/cutlass.h,sha256=1Sym0MlpB5w6F91GMqc0YM8OBa9IeynhA6a32E7Y1yc,6478
 cutlass_library/source/include/cutlass/device_kernel.h,sha256=io2cBuN8ErvjgMtg8f-qWQ_6XO-2Xk4PeNSKWZc3CYI,4321
-cutlass_library/source/include/cutlass/fast_math.h,sha256=ZLvIlNJgjEa9pIxMmed8SGDGdRRL0xfEik2QL-i7iuA,29056
-cutlass_library/source/include/cutlass/float8.h,sha256=n0P25Bmp_I6q2iV0iSGyJ8S-CzFr0wbU9DZCvCetHPg,36163
+cutlass_library/source/include/cutlass/fast_math.h,sha256=w62sM0phcoBIj5q1uEMYcySN87qFoSnsU_p5vJlsv2M,28949
+cutlass_library/source/include/cutlass/float8.h,sha256=Yx39WJa9od_SEZwo_DSn1iGyax6hENgqrp9d9ImQqOs,37129
 cutlass_library/source/include/cutlass/floating_point_nvrtc.h,sha256=Yxa1KP1RAV3WfNJgiFryP4fWiuhEdsjo1NmVpvBpK0o,2645
-cutlass_library/source/include/cutlass/functional.h,sha256=3XKeMYRnZ7-YE6Ryu9u9hn2bu-odtzP9X0Px-subywo,17280
+cutlass_library/source/include/cutlass/functional.h,sha256=Kwe69usR9OTLseiD3D_SwhQ5581ngtk2ykHDytlD4K0,17870
 cutlass_library/source/include/cutlass/gemm_coord.h,sha256=Rb-n9ZBAWSDF7l0ANPWXsqMuAXsZN3480dphqb-0F9w,10599
 cutlass_library/source/include/cutlass/gemm_coord.hpp,sha256=5dH0wmDXLU_KtjUbLRO2v2UxFa64FZbq8vGENYcYWNQ,2875
-cutlass_library/source/include/cutlass/half.h,sha256=v4XkaSwcqa5D2kCbijgj9AhKgqRhTbAEPr8ccybhSoA,24024
-cutlass_library/source/include/cutlass/integer_subbyte.h,sha256=GTREGivAxOjyhm6B6Lh25dXoaJIXwJaL4nSfbvJNN_Q,7353
-cutlass_library/source/include/cutlass/kernel_hardware_info.h,sha256=7rYxax2ebAKVqRQOBtLgnvtBh3sRIPWUPC1R4ly9fw8,3194
+cutlass_library/source/include/cutlass/half.h,sha256=4vTCMX40Eeoehx1e_KA0NgeiIy-UFrxSWGouG077qqc,23630
+cutlass_library/source/include/cutlass/integer_subbyte.h,sha256=Ewc8yh9iQqCmcmEpy6V4qnRxfd8B4nhuB3JNJrOqCkc,7942
+cutlass_library/source/include/cutlass/kernel_hardware_info.h,sha256=VmNfWmcojD6Az9c5sC6_inuAwKop6FEackFp32xpUEA,2784
 cutlass_library/source/include/cutlass/kernel_hardware_info.hpp,sha256=ukaN_XCYpMLSsLkzldRxl0ChSqPifMgB6Iz7M7ojy_g,2006
 cutlass_library/source/include/cutlass/kernel_launch.h,sha256=YEncj48kenU3bBYNLRsuTB8nPxKScxl5S9FqmJICn7o,2801
 cutlass_library/source/include/cutlass/matrix.h,sha256=5G30EFAwhsZBLu8RO6OaXS6rwf1OZwiDYtPSpNQiK64,364115
 cutlass_library/source/include/cutlass/matrix_coord.h,sha256=hefSG0tAe5tEN2Sr8WtkgJWVSFIwp5XkchaflZEchd8,4991
 cutlass_library/source/include/cutlass/matrix_shape.h,sha256=cgO5boGIO5-YG7RW7d2nv8-ppCWS7enBJ-ASbywQsyU,2726
-cutlass_library/source/include/cutlass/numeric_conversion.h,sha256=1Swdfp4dYlqFxUomGzlD82OorQP5T46TBuNiPELAofw,122014
-cutlass_library/source/include/cutlass/numeric_size.h,sha256=XJGIdFGvZH2_qTbR4eyQODx7kb06TFaY6hPzxqP6aqE,3605
-cutlass_library/source/include/cutlass/numeric_types.h,sha256=AJdmT-3w441LQvv78iiQ08spstxJB3B-hUTLVdOVTgI,3711
+cutlass_library/source/include/cutlass/numeric_conversion.h,sha256=lKwYzgv05DWX3g8iI3utaB8nEz4Qpgpuuf-Fo1fqBfw,125928
+cutlass_library/source/include/cutlass/numeric_size.h,sha256=vxFStr7LL7Dr5KwZ97wUi8Ngmai74-jiPYfWIY4ZgsU,3129
+cutlass_library/source/include/cutlass/numeric_types.h,sha256=Wsyt3bFonzarLvtVV4Bs_q21t6UE-N0waksFku6-Z5w,3385
 cutlass_library/source/include/cutlass/pitch_linear_coord.h,sha256=kdDypbltTj4oi3lD5hh03NaqicVIiJuvEXmDaYtdIl0,5492
 cutlass_library/source/include/cutlass/predicate_vector.h,sha256=EepOK0FDUg3umxUgQP8AkxJCZu1zXLhIR8KNVYP8rYw,16279
 cutlass_library/source/include/cutlass/quaternion.h,sha256=Ew0xUdZ7_dRLXeaA1AkJJuxhSEZxkSYIXxzioj22O0Q,20891
 cutlass_library/source/include/cutlass/real.h,sha256=B2qg1UB62p8aeaV5X1Gv2_pBxcmrsdOrUylnBvlZFTg,2369
 cutlass_library/source/include/cutlass/relatively_equal.h,sha256=y432_hPZH37PdIkr-ss0cMbVLLQ6a77rJX0tq1bGW8w,6572
 cutlass_library/source/include/cutlass/semaphore.h,sha256=p95G1Lp1TwHidlB88MkBOBsrUdievbfD3YKrn0aZHV0,3984
 cutlass_library/source/include/cutlass/subbyte_reference.h,sha256=0goXlnu7m0eQmmdJsld7EI0t4MTjCso99j1TeHkKd-Y,38253
 cutlass_library/source/include/cutlass/tensor_coord.h,sha256=FEz5FKss-eJO9PuvmLODvIXSZLFb0JmWFDHvndoTB1s,8964
 cutlass_library/source/include/cutlass/tensor_ref.h,sha256=HQFoHggWDrWlcb-LHY4vu8oJmgJwaJL1XMc3Z3VYz2g,12207
 cutlass_library/source/include/cutlass/tensor_ref_planar_complex.h,sha256=l1vYznd4jS6fBOy1SIRQBcKOoTRLvO9VDn5XhYWHEno,11201
 cutlass_library/source/include/cutlass/tensor_view.h,sha256=QLU7KdJf8PuVJCjYpFJLNkunbcmMsmP396xvrxOuGNY,9509
 cutlass_library/source/include/cutlass/tensor_view_planar_complex.h,sha256=quF5wUihypxcnobdLvFMysFnc4Jpy4ojZaqTSK8AuO8,10250
 cutlass_library/source/include/cutlass/tfloat32.h,sha256=RwN7vuC7cqo0oNALpi0nZdXEILFLoV70E0VgDj-VsoY,13017
 cutlass_library/source/include/cutlass/trace.h,sha256=WvJ-f3boqn0QAGjndB95Ho4fPY6f9PTc9TpdJF4PTAw,2581
-cutlass_library/source/include/cutlass/uint128.h,sha256=dv-qDPpNXoqOfFVFf0WLLbLXSSWBAQ7qB1FaXrViuQI,8322
-cutlass_library/source/include/cutlass/version.h,sha256=aj_ChTR2LjlDjU2uLvCPkXB2hp-KzWCFWakCOghzLlw,2899
+cutlass_library/source/include/cutlass/uint128.h,sha256=9tKo7bJBeTRW5JJVLSS1AHUdQLp2zJWB-AZPu9La3sM,7899
+cutlass_library/source/include/cutlass/version.h,sha256=QH37HzbXbSK-6waUSRggx76oFtw2bK59T8iCsGl6qXs,2899
 cutlass_library/source/include/cutlass/wmma_array.h,sha256=eTH36zedPKrMuuEYsWwTiaBkYJ-6g-FBX8nvKxHkJV8,4540
-cutlass_library/source/include/cutlass/workspace.h,sha256=w2k5VVr3hoMvt9m2UWlrEb0CKA-3aoaYnTQ9mAjnhck,4964
-cutlass_library/source/include/cutlass/arch/arch.h,sha256=8sY2XLkqyOc12_Hs-QxmFCinckYr7RGCOUzB5JW_Vzk,3564
-cutlass_library/source/include/cutlass/arch/barrier.h,sha256=DPnx71ByW-LyVnEYFeqofYkxBajeMsXINglEa9I4-mg,19807
+cutlass_library/source/include/cutlass/workspace.h,sha256=0sVMB2CmK6ySy0slj-J-onYi308yr9y3akxAWp6WdLU,5116
+cutlass_library/source/include/cutlass/arch/arch.h,sha256=Bq0F4d1co97T3feiYMgKM0GoIeFP9puFKlA3GMteEKk,3628
+cutlass_library/source/include/cutlass/arch/barrier.h,sha256=ecqgFdeKN6EdQFjTqgpsFq0MdBbhf8pL7d5nfotE-jU,20151
 cutlass_library/source/include/cutlass/arch/cache_operation.h,sha256=K0naWy6CmN-1jrxwRX0KQI4mW3vzSHFcP8uUpoX1GF8,2691
-cutlass_library/source/include/cutlass/arch/memory.h,sha256=rZoJJ5kWDEB4hkPTak5Ll_unr0NtKQ89qHfhKDQot9M,18266
+cutlass_library/source/include/cutlass/arch/memory.h,sha256=UInHAJKl6O3Bu4Y7t7ud5GuUyythJDrLsYL7EaXNygI,18305
 cutlass_library/source/include/cutlass/arch/memory_sm75.h,sha256=UtNeGZ8iF7IDr8gfXjSOSzaolZYaTQ2wmC5ekIzs2j8,8260
 cutlass_library/source/include/cutlass/arch/memory_sm80.h,sha256=TLQjZc6Q2z0Ch0Cuqi8RR07Sxdh312Gn5O5cpHj3I6Y,15163
-cutlass_library/source/include/cutlass/arch/mma.h,sha256=g-z8bKmo7EszY_-q9dYOMlf4EJjjPBVZB2dIKxbacng,9252
+cutlass_library/source/include/cutlass/arch/mma.h,sha256=i6jkjphiq4uFikvaBMThMkb1nKfLtxv1iPU-teTaGfY,9800
 cutlass_library/source/include/cutlass/arch/mma_sm50.h,sha256=29EW41BGGcR-2vshF1hnC_iwSvRo7kTgbzUBHQbo27E,11096
 cutlass_library/source/include/cutlass/arch/mma_sm60.h,sha256=aMMvx2Kz-_khD0jnvY0T-P7NU1J0-3eK2WfuK3sMThU,7040
 cutlass_library/source/include/cutlass/arch/mma_sm61.h,sha256=FGtumT5zbgkm3Piv8w6FR7mtYbEl3N8uKh44XNGypnA,4193
 cutlass_library/source/include/cutlass/arch/mma_sm70.h,sha256=Mn6J0-S7F3YE-1K10xH3-d1Y54txy2RJgeyJDdxpwZA,16554
 cutlass_library/source/include/cutlass/arch/mma_sm75.h,sha256=hNl-QMm3kzO8hM74dlC6gi2eMnz32rSp35SAw6Cz0Sk,31990
 cutlass_library/source/include/cutlass/arch/mma_sm80.h,sha256=r76WJHA5uX_FmYjB6iVIeur-KMACNvjMAdWXnvlJvc8,57636
+cutlass_library/source/include/cutlass/arch/mma_sm89.h,sha256=EeoWegoWJJ_d-1VAnVSPg5nNGWSRDqqK8DGt1I0tuk0,11290
 cutlass_library/source/include/cutlass/arch/mma_sm90.h,sha256=Sbr7Jg1Xv1NUKh25nKemWvr6ichrQ9vR-p_Fp4Y1yQ8,8419
 cutlass_library/source/include/cutlass/arch/mma_sparse_sm80.h,sha256=QL5Jjb6N0w_37dp0NLS7350MsjEfpNGe1ZdGYRlrdL0,43978
+cutlass_library/source/include/cutlass/arch/mma_sparse_sm89.h,sha256=KjcjxEQfudSpROvaJzpJZx8D6s_67GxBWo6DAcQARiU,12126
 cutlass_library/source/include/cutlass/arch/reg_reconfig.h,sha256=8zuU84uqAUUccX2QmpfbvyjeYngE6pL6HwI4mIBkkoI,2621
 cutlass_library/source/include/cutlass/arch/simd.h,sha256=NVtCNrRzQEs5HzwpxrSYN5w8o4kTSrfzyrFQhbf50jc,3998
 cutlass_library/source/include/cutlass/arch/simd_sm60.h,sha256=12QgMIcSuLqQkdVuUP5Bzjy3JutXb8j8WaIZxigmjBE,3590
 cutlass_library/source/include/cutlass/arch/simd_sm61.h,sha256=BhYCVB136gwCVjNpcerZXfBbysdk-WDll08ejAMPIks,5102
 cutlass_library/source/include/cutlass/arch/wmma.h,sha256=z5rXLlWrh-QKyLLCez3KEvzA98rG_R5vHbMJWZalcrw,8473
 cutlass_library/source/include/cutlass/arch/wmma_sm70.h,sha256=7MpeXWm-awnRPz0JQFA-FPfGpJwTUtFsURdCg_YjaN0,5286
 cutlass_library/source/include/cutlass/arch/wmma_sm72.h,sha256=8b874P9eq7aWoNr8DXwamK5hL8JeudATpXNCyhhGApk,7746
 cutlass_library/source/include/cutlass/arch/wmma_sm75.h,sha256=pqGwgq7vKuZGgyqE7pA4JpPRvDosbetBmmlzr_szXPo,7616
-cutlass_library/source/include/cutlass/conv/conv2d_problem_size.h,sha256=AuY2YcchT19WeyD6YElpRqIyT4ehLqJvwyqVV2rhmbY,23019
-cutlass_library/source/include/cutlass/conv/conv3d_problem_size.h,sha256=OTYz6B66wRo-QkmMD5W5p1Azi4NHYU8P7FsHOe9sIq0,16660
-cutlass_library/source/include/cutlass/conv/convolution.h,sha256=eVGGxC4qA1xt50zEAv88ZnuFOUXBSLML9aieWdHcjH0,7140
+cutlass_library/source/include/cutlass/conv/conv2d_problem_size.h,sha256=IzuYkFoH8ycn4N11vrmPFI3sjZpCFfZ2pwCXmZGTabs,23132
+cutlass_library/source/include/cutlass/conv/conv3d_problem_size.h,sha256=WucLRI6nC6C5P67riTSIh_07hQ-TgI_U3CXaqcjw7Uo,18158
+cutlass_library/source/include/cutlass/conv/convnd_problem_shape.hpp,sha256=6fCt7RjnQ7hbNFzpCwwtvvBcc0Uxcix6HAlBpXUuU8k,22931
+cutlass_library/source/include/cutlass/conv/convolution.h,sha256=csBa1evMBi4_XBOAF-I5SZ6jbcl2YzYdFCIQ-1eVB1M,7293
+cutlass_library/source/include/cutlass/conv/dispatch_policy.hpp,sha256=Dv2rUNkHD7d33UbpHoa6CZnwBwy-ZC2wAnrZGxM4Z0k,3636
+cutlass_library/source/include/cutlass/conv/collective/collective_builder.hpp,sha256=9u9_ox8MRnLta6xHQcc3Wf54hJF1S0Sf_Egt-EEAZJY,3783
+cutlass_library/source/include/cutlass/conv/collective/collective_conv.hpp,sha256=4NKTzIg6BXZKA75Fy9GZJEyi4NBTlxjhvVYNQu5w8Pk,2853
+cutlass_library/source/include/cutlass/conv/collective/detail.hpp,sha256=WCnRoAEl3BWPyWoy4kwlpYx3azOqA1INXVu2UfNYpl0,10716
+cutlass_library/source/include/cutlass/conv/collective/sm90_implicit_gemm_gmma_ss_warpspecialized.hpp,sha256=WW8oqSJ4E9epvnASSY8QeWFl4wjcA9J410_wgXyXxDw,27380
+cutlass_library/source/include/cutlass/conv/collective/builders/sm90_common.inl,sha256=ZVn3rZRiQzBpsBZsIf6Eix4zBKy69aVuY6OI0OKfhr0,4145
+cutlass_library/source/include/cutlass/conv/collective/builders/sm90_gmma_builder.inl,sha256=xaYIWuTZPoO7qofs_n_HVJBulRqUO19rBLG6X-Iwa9s,10983
+cutlass_library/source/include/cutlass/conv/device/conv_universal_adapter.hpp,sha256=SQ8HtJb-07M6LI4k4K3DIc6XwuRPLHo29K6CGwbGGGs,16194
 cutlass_library/source/include/cutlass/conv/device/direct_convolution.h,sha256=x7ip9L5K3uIZR7NGS1pnTwT9JlW8S9AgPSXtl3BYSqg,9743
-cutlass_library/source/include/cutlass/conv/device/implicit_gemm_convolution.h,sha256=re-5ebPSAKObmu0SDhk_E5eMB77pzdlP-NM6PBJrS0g,12078
+cutlass_library/source/include/cutlass/conv/device/implicit_gemm_convolution.h,sha256=26wNEM5tHW-4JbA4L1wG9HfA5i5Skq6alUiWoogRsFI,13380
 cutlass_library/source/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h,sha256=DM2ulv1B4rrRMw6avKkHrBp49mFWMEIqbJwVM3iSKDA,10044
-cutlass_library/source/include/cutlass/conv/kernel/default_conv2d.h,sha256=MpurDVX8agzAHc0RPnxxfEmbuyKjgUpHBlves3s8hPk,7671
+cutlass_library/source/include/cutlass/conv/kernel/conv_universal.hpp,sha256=Q3LPFaQGBWqW00iTx6NdL-g-4kjkEQcKcD0JeSx7GeM,2888
+cutlass_library/source/include/cutlass/conv/kernel/default_conv2d.h,sha256=l3cGWWFo3xhDMQ76qenMWGnoqizj8W2UooqSYMPmdJ8,8632
 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_dgrad.h,sha256=g1HyxidSv98pRNdCR4152hZld1f0ykpiMlW-8_CeAAc,53546
-cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop.h,sha256=xRswqzMAeL323awVZ6q_X1C3RKue7hJE0qKwwk7mVPk,56838
-cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h,sha256=uKqTZIjiDPXjVUvQJAYCynVemTqFoLKZPYO_H-1_DMc,11953
-cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h,sha256=HvdCHVoKgJ5oZCJ2AYLXZ5P16G8HVDityjWFMT9ciEQ,4689
-cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h,sha256=K9aOMrnL3sZO-uALPWeQoj6thTmtxKLFmphtPvAiY6I,4659
-cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_group_fprop.h,sha256=6yJdil7PRZFQTT_wDvzhRNc4OluYrLMi1oKFDw1lRiI,19603
+cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop.h,sha256=40QG6V0VsSxPT9okS9a5L4CcYQn-IG9YMYankRxacc0,57134
+cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h,sha256=hp8zRgain660OpC8BLqMqJd4b812VDSf_-CGuyq65h0,11951
+cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_absmax.h,sha256=T_1LD0rrwhK6LoZU0hqOLvVydi9m9fQvUAdRcoAMVZE,4609
+cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h,sha256=bFnjehgclD04oHqVoUFCTMi3rPXiOBr2vgYwP9WJq8s,6978
+cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h,sha256=D1lMEutUQTgzicEvTpjfeqvr6NTnbVXIR3n3CY13Ocw,4657
+cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_group_fprop.h,sha256=pahHLG1xHuGFkF77Ay7okyGvnJl_imCz5gmAwRNtsGg,19662
 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_wgrad.h,sha256=XrrnKvp9P9JYYyve8xVxUqE0BLucC7fjZzJEUfz7m84,28745
 cutlass_library/source/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h,sha256=d4OMbh7VeVhjloH9O6UL83Etwpwl7iFYoyNcoaLgcyg,10459
-cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_dgrad.h,sha256=KTGXCeJHvfcsAFwj7hH64XV3fHvJ5HvAyuPlPWewpT4,9324
-cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop.h,sha256=mhIryRe1LQf2QqjNRJshmg00eA2WQaqWMfVJMxAVwBA,14864
-cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h,sha256=oJ4E0518VSlatg7nRM1njBPlgXseDwHiNbdlUrTvY9c,11980
-cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_wgrad.h,sha256=uwAp03MWS9aADHMNMhkrehQyl_AhtY5ivjmGqSFbTpc,14883
-cutlass_library/source/include/cutlass/conv/kernel/default_depthwise_fprop.h,sha256=Fsqj4UoqxfxLt4fwO-P6SlCZL2LSQsbIOeK98DdfHHM,19293
-cutlass_library/source/include/cutlass/conv/kernel/direct_convolution.h,sha256=W6JfX4KM_vI6Qwk1-ZZFanAcLdszYnHpmJ5-QyF9Xic,18026
-cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution.h,sha256=pNmgUKZCfrthROvBZ7PGn6A3QVKyHHWdMDSry9MiIjY,15413
-cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h,sha256=r7b_24fFfPOlnl5aUikyaIklC3tqRNZg7U_cI3Mc1g4,15690
-cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h,sha256=1OUjH8d01JYimf0FaXpDDuIzCJMCjD3rdO6sdAkuC5E,17222
-cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h,sha256=TVMExqx6k95SDi5cXxTqFpTHr5vl1fJWLYYUgglAHto,16730
+cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_dgrad.h,sha256=w236fncN2R99Vdo-r-PxV6pUO3rDbbfS1TKiuLGzxxA,20919
+cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop.h,sha256=mpwebLdIbkslHTgsKmpa1O7OF1D05jKr-93RE4fx-zY,27125
+cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h,sha256=shIy5XNEA9OSUVrzgB6L0itJnPX0jUCzYukBsBWbQ7Y,11978
+cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_fprop_with_broadcast.h,sha256=4gfkt01yBppPLbsdt7cV547NnDpyXWDiZ8PGK77pPT4,6953
+cutlass_library/source/include/cutlass/conv/kernel/default_conv3d_wgrad.h,sha256=1sMP_T-_JzEqDErJCEAVaUhjoAje0BbVJzvfnO2OQ84,26380
+cutlass_library/source/include/cutlass/conv/kernel/default_deconv2d.h,sha256=Bc8CEftdpf0ssATHpj2fI8OjSq0RUGy1QEXGrwc_688,27986
+cutlass_library/source/include/cutlass/conv/kernel/default_deconv2d_with_broadcast.h,sha256=jOiWbjvyOU7DZBAL2EdFojjeUlWpk5rHbWOpkLit3dA,8912
+cutlass_library/source/include/cutlass/conv/kernel/default_deconv3d.h,sha256=2iMR0k99kFOsvIojxw-q8UyrfQ1e_0hao82mXbBz4FI,15389
+cutlass_library/source/include/cutlass/conv/kernel/default_deconv3d_with_broadcast.h,sha256=TDzxAxr1GIHikmgA_qTmHEsH6XJ8UsQ26oSnp9uwZZ4,8917
+cutlass_library/source/include/cutlass/conv/kernel/default_depthwise_fprop.h,sha256=I4Gmg7jkaqfa2DPgzNHkxx3nhoVdiLPovvdAss7PeVg,19289
+cutlass_library/source/include/cutlass/conv/kernel/direct_convolution.h,sha256=5S8SNj_q35_J0UoK5ZSmd4hvQ5P5-yroWtVdLDpbWBg,18078
+cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution.h,sha256=HX9VT9L8hfFWKfUpufakxjvyuQtRUNXxCViYrEMMWf0,15458
+cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h,sha256=pCnBGfbUMMC36N6pmy-RP-qqHjYbo8sH3mBPB64r7dM,15729
+cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h,sha256=gJcXx3LC3t0rUBczxicWkShLeXIY8PPp5BrDi-tK0j8,17257
+cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_with_absmax.h,sha256=zdZY5TikxG-ksYtNV4v4KvfVjt3HQnhfM864Ecgf1QQ,16581
+cutlass_library/source/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h,sha256=FzWHQ5daRnbhew0SxLWtgp2sBi-zdafC4g4O-4pJFNU,16738
+cutlass_library/source/include/cutlass/conv/kernel/sm90_implicit_gemm_tma_warpspecialized.hpp,sha256=dscCtlz0UZ7FTSg8Szcw6OsGNm5Sv5lAQxutkgCAEXo,16865
 cutlass_library/source/include/cutlass/conv/thread/depthwise_mma.h,sha256=mI7cuc4eaHsQiTN_w4j_8ojIb6g1CWAs47QdjkS8Tdo,9689
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h,sha256=drk3ZnlPEyBse1DMAoB29cfGbbBKC1uhA_dlOAau_yY,15306
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h,sha256=ERamiSzIgWIHOTz7PSoyHex3hXwM1XGQTSkX-lZtNlA,19735
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h,sha256=_NiX6BeQvrJT3VFNTf8wfT6HlPdBpCSOvioSYqcKVv4,18940
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h,sha256=rpXdaooq9NstDHvq9M5LwvuONsTXuqwCYQxUeigC2PU,26136
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h,sha256=4AJ4MnxzRx8TaCUW4IPXGttQMwx4_M1zxCkbwrebqYc,10977
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h,sha256=pJ1Lh0lf9N2dCvPGV78Fn-8ak2SX8nLZ4YjL5RTR2l8,11529
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h,sha256=NDOQ8PRXIAFNSFJoHBlkSTCVgj882RS5lkvk69PSIHQ,11333
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h,sha256=QhQUuboVuWzrvHINrLm-RLB4OBP2N2oGk9vjr8VMGJ4,13688
-cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h,sha256=jQzjHzwSlLummFo05JJY1BJdjiQpeCdVGwmc8VHQqUQ,10651
+cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h,sha256=PXaXDW3oAPRA3SamDl_HtJeB4A3CVYSGb4HbGQjnUkM,11164
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h,sha256=muurJmO-rHgoNECsY9GGtg51RyCi31huE-YbO6a8DTk,9314
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h,sha256=XpVE_29Duj3hx7JaSFZVB7Uu4vV1cGtORS7tRNJVHr0,9018
-cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h,sha256=CEzM39yylrO0iXY2B6uiPhB5XxaRYOtZILlJnma1DMY,10411
+cutlass_library/source/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h,sha256=0kSwL0eEmtbwGMJVQBjjZJPIgKlZfn1FXtZ1yb0OuDo,10689
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_params.h,sha256=-N5WHFiP_NntPlOdU95O8DxnZTGrE5N7ChEidYcX3Lw,30197
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_tile_iterator.h,sha256=16lTjsamV0ymZPE7g2F_1SSXwiIoHW9WlOU0Rwse4Qo,11202
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h,sha256=TfKc9uC0kD0XE3QSW-5Kz1HECv1yQ4FgsKDczgLrhHc,10349
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h,sha256=7JuNlmBxZO2KJbYP_ympn1UeqSIJJWErKJIuCWx4HII,11519
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h,sha256=JN7fbhEjdPHhl7mVVZb6U22Ka0wDs5UgjpwrhwASJto,9043
 cutlass_library/source/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h,sha256=iO8E8lGpnljSj3W6IXCb-NvLi33fkjmpfX-xd6snVsg,10832
 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h,sha256=FfEEZG3Q5QNdIxnsVFqRpsFOqmBGuzG6vlZ8tskG6gQ,8450
 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h,sha256=wqdLbsfBiAtJphMgV7qZfmrOC-ujJOy78F_i3EF3DAk,9569
 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h,sha256=pzo5NgaMiZ6h_SRT3WD4gXgAtQO30870eUP1zV3Y_B8,11020
 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h,sha256=kBALaNofS_eUGBAj_DVvQ-Xsc6APsHvhL9rrB4OqiME,15014
 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h,sha256=-Aij-vww1KrjDaEw3h-hM8NQg8JplkSq9Ve5kkHZuF0,9634
 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h,sha256=LILebmnKXoabSRg3ZDtQz3X717zJ6Q4qUdR2d5q7vj0,15132
-cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h,sha256=u7BA02mCsaVUoaffejaXpK6MTWsyrfmZOrz1iZBPJPY,7945
-cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h,sha256=MKOxsjiqt9WvEXggSBlwNAx-lrk2ZWdPEdVcm3L40Ec,8891
+cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h,sha256=NRqeTDDMNP0GXkCwg2MT91-nA-XHKXqa2eNSuir44IQ,8307
+cutlass_library/source/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h,sha256=G9O7ns4oICNAOJHwerzeOW7HQ05EBbeCrtHneRieBVY,9126
 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_params.h,sha256=7wX1cKpgBCQUmrqDnnceWKnsZlE7alPTOgaIxJ9dJXQ,18249
 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h,sha256=N65tSTZ0MFPZfZiHBzoXCps6rGa_kG5KaiOY4DsKThE,9971
 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h,sha256=_ABssg6tJfnlPZU_xxPT0dSA0tvTEg3jX8Xw07DIMZ0,12024
 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h,sha256=lBIMsKzbaIFlC45GJZ7TNKz_mm8Y0q4aeyscBosLnCw,8821
 cutlass_library/source/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h,sha256=R7a467BTqUJo8XCvE3GXKnJ8EcOsx17xjPSOcNrZWzg,10744
 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h,sha256=j4dIhOXl26ElzEc38qJ7Pgnw7iJodNEMHTCj5SL9Zds,8871
 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h,sha256=Uqj_rELhxilGtmFFSUli5u1aRVOmiA_sy2yGMo1MgM8,10747
 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h,sha256=km-WGLkQKlehy6euCcNQGEqwFH13DjEO0_qdTKX8NZo,9899
 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h,sha256=dmTZKvkKkfk5tglUxLbwiuLmzj3MjzfV76GuOU9pf3s,20899
 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h,sha256=I_kpjVoklUuiOLpmC_d4zDIH0KLNTbEKXOsEEbryJpQ,8921
 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h,sha256=nc81QZ4KDQJ4ahGj_CMi9f3g2pSQLPsGIiOq8UnK7AU,12745
 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_mma_base.h,sha256=d1s4pqZQan8qybipow3J51tHJECJAGPAvmIR5tE93z4,8097
 cutlass_library/source/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h,sha256=sI_6bb3qC1BTBbIMTGPJpNaE8AZCH-uV9yXaA5XTktE,36697
 cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h,sha256=-_Meo10l6rnQIVnzf19DXwrID_1tiMnYmJX7Pb2hlxI,30106
-cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_multistage.h,sha256=9cekOTKtlkaOi2Afq4zzUkul2cqFgyVIaHxNJnEn5J8,19823
+cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_multistage.h,sha256=Ntl6Z-P6it3opExuf-0QD3XC9-abQAAW8eHDWpoj3e0,19808
 cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h,sha256=O43G5X_IeETISZCWwM5EUthhWZmfMlNyRYraLIn9NQ4,12175
 cutlass_library/source/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h,sha256=BNf8Wvred65kXRW8mJwXF7GsJWZNFsqzvL_G6MKsZfY,26320
 cutlass_library/source/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h,sha256=UKvQWGPW3a7n0MV-XzOdCeuWfU7s6FMCn6ho8Yv0loE,16915
 cutlass_library/source/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h,sha256=vpD9WlRVx8Sv65bhid3VS-3SD9mYRa1W129befwFXig,12476
 cutlass_library/source/include/cutlass/conv/threadblock/threadblock_swizzle.h,sha256=-Opuvc0GI6zHrXJODurKjkLr6n6WJaizHwi7s_mz-GU,8050
 cutlass_library/source/include/cutlass/conv/warp/mma_depthwise_simt.h,sha256=TE3yPSjHzwuwj_4UZ9hAPpG4n47FJVzCUWZyoOyg2MY,12392
 cutlass_library/source/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h,sha256=q2YR-gc41qWZ-V4B4RzXLh-bsW0-e0u2dv0dC9gN33E,30655
 cutlass_library/source/include/cutlass/conv/warp/scale_bias_relu_transform.h,sha256=rzAQl5owq35PvugU96_CBNGb25UpUuuzXmuhdizD8rE,8704
 cutlass_library/source/include/cutlass/detail/collective.hpp,sha256=X109PaRCoVV4BAqQPDbpsP-Dux-eZQZoQtGpy15Wk_0,3048
 cutlass_library/source/include/cutlass/detail/dependent_false.hpp,sha256=nNqgIahe8o2sU6dN8LaYjTps__rf8jHrqAr-6mvBW14,3710
 cutlass_library/source/include/cutlass/detail/helper_macros.hpp,sha256=a3ulwKhR5UyL4n_B1DdO6F80jO9MflNOIY3KJds84uQ,5745
-cutlass_library/source/include/cutlass/detail/layout.hpp,sha256=4-8jW9xKV1StzUf-kDNrIu5LrMm_W912wp2ssZLbr4g,9629
+cutlass_library/source/include/cutlass/detail/layout.hpp,sha256=OyWkXkoQo8OQiGKNkKCBdaibYsbT1Uz4WdlFXOi-aF0,12305
 cutlass_library/source/include/cutlass/detail/mma.hpp,sha256=K1WQPziFZGBRh6KQU4gNVcaPFPAzPnE_2SFtHSBWZbI,3089
-cutlass_library/source/include/cutlass/epilogue/dispatch_policy.hpp,sha256=GCNlG1lJfPM03ykjutwM1jFo9zmc50JGg2kQK0yUoGs,6054
-cutlass_library/source/include/cutlass/epilogue/collective/collective_builder.hpp,sha256=mrDUWulJcQrksqof6CSKW8wfo9VfVBPaUQ1Wl65AyYQ,4407
+cutlass_library/source/include/cutlass/epilogue/dispatch_policy.hpp,sha256=XsofNLLiI6gbYxzY9NYNjClhN4UHm__0HHWaVKkh3CU,6133
+cutlass_library/source/include/cutlass/epilogue/collective/collective_builder.hpp,sha256=ezD5r9jXvkpUw6rJwFUgdrt5LzF24n8Su_XVUjlPU30,4425
 cutlass_library/source/include/cutlass/epilogue/collective/collective_epilogue.hpp,sha256=NaSgXM9cvfhgq45BaUayGCfhIokeh948V9uvY3x6Wv0,2957
-cutlass_library/source/include/cutlass/epilogue/collective/default_epilogue.hpp,sha256=pIkJeVJdO_169VPU8oLJ0Twwf0Y-mjx739Cc-9oHsRA,9109
-cutlass_library/source/include/cutlass/epilogue/collective/default_epilogue_array.hpp,sha256=o8YZwr-70eJqekl9tgCPWxG1j600OWqKzIaSQcdjsKU,10398
-cutlass_library/source/include/cutlass/epilogue/collective/detail.hpp,sha256=s4icOCFMmO2Kgih79qtFWb178b_sJiBn2E3WiO__tPo,8237
-cutlass_library/source/include/cutlass/epilogue/collective/epilogue_tensor_broadcast.hpp,sha256=WxnjBKRueLGq37L4yrCB7vB7E0bXwUOCvg4x7NXdiRA,11300
-cutlass_library/source/include/cutlass/epilogue/collective/sm70_epilogue_vectorized.hpp,sha256=zt-y-rXBghtQCXege4vQjZv952f7iZDsdVE63SPHQKg,13967
-cutlass_library/source/include/cutlass/epilogue/collective/sm90_epilogue_tma_warpspecialized.hpp,sha256=HtERpsGk7rFxFQ-cRL38k6qVudBxGsQMJTSeWFhJqvw,33929
-cutlass_library/source/include/cutlass/epilogue/collective/sm90_epilogue_tma_warpspecialized_bias_elementwise.hpp,sha256=UCdBDq8q9vuvTonEiasL_ZkW7ByR6WlRLH3_MPAl0ek,5369
-cutlass_library/source/include/cutlass/epilogue/collective/builders/sm90_builder.inl,sha256=CX1qqmOiuBi4J6ptF8sWghcb03toEcWHdWsZRihYTlM,28149
+cutlass_library/source/include/cutlass/epilogue/collective/default_epilogue.hpp,sha256=oOtOKhtjJLHaGExZQXxt71hos9q-IHZW280kgoRjKEQ,9243
+cutlass_library/source/include/cutlass/epilogue/collective/default_epilogue_array.hpp,sha256=B3zXXHmCzz-GVD9cG-NNpRul7sLq5KlfU98c_cmmvnQ,10494
+cutlass_library/source/include/cutlass/epilogue/collective/detail.hpp,sha256=XQlCtUGUS7qOR12dVsbq8pg7d1HMU6AJaXe3avtXl0E,9625
+cutlass_library/source/include/cutlass/epilogue/collective/epilogue_tensor_broadcast.hpp,sha256=SB4VqIgF0aU0hez8NQuN4WtwF9k1YfQ-B9L6DUhr6XY,11386
+cutlass_library/source/include/cutlass/epilogue/collective/sm70_epilogue_vectorized.hpp,sha256=JKNHxu5h3-dWN5niYA7Jisi0FOzUatnZReQtpYbIlQY,14012
+cutlass_library/source/include/cutlass/epilogue/collective/sm90_epilogue_tma_warpspecialized.hpp,sha256=yGIFYF3p-uZZyLyDJc4uZx2MY0a9XqDH8NnNrbR2vRo,34828
+cutlass_library/source/include/cutlass/epilogue/collective/sm90_epilogue_tma_warpspecialized_bias_elementwise.hpp,sha256=t7DG2PEfidXS4-7lvYRa6sBFn1jdjpRNMg87Vj0q5gM,5427
+cutlass_library/source/include/cutlass/epilogue/collective/builders/sm90_builder.inl,sha256=RWrPRoemWkjdb4EAKAM-uSaOQ74AGD6-10Tcs7Qo_1g,29518
 cutlass_library/source/include/cutlass/epilogue/fusion/callbacks.hpp,sha256=vB9wddj1FKSnaL7Q_4EEKRecsqyEdyPOCstU-kC9J7E,4128
-cutlass_library/source/include/cutlass/epilogue/fusion/operations.hpp,sha256=PM7LfsR3umFjgMVlUwLkExEuKOtDP8JxtIq7OGbqh3c,12104
-cutlass_library/source/include/cutlass/epilogue/fusion/sm90_callbacks_tma_warpspecialized.hpp,sha256=f4V9Hnt0wCWYGLH367sxGto0r4TnlNNP9d7V6S7-Smg,49837
-cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_compute_tma_warpspecialized.hpp,sha256=k9kKvuT8crltG-0x9G2Q5qyo8rczoKbtt03Dqz7YK2Q,28095
-cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_load_tma_warpspecialized.hpp,sha256=S0nENOYNhUWqLVCFBBIoB0E9BfR8SSPHfVhdCM2mRig,31364
-cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_store_tma_warpspecialized.hpp,sha256=Cktsq__wKjqdizhMR10-kJrud_bo-7m5IlzR0TjGNSY,45042
-cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_tma_warpspecialized.hpp,sha256=pFFEgG_8f_6iPJwYP8a4e5vDXX11Gr9Btl69_MOq9Pw,37837
-cutlass_library/source/include/cutlass/epilogue/thread/activation.h,sha256=pijtY01Rz51GO3zTOPNbogD2wfF4cHTlji9UZn42454,17386
+cutlass_library/source/include/cutlass/epilogue/fusion/operations.hpp,sha256=ZcCadGanJ7nOLB5fn_8ZE3_4kUYIhWY5u9UBl__zAFE,12105
+cutlass_library/source/include/cutlass/epilogue/fusion/sm90_callbacks_tma_warpspecialized.hpp,sha256=OhrpQT-VX28YINPV6CH5Y2b-0seGFf_Osk492aHvPFE,55072
+cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_compute_tma_warpspecialized.hpp,sha256=Xfjo9mcvawpsGMMqcho7RMAxEyvSW9oG8VqV6Kqxm_M,28888
+cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_load_tma_warpspecialized.hpp,sha256=MfoINacw8fzmQfUxThipP6qGoLLlD8qTlDysxGYIy2o,32867
+cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_store_tma_warpspecialized.hpp,sha256=U_7z2JSWTzfeKC2z8aAEbphcGagOelcmYVlS_03S27o,57933
+cutlass_library/source/include/cutlass/epilogue/fusion/sm90_visitor_tma_warpspecialized.hpp,sha256=bYYaEh0VEC91h5FivSEetLJTZtWx3Bkf4Q3w1Bsls7k,39461
+cutlass_library/source/include/cutlass/epilogue/thread/activation.h,sha256=PTVNr1CIDKv5RR5VE-V_rDpHgB5ZRjF3N-O-f_-N0C8,17909
 cutlass_library/source/include/cutlass/epilogue/thread/conversion_op.h,sha256=LvoPmHMlGZKFcqwaQM3oP74jgUwtr022My3j7m9T-Lo,4691
 cutlass_library/source/include/cutlass/epilogue/thread/detail.hpp,sha256=YXeU0e_KVXvRdZ9mCyqkpJv0txz5dErGO21CuV8ztH0,2281
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination.h,sha256=928ggzxPhCby_0SqDCsBcbrFDgaLYhEPzDLnl2zW7kM,18989
-cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h,sha256=_oyLBvONjqVRfhLEpJ1g1EUws7lzTx_Nn9XrNjxmBDo,9323
+cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h,sha256=NS4Otd8RMedCfwkxCUN6PryCiHwELk4hQiTx-5Nolmg,13180
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_bias_relu.h,sha256=LRLnoRWOzl12N8AF37332tFW4iqjSnefqlLXD7E0Cmc,18161
-cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_clamp.h,sha256=JdzdH1GZU1jesrS_ZdtDI9Q0pvjTpVh25KlQ8qaW3I0,23752
+cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_clamp.h,sha256=dyJQ7aic7n_HFCZFNNkMca9V43pWGT3LjMqxh8UWaiU,23370
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_dgelu.h,sha256=pUu4b4Xt_vSIkZFTZUw56R15z8UFFPNHX9UhnKQOso8,9066
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_drelu.h,sha256=KTR_U3pnFfrHXyycGzlhhOeNiI5_QSECLbuF3m747GE,15195
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_gelu.h,sha256=RBRiPVq4_vVdAw4XNa5raEskxhYxJkDHsxwHHomUhgc,3669
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_generic.h,sha256=6XkO24cyko8WEj9ALRmEqGwWAB4Mgaa5dhDXTObdxrk,10056
+cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_generic_with_scaling.h,sha256=7eEvRSm3vud40po9UFTtv-gRt_R_-3tWpFkIgOaZUf8,13185
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_hardswish.h,sha256=dKCrA6lKVmKvrKSsHf499U9cG6QZTCAEmEsY-jDmKLg,3693
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h,sha256=eNPOmrMCO7eG-9-82wADVXJXsnN69C4p_N8ttYEAwRk,8399
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_params.h,sha256=ZVNpq-g2P2UfE3llmeTlHbzvLttcr3OJHEecV5AdEII,3046
-cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_planar_complex.h,sha256=uWHH-DtudOJlg6PRGlZAt9TTRIn88q6Fw4Fo4Rym_AQ,9351
+cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_planar_complex.h,sha256=o6ZKktaVdblWr6gWzVy679s9pu2z6cAhhzLAMQS9Bww,9278
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_relu.h,sha256=J9qsuPDJIPJYgR1yL7aCsKBpCh8G809qm_o03pQiuOQ,20596
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_relu0.h,sha256=ABZk2znukkHsDRB0XtAFacqPNVfhmdvgdU9VWPtHV5w,19458
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_residual_block.h,sha256=uYT_G-kHdhxqBJqJpuLvER2MhQwYcaHmBGYNYjxPbfw,11995
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_sigmoid.h,sha256=5K9mCXGOEJewa6KsnhkkIUIyHEnO9TYPWHNXJut50HI,3688
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_silu.h,sha256=QswbI5_D-ReGg6-SBm1X7cX9vYKTO76zImuVO_ZEfyo,3669
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_tensor_broadcast.hpp,sha256=dIMkPU_h5kwtOhsrK5r8kux8JO2cvCnw2IqIPPtnHgI,9786
 cutlass_library/source/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h,sha256=WRjpQ2MSaobVGjxYgTye1hkNabMmM1QFqyl2F7NYlPM,8662
 cutlass_library/source/include/cutlass/epilogue/thread/reduction_op.h,sha256=4oUrJ8AsgZf83FcgdVsA6an89UMAiErH0_Pyl_cyWEg,3416
 cutlass_library/source/include/cutlass/epilogue/thread/scale_type.h,sha256=3mfuLqcDCWU1QkSPDcVbjkY7VYzhf9pP_3BZYSWcMwg,3048
 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h,sha256=cIn3bm01mGvVvh7pJegbEBlkBj9opJK5bpmhemMRrc4,9142
 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h,sha256=WY38SoGfteUxclQUE-LGyydeOv7wXN4OQvMTZkmHWMM,9441
 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h,sha256=o2ONzBmlgT_0sLmQsKve4UjNXZxXt1qslzwzPzzPgSU,3234
 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h,sha256=4jDRzDkd_r5dHcno86eUY-K4iEpeAAIImFkKJhvWPjc,7209
-cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_simt.h,sha256=t6GyVrnQvpdYWLA1F7Yt4l0YMmnnHyXrhsm_thVdkGM,13385
-cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h,sha256=Ezws8jQFMoF4UieDexbd1H1YTGtWYNPZRW1X4xgB-3k,28290
-cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h,sha256=yDixqskc3GzP-qXNZ6QM3jfh0Ywp0ic2LyGfQPVogqo,7129
+cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_simt.h,sha256=OY9O68y8kbWmjyOfh34o7XvQlQOwQOWqMiUha_nRYuQ,14255
+cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h,sha256=P34natcxjZOlagafJPj2OgWDBoKQpK0PwRAr2vLgVJ0,29301
+cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h,sha256=iNFbLxHUZi_Pfle-OAIuCV9KpblecKdGcyFGO9lUvS8,7134
 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h,sha256=oTy0Tdo53vCYmo81hKTqjqyDLrBMhjZHE5xl9hxxGco,10846
-cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h,sha256=jPj5izk0a1H7ci6yU5WYXeXFr2y6iyaSRNePiTKnptk,7424
+cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_with_absmax.h,sha256=KST1XmWU4IYAN7_H1v3wQ9Mrm1NvghKyoN2C4QsTSIM,4264
+cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h,sha256=m7bhbpy_8iXqDzz-f6Tt6pB8_icdCT_v9-gyjT90lzE,10301
 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h,sha256=wPKAbZm__6QJd19x9x6WazwAcVDMHsWOFaYkrwPpHso,5763
 cutlass_library/source/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h,sha256=t8LJz9LROvaA94fIpZxvlFPczUVw3SXckdAkj6XYTTA,5947
 cutlass_library/source/include/cutlass/epilogue/threadblock/default_thread_map_simt.h,sha256=_sWBFGXo9xVw5ovwIv45HmKN7bMTaDmpff_GkfljwbA,4409
 cutlass_library/source/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h,sha256=SgnkWO2NXYXVGTd6d7l6S8svZavsEp1Pc4ix-GWCbWM,7398
 cutlass_library/source/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h,sha256=nYJxvAc8pFqqJ292igxVcf--m2_JG_bIwrhZZm9Oxso,7303
 cutlass_library/source/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h,sha256=mXYCtQwefkslxD-gQ6N1lmjMMc5A7cdw0-F9kvxmaqo,4098
 cutlass_library/source/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h,sha256=qR8qLQsCo9p3O4LGqh54J6He4hXSnFxabhQEseONTJI,4678
@@ -624,189 +656,197 @@
 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_depthwise.h,sha256=dSWcSJMUpzMawfhl5eEqVBomIcMUjSy9Mh2gTMuO6D0,13424
 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_direct_store.h,sha256=HYHnXzjQNcBSv1BuIyzCKh-WeZpA91d5FmOX5KeBFqo,13933
 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h,sha256=NRJThxvzdfh1OyGXyUWwgImCwtYh4YDRWy5Q4D0P5Ew,7401
 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h,sha256=VjsvpW9tLYGdCxVJP1wPZ-r72n9zzq1C8Hf2jSxpRYU,14610
 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h,sha256=0pxopqMsVBgB7WHzeDbQWF4PyZL7lMiEUgYxS819SDc,9073
 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_streamk_with_broadcast.h,sha256=n-sUgmtcTjGmAC7cGd-srcOsH_nN7ImXq_hRUPZmq1M,15321
 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h,sha256=BSqXFZCpP58ET1m8LcIIIE-zwnd7hWiXUT8ktaeK8lU,16804
+cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_absmax.h,sha256=lzraTHrYs5ElOV7kjzGMHN_Dox2FmiKV4QSCD4dAp3I,34018
 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h,sha256=-O6z9hQEZSVD9jAGHQ8uBxgAfvjpcvJ00n-XkUGEJ-M,58727
 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h,sha256=ig_4T7h0LQtW1BUEhXGDmV0VyD8M07qlVc6VSkxpkpQ,29199
 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h,sha256=pTwhO8D8o-C1sW5WVuuMt6QYTLJm7Pr79KDT4RBqWMI,13454
 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_with_visitor_callbacks.h,sha256=ZccK776fv-W-ZzI7i4apMzs_7r5i3_yvpWit9u26j_8,17169
 cutlass_library/source/include/cutlass/epilogue/threadblock/epilogue_workspace.h,sha256=GFKx2m9day6bWrEhNs4jNE1JSNRNTHbXx-HsQ1lXzxk,7308
 cutlass_library/source/include/cutlass/epilogue/threadblock/interleaved_epilogue.h,sha256=wXVMjicqqTtKaf5a2TFOhSbieRqIbAI8GRfwh-5VcM0,14359
-cutlass_library/source/include/cutlass/epilogue/threadblock/output_iterator_parameter.h,sha256=F8e2fkMWXZPnjHcTiZcTOeqf_v4qrmr910sF9PA_6ik,4741
+cutlass_library/source/include/cutlass/epilogue/threadblock/output_iterator_parameter.h,sha256=4xTcrAm8sUUquZg1Q_qZlOYG4vXcN2mnpD_Plcr7lZ4,6881
 cutlass_library/source/include/cutlass/epilogue/threadblock/output_tile_thread_map.h,sha256=AUYTEcZD0aCLfXiQIjYcGkKSQFUhphbg1wNvaasGv1A,19842
-cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h,sha256=lGNgD1VR5i6E0CVFZVAlMth9hRyj01lO4_Je_xYf7ds,40972
+cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h,sha256=LGvaKQy2pE8rYlNiNrkAwuWzGP3LCp56PkBLGD6lhY0,41699
 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h,sha256=m33q94rp2Q-8kYBCRVyOGEhirwhemrrg7W3l--hnioQ,18821
 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h,sha256=Y6V5q40I8ee2b75b_fILeH2l604IZymJY-QPU7PK8sk,5636
 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h,sha256=2egttxLW0aVd3EPwHHRJLXtj2XRE2mjY2jFE31Hacn4,21249
+cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_conv.h,sha256=BfUxADo7DW0kQXz5-3nEzPqSKMLvUyhKlRQHwNYaObQ,17563
 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h,sha256=F6HDcZMIwfinzQyi1nNy6F1MFBD-YWjz2EIFCtJU1EI,13873
-cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h,sha256=Oh9fGzfHKtc_d5CCPLslS4sUcbVC1pfr5UFkajn53-M,15397
+cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h,sha256=E9kbACg5AOgKJPQpwBJ8Yy7jMUz328ZzOOXAO07TWEk,14985
 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h,sha256=0_CT0SmXMAqCvCOGSMr1Vx4jI_gqICzHMABrdIEafjk,9146
 cutlass_library/source/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h,sha256=lw0P10zldsRIVRKIynbsfiJILM6Wf6ErPPXreioZ1t0,15534
 cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator.h,sha256=LZ5II30uU_5Ppt-lxGdnFBbZhpo3mEJxvuFIykf2v4Q,7487
 cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h,sha256=8eXaZuQfnQvwps8tDZ8mTG38leGPvwrc9a9sOe4fzPM,18100
-cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h,sha256=1-4_V2fDqsfS4ErNj2cAnxqugB5UIaOBUEDtwNrVluY,7394
-cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_2x.hpp,sha256=fJSS03gz3N7WlbYTVb_vWz-6zulZNeX8fdCcm_6YPHI,14544
+cutlass_library/source/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_linear.h,sha256=212gfVHAbpfMztazJZAjjf4eDJoCEWcu6TtEPpFcRrQ,7396
+cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_2x.hpp,sha256=pxxNUXweNcghTwYb9FegUZdKeyt-5kkpWIqffnlOgHE,14524
 cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_compute.hpp,sha256=WytKj_7GGqFivimrDqvk3qAn_qB8Pqqwnyq7J84OExY,4387
-cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_load.hpp,sha256=5q5AeL7ordR6BLYOLLIiJp4m2lbDmZTz1QBOjr3094w,17161
-cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_store.hpp,sha256=-kGH-o6XTSHkbsoiPm9Zy3udrjCPyY7ZpvQhbVXDZhk,24942
+cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_load.hpp,sha256=tSqM0vfhbq8v70yyJDAJU6Yq7GlEKjkrFyCdF2mgGek,17753
+cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitor_store.hpp,sha256=75o_b0967BF5dfQePODQthSvisazuCB6s9bnX78QbkQ,25534
 cutlass_library/source/include/cutlass/epilogue/threadblock/fusion/visitors.hpp,sha256=Xqbpfnk9MJf3P9bUPohxBI9-QEtsZOvKyO_9yINvLC0,2171
 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h,sha256=7RX9yWYLXzeh1PNY3M87Epfd8BsndUgTk5Pd82dvxWM,7055
 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h,sha256=0hnJilOMuvfojlsiGIyBz-TPTcBtKi8HRnAwzjPcgNw,7736
 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_simt.h,sha256=zOJhOGmt2aYJCGFyPXoS2uogOEvrhrGypBVFOJXkS-c,5880
 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h,sha256=bG_nTw8wtkX7HmmG3y1--wA5f8f53Z5yzgxzaVPeD5s,9883
 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h,sha256=CHC3M-Vg9uYAS8a2SJjsgKrnB26Qq2bBUbs7L3a31PM,8924
 cutlass_library/source/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h,sha256=SIgS0vyw62sesJgRdS3YWWoK__XV-U20Ghqzvw5t1w4,6045
 cutlass_library/source/include/cutlass/epilogue/warp/simt_policy.h,sha256=eJfkxpA_XNkTdVEabuIRiaXk9Ovy8iLpVBQ4j1fSE_Y,4864
 cutlass_library/source/include/cutlass/epilogue/warp/tensor_op_policy.h,sha256=vbuvJXqcK1LsnixHEcpDxQ8hoFCOmoR2ev3QFmnBa9Q,5979
 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_simt.h,sha256=gB1mgP9cU5MAvAyLiIhFwWuBZHWxsxZEyDJbJsUJxXg,25658
 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h,sha256=20cUJFGkawYp3U4UEvai9AYaK3Dfy1gWsNpSFkkw154,20290
-cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h,sha256=cy7K49fWY7XkzqbkiNZya44njXpciTpok3JSEoSEwiI,22921
+cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h,sha256=4xp3xl0To5w5nIyPERP69pKlDRMhRDuqMLsImWDGBnM,33300
 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h,sha256=6ISpnXfU4Tweapy9Q9sjJf4WjrWrd4pVDeyY2HZEmxU,14250
 cutlass_library/source/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h,sha256=8e_KVtOoGh-JuJAdcmd0n6iAqPG1aXU2Zyt8C6sWJsg,7704
 cutlass_library/source/include/cutlass/epilogue/warp/volta_tensor_op_policy.h,sha256=jr-EfbzSXPR9G3YbzKob8mG68rJE1iomQ6qwfy65I3Y,7485
 cutlass_library/source/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h,sha256=TvCA8NPvcPCo7yo0-T81pX7up4eUgcXO3yoGyA6K81A,3916
-cutlass_library/source/include/cutlass/gemm/dispatch_policy.hpp,sha256=nxHnwafvy1o-5g4CVvW6NNv3mH-dGk5X5IPO6YZiIL4,9823
+cutlass_library/source/include/cutlass/gemm/dispatch_policy.hpp,sha256=HkgKg4TEVZIuAQLwnD5Sy_mKYTUaIed1bQp-VZnLkvw,10458
 cutlass_library/source/include/cutlass/gemm/gemm.h,sha256=55JkA2yyc9kszAgrJ_sMeTs0YI-H85Ia_aoUbdvDCTA,4630
-cutlass_library/source/include/cutlass/gemm/gemm_enumerated_types.h,sha256=CSwZDOJK2YQAslYzlIV2u_UqbGrP9MnN2MO7UsVFbmU,3568
+cutlass_library/source/include/cutlass/gemm/gemm_enumerated_types.h,sha256=briRzA2Qv3RGN_3RHX72bjS_RnMbNp7wkjPOW_nxW9A,3174
 cutlass_library/source/include/cutlass/gemm/group_array_problem_shape.hpp,sha256=bKGHxsf4qJDRERgOgwf_w7KY5xrrpA5eS6lxYj4GKCo,4265
-cutlass_library/source/include/cutlass/gemm/collective/collective_builder.hpp,sha256=CuFZutNaeB65-99rduP8GzpGPXNEFiAfHmBcNGkkukU,3589
+cutlass_library/source/include/cutlass/gemm/collective/collective_builder.hpp,sha256=DwhCyn7MC1l5fjtQUTeOU42ExAu_ZyaegdZC8VMVj8o,3771
 cutlass_library/source/include/cutlass/gemm/collective/collective_mma.hpp,sha256=pDFza1Hfr0ezp2RRxpiCyK7sGTRnVsHeHgBe1fzgWic,3668
 cutlass_library/source/include/cutlass/gemm/collective/fp8_accumulation.hpp,sha256=f9aCwWbfC_xBYCPYq6hb0-oduUxI-mufOJNnp7YGt4k,4768
 cutlass_library/source/include/cutlass/gemm/collective/sm70_mma_twostage.hpp,sha256=GxrVRjPBV29xgeLVHhTIy6eckLehz39sN-FxV_m5PR8,22247
-cutlass_library/source/include/cutlass/gemm/collective/sm80_mma_multistage.hpp,sha256=cYKZVxvVEv_4b4SjDPPC8K-3ve2vhvdhG5OlbjR2I8Q,27570
-cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_array_tma_gmma_ss_warpspecialized.hpp,sha256=xQffhEvvKm79gLgLHDLNtoifLPzZZZJbB41G94az2x4,33101
-cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_multistage_gmma_rs_warpspecialized.hpp,sha256=qXkmwQLKTwmuVbOF4RGsXD0vhE4Mhez1nAzPMJprzpg,28282
-cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_multistage_gmma_ss_warpspecialized.hpp,sha256=-prdbO3OuhhkEUgwmvYTMGt4IsNEthqQXOIq9Cm4xVs,19361
-cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized.hpp,sha256=pi5maf-n5l7f4_ol_9sM9-t6mg3PU479SKN3EqRyl4s,33171
-cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized_mixed_input.hpp,sha256=iaIw5rBxT0KDvFZ7QPCHsNabiDTHQESwvDbc_WXxGho,66973
-cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss.hpp,sha256=ykNocq32GhehhKqD7PokS0q0-qYRQ6YvHqIVP9dC9Gk,22389
-cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp,sha256=6Z9415cqTaLgvwwSK5eydIcyYfauVc_SE1PvNjQOlkc,23844
-cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8.hpp,sha256=CL68GfWSZ-joP5GOUMYbjv0sVzio4FL7oMO_REjN9Bg,23982
-cutlass_library/source/include/cutlass/gemm/collective/builders/sm90_common.inl,sha256=lqcip_vbJA9eG6CYc7jh-VF96olFdgv-h6RSvcHXr9g,15283
-cutlass_library/source/include/cutlass/gemm/collective/builders/sm90_gmma_builder.inl,sha256=A9PD2t7BwRcHXYgLw-JmJZ-22ZF5dmVDmGzlDPuFJMo,43430
+cutlass_library/source/include/cutlass/gemm/collective/sm80_mma_multistage.hpp,sha256=VoM2sScMsRoxkv2ulGOzZu8e_BIyDe2PrNL5YqB1fJw,27955
+cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_array_tma_gmma_ss_warpspecialized.hpp,sha256=cK3FN7jzeLe-LdcP6KHXddcIArza956IctW0upID8ko,33255
+cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_multistage_gmma_rs_warpspecialized.hpp,sha256=u5QRezL3Sb02oyYexGX2ohsWVkuc5UHSkGHCNMofjNA,28408
+cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_multistage_gmma_ss_warpspecialized.hpp,sha256=HZouRQZuUmDiaSE82S4u2kFOmtVBJwiUB3U8lu7tWVQ,19434
+cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized.hpp,sha256=D_sUOxyyGxrfyuatVVTbf2jlDfw06_Ls-kQmVcqQZgQ,33465
+cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized_mixed_input.hpp,sha256=uHrKBnySY_cGQFclN395tNb76eGrGYY_NFfSzqbO4BI,63323
+cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss.hpp,sha256=dcY8IC0Xy1Ps8KB47koi3PZf-YoWOIcsz-M5-0pAcOw,22544
+cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp,sha256=ULatu3wSNzzNR7F9unNq4n0il-g6zGYzoTHZ2boxchA,24000
+cutlass_library/source/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8.hpp,sha256=j041qbITZxqLVBH7a0FSd_eyxf6L6tJJ58n_i7j9OJo,24138
+cutlass_library/source/include/cutlass/gemm/collective/builders/sm90_common.inl,sha256=OhhwDlgwuVNg47djpg1mAJ2HY-ocGdNXDf1_Hz1qDKU,15520
+cutlass_library/source/include/cutlass/gemm/collective/builders/sm90_gmma_builder.inl,sha256=cKdAuWQAuTKbKlpNl3s5c5tCfcr_PxaR-8xHJRc2T5w,43793
 cutlass_library/source/include/cutlass/gemm/device/base_grouped.h,sha256=im8pgmwwKmAKrtlh_swObQ1vYUM5VobbAqzSLC5ApOg,16881
-cutlass_library/source/include/cutlass/gemm/device/default_gemm_configuration.h,sha256=aQ3GMCguGAaPqNd8k5AQno3qWR1UtWNsHAWNhrT-Ma0,24262
-cutlass_library/source/include/cutlass/gemm/device/ell_gemm.h,sha256=qOtbT8qbnqv1E9mas4pYmC5uxYMSACXzucEJ0_oNAkQ,27616
+cutlass_library/source/include/cutlass/gemm/device/default_gemm_configuration.h,sha256=ue9poD1kcuCN7ZJtSKo7UXOUcTJ1175Ttn9aRwpRFu0,27521
+cutlass_library/source/include/cutlass/gemm/device/ell_gemm.h,sha256=JirQE01JyPyWOU_4N2WsANwjvZRLM7jrbL7bFTqcVTg,27618
 cutlass_library/source/include/cutlass/gemm/device/gemm.h,sha256=yK4xLvhuuePaMEST_UT51DpeLWmZ3Y3in-Wuei8bxtE,25202
 cutlass_library/source/include/cutlass/gemm/device/gemm_array.h,sha256=GyX-iZMO25nZqUeEAROmUbphVa3kjJUf4PHlJrs-TmA,22367
 cutlass_library/source/include/cutlass/gemm/device/gemm_batched.h,sha256=iiksodutYPbjO2iet5vW-_q5WtI9LL__OooAI_Vz2GU,22375
 cutlass_library/source/include/cutlass/gemm/device/gemm_complex.h,sha256=uDQ7BHy0SxxiSsHukePTn28weDNlm0GsOchiDHMgrBA,22725
 cutlass_library/source/include/cutlass/gemm/device/gemm_grouped.h,sha256=YyymwZdyNmyb4NXRXygKo5DtbjMTZXfUUackRCXYaO0,2591
 cutlass_library/source/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h,sha256=fY1jAaIRmN-CN9lJ1VMM6NEcf6WfTyCNZZBcLe1jMEA,13736
 cutlass_library/source/include/cutlass/gemm/device/gemm_sparse.h,sha256=nDst85R8UGyDQEF3j1qyFTAyDdNBzk1AV6yv14f1l94,17329
+cutlass_library/source/include/cutlass/gemm/device/gemm_sparse_with_absmax.h,sha256=0TgsFXmN1-7mlftjiQBLL3qQTkr3V5l9Ippyktz3UMw,12258
 cutlass_library/source/include/cutlass/gemm/device/gemm_sparse_with_visitor.h,sha256=66zZTyt_U2uHCVvawwRIFajVPIBJCOzDo2pb_X7Sq4A,11362
 cutlass_library/source/include/cutlass/gemm/device/gemm_splitk_parallel.h,sha256=7mqFtSLVIFpT7JQNWXPQ30Aqw8i61iBFfpzTWCHuALg,20436
-cutlass_library/source/include/cutlass/gemm/device/gemm_universal.h,sha256=VUAc2B89ICDjNf4FAvUhSDvZcJOxT9HYvx3TzchyRxg,15600
-cutlass_library/source/include/cutlass/gemm/device/gemm_universal_adapter.h,sha256=vlKmG2_zft4FEN83NMmEfavAK_oxvlBj9z17wheQa8k,23658
+cutlass_library/source/include/cutlass/gemm/device/gemm_universal.h,sha256=C0v5vZybBGgeCfCrf-j-j3m7gZQ9j5CP7dD5wyVFH2M,15630
+cutlass_library/source/include/cutlass/gemm/device/gemm_universal_adapter.h,sha256=uYcvvmkqKvSZhwEsP9bAOiprAjBQ3D2e96rau6QQlxo,24236
 cutlass_library/source/include/cutlass/gemm/device/gemm_universal_base.h,sha256=a_sW1E1ao0qZQH7CSfBIEGABttyiRPocNZzr62SlZ1E,15624
 cutlass_library/source/include/cutlass/gemm/device/gemm_universal_streamk_with_broadcast.h,sha256=S00wfhH82tcz5rxgPEjN8S10Y3a-8MDXvuWsy3t2_gg,14027
+cutlass_library/source/include/cutlass/gemm/device/gemm_universal_with_absmax.h,sha256=iDh37WLKz4HeVY7BGXWLZgpxPxiEOpK7-sR9owHJgP0,13785
 cutlass_library/source/include/cutlass/gemm/device/gemm_universal_with_broadcast.h,sha256=rXjDZOd1MebdfnUjxMYRwQz6FNCqSWLRZJf3Uk412GE,13968
 cutlass_library/source/include/cutlass/gemm/device/gemm_with_k_reduction.h,sha256=Ztgm0hLxI8cWbmSfjXgaqGvtzZKnEU5JNN88LMnefbM,14853
 cutlass_library/source/include/cutlass/gemm/device/gemv.h,sha256=jQgomrvI0vPux1POTH4dlM1Nkjm2WPqVRZklLow3rFA,5961
 cutlass_library/source/include/cutlass/gemm/device/rank_2k.h,sha256=16uBd59m89md7CR0bjCgC6z6GHPMcqPk4QP94fadl3s,18127
 cutlass_library/source/include/cutlass/gemm/device/rank_2k_grouped.h,sha256=8p83T6HExJuQf6RwrkRVmD94_3v7deMKcLJE60HQLu8,2747
 cutlass_library/source/include/cutlass/gemm/device/rank_k.h,sha256=bMgdxJ3Sp0rW4XzMAKRhHDOQDDkoTzVgRq5VhlSvc7I,16719
 cutlass_library/source/include/cutlass/gemm/device/symm.h,sha256=GH1ZwUUXK7EXBWnVVrGSUDeRUGSwytqcne4dE2oW5Cc,21050
 cutlass_library/source/include/cutlass/gemm/device/trmm.h,sha256=q93hcXlHySrGymnLLSE9dXBChUBsq7DDXCQ_W0_5MtA,26464
 cutlass_library/source/include/cutlass/gemm/kernel/default_ell_gemm.h,sha256=OyyTQsULWdNTN393v0nkH1uO9h0iXaIbuXqRGEi_E_o,29360
-cutlass_library/source/include/cutlass/gemm/kernel/default_gemm.h,sha256=sXJoluf6OLjgNsDmST0dTNl2IbuyuJPXHcholQiUolI,39181
+cutlass_library/source/include/cutlass/gemm/kernel/default_gemm.h,sha256=mNwxykMDOnyMnqHyL4kv7dUYhdP-exrK4H8AGPRstlI,42401
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_complex.h,sha256=nfcEEPHVXBQ9wywiZJo8gGxo6ix_b9Eo3q2Si2X1CMI,16130
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_grouped.h,sha256=S9zcVEk74rhdIFtsUhhfJuhGRg1c6YWmZqrix_nJXxU,12385
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h,sha256=9G-ZF9fyN_kiN7i88TFP7GedgW4kw_KoJaSlrs4NkfY,6592
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h,sha256=7h74kA5ym7oPK6ZXEOgNJUVSxhiKhsBT0SAex-5N8eA,5848
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h,sha256=JNr5L1mOXSw_Yu61vHi3mIUiyNlV2QLI77lo9VgJ76o,11104
-cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_sparse.h,sha256=eHIcnaifGBNVq6wBThLrlz5pOZwyrDw5ANYIiyOnKjY,7983
+cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_sparse.h,sha256=TF7Z-oByuO1XIeg_QM4Kzn_ASATzOUYMsXvdG1BzTN4,10526
+cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_sparse_with_absmax.h,sha256=nAo6ZWIH9LBz90yhsxLz-tz_zz7IpV28UiC0OM_yCuk,6102
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_sparse_with_visitor.h,sha256=jt0VgvdEwYRR-Nn2hZeEzo6_zpzwJ5DMOhqWCp2HT0A,8175
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h,sha256=Xto7folc_Ecdgz3quA9v19i43OK0YMiVf06dLaoE5Wo,4932
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_streamk_with_broadcast.h,sha256=bDZUu0qinMHsAqt_cOMZ-ncVWESZffUcFR3yz5uR8LY,5446
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_universal.h,sha256=WNLXrifmkyqxMst9Mj0a-pFwkJ328SbkrPJ18NK7U7A,12332
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_universal_with_visitor.h,sha256=ByM9d7leT8Wln6EwqbGCnmPFA0ncwng4lWnXJoAMVck,5697
+cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_absmax.h,sha256=CxqRNXYBjtu14Wh8qUv5-fqJP0i7X04wUyinuG984I0,5115
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h,sha256=L_nVbnCw91B1dRoJ29p61BLFK8MC3cpus8YazgB8WHA,8123
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h,sha256=Bl8uiwTPFX_kcI5wBv_iqxn1BNkKdqZQ1fPLZLwShUQ,6457
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemm_with_reduction.h,sha256=W3BsHA_Yh3FSNJsY3baB8R0ig5Bp3RRNzKyQIHChS74,8084
 cutlass_library/source/include/cutlass/gemm/kernel/default_gemv.h,sha256=rJUpG-380ceK1GGDEUSBH1768qPpRbW02P42aj10n8A,5349
 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k.h,sha256=8RItQYlf4-IV_kCa6xWoWB-Ao8_dkXiz1qbY7ktRKEw,11560
 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k_complex.h,sha256=FgjoSZq83G-hsUpH1MC-X0Slo-vdpbDMEV13T4H4E80,20509
-cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k_grouped.h,sha256=C-JhKmJ5iRARgIUJ5rrPgPvp1MuS_Lk8YEsl5rZXyXY,12470
-cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k_universal.h,sha256=GyQBXiYc9fX7gsIAj0aavHO5rgYVr0ixWaSd6IGoH6k,10620
+cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k_grouped.h,sha256=60AkUGzPLthIP_2ia7r14TfvXb4aOzC9TVOjQ4VVKiE,12480
+cutlass_library/source/include/cutlass/gemm/kernel/default_rank_2k_universal.h,sha256=abs6M36FfSxn0nXBsWyTQRJgt7FXz_s_hm9RW6-36F8,10630
 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_k.h,sha256=uYrxFCcc43JO8XUKEUERTlrk19CYyPeF3azlmXAGRjY,9872
 cutlass_library/source/include/cutlass/gemm/kernel/default_rank_k_complex.h,sha256=xJwGIblFgupK3YJa7tRiF3u7RlBlIZ8cPhZFxtVYmSg,16990
-cutlass_library/source/include/cutlass/gemm/kernel/default_rank_k_universal.h,sha256=aMK4TreXPyzsotLE4dc9LhQiENJZkiCaD5AI-ARB2tI,9444
+cutlass_library/source/include/cutlass/gemm/kernel/default_rank_k_universal.h,sha256=0BW2IjY-RmxWRJUyvGiYeX1wcJsCqHAuU8IaJvBTEKM,9454
 cutlass_library/source/include/cutlass/gemm/kernel/default_symm.h,sha256=GVLWm62smmZbadtjSZ7j3Wr8PHJfvoWJdxNSWgOyVkk,13375
 cutlass_library/source/include/cutlass/gemm/kernel/default_symm_complex.h,sha256=E8UwHSmlI6-0PE0nfK7Vo2uTaKPYfGRNM6Y4cZd9S7o,21830
-cutlass_library/source/include/cutlass/gemm/kernel/default_symm_universal.h,sha256=fjqu_K2iAhPnvMAOQ4FeXTKu6Xa6jSZKETO2BiTy1Qw,10315
+cutlass_library/source/include/cutlass/gemm/kernel/default_symm_universal.h,sha256=5-1Jcl5bgZiQupa4KgOlthM1jBN_rFW3iHeJtqI7yUM,10325
 cutlass_library/source/include/cutlass/gemm/kernel/default_trmm.h,sha256=VNHly2x2zNO6dLuOyF1xpThCgMgsRtTrXyOsPUnFmeE,10873
 cutlass_library/source/include/cutlass/gemm/kernel/default_trmm_complex.h,sha256=UIkMqlDJeP_a6acRHdx6X7Y4erOo-uve8zJuOqn9CnY,10730
-cutlass_library/source/include/cutlass/gemm/kernel/default_trmm_universal.h,sha256=edy9gRGGEamsDcoAW-rZT45MaP_2w1sVGp9YU2H9cJA,10850
-cutlass_library/source/include/cutlass/gemm/kernel/ell_gemm.h,sha256=9XuQY-IoUyHnnnXm_f8-uTfV1gGcv3fgXBsj_cd54Fo,28916
+cutlass_library/source/include/cutlass/gemm/kernel/default_trmm_universal.h,sha256=_nUh9KlpbfwGl8qMnbQcCwTCn8ZAYwHC-T15Ppd-BCg,10860
+cutlass_library/source/include/cutlass/gemm/kernel/ell_gemm.h,sha256=3fufT7GphyuE_fkSoGo_0Osv2UBOls3gPqdK3DKwYxs,28837
 cutlass_library/source/include/cutlass/gemm/kernel/gemm.h,sha256=0FfEX62rk7BEJvfJglLeoRiE0bwwR6rkvLBHXlDl5Mc,13362
 cutlass_library/source/include/cutlass/gemm/kernel/gemm_array.h,sha256=p-c_gg5ed5bJ-PI3cZeBzDkJQI4f9svQX9uzT2IFfkA,8698
-cutlass_library/source/include/cutlass/gemm/kernel/gemm_batched.h,sha256=924xAPYykid7vr8qFt7lFL0xqOWliqVF5xG1lWr5Yy0,8766
-cutlass_library/source/include/cutlass/gemm/kernel/gemm_grouped.h,sha256=aNehu7kB8vsX2XTsx4rKZPHiazVySz1aYVHoRp-1YGs,14692
+cutlass_library/source/include/cutlass/gemm/kernel/gemm_batched.h,sha256=xAtIZ09ZgHNmQtHxzFJg8T0LN0aN8lmtxWN4movHuPA,8746
+cutlass_library/source/include/cutlass/gemm/kernel/gemm_grouped.h,sha256=E2vR_MaGTf8NNadwym9uQYtuNMjlXArniY_SwLRNCM4,14394
 cutlass_library/source/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h,sha256=Os_pjZJ2DU95HSI9TpnQjIhrp5OgndhPJN0X-PDuvXc,4690
-cutlass_library/source/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h,sha256=4PH76Q0LHEeS8TynTBVAyYqQx0we-l5j3Dsc910oRNA,15623
-cutlass_library/source/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h,sha256=egOlKhLjlRI_sakz-gCsqUNWNPuIu7ClTMN7czyX8Sw,27663
-cutlass_library/source/include/cutlass/gemm/kernel/gemm_params.h,sha256=0AuZSd_4mm5XuBQiI5eTkP8-2-f745xr9Nh8X-mUeZc,6144
+cutlass_library/source/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h,sha256=HrwB85lo3Kn0QP7-cosP6jTJiDA24E5KMmGv39xmU5M,15268
+cutlass_library/source/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h,sha256=hsuS-RrjmHxIrlcF7_9E4dlIbyW33VcuyOWP1XXzPHo,27550
+cutlass_library/source/include/cutlass/gemm/kernel/gemm_params.h,sha256=IEODEO6upgdakoZgKmzoXCx-cPSOQ1Q7UV215sBc9qY,5979
 cutlass_library/source/include/cutlass/gemm/kernel/gemm_pipelined.h,sha256=dccLUZU7eCr79q9fj2F11TFQkNuZAuXmrTkK7ealdsU,5150
-cutlass_library/source/include/cutlass/gemm/kernel/gemm_planar_complex.h,sha256=lms_Ba-TPWOZUXDLO8vQAP4Or4i6UAuQ65zlshT2iP4,23352
-cutlass_library/source/include/cutlass/gemm/kernel/gemm_planar_complex_array.h,sha256=wGsmjgk91R-2yXeZwAcaqI_B8UL-A6_XEPGmz2PyhFs,18941
+cutlass_library/source/include/cutlass/gemm/kernel/gemm_planar_complex.h,sha256=hh9XqPte20Eba0OVGdh-jEF6oDGDwLGUCyCHDY-DHqc,23348
+cutlass_library/source/include/cutlass/gemm/kernel/gemm_planar_complex_array.h,sha256=PZ9w0KyuzsiPG_yNVYpQXHmXXNP5ohA1IXbFlN86RG8,18886
 cutlass_library/source/include/cutlass/gemm/kernel/gemm_splitk_parallel.h,sha256=tfcgrdTUAO4Tasz_HPoEzZbQKw26FZDZ6Udo0teNFTw,8142
-cutlass_library/source/include/cutlass/gemm/kernel/gemm_streamk_with_fused_epilogue.h,sha256=yTdNFPNB6pC76vRVdb1k6WFo__UxYYna_pQkzabxHRI,80099
+cutlass_library/source/include/cutlass/gemm/kernel/gemm_streamk_with_fused_epilogue.h,sha256=OgPbpku-VPa5YM4HlNrnBUHP--gXqVmJUrgfi57AGT4,80158
 cutlass_library/source/include/cutlass/gemm/kernel/gemm_transpose_operands.h,sha256=3sOChDuHZ-WS112cVZyZr3z_qfV-rkaRuuwKhR9jZEM,4291
-cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal.h,sha256=Yo8hL1DFT9aVSYbP_xLK2BalOBxUWvX6HRLhSDh_NKY,23597
+cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal.h,sha256=7QTnYxU-dmoKsH9q1dhS1R9_R9v-TclAPDCQ0Bwtc98,23549
 cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal.hpp,sha256=B4vozc7KIL3qfkwJVvNMTGIwKSpZCnaaZ9Q153p_duM,4374
-cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_streamk.h,sha256=0oaC0LcUBVr1jpKvRpAwfQ06daYNqxyOS1iSPhMZl9g,39277
+cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_streamk.h,sha256=IZztRz83sNT1zPDmbwvcDrZSXG7aDBZGMvQE3dpPN9k,39338
 cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_with_visitor.h,sha256=hHW9xnCwQd1FMbb8U3kAORp2iZGobJUP5o_ecF8sDco,10423
-cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_with_visitor_streamk.h,sha256=hlnWoii1coz4rdghuHG5ZcR6JqcqvGaHp9sE_yo22mU,28721
-cutlass_library/source/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h,sha256=cpz9YxboFpGZxu2Mh2H0d3eJqY-ekahOsRDVwOMe8z0,48041
+cutlass_library/source/include/cutlass/gemm/kernel/gemm_universal_with_visitor_streamk.h,sha256=U_sop8iWZbeHGN_jIg5vswiSEb7M_f_8cAxOpiwq0Mo,28827
+cutlass_library/source/include/cutlass/gemm/kernel/gemm_with_absmax.h,sha256=I8jGmGwIHwx3PghHzfJtcxCx5na_5i4gcUY3BrnOGD0,24030
+cutlass_library/source/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h,sha256=3GLOXEcPzokmDWfAgXtK6nQ0BEDoLgWB5tRlbNZsIpA,47839
 cutlass_library/source/include/cutlass/gemm/kernel/gemm_with_k_reduction.h,sha256=BAn1oflPCehqtgIUH6qmOyoXE7qKi_9D1w9ZirB5QTw,23866
 cutlass_library/source/include/cutlass/gemm/kernel/gemv.h,sha256=HwqbHkACifYe7YmJdsfavPOVcw4or50kpqxDeZTSN3A,18393
 cutlass_library/source/include/cutlass/gemm/kernel/gemv_batched_strided.h,sha256=KMQN1duDd6CchpgWASw8WS11feuMB9c1X74sFD_rtiI,8954
 cutlass_library/source/include/cutlass/gemm/kernel/grouped_problem_visitor.h,sha256=DlHrs3yM9r_As2Prb0vha503DP3GJWl5nsvysdlSlGo,16765
-cutlass_library/source/include/cutlass/gemm/kernel/params_sparse_base.h,sha256=3fvDtUQOo4tD6b0X_w26tHIaDJ1XYH9NvglgbB0EW7k,3988
-cutlass_library/source/include/cutlass/gemm/kernel/params_universal_base.h,sha256=9XgZ3wO_tZQhzmpyhVpXM90WAFBTg12B99yph51NOXA,8404
-cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_grouped.h,sha256=9rjzzh7e8n5BlIXo7IycnR9EEkep3U2jc2kuT-_H_fY,23381
+cutlass_library/source/include/cutlass/gemm/kernel/params_sparse_base.h,sha256=54aYtdqsyaLP-qVTIz4P6gt9xJAuk4cTsBqiVXj_6OA,3934
+cutlass_library/source/include/cutlass/gemm/kernel/params_universal_base.h,sha256=F43tVjgr9m45f3xiXWLAgbNdlUbDU1XR8oRmTicSu40,8456
+cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_grouped.h,sha256=J6H0aVYMXFBm9KsxxPHuFizVGtJP8hRrd4ubR-y_hUM,23027
 cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h,sha256=eBsUYTC4nVcTjY_GvV5y0CzSiwaYBoZ7SGtG-dAZSts,16100
 cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h,sha256=wQWyUzCKlsOMbFmyoGFnC_1pMB9Bd0T50DBBQzI9PUs,4334
-cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_universal.h,sha256=SgOBw8GDhG34PQmdV22fiK3PyH4vFemTh338jsYTFcs,24584
-cutlass_library/source/include/cutlass/gemm/kernel/rank_k_universal.h,sha256=5ZphaLzcnsFg9YUH_cMpfGmU6SF20E5k6pQoHLiytYs,17989
-cutlass_library/source/include/cutlass/gemm/kernel/sm70_gemm.hpp,sha256=Iuj-s0AUMwFw62meLypar65VRrYwF86lXj-BvVZvEAU,11073
-cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_array_tma_warpspecialized_cooperative.hpp,sha256=G6HeYD4tWd7luPw7XKGdZn8azgeQFdYdu2_oiMdZHY8,35090
-cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma.hpp,sha256=lb7yc_7am_MbHtVzoX0B99ZjA8oRNoE_WxUU0-U5ka0,13122
-cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized.hpp,sha256=drNxcFQYQi0FwBOCKtQbuZSE15JOeKHd9biCHUqiHDs,18498
-cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_cooperative.hpp,sha256=voWmNyTszy3KiVnTLkr2__5dkvq0GW-hTuQBkwIiRVs,28936
-cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_pingpong.hpp,sha256=gweXm6muGSfVGDCRcFyNLVRmTvmhxWUJ9WIoRf0-g9Q,28129
-cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized.hpp,sha256=GJP8DdXdRTxJA4X6hIFjiDGDA85VvbvZLEDw8rjAcFk,18144
-cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized_cooperative.hpp,sha256=CQqrauTRcjQHCQAbDin-jt8RSH1wF23vrSq-w0_-dvY,22969
-cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized_pingpong.hpp,sha256=cX7WVTgNBKarEV0PHTMrhLVHNsUM3iSbdZ1X49TtaxY,23044
-cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler.hpp,sha256=qH0l0ti1bjQ0JZbpWmqIWZ0Vk9vH2xEVVR_tfrytL5M,4832
-cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler_group.hpp,sha256=iLKKB_SBXeABAsFlO9ShKmWRNKVCvYfv0keaU0ZNSKY,18485
-cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler_stream_k.hpp,sha256=9kYjXZaeLkImNWFg81qnABfTnD9LKEy4UvatVN-WjrM,39022
+cutlass_library/source/include/cutlass/gemm/kernel/rank_2k_universal.h,sha256=0t8qdfnytCwWpV4bcJqKxPcFUT43MTJ3kZPy-xeGHBc,24214
+cutlass_library/source/include/cutlass/gemm/kernel/rank_k_universal.h,sha256=gjowSMBZXrc6Tqja1IXmbnUWesVDgehzjDRXcNuXbCo,17597
+cutlass_library/source/include/cutlass/gemm/kernel/sm70_gemm.hpp,sha256=c0FTmMHyHqFlOFHG7lzh6uivdKEPaLpJE0as8rXL46Q,11132
+cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_array_tma_warpspecialized_cooperative.hpp,sha256=V3mT35ucIzIvo3na_W_KPHYuvprFui8hGPCpJJ_Xz5A,35170
+cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma.hpp,sha256=PddBZH1kE3wDVBpVjxKV6-Srm_TdWZH1rG_6RzBoucw,13178
+cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized.hpp,sha256=D3V1r1ZPbN7l3pBbCBMTk2ww-GP87-PiiIZ88Lk26aY,18554
+cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_cooperative.hpp,sha256=2FnxPOLhJPiiqjfFuVqy8hRSMPnUH8Y15ySEbkfYpeM,29017
+cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_pingpong.hpp,sha256=FTbftXXYYaiwe1OJtNNAhQKoAFcuSGEoerDfveQ9Ln0,28200
+cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized.hpp,sha256=_5TMPPBQ031U38Eugg2EtV1fFJAv5K9o7aGq0SBsGFY,18200
+cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized_cooperative.hpp,sha256=iLkI63ucnIQevnDYGT2zogBeuE4LeDrcBFCqN2UPA3g,23028
+cutlass_library/source/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized_pingpong.hpp,sha256=YR6VBDdLkni_mYS-LS3CKgPNZ83GoMltmceTrK5KEo0,23103
+cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler.hpp,sha256=U08eQNOVPdoqde5CiYo7kUQmq5oxgRP_AXA26vU8_mQ,5511
+cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler_group.hpp,sha256=1TUH2Kxjrh9EDAnT2rqDIdAF01DJwoYKbTSUP1v4naE,18488
+cutlass_library/source/include/cutlass/gemm/kernel/sm90_tile_scheduler_stream_k.hpp,sha256=M95ai0tw81RReiPg3a1aS1NCTKyIoZ4nhqotVVSAMUE,39150
 cutlass_library/source/include/cutlass/gemm/kernel/sparse_gemm.h,sha256=yaKM-_YrfreV4zAwrT2ZSJfOCTgjH4yJPwTiioc6v4o,13183
+cutlass_library/source/include/cutlass/gemm/kernel/sparse_gemm_with_absmax.h,sha256=dCcWeCH1Z8oo3Uv9YFSmZ__Dk5TBQLm9_-TN-tg9F3A,16350
 cutlass_library/source/include/cutlass/gemm/kernel/sparse_gemm_with_visitor.h,sha256=Qk9Py96V1KsiTj_Lw5Rt4Ox07rx6t0Tg11K4qQLTKu0,8144
-cutlass_library/source/include/cutlass/gemm/kernel/static_tile_scheduler.hpp,sha256=yzCaen3qG_NYuzivlD1Sqch1YnSUpkcQbmumo8gCU-4,16057
-cutlass_library/source/include/cutlass/gemm/kernel/symm_universal.h,sha256=5E0TanAlZpnpP79DHtFXZeUdmOiGjdw_dn3oVNqNZzg,23881
+cutlass_library/source/include/cutlass/gemm/kernel/static_tile_scheduler.hpp,sha256=Rn7jnC-zm38RHi1o99eOz4Wdx0MrQAs1ueqKmVRdvn0,16041
+cutlass_library/source/include/cutlass/gemm/kernel/symm_universal.h,sha256=a_o0BasS93rvqpeRB4zHf9Tnz4CfnSFY5p3rQXvWsQs,23516
 cutlass_library/source/include/cutlass/gemm/kernel/tile_scheduler.hpp,sha256=G0rNk7hh8SQKjOtA1lQ8dSYw4CQ69n0Xayc7aVUZJtk,4414
-cutlass_library/source/include/cutlass/gemm/kernel/tile_scheduler_params.h,sha256=E7VQL4p5l3vjbfx5EaN6KXgHK3-lJT2ztCHM12qrDI8,58068
-cutlass_library/source/include/cutlass/gemm/kernel/trmm_universal.h,sha256=9W2XIjuTBms0mEJ_zHM_x9pCId3J9k_8-uh65EYs-tE,19518
+cutlass_library/source/include/cutlass/gemm/kernel/tile_scheduler_params.h,sha256=p19QYPYOeD0lEz--PX-8WxxdQzMuk1qTsqywX-kyMOg,58093
+cutlass_library/source/include/cutlass/gemm/kernel/trmm_universal.h,sha256=bK2g07mi_2Dl8MF13NVEwA1NryHJnlCq071Kn6nWTRU,19233
 cutlass_library/source/include/cutlass/gemm/thread/mma.h,sha256=2MRS_i-kvRKGsVEomCTqcDlOBsu_8Ad0R3wnAPlHGzs,3567
 cutlass_library/source/include/cutlass/gemm/thread/mma_sm50.h,sha256=Cc6YaXZwSl6HqLmFBtJwWosgm9mk5mLcqOXFFJ0ByU0,15399
-cutlass_library/source/include/cutlass/gemm/thread/mma_sm60.h,sha256=ShRrELKa_c2MatlJzcbgGNMEOJADCBddRiS9tMTBcrA,29987
+cutlass_library/source/include/cutlass/gemm/thread/mma_sm60.h,sha256=WBcotkptoUMA-t7UvUeXiQWGGkBC7qoZZ07WobKlGe8,29390
 cutlass_library/source/include/cutlass/gemm/thread/mma_sm61.h,sha256=L4LXemunthAPp5MHsrwNwmWsusGsdNKpMcDWKQq71f8,8142
 cutlass_library/source/include/cutlass/gemm/threadblock/default_ell_mma.h,sha256=dqaxJQGdgy--_aN11olJT6GApJ3vU-5Kvpte8h4dVS0,31930
 cutlass_library/source/include/cutlass/gemm/threadblock/default_gemv_core.h,sha256=yX8Z5HyqS_GLhdTGXRls5f_KLcgpexDu9KBpIXa2mOE,6979
 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma.h,sha256=fBb2sVZ8vqyl3EHRB9HQdk47AIxIzgsMVwZI7TN7BxE,35582
 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core.h,sha256=jywszgmVA_emnBvb9xL0clDnjom1Wdin0eq7x15Ndj0,5123
 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_simt.h,sha256=vryuxIbuPRS4GKY7ZypATty4FVZmimvIWIyarzNEcmc,57426
 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sm70.h,sha256=TmMMvIccDQbyNzNjtXU9jInOAhSfJ8AbPFv8XYum0mo,19257
-cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sm75.h,sha256=HbBWtYEfvpUEMgS5dFWJQzYjIK1aAs_v9EdNRPvV3RY,42310
-cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sm80.h,sha256=oHSSzbOMUwxz_llOS7652ADMjb9VZy2JqZFmhVwMCUI,102804
+cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sm75.h,sha256=V8KremYnHzb4JGCNQDOpdDAsAtQd6bbOli5r_ya_XpA,44176
+cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sm80.h,sha256=Y0a7t0SPXTUwIGUV8FyMXXUCvSlxKntrfdTKIVHKNGg,104675
 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h,sha256=sbLiVgnzYTf2a48xtIkRoCBQOUv4O9Y96umEZgcwLko,32106
 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h,sha256=zqhihAnXIf4WuU4-I0NOIFQwj5MIfd74R_TEbdeSlgk,12650
 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h,sha256=mYb4taMnfK5k0BuawmanavYS8OBYTozJFXmM-AVeZJ0,7387
 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_core_wmma.h,sha256=003FuPT7vnRmNVH_6nL3ithvmb9LinkoK0u-D3nB40k,20975
 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h,sha256=x6n8-NjqGPSLpU6Q9BoXpiwUdNIK-hebpmb1WlrCGBA,7998
 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h,sha256=HRuKt2tnmsq30q0FGHknGNy14n6JRt0NG0vUXqR5ON0,5110
 cutlass_library/source/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h,sha256=cG5jmYLv7-uGvuSft29NKz28R7KbQ5S53UKn2KcPmvk,4627
@@ -821,140 +861,141 @@
 cutlass_library/source/include/cutlass/gemm/threadblock/ell_mma_multistage.h,sha256=ST3e4N0RaDdJiEx3SrBwDEV1-5qUovkSbmJBtS9HB9M,24233
 cutlass_library/source/include/cutlass/gemm/threadblock/ell_mma_pipelined.h,sha256=WOTbDKBVJzxAm3c_7lWT1DQSkBuY4JMRy0b4J4dWSn8,13837
 cutlass_library/source/include/cutlass/gemm/threadblock/gemv.h,sha256=XYPaGVBD_3MPImBMhAs03fBC_zA4v8v7S4BubgEOG9M,4726
 cutlass_library/source/include/cutlass/gemm/threadblock/index_remat.h,sha256=1w9IwEmHhXXWyOFWZCSS9d66OSE3jTTZM_jHkP65_LU,3652
 cutlass_library/source/include/cutlass/gemm/threadblock/mma_base.h,sha256=BeYAzVCh4t5ssLBA5m0jEz4qvXYCWrg_LtvbmaF7JWA,7823
 cutlass_library/source/include/cutlass/gemm/threadblock/mma_blas3_multistage.h,sha256=fk6jT8MrJ9vjmiJF_vTgl7L_RWM-WxBKDqPrth0g1t4,27600
 cutlass_library/source/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h,sha256=gF82nqMOfw_HNRoWHdkxjLO1i4P-mFGuIWH9UqFn9aU,32816
-cutlass_library/source/include/cutlass/gemm/threadblock/mma_multistage.h,sha256=bSRwY1m-tffMLr5ko2-Q9Iadm_bRh5kmx4zRKpmcJn4,27801
+cutlass_library/source/include/cutlass/gemm/threadblock/mma_multistage.h,sha256=NH8wPkfJsbDePmT8Ux5F0LU_Df0Y2ggvoiVRCPoKRtY,27786
 cutlass_library/source/include/cutlass/gemm/threadblock/mma_pipelined.h,sha256=Pr772DDyZeHL72z0kjQLX0Ft0wNRQnnczkt9t6pEzPg,15995
 cutlass_library/source/include/cutlass/gemm/threadblock/mma_planar_complex_base.h,sha256=s0e9F9SXbPY9xNkxFcspvE0xWya4nhrxT7807EfQP2U,6901
 cutlass_library/source/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h,sha256=BzNqWoZOI-erPMXCSSqgcobi5KWnzrJSGV4UoDmWoQk,22839
 cutlass_library/source/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h,sha256=XVXfjGuQKmcWUspu3hI1PJ7E8SSboQb4399OYdlu_8U,14747
 cutlass_library/source/include/cutlass/gemm/threadblock/mma_singlestage.h,sha256=BuPUj-KpKPtChvPCc8qBATuBzJA9HVSK8_c6jt5teXw,9864
 cutlass_library/source/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h,sha256=3xsJ8DZFGYi4i2__k2Il5vJKoYMONEhrgrcEGdEFmKE,27246
 cutlass_library/source/include/cutlass/gemm/threadblock/mma_sparse_base.h,sha256=nOLn6W-jHvA8DYrb_JNNEDpKET-XWPtgoqLy-_JVlfs,9210
 cutlass_library/source/include/cutlass/gemm/threadblock/mma_sparse_multistage.h,sha256=AqIvvVH9D6tPLwbXZ_vE5JW7KzGZUvtxo5ARlncJcXU,25557
 cutlass_library/source/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h,sha256=j_5EDCtocFKHsviiGqXrZn4Nxr7biCj18Jn8PTYX00Y,20395
 cutlass_library/source/include/cutlass/gemm/threadblock/threadblock_swizzle.h,sha256=d1UYOuX4SHmAw3fO6EU43U_H2kew6YVtPR-V1u7cV1U,15041
-cutlass_library/source/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h,sha256=jEgamyjtqiy83xvChYQP3uU3Elbhl1SnMCEmljVMYdM,26627
+cutlass_library/source/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h,sha256=I_dB4RyVePCxLi_uelO_HNjMr1F3TwRdbxKv77VDWAQ,26219
 cutlass_library/source/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h,sha256=7Dxc3MDD1Ck4xinpyoh6UihB10xa1UijcUJeqLCmYMQ,20553
 cutlass_library/source/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h,sha256=tQYxYouXJBM-1QA_A122JBR2dwUCFHSobXzRXgRl5AA,6684
 cutlass_library/source/include/cutlass/gemm/warp/default_mma_tensor_op.h,sha256=I5weqAUVG_xLB9g_tv_HMXL7o3_fNV1e_kPsV54ZmXg,5178
-cutlass_library/source/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h,sha256=bU2x5wv4lWj7i_f1_HTylcWDdvecsmfKM5hUHm_PjXI,12142
+cutlass_library/source/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h,sha256=2B1MkxMpC8R66ngkp17txNFdOT6KFd7AnFF1XLfcKAQ,12142
 cutlass_library/source/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h,sha256=ig2cOBucnoKkeTi4feGzMEiSvJSChjUQ1tFcZ3wPPLI,4053
 cutlass_library/source/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h,sha256=2j-zg-1A1dMkm3W9RkznytCgDV1mt_inD3olXVh_z9w,4685
 cutlass_library/source/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h,sha256=1JgL85UTHMBCwxnSNoLFQm9e2V4YxJk-qjNX9DTxZr0,5691
 cutlass_library/source/include/cutlass/gemm/warp/mma.h,sha256=nWLsvdy8vSkxdRV4udJD4WB1YAfVJMzUUExPvAP0Vyo,2619
 cutlass_library/source/include/cutlass/gemm/warp/mma_complex_tensor_op.h,sha256=xkSFoOzQ0Qm9eY52-lmZrIipRfbes5-j4j4zowjuWZY,37767
 cutlass_library/source/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h,sha256=PiVMZXMhUtU85QNV6B_b_wg8YXhiiX2FLo_SLPfwxJ8,23132
 cutlass_library/source/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h,sha256=9du0zayD6WK9n88eb1M738mhp6QlHQ0DV2ZlfAoCOJc,78519
 cutlass_library/source/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h,sha256=ie7gCP7qDSagnzGaeLECrYtmaSZKmUZ3njkhC0WJPjA,21178
 cutlass_library/source/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h,sha256=bZvPrJiyN_wbDnixLkLrdZEc8Q1ZQkl3tu93SxsH4MU,14589
-cutlass_library/source/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h,sha256=lU-YRH_vI23UXFpNMBkxewzTeSbqxOIUIKA4TyZFDiI,20271
+cutlass_library/source/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h,sha256=AYI860t4V6rMYdXgkoPjJ2nkjMW9SrUEGfTllSyKY1E,20271
 cutlass_library/source/include/cutlass/gemm/warp/mma_planar_complex.h,sha256=NxSntt1SeBHZC45yEvFoc27Pw9u4s6cTGr04M-DJ6nA,6144
 cutlass_library/source/include/cutlass/gemm/warp/mma_simt.h,sha256=SIgPuZ7emJc_V9He_Q2DxKohOTigpdJtOIJZXiTD9NM,8419
 cutlass_library/source/include/cutlass/gemm/warp/mma_simt_policy.h,sha256=wkpmWEtdcuuvfiR2lRUu1i95uxuw3cNwsFFpHPKMOs8,3079
 cutlass_library/source/include/cutlass/gemm/warp/mma_simt_tile_iterator.h,sha256=lmo1RqWw-ZZQIucNt0jJz8tL5ZfFQweRfLt2lQFIBfQ,59793
 cutlass_library/source/include/cutlass/gemm/warp/mma_sparse_tensor_op.h,sha256=wD3uMpmQDC8aC7rRre20NZdSzVOMeWGPOuEo3WM1guk,13497
 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op.h,sha256=n3m6EWojykJ7Y9nXDLHVfs8xG4oAJj7bh5hbaRpULOk,13956
 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h,sha256=IvqVGqJ89V7kIh4UGINl6nrhgs2gsjlrvyoEXT38fho,15721
 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h,sha256=N4bbac2Unf0ra73v-LT6YbKsl7YeYQ5J_fYyfMBWz5U,20472
 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_policy.h,sha256=bbl2ojjcUn5iENYqhBLybOVbufzLBWURv0BOQM-OPHc,2939
 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_sm70.h,sha256=-K0jArqncoA5exz4PqlpP7ZIoQmFvkQtXV2-SnetGIQ,8966
 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h,sha256=3kxkWXfGENsbThUNhDM2Gj3aeHFcXYdIaZOWzuCrqVs,11017
-cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h,sha256=55vny0CuuJO1LntyUnGxsIygKizmuQyEWGdCuJtgInY,135937
+cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h,sha256=xKfoxaJERmgsnwHFu0OMtW1bUQWLeHhHPh1uh8YOi5g,162811
 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h,sha256=joNKHLNAlp1yW7DfPFehh4rBfqhHNTLJWLykgXMhDS8,99553
 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h,sha256=imlqxMa7nU9ccANv9Sbdx_m7H2qOFvuQNIqcrszpewE,75040
 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h,sha256=V9tuUm4PkWVuBpGmrlKedxlmw7c2tfaLfo7hcIBIPJs,13151
 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h,sha256=QeCwwXofCN6KWmXd4qeswYPSQwWlpOUqigazQRQhT94,27101
 cutlass_library/source/include/cutlass/gemm/warp/mma_tensor_op_wmma.h,sha256=LXajakE-RpLPZXK53BVDq1AegEMuLhEGuyE8SicV470,7241
 cutlass_library/source/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h,sha256=l99mg_TEiIaeDXf9xg4cjLVWQyEwDkwBpnQkGHZyGxU,17303
 cutlass_library/source/include/cutlass/gemm/warp/scale_bias_tile_iterator.h,sha256=JGQT3tHbwoThI-XHULMCQlOsA4nXzTlaX24U4IOrz8o,19101
 cutlass_library/source/include/cutlass/gemm/warp/softmax_scale_bias_transform.h,sha256=g4dTfKAB-xppgRctUzz85twftbyiu6vwMZQk4obzJgw,4610
 cutlass_library/source/include/cutlass/gemm/warp/tile_iterator_planar_complex.h,sha256=yA4PSDAjPMWqf_p5agRixODPfB3luM191AbEYnDjsms,8728
 cutlass_library/source/include/cutlass/layout/layout.h,sha256=mpNgFLwrIvParrtw1J3e-GMkh14bqpp4q3qDU1M4cQY,3020
-cutlass_library/source/include/cutlass/layout/matrix.h,sha256=IBMwdiw6o44sxRCK99fgLDV7KIh_yIa2tS3UJVrygsU,35129
+cutlass_library/source/include/cutlass/layout/matrix.h,sha256=fTRBsP9rH-l1zmJj8lKpwWx-it7-vNQz-qoPj44idbU,34719
 cutlass_library/source/include/cutlass/layout/permute.h,sha256=RxD5iZW8HUtWqLJVuo0jT-zG4rVEpDWh0X784KmjSNI,24906
-cutlass_library/source/include/cutlass/layout/pitch_linear.h,sha256=HKIVksABZoV1p-LJSrnA02YtEG-8WSg90JIUZqUAzOQ,5107
-cutlass_library/source/include/cutlass/layout/tensor.h,sha256=rbasoEC59dBrOvn5HgEXogwlNXKxiTCTtkT3N66D2Bg,18798
+cutlass_library/source/include/cutlass/layout/pitch_linear.h,sha256=z_9QkDgGImfN4QwZXUMBw7SD3P2BPdwrnCz1yJuTTsI,4697
+cutlass_library/source/include/cutlass/layout/tensor.h,sha256=NFlktf8WninrjZcb9thmyfEOBt3AdO-TuIjO4kPCL0w,19044
 cutlass_library/source/include/cutlass/layout/tensor_op_multiplicand_sm70.h,sha256=PArEG0yagW5-Wm3udcYpZshdj3eN2bz_lj9xe18QX8c,29599
-cutlass_library/source/include/cutlass/layout/tensor_op_multiplicand_sm75.h,sha256=2Adz5_81uNS4_ahGLQoef5CMHpxfUZH7Mi6Hrhtehgo,33137
+cutlass_library/source/include/cutlass/layout/tensor_op_multiplicand_sm75.h,sha256=6-msksMSh-b2YsZwmP0ec80kWKTOsDMtN-n2np8RHs4,33494
 cutlass_library/source/include/cutlass/layout/tensor_op_multiplicand_sm80.h,sha256=mSmZKjonHk1txB3GvohQi-RUMZe-vDV7nrl298lo1-Q,29336
 cutlass_library/source/include/cutlass/layout/vector.h,sha256=moB_T8Ro556dM_ox1WdrzkAjUfFh2F8X4RXWkfSsQ0A,3354
 cutlass_library/source/include/cutlass/pipeline/pipeline.hpp,sha256=1U92rmdTbNXf1x_dwzcSaDwKK3L7OjvoQvtNPGFixTw,2091
-cutlass_library/source/include/cutlass/pipeline/sm90_pipeline.hpp,sha256=QZ_khzbroYCsov9qbyNnx0Q_pOudinUY5yfp9NFXkzU,37270
-cutlass_library/source/include/cutlass/platform/platform.h,sha256=BubCmy9ssTLLofoyasHrzq_vyYliGvcGWoPDTtsYKRg,28315
+cutlass_library/source/include/cutlass/pipeline/sm90_pipeline.hpp,sha256=w8rUXBqN8QJ8LJOwHzlAZxvpxBuyBdb-L11Z60OZHrg,37271
+cutlass_library/source/include/cutlass/platform/platform.h,sha256=nud3XgK-ed58KTSIEEU5gLvpBkWul9g6J5dvlgeBl4I,27863
 cutlass_library/source/include/cutlass/reduction/threadblock_swizzle.h,sha256=4GBj0nMifYnzdEg9ajxecfmvxBMIXK5VkEJAD7fX0f4,2936
-cutlass_library/source/include/cutlass/reduction/device/reduce_split_k.h,sha256=xU-V4-B52ld-pqu9zizuq4_CtM99ei9J-zbOh0-nLiU,6823
+cutlass_library/source/include/cutlass/reduction/device/reduce_split_k.h,sha256=3mTnXrJlKsvfoJW-x8SRhUwaAyzNOvBwJuag_dSKKZk,6749
 cutlass_library/source/include/cutlass/reduction/device/tensor_reduce.h,sha256=MEn-SW8fa2hLHDmV5R8mcpL2yWW2WePKTXw8GYXdSV8,8152
 cutlass_library/source/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h,sha256=byTlZ6eYWmlIVfHJcX8dyipdNU4btnyOjY5tPLT4gQ0,11579
 cutlass_library/source/include/cutlass/reduction/device/tensor_reduce_affine_strided.h,sha256=fiWY0o89aV-gOJe5H72RX0YzxAYBkUOai3iSLkfDF-A,11448
-cutlass_library/source/include/cutlass/reduction/kernel/reduce_softmax_final.h,sha256=tS0syrRVjJuaLqZIrTVGhlWUPTXKC3Mfoy91N96cb_Q,8762
+cutlass_library/source/include/cutlass/reduction/kernel/reduce_softmax_final.h,sha256=U3qY7geMhPPpvLx02Atr1Y8FphiE1n-zpgZoRmLtRFI,8815
 cutlass_library/source/include/cutlass/reduction/kernel/reduce_split_k.h,sha256=fEG_Yqh5372lUgJosnsPWyRr99Lwy5WRLkohZWIGG1I,7897
-cutlass_library/source/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h,sha256=Q3QR14to30nYhXcBRVP_apU6O22q7e5HKNYtPAo6N7E,20685
-cutlass_library/source/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h,sha256=Cli9YrOX2W7LSBM_QIn2gyH4ebSXMDusJm81IOucz1U,21662
+cutlass_library/source/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h,sha256=dp-0TEwrmuhv8FdDv6_8HqKqeLSZJLvgrc_Z-IhWtSc,20690
+cutlass_library/source/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h,sha256=m3cKB5a51fygY4KoxueAfWbHfaj0uyJ73FSDMk_fbHA,21672
 cutlass_library/source/include/cutlass/reduction/thread/reduce.h,sha256=OeNCgOmoZa04mbV_6a4fl_5Fox2DAdI4D4YbyUHFHMM,7208
 cutlass_library/source/include/cutlass/reduction/thread/reduction_operators.h,sha256=bDjzUZbrS9ao5sQGChiT-HoxVlhA-2l9xnpuuiFUp-0,6790
 cutlass_library/source/include/cutlass/thread/matrix.h,sha256=q6HMMREqncm6j71ygFxt1ciNZPNHxatqO6p-ehgBDfA,5893
 cutlass_library/source/include/cutlass/transform/pitch_linear_thread_map.h,sha256=tFespqXs6WaS1KbUJXLgwUs0IseJ7hdsGAo2W_9C5DQ,33349
 cutlass_library/source/include/cutlass/transform/collective/sm90_wgmma_transpose.hpp,sha256=BWVRf83SjkIbXtIjYVyTPOXdkssu2tiqLtzAvgtY1AA,33948
 cutlass_library/source/include/cutlass/transform/thread/transpose.h,sha256=HY44cmSMSdTAij25UlyaXH958CaMwQs5UML8NvjxMqU,3835
 cutlass_library/source/include/cutlass/transform/thread/unary_op.h,sha256=hBm8JFRkIkoBrqVp9HhNgce7vNGmQaOZcF7xn-PKa2k,4309
 cutlass_library/source/include/cutlass/transform/threadblock/ell_iterator.h,sha256=YzPOiji1jPRm21LEmIiv9pnCUtKgBLzR5bHcWCA1MmM,6181
 cutlass_library/source/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h,sha256=IP8U7vnuGU03QkqWd31J9p_D2S5Zj61dhxwwaGFhP80,44443
 cutlass_library/source/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h,sha256=EWVEZBPqO8TDsT0P6XGU6Ll8kHyKS_cpfNT2LiJoi9A,44309
 cutlass_library/source/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h,sha256=tPN1p-mnRmIQ66fbeaZFmRFkwKlJUn_B8d3d53dbdMA,12890
 cutlass_library/source/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h,sha256=PVXp-86uXYqcWcGp8YdFWSsHpOOqoS5fa_NJZCftSFw,11097
 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h,sha256=WIU0mI-kHYPYhvJIG3vFOqr2WBQYIfF-ClxIm9dmIzY,72537
 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h,sha256=7QSIiHgY6iyrgKhj6pjGDqrcPmvL-7rocd_0ZzhO644,28232
-cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h,sha256=TbDO1ppr-vgEYf96z3bUAVsCts9bdZ8Zqo8CFQZlp30,10805
+cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h,sha256=GxpLcyBB8lh_zhQAa8yQpMYawznfXxfuTgmbUMTvTL8,10395
 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h,sha256=dvqyCtluLdQ0ALjbPx844IyB2UM6UJCvoysZAWgxgD0,31412
 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_iterator.h,sha256=Wo2GKEickNVT_7RnPT-9VHaoK4x2e8LgF3PqBIZZQbk,62949
 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h,sha256=BmJMqtysIm89P6SZzUXpgcxb4nvc6KOP56uKqawIYxE,27175
 cutlass_library/source/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h,sha256=JEbI1DREI3_I2hvt1OEMW-JV90XNlBQMu4p5m5U2FyU,28064
 cutlass_library/source/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h,sha256=1CbYauNXrNINrCDkB1ZAMrHsWgXmryuK7Gdvh1Gp-jo,13088
 cutlass_library/source/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h,sha256=HJWCIt2ntzOq9jkY-SlCFliYzMNsHllPVRMS9hmKtU0,8232
 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator.h,sha256=b4SRIFsrEeQIoAZgn79EUKZbAlXYG016aDzr3MANMYQ,2638
 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h,sha256=Coti58_lDD63pN9aZC_8Zujw4NVvSGLKIZDAAjgnWY8,13283
 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h,sha256=X6peoL22kbF4vweLbN8WIVoTtzP87I6tk9d6-0llb3Y,18623
-cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h,sha256=A9QJcZmwVwAwuZmIZIC0kDjM9-CUkh6NYaaBKcGKB0M,27922
+cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h,sha256=PmrRcZZfGXDY8itv08LE32VUo8qFnnIoxLT_WiIxBRY,27938
 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h,sha256=OO7EUpESvZ_RYw0TNpdFo6HlqEnptbetVrIV-MUcNs4,47789
 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator.h,sha256=jZR6p6rbIzHtA1wX8Cd9h6sx7DyG1MXbB0WqXEav7Qc,2616
 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h,sha256=Ao0pmm0jU7DqwRmb-zOniLkjWmiLewkpWQxhqsxiq5s,16508
 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h,sha256=vwF8EZe9eQROgq-WtvPMpQJrIMPWPbAlba0eA2i-8WI,15486
-cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h,sha256=YRox1C0eswM9JXmcjX5PVyv7bWO_IXAqz47yC2Es-jY,36050
+cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h,sha256=BemZJLc4k66x3gVBZ5x6Wif73HrPKrT3HZ1S4dnBVZ4,35956
 cutlass_library/source/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h,sha256=2aUY2wIf4mRnlN7UJzzayoR9dRsqgfmqcmbpzYFpIWo,43663
 cutlass_library/source/include/cutlass/transform/threadblock/vector_iterator.h,sha256=PuycQoKeC6-nvsnUAgCZYJGb0jSv94sA3PktLC_3zH8,5226
 cutlass_library/source/include/cutlass/transform/warp/vector_fragment_iterator.h,sha256=T9UF88szOdrC2Zhq81NRCJCubOUF7Ci_TlHzsS1Mizk,8828
 cutlass_library/source/tools/CMakeLists.txt,sha256=K7et_ye_cs_K0G9oxdpMRGcEuOea6bAighKkhiTsLFM,2092
 cutlass_library/source/tools/library/CMakeLists.txt,sha256=U1O8YYDeUqiiN7Gud_w54iQZaGtOeiv8VLYcii1-J_0,10693
-cutlass_library/source/tools/library/include/cutlass/library/arch_mappings.h,sha256=anxYzE9JAw8uaLbhC_CbylZFsawaxY143f67c9yyBhc,4272
+cutlass_library/source/tools/library/include/cutlass/library/arch_mappings.h,sha256=RYr3zpz3xSielcqBGsGSb37Pq_GXAvMYjjeue2iuAMk,4414
 cutlass_library/source/tools/library/include/cutlass/library/descriptions.h,sha256=NKyhN1oiCpjm1dx1cnqnd6cH88np3dMp4vcm7HEnsdE,18334
 cutlass_library/source/tools/library/include/cutlass/library/handle.h,sha256=0znwov_8IUBjtz5vu8nBQVerVZWFJYqP7UxwDdILBxY,16150
-cutlass_library/source/tools/library/include/cutlass/library/library.h,sha256=QhezTZFPMr-Jk-iYgzt0Ijvmq8IqvwLOCNfC2jOmW1M,20135
+cutlass_library/source/tools/library/include/cutlass/library/library.h,sha256=n3JXxtEIj1K4F1uggS1wtEcYOnDIxROAQcFD4nE5Zcg,21225
 cutlass_library/source/tools/library/include/cutlass/library/manifest.h,sha256=xWCtH9GbveM_6RLL0-xvTr1_bcCQkRzIDmeV5H8Rkzo,4251
-cutlass_library/source/tools/library/include/cutlass/library/operation_table.h,sha256=Rcj-zvvVDL1LqTequ55kQLXYTBwQYRISHPha_p_rcTI,19027
+cutlass_library/source/tools/library/include/cutlass/library/operation_table.h,sha256=I8A5JNwTZZBq5JBnSdH0M1rB9MKc56lHsiwha37u-jM,19073
 cutlass_library/source/tools/library/include/cutlass/library/singleton.h,sha256=FeR9XD5jk5pjWAdyaLoU7-xV-8eyqrXs-QWQm00YALk,2724
 cutlass_library/source/tools/library/include/cutlass/library/types.h,sha256=tQRQ51p2Ld0mpcyl4NP2plwURHNEyckHKOFPv2sSi60,5908
 cutlass_library/source/tools/library/include/cutlass/library/util.h,sha256=JZISwQUwsfq11fAQE90rbYSV1v1GlXdtSdzwO2k9Yic,8140
 cutlass_library/source/tools/library/src/conv2d_operation.h,sha256=04WGs8WXMLUAH-pop9fZcLgSf0WdhwxwYwFCVZvzpDY,22377
 cutlass_library/source/tools/library/src/conv3d_operation.h,sha256=EnbEhwywdZ6Wb1iTnZJ58rcOENyjyoAYwiCVrNOxfl4,13850
+cutlass_library/source/tools/library/src/conv_operation_3x.hpp,sha256=Oi3b5jLObKvsincPWkJowyaJG_pEct5Wn26qjy3cZpo,35804
 cutlass_library/source/tools/library/src/gemm_operation.h,sha256=1FzmOaum4fZgQsTMLsBwGDKrNqOLA33rpFw0JPtfKMg,42608
-cutlass_library/source/tools/library/src/gemm_operation_3x.hpp,sha256=foxs9rdh8VpuohsuAog3yiyFZ-wN-riXYYq5j2_5Qc8,13766
+cutlass_library/source/tools/library/src/gemm_operation_3x.hpp,sha256=53fJzDWyYolCiRdGY50ROoCgYDE3akOhvrhcm7Ld9jg,13834
 cutlass_library/source/tools/library/src/handle.cu,sha256=Ba2XCrfdNUUpUQEfrOmWaSYWfCq4V1hA8LD3BaCpRIQ,36802
-cutlass_library/source/tools/library/src/library_internal.h,sha256=goASDrTeFtA2Lu9HFBoZWgG4lzlfY1jJwebsSXeiJR8,13269
+cutlass_library/source/tools/library/src/library_internal.h,sha256=FRv8n0TuSmNXpEnibbKFjrWWhEggUd3TsdiTfPnm33Q,13270
 cutlass_library/source/tools/library/src/manifest.cpp,sha256=BoyL9yjtKeqSPQAj04X762q-gBJolUrs351fToBTmSY,3634
 cutlass_library/source/tools/library/src/operation_table.cu,sha256=BRN7WAwrXgjwlxPiuxMoAfm5iMrykc7AnMCHtgwXdwA,5551
 cutlass_library/source/tools/library/src/rank_2k_operation.h,sha256=FF5BqX4kHhO2o8jDKoiQoqdslPcDcU9F8TCqKDpCkSw,12873
 cutlass_library/source/tools/library/src/rank_k_operation.h,sha256=jau8i_T09WwFW12XDfok-N-0G3JKduXMC_CKnJ239OU,11367
 cutlass_library/source/tools/library/src/singleton.cu,sha256=cs58wLWIza4h8fWBClj_qdtsM1uxC9O8jZmaNIE8neY,2669
 cutlass_library/source/tools/library/src/symm_operation.h,sha256=njx6X17VKYJJynVts6FlXtQ-ZJGK-Yupp55tGVubz8s,13134
 cutlass_library/source/tools/library/src/trmm_operation.h,sha256=3pkOlQz-Satqxm5jRx7L7al0TS7YKN5JwcridaquaQ4,11698
-cutlass_library/source/tools/library/src/util.cu,sha256=9YVmyTlIj-iqZYEFQGoqiDywSrznsYqvV4hidmS747A,46579
+cutlass_library/source/tools/library/src/util.cu,sha256=9v_ZqbF6hIMam5guJ9U86onbGfTyH77l-TK2Yr-AEbI,46683
 cutlass_library/source/tools/library/src/reduction/init_reduction_operations.cu,sha256=jU5BdlZjuNv43C7x7bU13KfNBNvBWWJPt25FtAUhPkU,3482
 cutlass_library/source/tools/library/src/reduction/reduction_device.cu,sha256=9q6DKHJa1FaMU8fLuOmA3Z_RKQM6yf-F9pDFog85eo8,8435
 cutlass_library/source/tools/library/src/reduction/reduction_operation.h,sha256=rF14xlEALPPl-x0ZqXarMztVVqgInOGst5w_kVeeAlw,10269
 cutlass_library/source/tools/library/src/reference/conv2d.cu,sha256=Fm4WlPvPwM-l8Hve2VRsG3GxD8ops2VYhGRQQAM-O6w,6746
 cutlass_library/source/tools/library/src/reference/conv3d.cu,sha256=yfLRaodH6XBKsovj6zSdxm0Y4Rs2qDxVpR7gdxsF2Wk,6286
 cutlass_library/source/tools/library/src/reference/conv_reference_operation.h,sha256=iHLwdFalU1DtfkT1sO1xmylQswEYvX5X7iw4oSmAvbA,17347
 cutlass_library/source/tools/library/src/reference/gemm_e4m3a_e4m3out.cu,sha256=Pe9xcPBk3aAQjj1nimTSdgKMVVDXdLllp7Ax-aNFWE0,5473
@@ -978,16 +1019,16 @@
 cutlass_library/source/tools/profiler/include/cutlass/profiler/conv3d_operation_profiler.h,sha256=GiD27ouOfRctKbgETBTwIwI7czJp74F79vFEhyo-khs,16101
 cutlass_library/source/tools/profiler/include/cutlass/profiler/cublas_helpers.h,sha256=pua-Xb1VFQc4sDyee2u5pxr1OUEd-YdHnEItEiTW_54,10623
 cutlass_library/source/tools/profiler/include/cutlass/profiler/cudnn_helpers.h,sha256=_-F6U-V5N2GM8BBCVclcBTfDV0xnMYzLySyr2Uf5hBI,20435
 cutlass_library/source/tools/profiler/include/cutlass/profiler/cutlass_profiler.h,sha256=EMuD8vPljtKI2m4Yq4A6R1XegYA9oJlKb_PeA1zuGcQ,3233
 cutlass_library/source/tools/profiler/include/cutlass/profiler/debug.h,sha256=Yu4ARWN981qCyW0lDkBThigPbRS8aBiTt6SU_MX_Wl0,2454
 cutlass_library/source/tools/profiler/include/cutlass/profiler/device_allocation.h,sha256=wHvgCZNfgQocI3DM6OYpBiogNYA6X70gWsuS1gm9N0M,7576
 cutlass_library/source/tools/profiler/include/cutlass/profiler/device_context.h,sha256=queZaReJYlSRZYykucus4iW5G2SrajDRt6ga4DN42t8,4290
-cutlass_library/source/tools/profiler/include/cutlass/profiler/enumerated_types.h,sha256=eo4LBL5ynGHq9FkPMOcjScXnCBreZQOCY9IFX_6gVb4,6421
-cutlass_library/source/tools/profiler/include/cutlass/profiler/gemm_operation_profiler.h,sha256=QFFAU47voipHF04ntJJ5Ww8Iqkgoy9pwpV-uJIMsmbo,8749
+cutlass_library/source/tools/profiler/include/cutlass/profiler/enumerated_types.h,sha256=8EMNJtWdbbw7WcF63WqCLrgk7md8ydhhyj_hJFxtxss,6428
+cutlass_library/source/tools/profiler/include/cutlass/profiler/gemm_operation_profiler.h,sha256=hXDltXUDjzTciPd2vm0-9EFoWgfjD-qa-X3vK6ETE3Y,8590
 cutlass_library/source/tools/profiler/include/cutlass/profiler/gpu_timer.h,sha256=-NddfUIkavZBLz8UYzvjNHc17RUnSh82_xAxsIUER4o,2725
 cutlass_library/source/tools/profiler/include/cutlass/profiler/operation_profiler.h,sha256=Vn6sOTzuEafVZMGWVsGkZsjbInsxvBO4UKDM3LjFBXg,7924
 cutlass_library/source/tools/profiler/include/cutlass/profiler/options.h,sha256=Jwo_MjaBgd1WyS1l_uX3blrHqKhlDOs-wdVt0BfpCJI,9273
 cutlass_library/source/tools/profiler/include/cutlass/profiler/performance_report.h,sha256=UlqakDxgv5WkwuLc9mGu0w-YmdGmOTG67a77ri8wxgE,4337
 cutlass_library/source/tools/profiler/include/cutlass/profiler/performance_result.h,sha256=WOHe80qbbJS6AHPvGnU7J82zMUpCiWRk5lfeUQJoD1Q,3941
 cutlass_library/source/tools/profiler/include/cutlass/profiler/problem_space.h,sha256=YOtRHOSmeD1pkph4nGnMAZigI4_4K8D3fxDdVjjtcx8,28189
 cutlass_library/source/tools/profiler/include/cutlass/profiler/rank_2k_operation_profiler.h,sha256=_MwoWF_KfQyCNavxoiMH1-fGY4C6kYI2KWfW0_Mmi60,6891
@@ -996,102 +1037,103 @@
 cutlass_library/source/tools/profiler/include/cutlass/profiler/sparse_gemm_operation_profiler.h,sha256=8B4pvmyTy5_tA_M7pR1nucGHwz864Z0kjair16_Yh00,6471
 cutlass_library/source/tools/profiler/include/cutlass/profiler/symm_operation_profiler.h,sha256=IzvytsikD9IX-g1D7vWByAueR671_KX127c9JmNGgvI,6933
 cutlass_library/source/tools/profiler/include/cutlass/profiler/trmm_operation_profiler.h,sha256=jY5mh9IFf1VJ4fNzjzjRXDzYxADGOy749WCZRoBsonM,6599
 cutlass_library/source/tools/profiler/src/conv2d_operation_profiler.cu,sha256=fEqxW1OtMAslFrl77rDDP5hkkl0Tz0Xg0evaPSvh7PU,54272
 cutlass_library/source/tools/profiler/src/conv3d_operation_profiler.cu,sha256=MrPBckLAWPf8SfFRemVo92Ugj0ll2_bTcTrxC5TgZSk,48776
 cutlass_library/source/tools/profiler/src/cublas_helpers.cu,sha256=DVTRFvZZWyAGeKoO2Kk8avgN2GmtF9nZKge53csgdAU,37138
 cutlass_library/source/tools/profiler/src/cudnn_helpers.cpp,sha256=Y0d9zYJLtBzydlK_wAkKdIXcki8wBKdnvlYSmGeYmWA,17066
-cutlass_library/source/tools/profiler/src/cutlass_profiler.cu,sha256=YWWUMgxIvrEgwP13O9g1nQpFJ2NGyVRGU-2hZhqsdyo,7391
+cutlass_library/source/tools/profiler/src/cutlass_profiler.cu,sha256=DC9zdk01KmPjo_kVdRn2HE9mLnYxtYrahYuvwBWnfsA,7351
 cutlass_library/source/tools/profiler/src/device_allocation.cu,sha256=y4ULjh9SrRJGDycOj_dUpT3jpdAZFtgSbEoR5xWQxQU,78653
 cutlass_library/source/tools/profiler/src/device_context.cu,sha256=pKFHuHxSigHlMwaufxSG9o6HYpYOqAHMp_7_9lux0Jk,8359
 cutlass_library/source/tools/profiler/src/enumerated_types.cpp,sha256=0k6iG2GjUna-kzqqcAh4MVsHWkrBlze1vx9z1qiXVWY,8313
-cutlass_library/source/tools/profiler/src/gemm_operation_profiler.cu,sha256=7oUuBhiXtxzrCxMo1kyuzPjbicsOTrhAo05sRJI3k2k,43920
+cutlass_library/source/tools/profiler/src/gemm_operation_profiler.cu,sha256=qNehPNseRi2yyblIjZx0b8CfL2DOt57YlppsgUhxT34,44294
 cutlass_library/source/tools/profiler/src/gpu_timer.cpp,sha256=jZ1pcRxvMhyMo7Gx7mSAKkQBx0nsCzPTfAM-WJ0fWNI,3892
 cutlass_library/source/tools/profiler/src/main.cpp,sha256=cwjqxr_P8ojHzeUDHrHdPep5zortTlss_5SaUMpV0XE,2374
-cutlass_library/source/tools/profiler/src/operation_profiler.cu,sha256=7ypiOK-Z3KZcffQr7I03uEp9i4qZCq09A713JCqX-tg,22898
+cutlass_library/source/tools/profiler/src/operation_profiler.cu,sha256=y6abazWEiadOnLl6bhKuoqhyk-OXzG3d62f1tExNAm0,26023
 cutlass_library/source/tools/profiler/src/options.cu,sha256=0lHeBNJ8tSRFrRD5TNeXXulsbGwnnQNW9DveMhc8zD0,28314
 cutlass_library/source/tools/profiler/src/performance_report.cpp,sha256=nHFWPrb7cjLQGLvr7FaED7XyYmNMDx4mKaRqsnEcdCQ,14227
 cutlass_library/source/tools/profiler/src/performance_result.cu,sha256=fe2RpVFIEWCJwt3pcLm12XFLiA5U-5BBEHLsg8wOjIg,2528
 cutlass_library/source/tools/profiler/src/problem_space.cpp,sha256=bCX0ZW_k_8DnM2kFbSmYzTL7-7ZA9J-8Z1KVYRA7mXs,38798
 cutlass_library/source/tools/profiler/src/rank_2k_operation_profiler.cu,sha256=d6Mf5znhfaOa905bXq5ufMZynQjx3P_xyj7kQABtEfM,25183
 cutlass_library/source/tools/profiler/src/rank_k_operation_profiler.cu,sha256=Wc04JlcDypL0BN6e_03dGrAsSNWtrHLlsQeyhWlvkuU,24378
 cutlass_library/source/tools/profiler/src/sparse_gemm_operation_profiler.cu,sha256=zunMUx_TA6PtalwtVssP10ZuOOxjDJnr4ot5fvnmf7E,20915
-cutlass_library/source/tools/profiler/src/symm_operation_profiler.cu,sha256=3ky5uTGAAnYwiZTsRRr4ijD0mmRrCmB8nLAcTKfPpvg,26753
+cutlass_library/source/tools/profiler/src/symm_operation_profiler.cu,sha256=q69xaHnFRu_OXZnRDChoVJXsIP0W781KU3rl1NW0iXk,26757
 cutlass_library/source/tools/profiler/src/trmm_operation_profiler.cu,sha256=LEmGasyTR7nFNxntN7m7L4yY5-rsNb9qn0U23BQzeKk,24567
 cutlass_library/source/tools/util/CMakeLists.txt,sha256=hcTPN6NUyI-0D0eHftWqtTkuTxWcuXxWecJOWPudCEY,2297
 cutlass_library/source/tools/util/include/cutlass/util/GPU_Clock.hpp,sha256=c2BbUiqQtH__ncXguOTF09dPzFOyPG4X9k5dB1qy_Mg,2410
-cutlass_library/source/tools/util/include/cutlass/util/command_line.h,sha256=8AdaqvlFs0u0xE8TF753RM7iOrE3UmXH-xDA7cR-qMY,9777
-cutlass_library/source/tools/util/include/cutlass/util/cublas_wrappers.hpp,sha256=r21_099LwI0idAEClBRJd8IcMAFWF1KxmNQ3OTDD9JE,19866
+cutlass_library/source/tools/util/include/cutlass/util/command_line.h,sha256=2fklpP_egpjFgn_HYJraKg3egOb0_LaYF7V_z7quqFA,9780
+cutlass_library/source/tools/util/include/cutlass/util/cublas_wrappers.hpp,sha256=iC4BVxj7m4O_tbe8wDzG144_Nv5kEAsfwZTeLVXOuOU,19875
 cutlass_library/source/tools/util/include/cutlass/util/debug.h,sha256=9p-coGJpbIepS9muVBfNpiQ84UwCY3lxB1Se7HsUHfE,5104
-cutlass_library/source/tools/util/include/cutlass/util/device_dump.h,sha256=Zp-JrAwEU9C8foRd14A0jF6LiDVPJMtp_X5CZbWBXN4,5953
+cutlass_library/source/tools/util/include/cutlass/util/device_dump.h,sha256=8bhMimFMvskFardPENYTt04dSLfK2IySonMAmH-XXZw,5958
 cutlass_library/source/tools/util/include/cutlass/util/device_groupnorm.h,sha256=28jeQ8o_EDygaratgocpj36hS0ZxSEmDBx3xy9eBe1c,17696
 cutlass_library/source/tools/util/include/cutlass/util/device_layernorm.h,sha256=KppjLxRy4cIZtQ2LtKVkUm6YqkgyFudzCwu1AwIUy4E,20881
 cutlass_library/source/tools/util/include/cutlass/util/device_memory.h,sha256=lTx4l6yQ6nfnAoujAQYnQuLb_-8z5mfplQBo4PI9dVo,10561
 cutlass_library/source/tools/util/include/cutlass/util/device_nchw_to_nhwc.h,sha256=sTYREzUeH2grZWkoZf1OqSPJY0NCgf61hiZL3PsYHuQ,5219
 cutlass_library/source/tools/util/include/cutlass/util/device_nhwc_padding.h,sha256=8_1Ev01ytBCD7twg-DYJ6tnQGe0oJsK0eb_Jres9Tgg,11075
 cutlass_library/source/tools/util/include/cutlass/util/device_nhwc_pooling.h,sha256=GF55O0_hHg5fygAR7Gs9uLr-sHJQP4TlG5IKCvhP6Uk,18653
 cutlass_library/source/tools/util/include/cutlass/util/device_nhwc_to_nchw.h,sha256=CLLe2eerYt4TvZmILSk_0nxsougUTsixpZeu0_r8Ns4,5214
-cutlass_library/source/tools/util/include/cutlass/util/device_rmsnorm.h,sha256=ec7zNqkG3EjpLl4LovCrjAbw6PP7nxoqCwf1Vpzg1io,7096
+cutlass_library/source/tools/util/include/cutlass/util/device_rmsnorm.h,sha256=0YB34YlSeFfVgnPh_cx1mDQiqX0MIh5vHft7XlWL9RA,7237
 cutlass_library/source/tools/util/include/cutlass/util/device_utils.h,sha256=HBfBt8GS8seFMKJ36bQ2YutzxyJUV0HSWe_qwpdNuLU,4007
 cutlass_library/source/tools/util/include/cutlass/util/distribution.h,sha256=WwjmvEp0GQINHo1t7xgitALx4i_DMM9iLumWooFvZzk,4846
 cutlass_library/source/tools/util/include/cutlass/util/exceptions.h,sha256=O4k4e3eAoDxIj0heLKgubWW-ewRmLGRlJkZHUWeROCg,2674
 cutlass_library/source/tools/util/include/cutlass/util/gett_commandline.hpp,sha256=PruM93bd7VM3RXwYZvg5pGFzkWbR9v5UbXCsM7_2VQM,13740
 cutlass_library/source/tools/util/include/cutlass/util/helper_cuda.hpp,sha256=m2spdCgbhnMQIOylVRcGlDWxyR2lrlPTW_7qtk8gdh0,3946
 cutlass_library/source/tools/util/include/cutlass/util/host_reorder.h,sha256=jcydlTnt07uJA8Oju4egnqXys7PXEkFgXwkzLPFIb5M,4821
-cutlass_library/source/tools/util/include/cutlass/util/host_tensor.h,sha256=onK_rXcab6t0eFuZV2Kc6lD2Pc_-4FGOFeOBkFc1kSE,18299
+cutlass_library/source/tools/util/include/cutlass/util/host_tensor.h,sha256=8Vkl6nrVm6S3WhiG6i-mze_oeNObHOlD6suXpM84iZc,18542
 cutlass_library/source/tools/util/include/cutlass/util/host_tensor_planar_complex.h,sha256=NmYgmxB-iU22Zt8tnJm2VYhspl1WpzLoOi-KXCrAN8k,20354
 cutlass_library/source/tools/util/include/cutlass/util/host_uncompress.h,sha256=4uTH7FJEoU89_oWNU9qZrePwy16Q4XFqjd28aHz71Xs,5890
 cutlass_library/source/tools/util/include/cutlass/util/index_sequence.h,sha256=c4M4dL8G6WxiQMCXXMYVC8PlwJhKS655N4iBDXvPL1o,1962
-cutlass_library/source/tools/util/include/cutlass/util/packed_stride.hpp,sha256=OBf7H2QI3wMc4JMsCw1sAZ27-Dhv3Vui6K--DwBjFvs,4686
-cutlass_library/source/tools/util/include/cutlass/util/print_error.hpp,sha256=f2jvI4a5b6C72FToEsMVM23TdLXa1yRrnCdyYfydymU,11254
+cutlass_library/source/tools/util/include/cutlass/util/packed_stride.hpp,sha256=c6IupFpuDqEAKklU9w6wFNpjigk3XUEdaPbj1wCRSAQ,18060
+cutlass_library/source/tools/util/include/cutlass/util/print_error.hpp,sha256=ujYMOrLHnD8WgL-0Ah7yyyWd3krlBi9HYj0RGEkzvkk,12399
 cutlass_library/source/tools/util/include/cutlass/util/tensor_view_io.h,sha256=ET_NDJ5X2mQLXVTIlM_c8yXpeY-7cEXBbmPmwY0p7f8,8341
 cutlass_library/source/tools/util/include/cutlass/util/type_traits.h,sha256=kaYoEm8YxOLHOXJTtzE8KK34LIhezM6-ozgz9i1PSoI,8809
 cutlass_library/source/tools/util/include/cutlass/util/reference/detail/inner_product.h,sha256=a6NEuxf_QrvGQ9nf2oZxIOTlFmvQcc1VkQK_7mCQWwM,4606
-cutlass_library/source/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h,sha256=3Pq9Tc2jNRpBL283FCsymcIgX6_Ds5U7C2_j4jpAM2Q,3527
+cutlass_library/source/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h,sha256=EwzGaaJbwRauxc9ut_Pudt2qkvMj2q-z34x_INzPKWU,3521
 cutlass_library/source/tools/util/include/cutlass/util/reference/device/convolution.h,sha256=hFh8QRoq-pbwGS4YbnFUdbIO4e3hmMNEwjCKWluW9NA,48350
 cutlass_library/source/tools/util/include/cutlass/util/reference/device/gemm.h,sha256=osgvFvDIryL62FLxPx_csgUzpG047iIOG9pJXKB7dHw,14296
 cutlass_library/source/tools/util/include/cutlass/util/reference/device/gemm_complex.h,sha256=nweVz1fl17BLWCz__5itPVtak4eevthW0RPajJ5K7L8,10652
 cutlass_library/source/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h,sha256=2FtpDTpSTlPyRz4VyiitKgTQeGbL3tOBT_MohRtGH4c,9652
 cutlass_library/source/tools/util/include/cutlass/util/reference/device/gett.hpp,sha256=maFMJHZUq9tJ1YaH3LmWvwiUQPkKsCyK5vl76I166Sc,5444
 cutlass_library/source/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h,sha256=MMU4A2_DSCVrqwR7UuNE1qWRfyCxvm-f9C1Nt0nvSzs,11615
 cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_compare.h,sha256=3kklNOPyrgr1pKhPOx7gfplRY5AZATomQi_PjKP2Pyg,7278
-cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_fill.h,sha256=C6OAImQNYinu0JkQCVTuz5K2WbAgsWxK2KbhUl4hwYQ,49624
+cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_fill.h,sha256=zLH7HW6VyYmnhhkli9sEdXtIiQgusIELwNEDA97wZhM,49333
 cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_foreach.h,sha256=sYg_Ra5ud8zngIC-QRiPllw6vsTDEs418SzQ4xHde5I,5454
-cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_reduce.h,sha256=pVOxz4T2In_fhJEfthHm7tmIkYbtp26BbgS-fww8u38,15964
+cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_reduce.h,sha256=YBJo-8n0mYhhNzrtGOQoNt8waFPR4WCiZ5Rc7s4ca5U,15983
 cutlass_library/source/tools/util/include/cutlass/util/reference/device/tensor_relu.h,sha256=Y3mIR1xyBCDF1R9bNg6oeuX5M7m7j488dVlGdBBAGx8,4589
 cutlass_library/source/tools/util/include/cutlass/util/reference/device/kernel/gemm.h,sha256=YE43slx2qwyw37LgOFl1KbMxRMbzKGW21mfsIcOcJGg,5381
 cutlass_library/source/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h,sha256=1PBO_O3utwu9k5rxzkZOzDSrVxnP3vtoVZwqst5mEro,6198
 cutlass_library/source/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h,sha256=FxCAFXYhsPZnnDVsT3go9afXKGMrceBnocsncUqS7B4,5127
 cutlass_library/source/tools/util/include/cutlass/util/reference/device/thread/gemm.h,sha256=2WVX16CsCysZlV8Bbypp0RVmlBbqWloBQ4BS269gJkQ,5872
-cutlass_library/source/tools/util/include/cutlass/util/reference/host/convolution.h,sha256=QbZi25iwrxbmYjRRGnaJ7fiJq70J_Fws9E5Bs5W8H9M,28652
+cutlass_library/source/tools/util/include/cutlass/util/reference/host/conv.hpp,sha256=KIlbIqrGxXnJxO5JX3dND7ji36kUHhudRxjETOr1s1c,26866
+cutlass_library/source/tools/util/include/cutlass/util/reference/host/convolution.h,sha256=ae3WLC9Q56B_v5bc4NAoOnaYvs_Qmve2JtIQ8oYyPl8,29064
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/error_metrics.h,sha256=24yrgrCIcGTQ_nLjFrFzRul83bpSv06SlaB0hbYWLGk,2766
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/gemm.h,sha256=Tqmer-SOge9HytCYdSTSJJIq50GQ1Oc5u8tljBRJG1c,20938
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/gemm_complex.h,sha256=X3R_n3my2BPgthW9vqIEhk75ZeZv1HHqpe9_-dCFQBE,7161
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h,sha256=pF0Zt9kWynybNvSy-p8aY2nYWe95-i_0usKNV0tJT90,7708
-cutlass_library/source/tools/util/include/cutlass/util/reference/host/gett.hpp,sha256=2dvEgCmil0hhnDq2GkbvgtkDQXqFwr-Jl3kO-fA-60k,22747
+cutlass_library/source/tools/util/include/cutlass/util/reference/host/gett.hpp,sha256=CohB1NshXGE1RoP5kFdkFGY_gS1UhRbZxyb-5djiXzU,22815
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/rank_2k.h,sha256=2mmSOKuCt4rpaDJ_tvO5wVcT_qi1OC29nTXZ59ZxnMA,9441
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h,sha256=UyUZYRigIBOt1RnY_l7CHf6Onrj4iNXufogPoqTVeL8,11444
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/rank_k_complex.h,sha256=6_UnCUFeyEsfLsErXIWkEVNXoaN2_ERRAO2JkVbScus,8148
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/symm.h,sha256=g9zv1mlYVsIAURsTipTblnsgBizZPzBWCl5Zyd_eFKM,10509
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/symm_complex.h,sha256=OddPCIsnKGDbeupwWQdGm_nZRKWbwCDB8KHnXhmvYas,12296
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_compare.h,sha256=L2_PuekXYlxNRQVSOmNBH77VbtkrUDKv4DPjJ0tSi0Q,11235
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_compare.hpp,sha256=vCIK0JPdYgJzLSawD5iXPvv_9oUsAsNd3ZWkn9Lw_4s,3339
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_copy.h,sha256=IGT4E-XFz3FATAMFoNR5Q400S-qQYbTC1KBGgbLp5I8,8317
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h,sha256=XfSkvL8UoHAWtCuJ-K9V1LG22BOBt_kdJRNwjmjwMMs,9027
-cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_fill.h,sha256=arvr763QWomdyKZ6pmdlBbTKQGhrzBSWwbqipKcqMOs,46990
+cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_fill.h,sha256=umuZXBkcV-8ORDLL08_6uo__wwKaXT3Iqe8YohDsZ7M,47111
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_fill.hpp,sha256=cuqyp-Uhs_Va46rferVlY60O-MNAddsubcWeMc4nTxg,12875
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_foreach.h,sha256=_zhpG5sddgEiW72pEsy-K3JKMcbE4Sp-uL-h0eU1jO8,4757
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_norm.h,sha256=knGlciavfU5JTEUiiUtkHWQlQsIhAdeXvg7nPJjgJkM,2133
-cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_reduce.h,sha256=WUjgl4-0UPA6YN87W5scgGPcan6GzqtrqwURwzF2Yu0,6111
+cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_reduce.h,sha256=g_uL9_BjYLqMsdNXzuOqy--X-1IbGM3Ezd_XWh-SdMY,6129
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/tensor_reduce.hpp,sha256=47u9cIUfycH0NZ6ckXI_LL3gbR_bNqGcEzEF5oDmkQE,5987
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/trmm.h,sha256=d2u3A3_zONIg_r5ojIfZW-c1Rgg86oKzBPlsxv76cxU,7670
 cutlass_library/source/tools/util/include/cutlass/util/reference/host/trmm_complex.h,sha256=Fe5B-4Apx_F_xcOIqqAc6gc_Q6-eSVsCKQAE-9QehY0,9874
 pycute/__init__.py,sha256=SHDs6erCdPs9tW0exi2HTNUG2IULr0LYnvx5ZWvZTpw,1889
 pycute/int_tuple.py,sha256=3SY3qtPl-fUmzXO7UbFUtjn3BUhWX75uNz7bQeBXt9A,7813
 pycute/layout.py,sha256=Z6YG2JGqGAN3Qb5JML-Pfwtv2RszyOJAUuDhbxpUtiM,12388
 pycute/swizzle.py,sha256=uYGLaDrqJCqWxDuCUth-ziVbKregUjfzH8eJjq3vegs,4475
 pycute/typing.py,sha256=KBzihoh5Vl9lO7aURO5vk-SavAnvOX0LtZf5NhUX4Qo,1981
-nvidia_cutlass-3.4.1.0.dist-info/LICENSE.txt,sha256=b8atQZ5NJlcaFMfE0mtlNFhlwg3YaG3c9ehnP1TT0jM,1547
-nvidia_cutlass-3.4.1.0.dist-info/METADATA,sha256=zpdFbLC85gT_gTiiNs9JCWK5jK5OiJAOtBPMQaGFNAs,28723
-nvidia_cutlass-3.4.1.0.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-nvidia_cutlass-3.4.1.0.dist-info/top_level.txt,sha256=XaZeQG9_-R-rnF3HvPI9xWK84o2INGhUlbV6FBA6eIo,31
-nvidia_cutlass-3.4.1.0.dist-info/RECORD,,
+nvidia_cutlass-3.5.0.0.dist-info/LICENSE.txt,sha256=b8atQZ5NJlcaFMfE0mtlNFhlwg3YaG3c9ehnP1TT0jM,1547
+nvidia_cutlass-3.5.0.0.dist-info/METADATA,sha256=P9Ns10WVKecqKlsx_ZHZCRLxmmVgzs3E2NJPWL6ZEPE,29356
+nvidia_cutlass-3.5.0.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+nvidia_cutlass-3.5.0.0.dist-info/top_level.txt,sha256=XaZeQG9_-R-rnF3HvPI9xWK84o2INGhUlbV6FBA6eIo,31
+nvidia_cutlass-3.5.0.0.dist-info/RECORD,,
```

